<!-- ## Simulation Fundamentals -->


### Generating Independent Uniform Observations

The simulations that we consider are generated by computers. A major strength of this approach is that they can be replicated, allowing us to check and improve our work. Naturally, this also means that they are not really random. Nonetheless, algorithms have been produced so that results appear to be random for all practical purposes. Specifically, they pass sophisticated tests of independence and can be designed so that they come from a single distribution - our *iid* assumption, identically and independently distributed. 

To get a sense as to what these algorithms do, we consider a historically prominent method.

**Linear Congruential Generator.** To generate a sequence of random numbers, start with $B_0$, a starting value that is known as a *seed*. This value is updated using the recursive relationship
$$B_{n+1} = a B_n + c  \text{ modulo }m, ~~ n=0, 1, 2, \ldots .$$
This algorithm is called a *linear congruential generator*. The case of
$c=0$ is called a *multiplicative* congruential generator; it is
particularly useful for really fast computations.

For illustrative values of $a$ and $m$, Microsoft's Visual Basic uses
$m=2^{24}$, $a=1,140,671,485$, and $c = 12,820,163$ (see
<https://en.wikipedia.org/wiki/Linear_congruential_generator>). This is the engine underlying
the random number generation in Microsoft's Excel program.

The sequence used by the analyst is defined as $U_n=B_n/m.$ The analyst
may interpret the sequence {$U_{i}$} to be (approximately) identically
and independently uniformly distributed on the interval (0,1). To
illustrate the algorithm, consider the following.

**Example `r chapnum`.1.1. Illustrative Sequence.**
Take $m=15$, $a=3$, $c=2$ and $B_0=1$. Then we have:

   step $n$  $B_n$                                      $U_n$
  ---------- --------------------------------- ------------------------
      0      $B_0=1$                           
      1      $B_1 =\mod(3 \times 1 +2) = 5$      $U_1 = \frac{5}{15}$
      2      $B_2 =\mod(3 \times 5 +2) = 2$      $U_2 = \frac{2}{15}$
      3      $B_3 =\mod(3 \times 2 +2) = 8$     $U_3 = \frac{8}{15}$
      4      $B_4 =\mod(3 \times 8 +2) = 11$    $U_4 = \frac{11}{15}$
  ---------- --------------------------------- ------------------------

Sometimes computer generated random results are known as *pseudo*-random
numbers to reflect the fact that they are machine generated and can be
replicated. That is, despite the fact that {$U_{i}$} appears to be
i.i.d, it can be reproduced by using the same seed number (and the same
algorithm). 


**Example `r chapnum`.1.2. Generating uniform random numbers in `R`.**
The following code shows how to generate three uniform (0,1) numbers in `R` using the `runif` command. The `set.seed()` function sets the initial seed. In many computer packages, the initial seed is set using the system clock unless specified otherwise.


##### Three Uniform Random Variates {-}

```{r myirischunk, results = 'asis', comment = ""}
set.seed(2017)
U <- runif(3)
knitr::kable(U, digits=5, align = "c", col.names = "Uniform")
```

***

The linear congruential generator is just one method of producing
pseudo-random outcomes. It is easy to understand and is (still) widely
used. The linear congruential generator does have limitations, including
the fact that it is possible to detect long-run patterns over time in
the sequences generated (recall that we can interpret *independence* to
mean a total lack of functional patterns). Not surprisingly, advanced
techniques have been developed that address some of this method's
drawbacks.

### Inverse Transform Method

With the sequence of uniform random numbers, we next transform them to a distribution of interest, say $F$. A prominent technique is the *inverse transform method,* defined as

$$
X_i=F^{-1}\left( U_i \right) .
$$

Here, recall from Section 4.1.1 that we introduced the inverse of the distribution function, $F^{-1}$, and referred to it also as the *quantile function.* Specifically, it is defined to be 

$$
F^{-1}(y) = \inf_x ~ \{ F(x) \ge y \} .
$$

Recall that $\inf$ stands for *infimum* or the greatest lower bound. It is essentially the smallest value of *x* that satisfies the inequality $\{F(x) \ge y\}$.
The result is that the sequence {$X_{i}$} is approximately *iid* with distribution function $F$.

The inverse transform result is available when the underlying random variable is continuous, discrete or a hybrid combination of the two. We now present a series of examples to illustrate its scope of applications.

**Example `r chapnum`.1.3. Generating exponential random numbers.**
Suppose that we would like to generate observations from an exponential distribution with scale
parameter $\theta$ so that $F(x) = 1 - e^{-x/\theta}$. To compute the
inverse transform, we can use the following steps: 
$$
\begin{aligned}
 y = F(x) &\Leftrightarrow  y = 1-e^{-x/\theta} \\
  &\Leftrightarrow -\theta \ln(1-y) = x = F^{-1}(y) .
\end{aligned}
$$

Thus, if $U$ has a uniform (0,1) distribution, then $X = -\theta \ln(1-U)$ has an exponential distribution with parameter $\theta$.

The following `R` code shows how we can start with the same three uniform random numbers as in *Example 6.1.2* and transform them to independent exponentially distributed random variables with a mean of 10. Alternatively, you can directly use the `rexp` function in `R` to generate random numbers from the exponential distribution. The algorithm built into this routine is different so even with the same starting seed number, individual realizations will differ.

```{r}
set.seed(2017)
U <- runif(3)
X1 <- -10*log(1-U)
set.seed(2017)
X2 <- rexp(3, rate = 1/10)
```

##### Three Uniform Random Variates {-}

```{r echo = FALSE}
outmat <- cbind(U,X1,X2)
colnames(outmat) <- c("Uniform","Exponential 1", "Exponential 2")
knitr::kable(outmat, digits=5)
```


***

**Example `r chapnum`.1.4. Generating Pareto random numbers.**
Suppose that we would like to generate observations from a Pareto distribution with parameters $\alpha$ and
$\theta$ so that $F(x) = 1 - \left(\frac{\theta}{x+\theta} \right)^{\alpha}$. To compute
the inverse transform, we can use the following steps: 

$$
\begin{aligned}
 y = F(x) &\Leftrightarrow 1-y = \left(\frac{\theta}{x+\theta} \right)^{\alpha} \\
  &\Leftrightarrow \left(1-y\right)^{-1/\alpha} = \frac{x+\theta}{\theta} = \frac{x}{\theta} +1 \\
    &\Leftrightarrow \theta \left((1-y)^{-1/\alpha} - 1\right) = x = F^{-1}(y) .\end{aligned}
$$
    
Thus, $X = \theta \left((1-U)^{-1/\alpha} - 1\right)$ has a Pareto distribution with parameters $\alpha$ and $\theta$.

***


**Inverse Transform Justification.** Why does the random variable $X = F^{-1}(U)$ have a distribution function $F$?

<h5 style="text-align: center;"><a id="displayTheory.1" href="javascript:toggleTheory('ShowTheory.1','displayTheory.1');"><i><strong>Show A Snippet of Theory</strong></i></a> </h5><div id="ShowTheory.1" style="display: none">

***

This is easy to establish in the continuous case. Because $U$ is a Uniform random variable on (0,1), we know that $\Pr(U \le y) = y$, for $0 \le y \le 1$. Thus, 

$$
\begin{aligned}
\Pr(X \le x) &= \Pr(F^{-1}(U) \le x) \\
 &= \Pr(F(F^{-1}(U)) \le F(x)) \\
&= \Pr(U \le F(x)) = F(x)
\end{aligned}
$$

as required. The key step is that $F(F^{-1}(u)) = u$ for each $u$, which is clearly true when $F$ is strictly increasing.

***

</div>

We now consider some discrete examples.

**Example `r chapnum`.1.5. Generating Bernoulli random numbers.**
Suppose that we wish to simulate random variables from a Bernoulli distribution with parameter $p=0.85$.

```{r BinaryDF, fig.cap='Distribution Function of a Binary Random Variable', out.width='50%', fig.asp=.75, fig.align='center', echo=FALSE}
time = seq(-1,2,0.01)
Ftime = c(rep(0,100),rep(.85,100),rep(1,101))

plot(time,Ftime, ylim=c(0,1), xlab="x",ylab="",pch=19, cex=.2)#,type="l")
mtext("F(x)", side=2, at=1.1, las=1, cex=1.2, adj=1.6)
segments(0,0,0,0.85)#,code=4)
segments(1,0.85,1,1)#,code=4)
symbols(0,.85,circles=.03, add=TRUE,bg="black",inches=FALSE)
symbols(0,0,circles=.03, add=TRUE,inches=FALSE)
symbols(1,.85,circles=.03, add=TRUE,inches=FALSE)
symbols(1,1,circles=.03, add=TRUE,bg="black",inches=FALSE)
```


A graph of the cumulative distribution function in Figure \@ref(fig:BinaryDF) shows that the quantile function can be written as
$$
\begin{aligned}
F^{-1}(y) = \left\{ \begin{array}{cc}
              0 & 0<y \leq 0.85 \\
              1 & 0.85 < y  \leq  1.0 .
            \end{array} \right.
\end{aligned}
$$

Thus, with the inverse transform we may define 
$$
\begin{aligned}
X = \left\{ \begin{array}{cc}
              0 & 0<U \leq 0.85  \\
              1 &  0.85 < U  \leq  1.0
            \end{array} \right.
\end{aligned}
$$
For illustration, we generate three random numbers to get

```{r}
set.seed(2017)
U <- runif(3)
X <- 1*(U > 0.85)
```
#### Three Random Variates {-}
```{r echo = FALSE}
outmat <- cbind(U,X)
colnames(outmat) <- c("Uniform","Binary X")
knitr::kable(outmat, digits=5)
```

**Example `r chapnum`.1.6. Generating random numbers from a discrete distribution.**
Consider the time of a machine failure in the first five years. The distribution of failure times is
given as:

##### Discrete Distribution {-}

```{r echo = FALSE, fig.align='center'}
time <- 1:5
probs <- c(0.1,0.2,0.1, 0.4, 0.2)
df <- cumsum(probs)
outmat <- rbind(time, probs, df)
rownames(outmat) <- c("Time","Probability","Distribution Function $F(x)$    ")
knitr::kable(outmat, align=c('rrrrr'), col.names = rep("$~~~~~~~~~~$",5))
```


```{r DiscreteDF, fig.cap='Distribution Function of a Discrete Random Variable', out.width='60%', fig.asp=.75, fig.align='center', echo=FALSE}
time = seq(0,6,0.01)
Ftime = c(rep(0,100),rep(0.1,100),rep(0.3,100),rep(0.4,100),
          rep(0.8,100),rep(1,101))
plot(time,Ftime, ylim=c(0,1), xlab="x",ylab="",pch=19, cex=.2)#,type="l")
mtext("F(x)", side=2, at=1.1, las=1, cex=1.2, adj=1.6)
segments(1,0,1,0.1)#,code=4)
segments(2,0.1,2,.3)#,code=4)
segments(3,0.3,3,.4)#,code=4)
segments(4,0.4,4,.8)#,code=4)
segments(5,0.8,5,1)#,code=4)

symbols(1,0,circles=.05, add=TRUE,inches=FALSE)
symbols(1,.1,circles=.05, add=TRUE,bg="black",inches=FALSE)
symbols(2,.1,circles=.05, add=TRUE,inches=FALSE)
symbols(2,.3,circles=.05, add=TRUE,bg="black",inches=FALSE)
symbols(3,.3,circles=.05, add=TRUE,inches=FALSE)
symbols(3,.4,circles=.05, add=TRUE,bg="black",inches=FALSE)
symbols(4,.4,circles=.05, add=TRUE,inches=FALSE)
symbols(4,.8,circles=.05, add=TRUE,bg="black",inches=FALSE)
symbols(5,.8,circles=.05, add=TRUE,inches=FALSE)
symbols(5,1,circles=.05, add=TRUE,bg="black",inches=FALSE)
```

Using the graph of the distribution function in Figure \@ref(fig:DiscreteDF), with the inverse transform we may define

$$
\small{
\begin{aligned}
X = \left\{ \begin{array}{cc}
              1 &   0<U  \leq 0.1  \\
              2 &  0.1 < U  \leq  0.3\\
              3 &  0.3 < U  \leq  0.4\\
              4 &  0.4 < U  \leq  0.8  \\
              5 &  0.8 < U  \leq  1.0     .
            \end{array} \right.
\end{aligned}
}
$$

***

For general discrete random variables there may not be an ordering of
outcomes. For example, a person could own one of five types of life
insurance products and we might use the following algorithm to generate
random outcomes: 

$$
{\small
\begin{aligned}
X = \left\{ \begin{array}{cc}
  \textrm{whole life} &   0<U  \leq 0.1  \\
 \textrm{endowment} &  0.1 < U  \leq  0.3\\
\textrm{term life} &  0.3 < U  \leq  0.4\\
  \textrm{universal life} &  0.4 < U  \leq  0.8  \\
  \textrm{variable life} &  0.8 < U  \leq  1.0 .
            \end{array} \right.
\end{aligned}
}
$$ 
            
Another analyst may use an alternative procedure such as: 

$$
{\small
\begin{aligned}
X = \left\{ \begin{array}{cc}
  \textrm{whole life} &   0.9<U<1.0  \\
 \textrm{endowment} &  0.7 \leq U < 0.9\\
\textrm{term life} &  0.6 \leq U < 0.7\\
  \textrm{universal life} &  0.2 \leq U < 0.6  \\
  \textrm{variable life} &  0 \leq U < 0.2 .
            \end{array} \right.
\end{aligned}
}
$$
            
            
Both algorithms produce (in the long-run) the same probabilities, e.g., $\Pr(\textrm{whole life})=0.1$, and so forth. So, neither is incorrect. You should be aware that "there is more than one way to skin a cat." (What an old expression!) Similarly, you could use an alternative algorithm for ordered outcomes (such as failure times 1, 2, 3, 4, or 5, above).

**Example `r chapnum`.1.7. Generating random numbers from a hybrid distribution.**
Consider a random variable that is 0
with probability 70% and is exponentially distributed with parameter
$\theta= 10,000$ with probability 30%. In an insurance application, this might correspond to a 70% chance of having no insurance claims and a 30%
chance of a claim - if a claim occurs, then it is exponentially distributed. The distribution function, depicted in Figure \@ref(fig:MixedDF), is given as

$$
\begin{aligned}
F(y) = \left\{ \begin{array}{cc}
              0 &  x<0  \\
              1 - 0.3 \exp(-x/10000) & x \ge 0 .
            \end{array} \right.
\end{aligned}
$$
            

```{r MixedDF, fig.cap='Distribution Function of a Hybrid Random Variable', out.width='60%', fig.asp=.75, fig.align='center', echo=FALSE}
time = seq(-1000,40000,10)
Ftime = 1 - .3*exp(-0.0001*time)
Ftime = Ftime*(time>0)
plot(time,Ftime, ylim=c(0,1), xlab="x",ylab="",pch=19, cex=.2)#,xaxt="n")
#axis(1, at=seq(0,40000, by=10000), font=10, cex=0.005, tck=0.01)
mtext("F(x)", side=2, at=1.1, las=1, cex=1.2, adj=1.6)
segments(0,0,0,0.7)#,code=4)
symbols(0,0,circles=500, add=TRUE,inches=FALSE)
symbols(0,.7,circles=500, add=TRUE,bg="black",inches=FALSE)
```


From Figure \@ref(fig:MixedDF), we can see that the inverse transform for generating random variables with this distribution function is

$$
\begin{aligned}
X = F^{-1}(U) = \left\{ \begin{array}{cc}
              0 &  0< U  \leq  0.7  \\
              -1000 \ln (\frac{1-U}{0.3}) & 0.7 < U < 1 .
            \end{array} \right.
\end{aligned}
$$

For discrete and hybrid random variables, the key is to draw a graph of the distribution function that allows you to visualize potential values of the inverse function.

### Simulation Precision

From the prior subsections, we now know how to generate independent simulated realizations from a distribution of interest. With these realizations, we can construct an empirical distribution and approximate the underlying distribution as precisely as needed. As we introduce more actuarial applications in this book, you will see that simulation can be applied in a wide variety of contexts.

Many of these applications can be reduced to the problem of approximating $\mathrm{E~}h(X)$, where $h(\cdot)$
is some known function.  Based on $R$ simulations (replications), we get $X_1,\ldots,X_R$. From this simulated sample, we calculate an average 

$$
\overline{h}_R=\frac{1}{R}\sum_{i=1}^{R} h(X_i)
$$
that we use as our simulated approximate (estimate) of $\mathrm{E~}h(X)$. To estimate the precision of this approximation, we use the simulation variance

$$
s_{h,R}^2 = \frac{1}{R-1} \sum_{i=1}^{R}\left( h(X_i) -\overline{h}_R
\right) ^2.
$$

From the independence, the standard error of the estimate is $s_{h,R}/\sqrt{R}$. This can be made as small as we like by increasing the number of replications $R$.


```{r warning=FALSE, message=FALSE, comment="", echo=FALSE}
# For the gamma distributions, use
alpha1 <- 2;      theta1 <- 100
alpha2 <- 2;      theta2 <- 200
# Deductibles
M <- 400

nSim <- 1e6  #number of simulations
```

**Example. `r chapnum`.1.8. Portfolio management.** 
In Section 3.4, we learned how to calculate the expected value of policies with deductibles. For an example of something that cannot be done with closed form expressions, we now consider two risks. This is a variation of a more complex example that will be covered as *Example 10.3.6*.

We consider two property risks of a telecommunications firm:

- $X_1$ - buildings, modeled using a gamma distribution with mean `r alpha1*theta1` and scale parameter `r theta1`.
- $X_2$ - motor vehicles, modeled using a gamma distribution with mean `r alpha2*theta2` and scale parameter `r theta2`.

Denote the total risk as $X = X_1 + X_2.$ For simplicity, you assume that these risks are independent. 

To manage the risk, you seek some insurance protection. You wish to manage internally small building and motor vehicles amounts, up to $M$, say. Your retained risk is $Y_{retained}=$ $\min(X_1 + X_2,M)$. The insurer's portion is $Y_{insurer} =  X- Y_{retained}$.

To be specific, we use $M=$ `r M` as well as $R=$ `r nSim` simulations. 

**a.** With the settings, we wish to determine the expected claim amount and the associated standard deviation of (i) that retained, (ii) that accepted by the insurer, and (iii) the total overall amount.

```{r warning=FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align='center', comment=""}

# Simulate the risks
nSim <- 1e6  #number of simulations
set.seed(2017) #set seed to reproduce work 
X1 <- rgamma(nSim,alpha1,scale = theta1)  
X2 <- rgamma(nSim,alpha2,scale = theta2)  

# Portfolio Risks
X         <- X1 + X2 
Yretained <- pmin(X, M)
Yinsurer  <- X - Yretained
```

Here is the code for the expected claim amounts.

```{r warning=FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align='center', comment=""}
# Expected Claim Amounts
ExpVec <- t(as.matrix(c(mean(Yretained),mean(Yinsurer),mean(X))))
sdVec <- t(as.matrix(c(sd(Yretained),sd(Yinsurer),sd(X))))
outMat <- rbind(ExpVec, sdVec)
colnames(outMat) <- c("Retained", "Insurer","Total")
row.names(outMat) <- c("Mean","Standard Deviation")
round(outMat,digits=2)
```

**b.** For insured claims, the standard error of the simulation approximation is $s_{h,R}/\sqrt{1000000} =$ `r 
round(sd(Yinsurer),digits=2)` $/\sqrt{1000000} =$  `r round(sd(Yinsurer)/sqrt(1000000), digits=3)`. For this example, simulation is quick and so a large value such as 1000000 is an easy choice. However, for more complex problems, the simulation size may be an issue. Figure \@ref(fig:PortfolioDF)
allows us to visualize the development of the approximation as the number of simulations increases.


```{r PortfolioDF, warning=FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align='center', comment="", fig.cap = 'Estimated Expected Insurer Claims versus Number of Simulations.'}
Yinsurefct <- function(numSim){
X1 <- rgamma(numSim,alpha1,scale = theta1)  
X2 <- rgamma(numSim,alpha2,scale = theta2)  
# Portfolio Risks
X         <- X1 + X2 
Yinsurer <- X - pmin(X, M)
return(Yinsurer)
}

R <- 1e3
nPath <- 20
set.seed(2017)
simU <- matrix(Yinsurefct(R*nPath),R,nPath)
sumP2 <- apply(simU, 2, cumsum)/(1:R)
matplot(1:R,sumP2[,1:20],type="l",col=rgb(1,0,0,.2), ylim=c(100, 400),
        xlab=expression(paste("Number of Simulations (", italic('R'), ")")), ylab="Expected Insurer Claims")
abline(h=mean(Yinsurer),lty=2)
bonds <- cbind(1.96*sd(Yinsurer)*sqrt(1/(1:R)),-1.96*sd(Yinsurer)*sqrt(1/(1:R)))
matlines(1:R,bonds+mean(Yinsurer),col="red",lty=1)
```

***

**Determination of Number of Simulations**

How many simulated values are recommended? 100? 1,000,000? We can use the central limit theorem to respond to this question. 

As one criterion for your confidence in the result, suppose that you wish to be within 1% of the mean with 95%
certainty. That is, you want $\Pr \left( |\overline{h}_R - \mathrm{E~}h(X)| \le 0.01 \mathrm{E~}h(X) \right) \le 0.95$. According to the central limit theorem, your estimate should be approximately normally distributed and so we want to have $R$ large enough to satisfy $0.01 \mathrm{E~}h(X)/\sqrt{\mathrm{Var~}h(X)/R}) \ge 1.96$. (Recall that 1.96 is the 97.5th percentile from the standard normal distribution.) Replacing $\mathrm{E~}h(X)$ and $\mathrm{Var~}h(X)$ with estimates, you continue your simulation until 

$$
\frac{.01\overline{h}_R}{s_{h,R}/\sqrt{R}}\geq 1.96
$$
or equivalently 

\begin{equation}
R \geq 38,416\frac{s_{h,R}^2}{\overline{h}_R^2}.
(\#eq:NumSimulations)
\end{equation}


This criterion is a direct application of the approximate normality. Note that $\overline{h}_R$ and $s_{h,R}$ are not known in
advance, so you will have to come up with estimates, either by doing a small pilot study in advance or by interrupting your procedure intermittently to see if the criterion is satisfied.


**Example. `r chapnum`.1.8. Portfolio management - continued** 

For our example, the average insurance claim is `r round(mean(Yinsurer), digits=3)` and the corresponding standard deviation is `r round(sd(Yinsurer), digits=3)`. Using equation \@ref(eq:NumSimulations), to be within 10% of the mean, we would only require at least `r round(38416*(sd(Yinsurer)/mean(Yinsurer))^2/1000,digits=2)` thousand simulations. However, to be within 1% we would want at least `r round(100*38416*(sd(Yinsurer)/mean(Yinsurer))^2/1000000,digits=2)` million simulations.

***

**Example. `r chapnum`.1.9. Approximation choices.** 
An important application of simulation is the approximation of $\mathrm{E~}h(X)$. In this example, we show that the choice of the $h(\cdot)$ function and the distribution of $X$ can play a role.

Consider the following question : what is $\Pr[X>2]$ when $X$ has a Cauchy distribution, with density $f(x) =\left(\pi(1+x^2)\right)^{-1}$, on the real line? The true value is 

$$
\Pr\left[X>2\right] = \int_2^\infty \frac{dx}{\pi(1+x^2)} .
$$
One can use an `R` numerical integration function (which works usually well on improper integrals)
```{r}
true_value <- integrate(function(x) 1/(pi*(1+x^2)),lower=2,upper=Inf)$value
```
which is equal to `r round(true_value,digits=5)`.

Alternatively, one can use simulation techniques to approximate that quantity. From calculus, you can check that the quantile function of the Cauchy distribution is $F^{-1}(y) = \tan \left( \pi(y-0.5) \right)$. Then, with simulated uniform (0,1) variates, $U_1, \ldots, U_R$, we can construct the estimator

$$
p_1 = \frac{1}{R}\sum_{i=1}^R \mathrm{I}(F^{-1}(U_i)>2) = \frac{1}{R}\sum_{i=1}^R \mathrm{I}(\tan \left( \pi(U_i-0.5) \right)>2) .
$$

```{r comment = ""}
Q <- function(u) tan(pi*(u-.5))
R <- 1e6
set.seed(1)
X <- Q(runif(R))
p1 <- mean(X>2)
se.p1 <- sd(X>2)/sqrt(R)
p1
se.p1
```


With one million simulations, we obtain an estimate of `r round(p1,digits=5)` with standard error `r round(se.p1*1000,digits=3)` (divided by 1000). One can prove that the variance of $p_1$ is of order $0.127/R$.

<!--It can be visualized below -->
```{r eval=FALSE, echo = FALSE}
true_value <- integrate(function(x) 1/(pi*(1+x^2)),lower=2,upper=Inf)$value
n <- 1e3
ns <- 20
set.seed(1)
simU <- matrix(Q(runif(n*ns))>2,n,ns)
sumP1 <- apply(simU, 2, cumsum)/(1:n)
matplot(1:n,sumP1[,1:20],type="l",col=rgb(1,0,0,.2),ylim=c(.1,.2),
        xlab="Number of simulations (n)",ylab="Approximation of the integral")
abline(h=true_value,lty=2)
bonds <- cbind(1.96*sqrt(.127/(1:n)),-1.96*sqrt(.127/(1:n)))
matlines(1:n,bonds+true_value,col="red",lty=1)
```

With other choices of $h(\cdot)$ and $F(\cdot)$, it is actually possible to reduce uncertainty even using the same number of simulations `R`. To begin, one can use the symmetry of the Cauchy distribution to write $\Pr[X>2]=0.5\cdot\Pr[|X|>2]$. With this, can construct a new estimator

$$
p_2 = \frac{1}{2R}\sum_{i=1}^R \mathrm{I}(|F^{-1}(U_i)|>2) .
$$


```{r echo = FALSE}
set.seed(1)
p2 <- mean(abs(Q(runif(R)))>2)/2
se.p2 <- sd(abs(Q(runif(R)))>2)/(2*sqrt(R))
```

With one million simulations, we obtain an estimate of `r round(p2,digits=5)` with standard error `r round(se.p2*1000,digits=3)` (divided by 1000). One can prove that the variance of $p_2$ is of order $0.052/R$.

<!--It can be visualized below -->
```{r eval=FALSE, echo = FALSE}
n <- 1e3
ns <- 20
set.seed(1)
simU <- matrix(abs(Q(runif(n*ns)))>2,n,ns)
sumP2 <- apply(simU, 2, cumsum)/(1:n)/2
matplot(1:n,sumP2[,1:20],type="l",col=rgb(1,0,0,.2),ylim=c(.1,.2),
        xlab="Number of simulations (n)",ylab="Approximation of the integral")
abline(h=true_value,lty=2)
bonds <- cbind(1.96*sqrt(.052/(1:n)),-1.96*sqrt(.052/(1:n)))
matlines(1:n,bonds+true_value,col="red",lty=1)
```

But one can go one step further. The improper integral can be written as a proper one by a simple symmetry property (since the function is symmetry and the integral on the real line is equal to $1$)
$$
\int_2^\infty \frac{dx}{\pi(1+x^2)}=\frac{1}{2}-\int_0^2\frac{dx}{\pi(1+x^2)} .
$$
From this expression, a natural approximation would be
$$
p_3 = \frac{1}{2}-\frac{1}{R}\sum_{i=1}^R h_3(2U_i), ~~~~~~\text{where}~h_3(x)=\frac{2}{\pi(1+x^2)} .
$$
```{r echo = FALSE}
h <- function(x) 2/(pi*(1+x^2))
set.seed(1)
p3 <- .5-mean(h(2*runif(R)))
se.p3 <- sd(h(2*runif(R)))/sqrt(R)
```

With one million simulations, we obtain an estimate of `r round(p3,digits=5)` 
with standard error `r signif(se.p3*1000,digits=3)` (divided by 1000).  One can prove that the variance of $p_3$ is of order $0.0285/R$.

<!--It can be visualized below -->
```{r eval=FALSE, echo = FALSE}
n <- 1e3
ns <- 20
set.seed(1)
simU <- matrix((h(2*runif(n*ns))),n,ns)
sumP3 <- .5-apply(simU, 2, cumsum)/(1:n)
matplot(1:n,sumP3[,1:20],type="l",col=rgb(1,0,0,.2),ylim=c(.1,.2),
        xlab="Number of simulations (n)",ylab="Approximation of the integral")
abline(h=true_value,lty=2)
bonds <- cbind(1.96*sqrt(.0285/(1:n)),-1.96*sqrt(.0285/(1:n)))
matlines(1:n,bonds+true_value,col="red",lty=1)
```

Finally, one can also consider some change of variable in the integral
$$
\int_2^\infty \frac{dx}{\pi(1+x^2)}=\int_0^{1/2}\frac{y^{-2}dy}{\pi(1-y^{-2})} .
$$
From this expression, a natural approximation would be
$$
p_4 = \frac{1}{R}\sum_{i=1}^R h_4(U_i/2),~~~~~\text{where}~h_4(x)=\frac{1}{2\pi(1+x^2)} .
$$
The expression seems rather similar to the previous one, 

```{r echo = FALSE}
set.seed(1)
h4 <- function(x) 1/(2*pi*(1+x^2))
p4 <- mean(h4(runif(R)/2))
se.p4 <- sd(h4(runif(R)/2))/sqrt(R)
```

With one million simulations, we obtain an estimate of `r round(p4,digits=5)` with standard error `r round(se.p4*1000,digits=3)` (divided by 1000). One can prove that the variance of $p_4$ is of order $0.00009/R$, which is much smaller than what we had so far !

<!--It can be visualized below -->
```{r eval=FALSE, echo = FALSE}
n <- 1e3
ns <- 20
set.seed(1)
simU <- matrix((h(runif(n*ns)/2)),n,ns)
sumP4 <- apply(simU, 2, cumsum)/(1:n)
matplot(1:n,sumP4[,1:20],type="l",col=rgb(1,0,0,.2),ylim=c(.1,.2),
        xlab="Number of simulations (n)",ylab="Approximation of the integral")
abline(h=true_value,lty=2)
bonds <- cbind(1.96*sqrt(.00009/(1:n)),-1.96*sqrt(.00009/(1:n)))
matlines(1:n,bonds+true_value,col="red",lty=1)
```

[Table 6.1] summarizes the four choices of $h(\cdot)$ and $F(\cdot)$ to approximate $\Pr[X>2] =$ `r round(true_value,digits=5)`. The standard error varies dramatically. Thus, if we have a desired degree of accuracy, then the *number of simulations* depends strongly on how we write the integrals we try to approximate.

[Table 6.1]:\#tab:61

<a id=tab:61></a>

<div align="center">
```{r ,tab:ApproximationChoices, echo = FALSE, message=FALSE, warning=FALSE}
library(htmlTable)
x1 <- c(
  "$p_1$", 
  "$\\frac{1}{R}\\sum_{i=1}^R \\mathrm{I}(F^{-1}(U_i)>2)$",
   "$~~~~~~F^{-1}(u)=\\tan \\left( \\pi(u-0.5) \\right)~~~~~~~$",round(c(p1,se.p1),digits=6))
x2 <- c(
  "$p_2$", 
  "$\\frac{1}{2R}\\sum_{i=1}^R \\mathrm{I}(|F^{-1}(U_i)|>2)$",
   "$F^{-1}(u)=\\tan \\left( \\pi(u-0.5) \\right)$",round(c(p2,se.p2),digits=6))
x3 <- c(
  "$p_3$", 
  "$\\frac{1}{2}-\\frac{1}{R}\\sum_{i=1}^R h_3(2U_i)$",
"$h_3(x)=\\frac{2}{\\pi(1+x^2)}$",round(c(p3,se.p3),digits=6))
x4 <- c(
  "$p_4$", 
  "$\\frac{1}{R}\\sum_{i=1}^R h_4(U_i/2)$",
  "$h_4(x)=\\frac{1}{2\\pi(1+x^2)}$",format(round(c(p4,se.p4),digits=6), scientific = FALSE))
outMat <- rbind(x1,x2,x3,x4)
rownames(outMat) <- NULL
htmlTable(outMat, header =  c("Estimator", "Definition",
                            "Support Function", "Estimate", " Standard Error"),
          caption = "Table 6.1. Summary of Four Choices to Approximate $\\Pr[X>2]$. ",
          align = "cccrr")     
```
</div>


### Simulation and Statistical Inference {#S:SimulationStatInference}

Simulations not only help us approximate expected values but are also useful in calculating other aspects of distribution functions. In particular, they are very useful when distributions of test statistics are too complicated to derive; in this case, one can use simulations to approximate the reference distribution. We now illustrate this with the *Kolmogorov-Smirnov* test that we learned about in Section 4.1.2.2. 


**Example. `r chapnum`.1.10. Kolmogorov-Smirnov Test of Distribution.** 
Suppose that we have available $n=100$ observations $\{x_1,\cdots,x_n\}$ that, unknown to the analyst, was generated from a gamma distribution with parameters $\alpha = 6$ and $\theta=2$. The analyst believes that the data come from a lognormal distribution with parameters 1 and 0.4 and would like to test this assumption.

The first step is to visualize the data. Figure \@ref(fig:KSTestData) provides a graph of a histogram and empirical distribution. For reference, superimposed are red dashed lines from the lognormal distribution. 


```{r KSTestData, fig.align='center', comment="", fig.cap = 'Histogram and Empirical Distribution Function of Data used in Kolmogorov-Smirnov Test. The red dashed lines are fits based on (incorrectly) hypothesized lognormal distribution.'}
set.seed(1)
n <- 100
x <- rgamma(n, 6, 2)
par(mfrow=c(1,2))
hist(x,probability = TRUE,main="Histogram", col="light blue",border="white",xlim=c(0,7),ylim=c(0,.4))
u=seq(0,7,by=.01)
lines(u,dlnorm(u,1,.4),col="red",lty=2)
vx = c(0,sort(x))
vy = (0:n)/n
plot(vx,vy,type="l",xlab="x",ylab="Cumulative Distribution",main="Empirical cdf")
lines(u,plnorm(u,1,.4),col="red",lty=2)
```

Recall that the Kolmogorov-Smirnov statistic equals the largest discrepancy between the empirical and the hypothesized distribution. This is $\max_x |F_n(x)-F_0(x)|$, where $F_0$ is the hypothesized lognormal distribution. We can calculate this directly as:

```{r comment=""}
# test statistic
D <- function(data, F0){
   F <- Vectorize(function(x) mean((data<=x)))
   n <- length(data)
   x <- sort(data)
   d1=abs(F(x+1e-6)-F0(x+1e-6))
   d2=abs(F(x-1e-6)-F0(x-1e-6))
   return(max(c(d1,d2)))
}
D(x,function(x) plnorm(x,1,.4))
```

Fortunately, for the lognormal distribution, `R` has built-in tests that allow us to determine this without complex programming:

```{r comment=""}
ks.test(x, plnorm, mean=1, sd=0.4)
```


However, for many distributions of actuarial interest, pre-built programs are not available. We can use simulation to test the relevance of the test statistic. Specifically, to compute the $p$-value, let us generate thousands of random samples from a $LN(1,0.4)$ distribution (with the same size), and compute empirically the distribution of the statistic,

```{r KSSimulatedDistribution, fig.align='center', comment="", fig.cap = 'Simulated Distribution of the Kolmogorov-Smirnov Test Statistic. The vertical red dashed line marks the test statistic for the sample of 100.'}
ns <- 1e4
d_KS <- rep(NA,ns)
# compute the test statistics for a large (ns) number of simulated samples
for(s in 1:ns) d_KS[s] <- D(rlnorm(n,1,.4),function(x) plnorm(x,1,.4))
hist(d_KS,probability = TRUE,col="light blue",border="white",xlab="Test Statistic",main="")
lines(density(d_KS),col="red")
abline(v=D(x,function(x) plnorm(x,1,.4)),lty=2,col="red")
mean(d_KS>D(x,function(x) plnorm(x,1,.4)))
```

The simulated distribution based on 10,000 random samples is summarized in Figure \@ref(fig:KSSimulatedDistribution). Here, the statistic exceeded the empirical value (`r format(D(x,function(x) plnorm(x,1,.4)),digits=4)`) in `r 100*mean(d_KS>D(x,function(x) plnorm(x,1,.4)))`% of the scenarios, while the *theoretical* $p$-value is `r round(ks.test(x,plnorm,mean=1,sd=.4)$p.value, digits=4)`. For both the simulation and the theoretical $p$-values, the conclusions are the same; the data do not provide sufficient evidence to reject the hypothesis of a lognormal distribution.


Although only an approximation, the simulation approach works in a variety of distributions and test statistics without needing to develop the nuances of the underpinning theory for each situation. We summarize the procedure for developing simulated distributions and *p*-values as follows:

1. Draw a sample of size *n*, say, $X_1, \ldots, X_n$, from a known distribution function $F$. Compute a statistic of interest, denoted as $\hat{\theta}(X_1, \ldots, X_n)$. Call this $\hat{\theta}^r$ for the *r*th replication.
2. Repeat this $r=1, \ldots, R$ times to get a sample of statistics, $\hat{\theta}^1, \ldots,\hat{\theta}^R$.
3. From the sample of statistics in Step 2, $\{\hat{\theta}^1, \ldots,\hat{\theta}^R\}$, compute a summary measure of interest, such as a *p*-value.
 

