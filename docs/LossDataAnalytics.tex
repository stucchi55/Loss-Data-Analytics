\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Loss Data Analytics},
            pdfauthor={An open text authored by the Actuarial Community},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{econPeriod}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Loss Data Analytics}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{An open text authored by the Actuarial Community}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
    \date{}
    \predate{}\postdate{}
  
\usepackage{booktabs}
\setcounter{secnumdepth}{2}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\emph{Date: 16 January 2020}

\subsubsection*{Book Description}\label{book-description}
\addcontentsline{toc}{subsubsection}{Book Description}

\textbf{Loss Data Analytics} is an interactive, online, freely available
text.

\begin{itemize}
\tightlist
\item
  The online version contains many interactive objects (quizzes,
  computer demonstrations, interactive graphs, video, and the like) to
  promote \emph{deeper learning}.
\item
  A subset of the book is available for \emph{offline reading} in pdf
  and EPUB formats.
\item
  The online text will be available in multiple languages to promote
  access to a \emph{worldwide audience}.
\end{itemize}

\subsubsection*{What will success look
like?}\label{what-will-success-look-like}
\addcontentsline{toc}{subsubsection}{What will success look like?}

The online text will be freely available to a worldwide audience. The
online version will contain many interactive objects (quizzes, computer
demonstrations, interactive graphs, video, and the like) to promote
deeper learning. Moreover, a subset of the book will be available in pdf
format for low-cost printing. The online text will be available in
multiple languages to promote access to a worldwide audience.

\subsubsection*{How will the text be
used?}\label{how-will-the-text-be-used}
\addcontentsline{toc}{subsubsection}{How will the text be used?}

This book will be useful in actuarial curricula worldwide. It will cover
the loss data learning objectives of the major actuarial organizations.
Thus, it will be suitable for classroom use at universities as well as
for use by independent learners seeking to pass professional actuarial
examinations. Moreover, the text will also be useful for the continuing
professional development of actuaries and other professionals in
insurance and related financial risk management industries.

\subsubsection*{Why is this good for the
profession?}\label{why-is-this-good-for-the-profession}
\addcontentsline{toc}{subsubsection}{Why is this good for the
profession?}

An online text is a type of open educational resource (OER). One
important benefit of an OER is that it equalizes access to knowledge,
thus permitting a broader community to learn about the actuarial
profession. Moreover, it has the capacity to engage viewers through
active learning that deepens the learning process, producing analysts
more capable of solid actuarial work.

Why is this good for students and teachers and others involved in the
learning process? Cost is often cited as an important factor for
students and teachers in textbook selection (see a recent post on the
\href{https://www.aei.org/publication/the-new-era-of-the-400-college-textbook-which-is-part-of-the-unsustainable-higher-education-bubble/}{\$400
textbook}). Students will also appreciate the ability to ``carry the
book around'' on their mobile devices.

\subsubsection*{Why loss data analytics?}\label{why-loss-data-analytics}
\addcontentsline{toc}{subsubsection}{Why loss data analytics?}

The intent is that this type of resource will eventually permeate
throughout the actuarial curriculum. Given the dramatic changes in the
way that actuaries treat data, loss data seems like a natural place to
start. The idea behind the name \emph{loss data analytics} is to
integrate classical loss data models from applied probability with
modern analytic tools. In particular, we recognize that big data
(including social media and usage based insurance) are here to stay and
that high speed computation is readily available.

\subsubsection*{Project Goal}\label{project-goal}
\addcontentsline{toc}{subsubsection}{Project Goal}

The project goal is to have the actuarial community author our textbooks
in a collaborative fashion. To get involved, please visit our
\href{https://sites.google.com/a/wisc.edu/loss-data-analytics/}{Open
Actuarial Textbooks Project Site}.

\section*{Acknowledgements}\label{acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

Edward Frees acknowledges the John and Anne Oros Distinguished Chair for
Inspired Learning in Business which provided seed money to support the
project. Frees and his Wisconsin colleagues also acknowledge a Society
of Actuaries Center of Excellence Grant that provided funding to support
work in dependence modeling and health initiatives. Wisconsin also
provided an education innovation grant that provided partial support for
the many students who have worked on this project.

We acknowledge the Society of Actuaries for permission to use problems
from their examinations.

We thank Rob Hyndman, Monash University, for allowing us to use his
excellent style files to produce the online version of the book.

We thank Yihui Xie and his colleagues at
\href{https://www.rstudio.com/}{Rstudio} for the
\href{https://bookdown.org/yihui/bookdown/}{R bookdown} package that
allows us to produce this book.

We also wish to acknowledge the support and sponsorship of the
\href{http://www.blackactuaries.org/}{International Association of Black
Actuaries} in our joint efforts to provide actuarial educational content
to all.

\includegraphics[width=0.25000\textwidth]{Figures/IABA.png}

\section*{Contributors}\label{contributors}
\addcontentsline{toc}{section}{Contributors}

The project goal is to have the actuarial community author our textbooks
in a collaborative fashion. The following contributors have taken a
leadership role in developing \emph{Loss Data Analytics}.

\begin{itemize}
\item
  \textbf{Zeinab Amin} is the Director of the Actuarial Science Program
  and Associate Dean for Undergraduate Studies of the School of Sciences
  and Engineering at the American University in Cairo (AUC). Amin holds
  a PhD in Statistics and is an Associate of the Society of Actuaries.
  Amin is the recipient of the 2016 Excellence in Academic Service Award
  and the 2009 Excellence in Teaching Award from AUC. Amin has designed
  and taught a variety of statistics and actuarial science courses.
  Amin's current area of research includes quantitative risk assessment,
  reliability assessment, general statistical modelling, and Bayesian
  statistics.
\item
  \textbf{Katrien Antonio}, KU Leuven
\item
  \textbf{Jan Beirlant}, KU Leuven
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Arthur Charpentier} is a professor in the Department of
  Mathematics at the Université du Québec á Montréal. Prior to that, he
  worked at a large general insurance company in Hong Kong, China, and
  the French Federation of Insurers in Paris, France. He received a MS
  on mathematical economics at Université Paris Dauphine and a MS in
  actuarial science at ENSAE (National School of Statistics) in Paris,
  and a PhD degree from KU Leuven, Belgium. His research interests
  include econometrics, applied probability and actuarial science. He
  has published several books (the most recent one on
  \emph{Computational Actuarial Science with R}, CRC) and papers on a
  variety of topics. He is a Fellow of the French Institute of
  Actuaries, and was in charge of the `Data Science for Actuaries'
  program from 2015 to 2018.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Curtis Gary Dean} is the Lincoln Financial Distinguished
  Professor of Actuarial Science at Ball State University. He is a
  Fellow of the Casualty Actuarial Society and a CFA charterholder. He
  has extensive practical experience as an actuary at American States
  Insurance, SAFECO, and Travelers. He has served the CAS and actuarial
  profession as chair of the Examination Committee, first
  editor-in-chief for \emph{Variance: Advancing the Science of Risk},
  and as a member of the Board of Directors and the Executive Council.
  He contributed a chapter to \emph{Predictive Modeling Applications in
  Actuarial Science} published by Cambridge University Press.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Edward W. (Jed) Frees} is an emeritus professor, formerly the
  Hickman-Larson Chair of Actuarial Science at the University of
  Wisconsin-Madison. He is a Fellow of both the Society of Actuaries and
  the American Statistical Association. He has published extensively (a
  four-time winner of the Halmstad and Prize for best paper published in
  the actuarial literature) and has written three books. He also is a
  co-editor of the two-volume series \emph{Predictive Modeling
  Applications in Actuarial Science} published by Cambridge University
  Press.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Guojun Gan} is an assistant professor in the Department of
  Mathematics at the University of Connecticut, where he has been since
  August 2014. Prior to that, he worked at a large life insurance
  company in Toronto, Canada for six years. He received a BS degree from
  Jilin University, Changchun, China, in 2001 and MS and PhD degrees
  from York University, Toronto, Canada, in 2003 and 2007, respectively.
  His research interests include data mining and actuarial science. He
  has published several books and papers on a variety of topics,
  including data clustering, variable annuity, mathematical finance,
  applied statistics, and VBA programming.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Lisa Gao} is a PhD candidate in the Risk and Insurance
  department at the University of Wisconsin-Madison. She holds a BMath
  in Actuarial Science and Statistics from the University of Waterloo
  and is an Associate of the Society of Actuaries.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{José Garrido}, Concordia University
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Lei (Larry) Hua} is an Associate Professor of Actuarial
  Science at Northern Illinois University. He earned a PhD degree in
  Statistics from the University of British Columbia. He is an Associate
  of the Society of Actuaries. His research work focuses on multivariate
  dependence modeling for non-Gaussian phenomena and innovative
  applications for financial and insurance industries.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Noriszura Ismail} is a Professor and Head of Actuarial Science
  Program, Universiti Kebangsaan Malaysia (UKM). She specializes in Risk
  Modelling and Applied Statistics. She obtained her BSc and MSc
  (Actuarial Science) in 1991 and 1993 from University of Iowa, and her
  PhD (Statistics) in 2007 from UKM. She also passed several papers from
  Society of Actuaries in 1994. She has received several research grants
  from Ministry of Higher Education Malaysia (MOHE) and UKM, totaling
  about MYR1.8 million. She has successfully supervised and
  co-supervised several PhD students (13 completed and 11 on-going). She
  currently has about 180 publications, consisting of 88 journals and 95
  proceedings.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Joseph H.T. Kim}, Ph.D., FSA, CERA, is Associate Professor of
  Applied Statistics at Yonsei University, Seoul, Korea. He holds a
  Ph.D.~degree in Actuarial Science from the University of Waterloo, at
  which he taught as Assistant Professor. He also worked in the life
  insurance industry. He has published papers in \emph{Insurance
  Mathematics and Economics}, \emph{Journal of Risk and Insurance},
  \emph{Journal of Banking and Finance}, \emph{ASTIN Bulletin}, and
  \emph{North American Actuarial Journal}, among others.
\end{itemize}

\begin{itemize}
\item
  \textbf{Nii-Armah Okine} is a dissertator at the business school of
  University of Wisconsin-Madison with a major in actuarial science. He
  obtained his master's degree in Actuarial science from Illinois State
  University. His research interests includes micro-level reserving,
  joint longitudinal-survival modeling, dependence modelling, micro
  insurance and machine learning.
\item
  \textbf{Margie Rosenberg} - University of Wisconsin
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Emine Selin Sarıdaş} is a doctoral candidate in the Statistics
  department of Mimar Sinan University. She holds a bachelor degree in
  Actuarial Science with a minor in Economics and a master degree in
  Actuarial Science from Hacettepe University. Her research interest
  includes dependence modeling, regression, loss models and life
  contingencies.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Peng Shi} is an associate professor in the Risk and Insurance
  Department at the Wisconsin School of Business. He is also the Charles
  \& Laura Albright Professor in Business and Finance. Professor Shi is
  an Associate of the Casualty Actuarial Society (ACAS) and a Fellow of
  the Society of Actuaries (FSA). He received a Ph.D.~in actuarial
  science from the University of Wisconsin-Madison. His research
  interests are problems at the intersection of insurance and
  statistics. He has won several research awards, including the Charles
  A. Hachemeister Prize, the Ronald Bornhuetter Loss Reserve Prize, and
  the American Risk and Insurance Association Prize.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Nariankadu D. Shyamalkumar (Shyamal)} is an associate
  professor in the Department of Statistics and Actuarial Science at The
  University of Iowa. He is an Associate of the Society of Actuaries,
  and has volunteered in various elected and non-elected roles within
  the SoA. Having a broad theoretical interest as well as interest in
  computing, he has published in prominent actuarial, computer science,
  probability theory, and statistical journals. Moreover, he has worked
  in the financial industry, and since then served as an independent
  consultant to the insurance industry. He has experience educating
  actuaries in both Mexico and the US, serving in the roles of directing
  an undergraduate program, and as a graduate adviser for both masters
  and doctoral students.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Jianxi Su} is an Assistant Professor at the Department of
  Statistics at Purdue University. He is the Associate Director of
  Purdue's Actuarial Science. Prior to joining Purdue in 2016, he
  completed the PhD at York University (2012-2015). He obtained the
  Fellow of the Society of Actuaries (FSA) in 2017. His research
  expertise are in dependence modelling, risk management, and pricing.
  During the PhD candidature, Jianxi also worked as a research associate
  at the Model Validation and ORSA Implementation team of Sun Life
  Financial (Toronto office).
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Tim Verdonck} is associate professor at the University of
  Antwerp. He has a degree in Mathematics and a PhD in Science:
  Mathematics, obtained at the University of Antwerp. During his PhD he
  successfully took the Master in Insurance and the Master in Financial
  and Actuarial Engineering, both at KU Leuven. His research focuses on
  the adaptation and application of robust statistical methods for
  insurance and finance data.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Krupa Viswanathan} is an Associate Professor in the Risk,
  Insurance and Healthcare Management Department in the Fox School of
  Business, Temple University. She is an Associate of the Society of
  Actuaries. She teaches courses in Actuarial Science and Risk
  Management at the undergraduate and graduate levels. Her research
  interests include corporate governance of insurance companies, capital
  management, and sentiment analysis. She received her Ph.D.~from The
  Wharton School of the University of Pennsylvania.
\end{itemize}

\section*{Reviewers}\label{reviewers}
\addcontentsline{toc}{section}{Reviewers}

Our goal is to have the actuarial community author our textbooks in a
collaborative fashion. Part of the writing process involves many
reviewers who generously donated their time to help make this book
better. They are:

\begin{itemize}
\tightlist
\item
  Yair Babab
\item
  Chunsheng Ban, Ohio State University
\item
  Vytaras Brazauskas, University of Wisconsin - Milwaukee
\item
  Chun Yong Chew, Universiti Tunku Abdul Rahman (UTAR)
\item
  Eren Dodd, University of Southampton
\item
  Gordon Enderle, University of Wisconsin - Madison
\item
  Rob Erhardt, Wake Forest University
\item
  Runhun Feng, University of Illinois
\item
  Liang (Jason) Hong, Robert Morris University
\item
  Fei Huang, Australian National University
\item
  Hirokazu (Iwahiro) Iwasawa
\item
  Himchan Jeong, University of Connecticut
\item
  Min Ji, Towson University
\item
  Paul Herbert Johnson, University of Wisconsin - Madison
\item
  Samuel Kolins, Lebonan Valley College
\item
  Andrew Kwon-Nakamura, Zurich North America
\item
  Ambrose Lo, University of Iowa
\item
  Mark Maxwell, University of Texas at Austin
\item
  Tatjana Miljkovic, Miami University
\item
  Bell Ouelega, American University in Cairo
\item
  Zhiyu (Frank) Quan, University of Connecticut
\item
  Jiandong Ren, Western University
\item
  Rajesh V. Sahasrabuddhe, Oliver Wyman
\item
  Ranee Thiagarajah, Illinois State University
\item
  Ping Wang, Saint Johns University
\item
  Chengguo Weng, University of Waterloo
\item
  Toby White, Drake University
\item
  Michelle Xia, Northern Illinois University
\item
  Di (Cindy) Xu, University of Nebraska - Lincoln
\item
  Lina Xu, Columbia University
\item
  Lu Yang, University of Amsterdam
\item
  Jorge Yslas, University of Copenhagen
\item
  Jeffrey Zheng, Temple University
\item
  Hongjuan Zhou, Arizona State University
\end{itemize}

\section*{For our Readers}\label{for-our-readers}
\addcontentsline{toc}{section}{For our Readers}

We hope that you find this book worthwhile and even enjoyable. For your
convenience, at our \href{https://openacttexts.github.io/}{Github
Landing site} (\url{https://openacttexts.github.io/}), you will find
links to the book that you can (freely) download for offline reading,
including a pdf version (for Adobe Acrobat) and an EPUB version suitable
for mobile devices.
\href{https://github.com/OpenActTexts/Loss-Data-Analytics/tree/master/Data}{Data}
for running our examples are available at the same site.

In developing this book, we are emphasizing the
\href{https://openacttexts.github.io/Loss-Data-Analytics/index.html}{online
version} that has lots of great features such as a glossary, code and
solutions to examples that you can be revealed interactively. For
example, you will find that the statistical code is hidden and can only
be seen by clicking on terms such as

We hide the code because we don't want to insist that you use the
\texttt{R} statistical software (although we like it). Still, we
encourage you to try some statistical code as you read the book -- we
have opted to make it easy to learn \texttt{R} as you go. We have even
set up a separate \href{https://openacttexts.github.io/LDARcode}{R Code
for Loss Data Analytics} site to explain more of the details of the
code.

Freely available, interactive textbooks represent a new venture in
actuarial education and we need your input. Although a lot of effort has
gone into the development, we expect hiccoughs. Please let your
instructor know about opportunities for improvement, write us through
our project site, or contact chapter contributors directly with
suggested improvements.

\chapter{Introduction to Loss Data Analytics}\label{C:Intro}

\emph{Chapter Preview}. This book introduces readers to methods of
analyzing insurance data. Section \ref{S:Intro} begins with a discussion
of why the use of data is important in the insurance industry. Section
\ref{S:PredModApps} gives a general overview of the purposes of
analyzing insurance data which is reinforced in the Section
\ref{S:LGPIF} case study. Naturally, there is a huge gap between the
broad goals summarized in the overview and a case study application;
this gap is covered through the methods and techniques of data analysis
covered in the rest of the text.

\section{Relevance of Analytics to Insurance Activities}\label{S:Intro}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Summarize the importance of insurance to consumers and the economy
\item
  Describe analytics
\item
  Identify data generating events associated with the timeline of a
  typical insurance contract
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Nature and Relevance of
Insurance}\label{nature-and-relevance-of-insurance}

This book introduces the process of using data to make decisions in an
insurance context. It does not assume that readers are familiar with
insurance but introduces insurance concepts as needed. If you are new to
insurance, then it is probably easiest to think about an insurance
policy that covers the contents of an apartment or house that you are
renting (known as renters insurance) or the contents and property of a
building that is owned by you or a friend (known as homeowners
insurance). Another common example is automobile insurance. In the event
of an accident, this policy may cover damage to your vehicle, damage to
other vehicles in the accident, as well as medical expenses of those
injured in the accident.

One way to think about the nature of insurance is who buys it. Renters,
homeowners, and auto insurance are examples of personal insurance in
that these are policies issued to people. Businesses also buy insurance,
such as coverage on their properties, and this is known as commercial
insurance. The seller, an insurance company, is also known as an
insurer. Even insurance companies need insurance; this is known as
reinsurance.

Another way to think about the nature of insurance is the type of risk
being covered. In the U.S., policies such as renters and homeowners are
known as property insurance whereas a policy such as auto that covers
medical damages to people is known as casualty insurance. In the rest of
the world, these are both known as nonlife or general insurance, to
distinguish them from life insurance.

Both life and non-life insurances are important components of the world
economy. The \citet{III2016} estimates that direct insurance premiums in
the world for 2014 was 2,654,549 for life and 2,123,699 for non-life;
these figures are in \emph{millions of U.S. dollars}. As noted earlier,
the total represents 6.2\% of the world GDP. Put another way, life
accounts for 55.5\% of insurance premiums and 3.4\% of world GDP whereas
non-life accounts for 44.5\% of insurance premiums and 2.8\% of world
GDP. Both life and non-life represent important economic activities.

Insurance may not be as entertaining as the sports industry (another
industry that depends heavily on data) but it does affect the financial
livelihoods of many. By almost any measure, insurance is a major
economic activity. On a global level, insurance premiums comprised about
6.2\% of the world gross domestic product (GDP) in 2014,
\citep{III2016}. As examples, premiums accounted for 18.9\% of GDP in
Taiwan (the highest in the study) and represented 7.3\% of GDP in the
United States. On a personal level, almost everyone owning a home has
insurance to protect themselves in the event of a fire, hailstorm, or
some other calamitous event. Almost every country requires insurance for
those driving a car. In sum, although not particularly entertaining,
insurance plays an important role in the economies of nations and the
lives of individuals.

\subsection{What is Analytics?}\label{what-is-analytics}

Insurance is a data-driven industry. Like all major corporations and
organizations, insurers use data when trying to decide how much to pay
employees, how many employees to retain, how to market their services
and products, how to forecast financial trends, and so on. These
represent general areas of activities that are not specific to the
insurance industry. Although each industry has its own data nuances and
needs, the collection, analysis and use of data is an activity shared by
all, from the internet giants to the small business, by public and
governmental organizations, and is not specific to the insurance
industry. You will find that the data collection and analysis methods
and tools introduced in this text are relevant for all.

In any data-driven industry, analytics is a key to deriving and
extracting information from data. But what is analytics? Making
data-driven business decisions has been described as business analytics,
business intelligence, and data science. These terms, among others, are
sometimes used interchangeably and sometimes refer to distinct
applications. \emph{Business intelligence} may focus on processes of
collecting data, often through databases and data warehouses, whereas
\emph{business analytics} utilizes tools and methods for statistical
analyses of data. In contrast to these two terms that emphasize business
applications, the term \emph{data science} can encompass broader data
related applications in many scientific domains. For our purposes, we
use the term analytics to refer to the process of using data to make
decisions. This process involves gathering data, understanding concepts
and models of uncertainty, making general inferences, and communicating
results.

When introducing data methods in this text, we will focus on losses that
arise from, or related to, obligations in insurance contracts. This
could be the amount of damage to one's apartment under a renter's
insurance agreement, the amount needed to compensate someone that you
hurt in a driving accident, and the like. We call these obligations
insurance claim. With this focus, we will be able to introduce and
directly use generally applicable statistical tools and techniques.

\subsection{Insurance Processes}\label{S:InsProcesses}

Yet another way to think about the nature of insurance is by the
duration of an insurance contract, known as the term. This text will
focus on short-term insurance contracts. By short-term, we mean
contracts where the insurance coverage is typically provided for a year
or six months. Most commercial and personal contracts are for a year so
that will be our default duration. An important exception is U.S. auto
policies that are often six months in length.

In contrast, we typically think of life insurance as a long-term
contract where the default is to have a multi-year contract. For
example, if a person 25 years old purchases a whole life policy that
pays upon death of the insured and that person does not die until age
100, then the contract is in force for 75 years.

There are other important differences between life and non-life
products. In life insurance, the benefit amount is often stipulated in
the contract provisions. In contrast, most non-life contracts provide
for compensation of insured losses which are unknown before the
accident. (There are usually limits placed on the compensation amounts.)
In a life insurance contract that stretches over many years, the time
value of money plays a prominent role. In a non-life contract, the
random amount of compensation takes priority.

In both life and non-life insurances, the frequency of claims is very
important. For many life insurance contracts, the insured event (such as
death) happens only once. In contrast, for non-life insurances such as
automobile, it is common for individuals (especially young male drivers)
to get into more than one accident during a year. So, our models need to
reflect this observation; we will introduce different frequency models
that you may also see when studying life insurance.

For short-term insurance, the framework of the probabilistic model is
straightforward. We think of a one-period model (the period length,
e.g., one year, will be specified in the situation).

\begin{itemize}
\item
  At the beginning of the period, the insured pays the insurer a known
  premium that is agreed upon by both parties to the contract.
\item
  At the end of the period, the insurer reimburses the insured for a
  (possibly multivariate) random loss.
\end{itemize}

This framework will be developed as we proceed; but we first focus on
integrating this framework with concerns about how the data may arise.
From an insurer's viewpoint, contracts may be only for a year but they
tend to be renewed. Moreover, payments arising from claims during the
year may extend well beyond a single year. One way to describe the data
arising from operations of an insurance company is to use a timeline
granular approach. A \textbf{process} approach provides an overall view
of the events occurring during the life of an insurance contract, and
their nature -- random or planned, loss events (claims) and contract
changes events, and so forth. In this micro oriented view, we can think
about what happens to a contract at various stages of its existence.

Figure \ref{fig:StochOperations} traces a timeline of a typical
insurance contract. Throughout the life of the contract, the company
regularly processes events such as premium collection and valuation,
described in Section \ref{S:PredModApps}; these are marked with an
\textbf{x} on the timeline. Non-regular and unanticipated events also
occur. To illustrate, \(\mathrm{t}_2\) and \(\mathrm{t}_4\) mark the
event of an insurance claim (some contracts, such as life insurance, can
have only a single claim). Times \(\mathrm{t}_3\) and \(\mathrm{t}_5\)
mark events when a policyholder wishes to alter certain contract
features, such as the choice of a deductible or the amount of coverage.
From a company perspective, one can even think about the contract
initiation (arrival, time \(\mathrm{t}_1\)) and contract termination
(departure, time \(\mathrm{t}_6\)) as uncertain events. (Alternatively,
for some purposes, you may condition on these events and treat them as
certain.)





\begin{figure}

{\centering \includegraphics[width=1\linewidth]{LossDataAnalytics_files/figure-latex/StochOperations-1} 

}

\caption{Timeline of a Typical Insurance Policy. Arrows
mark the occurrences of random events. Each x marks the time of
scheduled events that are typically non-random.}\label{fig:StochOperations}
\end{figure}

\section{Insurance Company Operations}\label{S:PredModApps}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Describe five major operational areas of insurance companies.
\item
  Identify the role of data and analytics opportunities within each
  operational area.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Armed with insurance data, the end goal is to use data to make
decisions. We will learn more about methods of analyzing and
extrapolating data in future chapters. To begin, let us think about why
we want to do the analysis. We will take the insurance company's
viewpoint (not the insured person) and introduce ways of bringing money
in, paying it out, managing costs, and making sure that we have enough
money to meet obligations. The emphasis is on insurance-specific
operations rather than on general business activities such as
advertising, marketing, and human resources management.

Specifically, in many insurance companies, it is customary to aggregate
detailed insurance processes into larger operational units; many
companies use these functional areas to segregate employee activities
and areas of responsibilities. Actuaries, other financial analysts, and
insurance regulators work within these units and use data for the
following activities:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Initiating Insurance}. At this stage, the company makes a
  decision as to whether or not to take on a risk (the underwriting
  stage) and assign an appropriate premium (or rate). Insurance
  analytics has its actuarial roots in \emph{ratemaking}, where analysts
  seek to determine the right price for the right risk.
\item
  \textbf{Renewing Insurance}. Many contracts, particularly in general
  insurance, have relatively short durations such as 6 months or a year.
  Although there is an implicit expectation that such contracts will be
  renewed, the insurer has the opportunity to decline coverage and to
  adjust the premium. Analytics is also used at this policy renewal
  stage where the goal is to retain profitable customers.
\item
  \textbf{Claims Management}. Analytics has long been used in (1)
  detecting and preventing claims fraud, (2) managing claim costs,
  including identifying the appropriate support for claims handling
  expenses, as well as (3) understanding excess layers for reinsurance
  and retention.
\item
  \textbf{Loss Reserving}. Analytic tools are used to provide management
  with an appropriate estimate of future obligations and to quantify the
  uncertainty of those estimates.
\item
  \textbf{Solvency and Capital Allocation}. Deciding on the requisite
  amount of capital and on ways of allocating capital among alternative
  investments are also important analytics activities. Companies must
  understand how much capital is needed so that they will have
  sufficient flow of cash available to meet their obligations at the
  times they are expected to materialize (solvency). This is an
  important question that concerns not only company managers but also
  customers, company shareholders, regulatory authorities, as well as
  the public at large. Related to issues of how much capital is the
  question of how to allocate capital to differing financial projects,
  typically to maximize an investor's return. Although this question can
  arise at several levels, insurance companies are typically concerned
  with how to allocate capital to different lines of business within a
  firm and to different subsidiaries of a parent firm.
\end{enumerate}

Although data represent a critical component of solvency and capital
allocation, other components including the local and global economic
framework, the financial investments environment, and quite specific
requirements according to the regulatory environment of the day, are
also important. Because of the background needed to address these
components, we will not address solvency, capital allocation, and
regulation issues in this text.

Nonetheless, for all operating functions, we emphasize that analytics in
the insurance industry is not an exercise that a small group of analysts
can do by themselves. It requires an insurer to make significant
investments in their information technology, marketing, underwriting,
and actuarial functions. As these areas represent the primary end goals
of the analysis of data, additional background on each operational unit
is provided in the following subsections.

\subsection{Initiating Insurance}\label{initiating-insurance}

Setting the price of an insurance product can be a perplexing problem.
This is in contrast to other industries such as manufacturing where the
cost of a product is (relatively) known and provides a benchmark for
assessing a market demand price. Similarly, in other areas of financial
services, market prices are available and provide the basis for a
market-consistent pricing structure of products. However, for many lines
of insurance, the cost of a product is uncertain and market prices are
unavailable. Expectations of the random cost is a reasonable place to
start for a price. (If you have studied finance, then you will recall
that an expectation is the optimal price for a risk-neutral insurer.) It
has been traditional in insurance pricing to begin with the expected
cost. Insurers then add margins to this, to account for the product's
riskiness, expenses incurred in servicing the product, and an allowance
for profit/surplus of the company.

Use of expected costs as a foundation for pricing is prevalent in some
lines of the insurance business. These include automobile and homeowners
insurance. For these lines, analytics has served to sharpen the market
by making the calculation of the product's expected cost more precise.
The increasing availability of the internet to consumers has also
promoted transparency in pricing; in today's marketplace, consumers have
ready access to competing quotes from a host of insurers. Insurers seek
to increase their market share by refining their risk classification
systems, thus achieving a better approximation of the products' prices
and enabling cream-skimming underwriting strategies (``cream-skimming''
is a phrase used when the insurer underwrites only the best risks).
Recent surveys (e.g., \citet{survey2013}) indicate that pricing is the
most common use of analytics among insurers.

\emph{Underwriting}, the process of classifying risks into homogeneous
categories and assigning policyholders to these categories, lies at the
core of ratemaking. Policyholders within a class (category) have similar
risk profiles and so are charged the same insurance price. This is the
concept of an actuarially fair premium; it is fair to charge different
rates to policyholders only if they can be separated by identifiable
risk factors. An early article, \emph{Two Studies in Automobile
Insurance Ratemaking} \citep{bailey1960}, provided a catalyst to the
acceptance of analytic methods in the insurance industry. This paper
addresses the problem of classification ratemaking. It describes an
example of automobile insurance that has five use classes
cross-classified with four merit rating classes. At that time, the
contribution to premiums for use and merit rating classes were
determined independently of each other. Thinking about the interacting
effects of different classification variables is a more difficult
problem.

\subsection{Renewing Insurance}\label{renewing-insurance}

Insurance is a type of financial service and, like many service
contracts, insurance coverage is often agreed upon for a limited time
period at which time coverage commitments are complete. Particularly for
general insurance, the need for coverage continues and so efforts are
made to issue a new contract providing similar coverage, when the
existing contract comes to the end of its term. This is called policy
renewal. Renewal issues can also arise in life insurance, e.g., term
(temporary) life insurance. At the same time other contracts, such as
life annuities, terminate upon the insured's death and so issues of
renewability are irrelevant.

In the absence of legal restrictions, at renewal the insurer has the
opportunity to:

\begin{itemize}
\item
  accept or decline to underwrite the risk; and
\item
  determine a new premium, possibly in conjunction with a new
  classification of the risk.
\end{itemize}

Risk classification and rating at renewal is based on two types of
information. First, at the initial stage, the insurer has available many
rating variables upon which decisions can be made. Many variables will
not change, e.g., sex, whereas others are likely to have changed, e.g.,
age, and still others may or may not change, e.g., credit score. Second,
unlike the initial stage, at renewal the insurer has available a history
of policyholder's loss experience, and this history can provide insights
into the policyholder that are not available from rating variables.
Modifying premiums with claims history is known as \emph{experience
rating}, also sometimes referred to as \emph{merit rating}.

Experience rating methods are either applied retrospectively or
prospectively. With retrospective methods, a refund of a portion of the
premium is provided to the policyholder in the event of favorable (to
the insurer) experience. Retrospective premiums are common in life
insurance arrangements (where policyholders earn dividends in the U.S.,
bonuses in the U.K., and profit sharing in Israeli term life coverage).
In general insurance, prospective methods are more common, where
favorable insured experience is rewarded through a lower renewal
premium.

Claims history can provide information about a policyholder's risk
appetite. For example, in personal lines it is common to use a variable
to indicate whether or not a claim has occurred in the last three years.
As another example, in a commercial line such as worker's compensation,
one may look to a policyholder's average claim frequency or severity
over the last three years. Claims history can reveal information that is
otherwise hidden (to the insurer) about the policyholder.

\subsection{Claims and Product
Management}\label{claims-and-product-management}

In some of areas of insurance, the process of paying claims for insured
events is relatively straightforward. For example, in life insurance, a
simple death certificate is all that is needed to pay the benefit amount
as provided in the contract. However, in non-life areas such as property
and casualty insurance, the process can be much more complex. Think
about even a relatively simple insured event such as automobile
accident. Here, it is often required to determine which party is at
fault, one needs to assess damage to all of the vehicles and people
involved in the incident, both insured and non-insured, the expenses
incurred in assessing the damages must be assessed, and so forth. The
process of determining coverage, legal liability, and settling claims is
known as claims adjustment.

Insurance managers sometimes use the phrase claims leakage to mean
dollars lost through claims management inefficiencies. There are many
ways in which analytics can help manage the claims process, c.f.,
\citet{SASsurvey}. Historically, the most important has been fraud
detection. The claim adjusting process involves reducing information
asymmetry (the claimant knows what happened; the company knows some of
what happened). Mitigating fraud is an important part of the claims
management process.

Fraud detection is only one aspect of managing claims. More broadly, one
can think about claims management as consisting of the following
components:

\begin{itemize}
\item
  \textbf{Claims triaging}. Just as in the medical world, early
  identification and appropriate handling of high cost claims (patients,
  in the medical world), can lead to dramatic savings. For example, in
  workers compensation, insurers look to achieve early identification of
  those claims that run the risk of high medical costs and a long payout
  period. Early intervention into these cases could give insurers more
  control over the handling of the claim, the medical treatment, and the
  overall costs with an earlier return-to-work.
\item
  \textbf{Claims processing}. The goal is to use analytics to identify
  routine situations that are anticipated to have small payouts. More
  complex situations may require more experienced adjusters and legal
  assistance to appropriately handle claims with high potential payouts.
\item
  \textbf{Adjustment decisions}. Once a complex claim has been
  identified and assigned to an adjuster, analytic driven routines can
  be established to aid subsequent decision-making\\
  processes. Such processes can also be helpful for adjusters in
  developing case reserves, an estimate of the insurer's future
  liability. This is an important input to the insurer's loss reserves,
  described in Section \ref{S:Reserving}.
\end{itemize}

In addition to the insured's reimbursement for losses, the insurer also
needs to be concerned with another source of revenue outflow, expenses.
loss adjustment expenses are part of an insurer's cost of managing
claims. Analytics can be used to reduce expenses directly related to
claims handling (allocated) as well as general staff time for overseeing
the claims processes (unallocated). The insurance industry has high
operating costs relative to other portions of the financial services
sectors.

In addition to claims payments, there are many other ways in which
insurers use data to manage their products. We have already discussed
the need for analytics in underwriting, that is, risk classification at
the initial acquisition and renewal stages. Insurers are also interested
in which policyholders elect to renew their contract and, as with other
products, monitor customer loyalty.

Analytics can also be used to manage the portfolio, or collection, of
risks that an insurer has acquired. When the risk is initially obtained,
the insurer's risk can be managed by imposing contract parameters that
modify contract payouts. Chapters \ref{C:Severity} and \ref{C:PortMgt}
describe common modifications including coinsurance, deductibles and
policy upper limits.

After the contract has been agreed upon with an insured, the insurer may
still modify its net obligation by entering into a reinsurance
agreement. This type of agreement is with a reinsurer, an insurer of an
insurer. It is common for insurance companies to purchase insurance on
its portfolio of risks to gain protection from unusual events, just as
people and other companies do.

\subsection{Loss Reserving}\label{S:Reserving}

An important feature that distinguishes insurance from other sectors of
the economy is the timing of the exchange of considerations. In
manufacturing, payments for goods are typically made at the time of a
transaction. In contrast, for insurance, money received from a customer
occurs in advance of benefits or services; these are rendered at a later
date when the insured event occurs. This leads to the need to hold a
reservoir of wealth to meet future obligations in respect to obligations
made, and to gain the trust of the insureds that the company will be
able to fulfill its commitments. The size of this reservoir of wealth,
and the importance of ensuring its adequacy, is a major concern for the
insurance industry.

Setting aside money for unpaid claims is known as loss reserving; in
some jurisdictions, reserves are also known as \emph{technical
provisions}. We saw in Figure \ref{fig:StochOperations} several times at
which a company summarizes its financial position; these times are known
as valuation dates. Claims that arise prior to valuation dates have
typically been paid, are in the process of being paid, or are about to
be paid; claims in the future of these valuation dates are unknown. A
company must estimate these outstanding liabilities when determining its
financial strength. Accurately determining loss reserves is important to
insurers for many reasons.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Loss reserves represent an anticipated claim that the insurer owes its
  customers. Under-reserving may result in a failure to meet claim
  liabilities. Conversely, an insurer with excessive reserves may
  present a weaker financial position than it truly has.
\item
  Reserves provide an estimate for the unpaid cost of insurance that can
  be used for pricing contracts.
\item
  Loss reserving is required by laws and regulations. The public has a
  strong interest in the financial strength and solvency of insurers.
\item
  In addition to insurance company management and regulators, other
  stakeholders such as investors and customers make decisions that
  depend on company loss reserves.
\end{enumerate}

Loss reserving is a topic where there are substantive differences
between life and general (also known as property and casualty, or
non-life), insurance. In life insurance, the severity (amount of loss)
is often not a source of uncertainty as payouts are specified in the
contract. The frequency, driven by mortality of the insured, is a
concern. However, because of the length of time for settlement of life
insurance contracts, the time value of money uncertainty as measured
from issue to date of payment can dominate frequency concerns. For
example, for an insured who purchases a life contract at age 20, it
would not be unusual for the contract to still be open in 60 years time,
when the insured celebrates his or her 80th birthday. See, for example,
\citet{bowers1986actuarial} or \citet{dickson2013actuarial} for
introductions to reserving for life insurance.

\section{Case Study: Wisconsin Property Fund}\label{S:LGPIF}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, we use the Wisconsin Property Fund as a case study. You
learn how to:

\begin{itemize}
\tightlist
\item
  Describe how data generating events can produce data of interest to
  insurance analysts.
\item
  Produce relevant summary statistics for each variable.
\item
  Describe how these summary statistics can be used in each of the major
  operational areas of an insurance company.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Let us illustrate the kind of data under consideration and the goals
that we wish to achieve by examining the Local Government Property
Insurance Fund (LGPIF), an insurance pool administered by the Wisconsin
Office of the Insurance Commissioner. The LGPIF was established to
provide property insurance for local government entities that include
counties, cities, towns, villages, school districts, and library boards.
The fund insures local government property such as government buildings,
schools, libraries, and motor vehicles. The fund covers all property
losses except those resulting from flood, earthquake, wear and tear,
extremes in temperature, mold, war, nuclear reactions, and embezzlement
or theft by an employee.

The fund covers over a thousand local government entities who pay
approximately \textbackslash{}\$25 million in premiums each year and
receive insurance coverage of about \textbackslash{}\$75 billion. State
government buildings are not covered; the LGPIF is for local government
entities that have separate budgetary responsibilities and who need
insurance to moderate the budget effects of uncertain insurable events.
Coverage for local government property has been made available by the
State of Wisconsin since 1911, thus providing a wealth of historical
data.

In this illustration, we restrict consideration to claims from coverage
of building and contents; we do not consider claims from motor vehicles
and specialized equipment owned by local entities (such as snow plowing
machines). We also consider only claims that are closed, with
obligations fully met.

\subsection{Fund Claims Variables: Frequency and
Severity}\label{S:OutComes}

At a fundamental level, insurance companies accept premiums in exchange
for promises to compensate a policyholder upon the occurrence of an
insured event. Indemnification is the compensation provided by the
insurer for incurred hurt, loss, or damage that is covered by the
policy. This compensation is also known as a \emph{claim}. The extent of
the payout, known as the \emph{severity}, is a key financial expenditure
for an insurer.

In terms of money outgo, an insurer is indifferent to having ten claims
of 100 when compared to one claim of 1,000. Nonetheless, it is common
for insurers to study how often claims arise, known as the
\emph{frequency} of claims. The frequency is important for expenses, but
it also influences contractual parameters (such as deductibles and
policy limits that are described later) that are written on a per
occurrence basis, is routinely monitored by insurance regulators, and
can be a key driver in the overall indemnification obligation of the
insurer. We shall consider the frequency and severity as the two main
claim variables that we wish to understand, model, and manage.

To illustrate, in 2010 there were 1,110 policyholders in the property
fund who experienced a total of 1,377 claims. Table
\ref{tab:Frequency2010} shows the distribution. Almost two-thirds
(0.637) of the policyholders did not have any claims and an additional
18.8\% had only one claim. The remaining 17.5\% (=1 - 0.637 - 0.188) had
more than one claim; the policyholder with the highest number recorded
239 claims. The average number of claims for this sample was 1.24
(=1377/1110).

\begin{longtable}[]{@{}llllllllllll@{}}
\caption{\label{tab:Frequency2010} 2010 Claims Frequency
Distribution}\tabularnewline
\toprule
Type & & & & & & & & & & &\tabularnewline
\midrule
\endfirsthead
\toprule
Type & & & & & & & & & & &\tabularnewline
\midrule
\endhead
Number & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 or more &
Sum\tabularnewline
Count & 707 & 209 & 86 & 40 & 18 & 12 & 9 & 4 & 6 & 19 &
1,110\tabularnewline
Claims & 0 & 209 & 172 & 120 & 72 & 60 & 54 & 28 & 48 & 617 &
1,377\tabularnewline
Proportion & 0.637 & 0.188 & 0.077 & 0.036 & 0.016 & 0.011 & 0.008 &
0.004 & 0.005 & 0.017 & 1.000\tabularnewline
\bottomrule
\end{longtable}

For the severity distribution, a common approach is to examine the
distribution of the sample of 1,377 claims. However, another common
approach is to examine the distribution of the average claims of those
policyholders with claims. In our 2010 sample, there were 403
(=1110-707) such policyholders. For 209 of these policyholders with one
claim, the average claim equals the only claim they experienced. For the
policyholder with highest frequency, the average claim is an average
over 239 separately reported claim events. This average is also known as
the pure premium or \emph{loss cost}.

Table \ref{tab:Severity2010} summarizes the sample distribution of
average severities from the 403 policyholders who made a claim; it shows
that the average claim amount was 56,330 (all amounts are in U.S.
Dollars). However, the average gives only a limited look at the
distribution. More information can be gleaned from the summary
statistics which show a very large claim in the amount of 12,920,000.
Figure \ref{fig:SeverityFig} provides further information about the
distribution of sample claims, showing a distribution that is dominated
by this single large claim so that the histogram is not very helpful.
Even when removing the large claim, you will find a distribution that is
skewed to the right. A generally accepted technique is to work with
claims in logarithmic units especially for graphical purposes; the
corresponding figure in the right-hand panel is much easier to
interpret.

\begin{longtable}[]{@{}rrrrrr@{}}
\caption{\label{tab:Severity2010} 2010 Average Severity
Distribution}\tabularnewline
\toprule
\begin{minipage}[b]{0.12\columnwidth}\raggedleft\strut
Minimum\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedleft\strut
First Quartile\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\raggedleft\strut
Median\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\raggedleft\strut
Mean\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedleft\strut
Third Quartile\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\raggedleft\strut
Maximum\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.12\columnwidth}\raggedleft\strut
Minimum\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedleft\strut
First Quartile\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\raggedleft\strut
Median\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\raggedleft\strut
Mean\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedleft\strut
Third Quartile\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\raggedleft\strut
Maximum\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
167\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedleft\strut
2,226\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedleft\strut
4,951\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedleft\strut
56,330\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedleft\strut
11,900\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedleft\strut
12,920,000\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{LossDataAnalytics_files/figure-latex/SeverityFig-1} 

}

\caption{Distribution of Positive Average Severities}\label{fig:SeverityFig}
\end{figure}

\subsection{Fund Rating Variables}\label{S:FundVariables}

Developing models to represent and manage the two outcome variables,
frequency and severity, is the focus of the early chapters of this text.
However, when actuaries and other financial analysts use those models,
they do so in the context of external variables. In general statistical
terminology, one might call these explanatory or predictor variables;
there are many other names in statistics, economics, psychology, and
other disciplines. Because of our insurance focus, we call them rating
variables as they will be useful in setting insurance rates and
premiums.

We earlier considered observations from a sample of 1,110 policyholders
which may seem like a lot. However, as we will see in our forthcoming
applications, because of the preponderance of zeros and the skewed
nature of claims, actuaries typically yearn for more data. One common
approach that we adopt here is to examine outcomes from multiple years,
thus increasing the sample size. We will discuss the strengths and
limitations of this strategy later but, at this juncture, we just wish
to show the reader how it works.

Specifically, Table \ref{tab:CoverageBCIM} shows that we now consider
policies over five years of data, 2006, \ldots{}, 2010, inclusive. The
data begins in 2006 because there was a shift in claim coding in 2005 so
that comparisons with earlier years are not helpful. To mitigate the
effect of open claims, we consider policy years prior to 2011. An open
claim means that not all of the obligations for the claim are known at
the time of the analysis; for some claims, such an injury to a person in
an auto accident or in the workplace, it can take years before costs are
fully known.

\begin{longtable}[]{@{}lrrrr@{}}
\caption{\label{tab:CoverageBCIM} Claims Summary by
Policyholder}\tabularnewline
\toprule
\begin{minipage}[b]{0.17\columnwidth}\raggedright\strut
Year\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\raggedleft\strut
Average Frequency\strut
\end{minipage} & \begin{minipage}[b]{0.17\columnwidth}\raggedleft\strut
Average Severity\strut
\end{minipage} & \begin{minipage}[b]{0.16\columnwidth}\raggedleft\strut
Average Coverage\strut
\end{minipage} & \begin{minipage}[b]{0.16\columnwidth}\raggedleft\strut
Number of Policyholders\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.17\columnwidth}\raggedright\strut
Year\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\raggedleft\strut
Average Frequency\strut
\end{minipage} & \begin{minipage}[b]{0.17\columnwidth}\raggedleft\strut
Average Severity\strut
\end{minipage} & \begin{minipage}[b]{0.16\columnwidth}\raggedleft\strut
Average Coverage\strut
\end{minipage} & \begin{minipage}[b]{0.16\columnwidth}\raggedleft\strut
Number of Policyholders\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
2006\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedleft\strut
0.951\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedleft\strut
9,695\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedleft\strut
32,498,186\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedleft\strut
1,154\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
2007\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedleft\strut
1.167\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedleft\strut
6,544\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedleft\strut
35,275,949\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedleft\strut
1,138\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
2008\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedleft\strut
0.974\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedleft\strut
5,311\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedleft\strut
37,267,485\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedleft\strut
1,125\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
2009\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedleft\strut
1.219\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedleft\strut
4,572\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedleft\strut
40,355,382\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedleft\strut
1,112\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
2010\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedleft\strut
1.241\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedleft\strut
20,452\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedleft\strut
41,242,070\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedleft\strut
1,110\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Table \ref{tab:CoverageBCIM} shows that the average claim varies over
time, especially with the high 2010 value (that we saw was due to a
single large claim)\footnote{Note that the average severity in Table
  \ref{tab:CoverageBCIM} differs from that reported in Table
  \ref{tab:Severity2010}. This is because the former includes
  policyholders with zero claims where as the latter does not. This is
  an important distinction that we will address in later portions of the
  text.}. The total number of policyholders is steadily declining and,
conversely, the coverage is steadily increasing. The coverage variable
is the amount of coverage of the property and contents. Roughly, you can
think of it as the maximum possible payout of the insurer. For our
immediate purposes, the coverage is our first rating variable. Other
things being equal, we would expect that policyholders with larger
coverage will have larger claims. We will make this vague idea much more
precise as we proceed, and also justify this expectation with data.

For a different look at the 2006-2010 data, Table \ref{tab:DeductCov}
summarizes the distribution of our two outcomes, frequency and claims
amount. In each case, the average exceeds the median, suggesting that
the two distributions are right-skewed. In addition, the table
summarizes our continuous rating variables, coverage and deductible
amount. The table also suggests that these variables also have
right-skewed distributions.

\begin{longtable}[]{@{}lrrrr@{}}
\caption{\label{tab:DeductCov} Summary of Claim Frequency and Severity,
Deductibles, and Coverages}\tabularnewline
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright\strut
\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedleft\strut
Minimum\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedleft\strut
Median\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedleft\strut
Average\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\raggedleft\strut
Maximum\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright\strut
\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedleft\strut
Minimum\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedleft\strut
Median\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedleft\strut
Average\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\raggedleft\strut
Maximum\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.23\columnwidth}\raggedright\strut
Claim Frequency\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
0\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
1.109\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedleft\strut
263\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright\strut
Claim Severity\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
0\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
9,292\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedleft\strut
12,922,218\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright\strut
Deductible\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
500\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
1,000\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
3,365\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedleft\strut
100,000\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright\strut
Coverage (000's)\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
8.937\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
11,354\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
37,281\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedleft\strut
2,444,797\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

The following display describes the rating variables considered in this
chapter. Hopefully, these are variables that you think might naturally
be related to claims outcomes. You can learn more about them in
\citet{frees2016multivariate}. To handle the skewness, we henceforth
focus on logarithmic transformations of coverage and deductibles.

\textbf{Description of Rating Variables} \[{\small \begin{matrix}
\begin{array}{ l | l}
\hline
Variable    & Description \\
\hline
\text{EntityType}   & \text{Categorical variable that is one of six types:  (Village, City,} \\
& ~~~~ \text{County, Misc, School, or Town)} \\
\text{LnCoverage}   & \text{Total building and content coverage, in logarithmic millions of dollars}\\
\text{LnDeduct}     & \text{Deductible, in logarithmic dollars} \\
\text{AlarmCredit}  & \text{Categorical variable that is one of four types:  (0, 5, 10, or 15)} \\
 &  ~~~~   \text{for automatic smoke alarms in main rooms} \\
\text{NoClaimCredit}    & \text{Binary variable to indicate no claims in the past two years} \\
\text{Fire5 }           & \text{Binary variable to indicate the fire class is below 5} \\
& ~~~~ \text{(The range of fire class is 0 to 10)} \\
\hline
\end{array}
\end{matrix}}\]

To get a sense of the relationship between the non-continuous rating
variables and claims, Table \ref{tab:ClaimRateVar} relates the claims
outcomes to these categorical variables. Table \ref{tab:ClaimRateVar}
suggests substantial variation in the claim frequency and average
severity of the claims by entity type. It also demonstrates higher
frequency and severity for the \({\tt Fire5}\) variable and the reverse
for the \({\tt NoClaimCredit}\) variable. The relationship for the
\({\tt Fire5}\) variable is counter-intuitive in that one would expect
lower claim amounts for those policyholders in areas with better public
protection (when the protection code is five or less). Naturally, there
are other variables that influence this relationship. We will see that
these background variables are accounted for in the subsequent
multivariate regression analysis, which yields an intuitive, appealing
(negative) sign for the \({\tt Fire5}\) variable.

\begin{longtable}[]{@{}lrrr@{}}
\caption{\label{tab:ClaimRateVar} Claims Summary by Entity Type, Fire Class,
and No Claim Credit}\tabularnewline
\toprule
\begin{minipage}[b]{0.27\columnwidth}\raggedright\strut
Variable\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedleft\strut
Number of Policies\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedleft\strut
Claim Frequency\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedleft\strut
Average Severity\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.27\columnwidth}\raggedright\strut
Variable\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedleft\strut
Number of Policies\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedleft\strut
Claim Frequency\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedleft\strut
Average Severity\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
\emph{EntityType}\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
Village\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
1,341\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
0.452\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
10,645\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
City\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
793\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
1.941\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
16,924\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
County\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
328\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
4.899\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
15,453\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
Misc\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
609\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
0.186\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
43,036\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
School\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
1,597\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
1.434\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
64,346\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
Town\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
971\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
0.103\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
19,831\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
\emph{Fire}\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
Fire5=0\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
2,508\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
0.502\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
13,935\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
Fire5=1\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
3,131\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
1.596\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
41,421\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
\emph{No Claims Credit}\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
NoClaimCredit=0\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
3,786\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
1.501\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
31,365\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
NoClaimCredit=1\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
1,853\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
0.310\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
30,499\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.27\columnwidth}\raggedright\strut
\textbf{Total}\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
5,639\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
1.109\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedleft\strut
31,206\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Table \ref{tab:RateAlarmCredit} shows the claims experience by alarm
credit. It underscores the difficulty of examining variables
individually. For example, when looking at the experience for all
entities, we see that policyholders with no alarm credit have on average
lower frequency and severity than policyholders with the highest (15\%,
with 24/7 monitoring by a fire station or security company) alarm
credit. In particular, when we look at the entity type School, the
frequency is 0.422 and the severity 25,523 for no alarm credit, whereas
for the highest alarm level it is 2.008 and 85,140. This may simply
imply that entities with more claims are the ones that are likely to
have an alarm system. Summary tables do not examine multivariate
effects; for example, Table \ref{tab:ClaimRateVar} ignores the effect of
size (as we measure through coverage amounts) that affect claims.

\begin{longtable}[]{@{}lrrrrrr@{}}
\caption{\label{tab:RateAlarmCredit} Claims Summary by Entity Type and Alarm
Credit (AC) Category}\tabularnewline
\toprule
\begin{minipage}[b]{0.10\columnwidth}\raggedright\strut
Entity Type\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedleft\strut
AC0 Claim Frequency\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedleft\strut
AC0 Avg. Severity\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedleft\strut
AC0 Num. Policies\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedleft\strut
AC5 Claim Frequency\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedleft\strut
AC5 Avg. Severity\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedleft\strut
AC5 Num. Policies\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.10\columnwidth}\raggedright\strut
Entity Type\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedleft\strut
AC0 Claim Frequency\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedleft\strut
AC0 Avg. Severity\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedleft\strut
AC0 Num. Policies\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedleft\strut
AC5 Claim Frequency\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedleft\strut
AC5 Avg. Severity\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedleft\strut
AC5 Num. Policies\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.10\columnwidth}\raggedright\strut
Village\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.326\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
11,078\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
829\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.278\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
8,086\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
54\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\raggedright\strut
City\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.893\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
7,576\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
244\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
2.077\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
4,150\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
13\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\raggedright\strut
County\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
2.140\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
16,013\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
50\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
-\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
-\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
1\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\raggedright\strut
Misc\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.117\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
15,122\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
386\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.278\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
13,064\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
18\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\raggedright\strut
School\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.422\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
25,523\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
294\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.410\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
14,575\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
122\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\raggedright\strut
Town\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.083\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
25,257\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
808\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.194\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
3,937\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
31\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\raggedright\strut
Total\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.318\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
15,118\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
2,611\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.431\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
10,762\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
239\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}lrcrrrr@{}}
\caption{Claims Summary by Entity Type and Alarm Credit (AC)
Category}\tabularnewline
\toprule
\begin{minipage}[b]{0.10\columnwidth}\raggedright\strut
Entity Type\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedleft\strut
AC10 Claim Frequency\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\centering\strut
AC10 Avg. Severity\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedleft\strut
AC10 Num. Policies\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedleft\strut
AC15 Claim Frequency\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedleft\strut
AC15 Avg. Severity\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedleft\strut
AC15 Num. Policies\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.10\columnwidth}\raggedright\strut
Entity Type\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedleft\strut
AC10 Claim Frequency\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\centering\strut
AC10 Avg. Severity\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedleft\strut
AC10 Num. Policies\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\raggedleft\strut
AC15 Claim Frequency\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedleft\strut
AC15 Avg. Severity\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedleft\strut
AC15 Num. Policies\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.10\columnwidth}\raggedright\strut
Village\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.500\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
8,792\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
50\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.725\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
10,544\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
408\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\raggedright\strut
City\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
1.258\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
8,625\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
31\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
2.485\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
20,470\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
505\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\raggedright\strut
County\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
2.125\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
11,688\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
8\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
5.513\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
15,476\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
269\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\raggedright\strut
Misc\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.077\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
3,923\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
26\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.341\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
87,021\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
179\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\raggedright\strut
School\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.488\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
11,597\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
168\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
2.008\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
85,140\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
1,013\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\raggedright\strut
Town\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.091\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
2,338\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
44\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.261\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
9,490\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
88\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\raggedright\strut
Total\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
0.517\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
10,194\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
327\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\raggedleft\strut
2.093\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
41,458\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedleft\strut
2,462\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\subsection{Fund Operations}\label{fund-operations}

We have now seen the Fund's two outcome variables: a count variable for
the number of claims, and a continuous variable for the claims amount.
We have also introduced a continuous rating variable (coverage); a
discrete quantitative variable (logarithmic deductibles); two binary
rating variables (no claims credit and fire class); and two categorical
rating variables (entity type and alarm credit). Subsequent chapters
will explain how to analyze and model the distribution of these
variables and their relationships. Before getting into these technical
details, let us first think about where we want to go. General insurance
company functional areas are described in Section \ref{S:PredModApps};
let us now think about how these areas might apply in the context of the
property fund.

\subsubsection*{Initiating Insurance}\label{initiating-insurance-1}
\addcontentsline{toc}{subsubsection}{Initiating Insurance}

Because this is a government sponsored fund, we do not have to worry
about selecting good or avoiding poor risks; the fund is not allowed to
deny a coverage application from a qualified local government entity. If
we do not have to underwrite, what about how much to charge?

We might look at the most recent experience in 2010, where the total
fund claims were approximately 28.16 million USD
(\(=1377 \text{ claims} \times 20452 \text{ average severity}\)).
Dividing that among 1,110 policyholders, that suggests a rate of 24,370
( \(\approx\) 28,160,000/1110). However, 2010 was a bad year; using the
same method, our premium would be much lower based on 2009 data. This
swing in premiums would defeat the primary purpose of the fund, to allow
for a steady charge that local property managers could utilize in their
budgets.

Having a single price for all policyholders is nice but hardly seems
fair. For example, Table \ref{tab:ClaimRateVar} suggests that Schools
have much higher claims than other entities and so should pay more.
However, simply doing the calculation on an entity by entity basis is
not right either. For example, we saw in Table \ref{tab:RateAlarmCredit}
that had we used this strategy, entities with a 15\% alarm credit (for
good behavior, having top alarm systems) would actually wind up paying
more.

So, we have the data for thinking about the appropriate rates to charge
but will need to dig deeper into the analysis. We will explore this
topic further in Chapter \ref{C:PremiumFoundations} on \emph{premium
calculation fundamentals}. Selecting appropriate risks is introduced in
Chapter \ref{C:RiskClass} on \emph{risk classification}.

\subsubsection*{Renewing Insurance}\label{renewing-insurance-1}
\addcontentsline{toc}{subsubsection}{Renewing Insurance}

Although property insurance is typically a one-year contract, Table
\ref{tab:CoverageBCIM} suggests that policyholders tend to renew; this
is typical of general insurance. For renewing policyholders, in addition
to their rating variables we have their claims history and this claims
history can be a good predictor of future claims. For example, Table
\ref{tab:ClaimRateVar} shows that policyholders without a claim in the
last two years had much lower claim frequencies than those with at least
one accident (0.310 compared to 1.501); a lower predicted frequency
typically results in a lower premium. This is why it is common for
insurers to use variables such as \({\tt NoClaimCredit}\) in their
rating. We will explore this topic further in Chapter
\ref{C:Credibility} on \emph{experience rating}.

\subsubsection*{Claims Management}\label{claims-management}
\addcontentsline{toc}{subsubsection}{Claims Management}

Of course, the main story line of the 2010 experience was the large
claim of over 12 million USD, nearly half the claims for that year. Are
there ways that this could have been prevented or mitigated? Are their
ways for the fund to purchase protection against such large unusual
events? Another unusual feature of the 2010 experience noted earlier was
the very large frequency of claims (239) for one policyholder. Given
that there were only 1,377 claims that year, this means that a single
policyholder had 17.4 \% of the claims. This also suggests opportunities
for managing claims, the subject of Chapter \ref{C:PortMgt}.

\subsubsection*{Loss Reserving}\label{loss-reserving}
\addcontentsline{toc}{subsubsection}{Loss Reserving}

In our case study, we look only at the one year outcomes of closed
claims (the opposite of open). However, like many lines of insurance,
obligations from insured events to buildings such as fire, hail, and the
like, are not known immediately and may develop over time. Other lines
of business, including those where there are injuries to people, take
much longer to develop. Chapter \ref{C:LossReserves} introduces this
concern and \emph{loss reserving}, the discipline of determining how
much the insurance company should retain to meet its obligations.

\section{Further Resources and
Contributors}\label{Intro-further-reading-and-resources}

\subsubsection*{Contributor}\label{contributor}
\addcontentsline{toc}{subsubsection}{Contributor}

\begin{itemize}
\tightlist
\item
  \textbf{Edward W. (Jed) Frees}, University of Wisconsin-Madison, is
  the principal author of the initial version of this chapter. Email:
  \href{mailto:jfrees@bus.wisc.edu}{\nolinkurl{jfrees@bus.wisc.edu}} for
  chapter comments and suggested improvements.
\item
  Chapter reviewers include: Yair Babad, Chunsheng Ban, Aaron Bruhn,
  Gordon Enderle, Hirokazu (Iwahiro) Iwasawa, Bell Ouelega.
\end{itemize}

This book introduces loss data analytic tools that are most relevant to
actuaries and other financial risk analysts. We have also introduced you
to many new insurance terms; more terms can be found at the
\citet{NAICGlossary}. Here are a few reference cited in the chapter.

\chapter{Frequency Modeling}\label{C:Frequency-Modeling}

\emph{Chapter Preview.} A primary focus for insurers is estimating the
magnitude of aggregate claims it must bear under its insurance
contracts. Aggregate claims are affected by both the frequency of
insured events and the severity of the insured event. Decomposing
aggregate claims into these two components, each of which warrant
significant attention, is essential for analysis and pricing. This
chapter discusses frequency distributions, summary measures, and
parameter estimation techniques.

In Section \ref{S:frequency-distributions}, we present terminology and
discuss reasons why we study frequency and severity separately. The
foundations of frequency distributions and measures are presented in
Section \ref{S:basic-frequency-distributions} along with three principal
distributions: the binomial, the Poisson, and the negative binomial.
These three distributions are members of what is known as the (a,b,0)
class of distributions, a distinguishing, identifying feature which
allows for efficient calculation of probabilities, further discussed in
Section \ref{S:the-a-b-0-class}. When fitting a dataset with a
distribution, parameter values need to be estimated and in Section
\ref{S:estimating-frequency-distributions}, the procedure for maximum
likelihood estimation is explained. For insurance datasets, the
observation at 0, denoting the occurrence of zero of a particular event,
is notable and may deserve additional attention. By nature of the
dataset, and explained further in Section
\ref{S:other-frequency-distributions}, it may be impossible to have zero
of the studied event, or the probability at zero could be of such a
magnitude that direct fitting would lead to improper estimates. Zero
truncation or modification techniques allow for more appropriate
distribution fit. Noting that our insurance portfolio could consist of
different sub-groups, each with its own set of individual
characteristics, Section \ref{S:mixture-distributions} introduces
mixture distributions and methodology to allow for this heterogeneity
within a portfolio. Section \ref{S:goodness-of-fit} describes Goodness
of Fit which measures the reasonableness of the parameter estimates.
Exercises are presented in Section \ref{S:exercises} and Section
\ref{S:rcode} concludes the chapter with \texttt{R} Code for plots
depicted in Section \ref{S:estimating-frequency-distributions}.

\section{Frequency Distributions}\label{S:frequency-distributions}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to summarize the importance of frequency
modeling in terms of

\begin{itemize}
\tightlist
\item
  contractual,
\item
  behavioral,
\item
  database and
\item
  regulatory/administrative motivations.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{How Frequency Augments Severity
Information}\label{S:how-frequency-augments-severity-information}

\subsubsection{Basic Terminology}\label{S:basic-terminology}

In this chapter, \textbf{loss}, also referred to as ground-up loss,
denotes the amount suffered by the insured. We use \textbf{claim} to
denote the indemnification upon the occurrence of an insured event, thus
the amount paid by the insurer. While some texts use \textbf{loss} and
\textbf{claim} interchangeably, we wish to make a distinction here to
recognize how insurance contractual provisions, such as deductibles and
limits, affect the size of the claim stemming from a loss. Frequency
represents how often an insured event occurs, typically within a policy
contract. Here, we focus on count random variables that represent the
number of claims, that is, how frequently an event occurs. Severity
denotes the amount, or size, of each payment for an insured event. In
future chapters, the aggregate model, which combines frequency models
with severity models, is examined.

\subsubsection{The Importance of
Frequency}\label{S:the-importance-of-frequency}

Recall from Chapter \ref{C:Intro} that setting the price of an insurance
good can be a complex problem. In manufacturing, the cost of a good is
(relatively) known. In other financial service areas, market prices are
available. In insurance, we can generalize the price setting as follows:
start with an expected cost. Add ``margins'' to account for the
product's riskiness, expenses incurred in servicing the product, and a
profit/surplus allowance for the insurer.

That expected cost for insurance can be defined as the expected number
of claims times the expected amount per claim, that is, expected
\emph{frequency times severity}. The focus on claim count allows the
insurer to consider those factors which directly affect the occurrence
of a loss, thereby potentially generating a claim. The frequency process
can then be modeled.

\subsubsection{Why Examine Frequency
Information}\label{S:why-examine-frequency-information}

Insurers and other stakeholders, including governmental organizations,
have various motivations for gathering and maintaining frequency
datasets.

\begin{itemize}
\item
  \textbf{Contractual.} In insurance contracts, it is common for
  particular deductibles and policy limits to be listed and invoked for
  each occurrence of an insured event. Correspondingly, the claim count
  data generated would indicate the number of claims which meet these
  criteria, offering a unique claim frequency measure. Extending this,
  models of total insured losses would need to account for deductibles
  and policy limits for each insured event.
\item
  \textbf{Behaviorial.} In considering factors that influence loss
  frequency, the risk-taking and risk-reducing behavior of individuals
  and companies should be considered. Explanatory (rating) variables can
  have different effects on models of how often an event occurs in
  contrast to the size of the event.

  \begin{itemize}
  \item
    In healthcare, the decision to utilize healthcare by individuals,
    and minimize such healthcare utilization through preventive care and
    wellness measures, is related primarily to his or her personal
    characteristics. The cost per user is determined by those personal,
    the medical condition, potential treatment measures, and decisions
    made by the healthcare provider (such as the physician) and the
    patient. While there is overlap in those factors and how they affect
    total healthcare costs, attention can be focused on those separate
    drivers of healthcare visit frequency and healthcare cost severity.
  \item
    In personal lines, prior claims history is an important underwriting
    factor. It is common to use an indicator of whether or not the
    insured had a claim within a certain time period prior to the
    contract. Also, the number of claims incurred by the insured in
    previous periods has predictive power.
  \item
    In homeowners insurance, in modeling potential loss frequency, the
    insurer could consider loss prevention measures that the homeowner
    has adopted, such as visible security systems. Separately, when
    modeling loss severity, the insurer would examine those factors that
    affect repair and replacement costs.
  \end{itemize}
\item
  \textbf{Databases}. Insurers may hold separate data files that suggest
  developing separate frequency and severity models. For example, a
  policyholder file is established when a policy is written. This file
  records much underwriting information about the insured(s), such as
  age, gender, and prior claims experience, policy information such as
  coverage, deductibles and limitations, as well as the insurance claims
  event. A separate file, known as the ``claims'' file, records details
  of the claim against the insurer, including the amount. (There may
  also be a ``payments'' file that records the timing of the payments
  although we shall not deal with that here.) This recording process
  could then extend to insurers modeling the frequency and severity as
  separate processes.
\item
  \textbf{Regulatory and Administrative.} Insurance is a highly
  regulated and monitored industry, given its importance in providing
  financial security to individuals and companies facing risk. As part
  of its duties, regulators routinely require the reporting of claims
  numbers as well as amounts. This may be due to the fact that there can
  be alternative definitions of an ``amount,'' e.g., paid versus
  incurred, and there is less potential error when reporting claim
  numbers. This continual monitoring helps ensure financial stability of
  these insurance companies.
\end{itemize}

\section{Basic Frequency
Distributions}\label{S:basic-frequency-distributions}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Determine quantities that summarize a distribution such as the
  distribution and survival function, as well as moments such as the
  mean and variance
\item
  Define and compute the moment and probability generating functions
\item
  Describe and understand relationships among three important frequency
  distributions, the binomial, Poisson, and negative binomial
  distributions
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, we will introduce the distributions that are commonly
used in actuarial practice to model count data. The claim count random
variable is denoted by \(N\); by its very nature it assumes only
non-negative integer values. Hence the distributions below are all
discrete distributions supported on the set of non-negative integers
\(\{0, 1, \ldots \}\).

\subsection{Foundations}\label{S:foundations}

Since \(N\) is a discrete random variable taking values in
\(\{0, 1, \ldots \}\), the most natural full description of its
distribution is through the specification of the probabilities with
which it assumes each of the non-negative integer values. This leads us
to the concept of the probability mass function (pmf) of \(N\), denoted
as \(p_N(\cdot)\) and defined as follows:

\begin{equation}
p_N(k)=\Pr(N=k), \quad \hbox{for } k=0,1,\ldots
\end{equation}

We note that there are alternate complete descriptions, or
characterizations, of the distribution of \(N\); for example, the
distribution function of \(N\) denoted by \(F_N(\cdot)\) and defined
below is another such:

\begin{equation}
F_N(x):=\begin{cases}
\sum\limits_{k=0}^{\lfloor x \rfloor } \Pr(N=k), &x\geq 0;\\
0, & \hbox{otherwise}.
\end{cases}
\end{equation}

In the above, \(\lfloor \cdot \rfloor\) denotes the floor function;
\(\lfloor x \rfloor\) denotes the greatest integer less than or equal to
\(x\). We note that the survival function of \(N\), denoted by
\(S_N(\cdot)\), is defined as the ones'-complement of \(F_N(\cdot)\),
\emph{i.e.} \(S_N(\cdot):=1-F_N(\cdot)\). Clearly, the latter is another
characterization of the distribution of \(N\).

Often one is interested in quantifying a certain aspect of the
distribution and not in its complete description. This is particularly
useful when comparing distributions. A \emph{center of location} of the
distribution is one such aspect, and there are many different measures
that are commonly used to quantify it. Of these, the mean is the most
popular; the mean of \(N\), denoted by \(\mu_N\),\footnote{For
  convenience, we have indexed \(\mu_N\) with the random variable \(N\)
  instead of \(F_N\) or \(p_N\), even though it is solely a function of
  the distribution of the random variable.} is defined as

\begin{equation}
\mu_N=\sum_{k=0}^\infty k~p_N(k).
\end{equation}

We note that \(\mu_N\) is the expected value of the random variable
\(N\), \emph{i.e.} \(\mu_N=\mathrm{E}[N]\). This leads to a general
class of measures, the moments of the distribution; the \(r\)-th moment
of \(N\), for \(r> 0\), is defined as \(\mathrm{E}{[N^r]}\) and denoted
by \(\mu_N'(r)\). Hence, for \(r>0\), we have\\

\begin{equation}
\mu_N'(r)= \mathrm{E}{[N^r]}= \sum_{k=0}^\infty k^r~p_N(k).
\end{equation}

We note that \(\mu_N'(\cdot)\) is a well-defined non-decreasing function
taking values in \([0,\infty]\), as \(\Pr(N\in\{0, 1, \ldots \})=1\);
also, note that \(\mu_N=\mu_N'(1)\). In the following, when we refer to
a moment it will be implicit that it is finite unless mentioned
otherwise.

Another basic aspect of a distribution is its dispersion, and of the
various measures of dispersion studied in the literature, the standard
deviation is the most popular. Towards defining it, we first define the
variance of \(N\), denoted by \(\mathrm{Var}[N]\), as
\(\mathrm{Var}[N]:=\mathrm{E}{[(N-\mu_N)^2]}\) when \(\mu_N\) is finite.
By basic properties of the expected value of a random variable, we see
that \(\mathrm{Var}[N]:=\mathrm{E}[N^2]-[\mathrm{E}(N)]^2\). The
standard deviation of \(N\), denoted by \(\sigma_N\), is defined as the
square-root of \(\mathrm{Var}~N\). Note that the latter is well-defined
as \(\mathrm{Var}[N]\), by its definition as the average squared
deviation from the mean, is non-negative; \(\mathrm{Var}[N]\) is denoted
by \(\sigma_N^2\). Note that these two measures take values in
\([0,\infty]\).

\subsection{Moment and Probability Generating
Functions}\label{S:generating-functions}

Now we will introduce two generating functions that are found to be
useful when working with count variables. Recall that for a discrete
random variable, the moment generating function (mgf) of \(N\), denoted
as \(M_N(\cdot)\), is defined as \[
M_N(t) = \mathrm{E}~{[e^{tN}]} = \sum^{\infty}_{k=0} ~e^{tk}~ p_N(k), \quad t\in \mathbb{R}.
\] We note that while \(M_N(\cdot)\) is well defined as it is the
expectation of a non-negative random variable (\(e^{tN}\)), though it
can assume the value \(\infty\). Note that for a count random variable,
\(M_N(\cdot)\) is finite valued on \((-\infty,0]\) with \(M_N(0)=1\).
The following theorem, whose proof can be found in \citep{billingsley}
(pages 285-6), encapsulates the reason for its name.

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:freq-thm1}{}{\label{thm:freq-thm1} }Let \(N\) be a
count random variable such that \(\mathrm{E}~{[e^{t^\ast N}]}\) is
finite for some \(t^\ast>0\). We have the following:

All moments of \(N\) are finite, \emph{i.e.}

\[
\mathrm{E}{[N^r]}<\infty, \quad r > 0.
\]

The \emph{mgf} can be used to \emph{generate} its moments as follows: \[
\left.\frac{{\rm d}^m}{{\rm d}t^m} M_N(t)\right\vert_{t=0}=\mathrm{E}{N^m}, \quad m\geq 1.
\] The \emph{mgf} \(M_N(\cdot)\) characterizes the distribution; in
other words it uniquely specifies the distribution.
\EndKnitrBlock{theorem}

Another reason that the \emph{mgf} is very useful as a tool is that for
two independent random variables \(X\) and \(Y\), with their mgfs
existing in a neighborhood of \(0\), the \emph{mgf} of \(X+Y\) is the
product of their respective mgfs.

A related generating function to the \emph{mgf} is called the
probability generating function (pgf), and is a useful tool for random
variables taking values in the non-negative integers. For a random
variable \(N\), by \(P_N(\cdot)\) we denote its \emph{pgf} and we define
it as follows\footnote{\(0^0 = 1\)}:

\begin{equation}
P_N(s):=\mathrm{E}~{[s^N]}, \quad s\geq 0.
\end{equation}

It is straightforward to see that if the \emph{mgf} \(M_N(\cdot)\)
exists on \((-\infty,t^\ast)\) then \[
P_N(s)=M_N(\log(s)), \quad s<e^{t^\ast}.
\] Moreover, if the \emph{pgf} exists on an interval \([0,s^\ast)\) with
\(s^\ast>1\), then the \emph{mgf} \(M_N(\cdot)\) exists on
\((-\infty,\log(s^\ast))\), and hence uniquely specifies the
distribution of \(N\) by Theorem \ref{thm:freq-thm1}. The following
result for \emph{pgf} is an analog of Theorem \ref{thm:freq-thm1}, and
in particular justifies its name.

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:pgfthm}{}{\label{thm:pgfthm} }Let \(N\) be a count
random variable such that \(\mathrm{E}~{(s^{\ast})^N}\) is finite for
some \(s^\ast>1\). We have the following:

All moments of \(N\) are finite, \emph{i.e.} \[
\mathrm{E}~{N^r}<\infty, \quad r\geq 0.
\] The \texttt{r\ Gloss(\textquotesingle{}pmf\textquotesingle{})} of
\(N\) can be derived from the \emph{pgf} as follows: \[
p_N(m)=\begin{cases} 
P_N(0), &m=0;\cr
&\cr
\left(\frac{1}{m!}\right) \left.\frac{{\rm d}^m}{{\rm d}s^m} P_N(s)\right\vert_{s=0}\;, &m\geq 1.\cr
\end{cases}
\] The factorial moments of \(N\) can be derived as follows: \[
\left.\frac{{\rm d}^m}{{\rm d}s^m} P_N(s)\right\vert_{s=1}=\mathrm{E}~{\prod\limits_{i=0}^{m-1} (N-i)}, \quad m\geq 1.
\] The \emph{pgf} \(P_N(\cdot)\) characterizes the distribution; in
other words it uniquely specifies the distribution.
\EndKnitrBlock{theorem}

\subsection{Important Frequency
Distributions}\label{S:important-frequency-distributions}

In this sub-section we will study three important frequency
distributions used in statistics, namely the binomial, the Poisson, and
the negative binomial distributions. In the following, a risk denotes a
unit covered by insurance. A risk could be an individual, a building, a
company, or some other identifier for which insurance coverage is
provided. For context, imagine an insurance data set containing the
number of claims by risk or stratified in some other manner. The above
mentioned distributions also happen to be the most commonly used in
insurance practice for reasons, some of which we mention below.

\begin{itemize}
\item
  These distributions can be motivated by natural random experiments
  which are good approximations to real life processes from which many
  insurance data arise. Hence, not surprisingly, they together offer a
  reasonable fit to many insurance data sets of interest. The
  appropriateness of a particular distribution for the set of data can
  be determined using standard statistical methodologies, as we discuss
  later in this chapter.
\item
  They provide a rich enough basis for generating other distributions
  that even better approximate or well cater to more real situations of
  interest to us.

  \begin{itemize}
  \item
    The three distributions are either one-parameter or two-parameter
    distributions. In fitting to data, a parameter is assigned a
    particular value. The set of these distributions can be enlarged to
    their convex hulls by treating the parameter(s) as a random variable
    (or vector) with its own probability distribution, with this larger
    set of distributions offering greater flexibility. A simple example
    that is better addressed by such an enlargement is a portfolio of
    claims generated by insureds belonging to many different risk
    classes.
  \item
    In insurance data, we may observe either a marginal or inordinate
    number of zeros, that is, zero claims by risk. When fitting to the
    data, a frequency distribution in its standard specification often
    fails to reasonably account for this occurrence. The natural
    modification of the above three distributions, however, accommodate
    this phenomenon well towards offering a better fit.
  \item
    In insurance we are interested in total claims paid, whose
    distribution results from compounding the fitted frequency
    distribution with a severity distribution. These three distributions
    have properties that make it easy to work with the resulting
    aggregate severity distribution.
  \end{itemize}
\end{itemize}

\subsubsection{Binomial Distribution}\label{S:binomial-distribution}

We begin with the binomial distribution which arises from any finite
sequence of identical and independent experiments with binary outcomes.
The most canonical of such experiments is the (biased or unbiased) coin
tossing experiment with the outcome being heads or tails. So if \(N\)
denotes the number of heads in a sequence of \(m\) independent coin
tossing experiments with an identical coin which turns heads up with
probability \(q\), then the distribution of \(N\) is called the binomial
distribution with parameters \((m,q)\), with \(m\) a positive integer
and \(q\in[0,1]\). Note that when \(q=0\) (resp., \(q=1\)) then the
distribution is degenerate with \(N=0\) (resp., \(N=m\)) with
probability \(1\). Clearly, its support when \(q\in(0,1)\) equals
\(\{0,1,\ldots,m\}\) with \emph{pmf} given by \footnote{In the following
  we will suppress the reference to \(N\) and denote the \emph{pmf} by
  the sequence \(\{p_k\}_{k\geq 0}\), instead of the function
  \(p_N(\cdot)\).}

\begin{equation*}
p_k:= \binom{m}{k} q^k (1-q)^{m-k}, \quad k=0,\ldots,m.
\end{equation*}

where \[\binom{m}{k} = \frac{m!}{k!(m-k)!}\]

The reason for its name is that the \emph{pmf} takes values among the
terms that arise from the binomial expansion of \((q +(1-q))^m\). This
realization then leads to the the following expression for the
\emph{pgf} of the binomial distribution: \[
P_N(z):= \sum_{k=0}^m z^k \binom{m}{k} q^k (1-q)^{m-k} = \sum_{k=0}^m  \binom{m}{k} (zq)^k (1-q)^{m-k} = (qz+(1-q))^m = (1+q(z-1))^m.
\] Note that the above expression for the \emph{pgf} confirms the fact
that the binomial distribution is the m-convolution of the Bernoulli
distribution, which is the binomial distribution with \(m=1\) and
\emph{pgf} \((1+q(z-1))\). Also, note that the \emph{mgf} of the
binomial distribution is given by \((1+q(e^t-1))^m\).

The central moments of the binomial distribution can be found in a few
different ways. To emphasize the key property that it is a
\(m\)-convolution of the Bernoulli distribution, we derive below the
moments using this property. We begin by observing that the Bernoulli
distribution with parameter \(q\) assigns probability of \(q\) and
\(1-q\) to \(1\) and \(0\), respectively. So its mean equals \(q\)
(\(=0\times (1-q) + 1\times q\)); note that its raw second moment equals
its mean as \(N^2=N\) with probability \(1\). Using these two facts we
see that the variance equals \(q(1-q)\). Moving on to the binomial
distribution with parameters \(m\) and \(q\), using the fact that it is
the \(m\)-convolution of the Bernoulli distribution, we write \(N\) as
the sum of \(N_1,\ldots,N_m\), where \(N_i\) are iid Bernoulli variates.
Now using the moments of Bernoulli and linearity of the expectation, we
see that \[
\mathrm{E}[{N}]=\mathrm{E}[{\sum_{i=1}^m N_i}] = \sum_{i=1}^m ~\mathrm{E}[N_i] = mq.
\] Also, using the fact that the variance of the sum of independent
random variables is the sum of their variances, we see that\\
\[
\mathrm{Var}[{N}]=\mathrm{Var}~\left({\sum_{i=1}^m N_i}\right)=\sum_{i=1}^m \mathrm{Var}[{N_i}] = mq(1-q).
\] Alternate derivations of the above moments are suggested in the
exercises. One important observation, especially from the point of view
of applications, is that the mean is greater than the variance unless
\(q=0\).

\subsubsection{Poisson Distribution}\label{S:poisson-distribution}

After the binomial distribution, the Poisson distribution (named after
the French polymath Simeon Denis Poisson) is probably the most well
known of discrete distributions. This is partly due to the fact that it
arises naturally as the distribution of the count of the random
occurrences of a type of event in a certain time period, if the rate of
occurrences of such events is a constant. It also arises as the
asymptotic limit of the binomial distribution with
\(m\rightarrow \infty\) and \(mq\rightarrow \lambda\).

The Poisson distribution is parametrized by a single parameter usually
denoted by \(\lambda\) which takes values in \((0,\infty)\). Its
\emph{pmf} is given by \[
p_k = \frac{e^{-\lambda}\lambda^k}{k!}, k=0,1,\ldots
\] It is easy to check that the above specifies a \emph{pmf} as the
terms are clearly non-negative, and that they sum to one follows from
the infinite Taylor series expansion of \(e^\lambda\). More generally,
we can derive its \emph{pgf}, \(P(\cdot)\), as follows: \[
P_N(z)= \sum_{k=0}^\infty p_k z^k = \sum_{k=0}^\infty  \frac{e^{-\lambda}\lambda^kz^k}{k!} = e^{-\lambda} e^{\lambda z}
= e^{\lambda(z-1)}, \forall z\in\mathbb{R}.
\] From the above, we derive its \emph{mgf} as follows: \[
M_N(t)=P_N(e^t)=e^{\lambda(e^t-1)}, t\in \mathbb{R}.
\] Towards deriving its mean, we note that for the Poisson distribution
\[
kp_k=\begin{cases}
0,  &k=0;\cr
\lambda~p_{k-1}, &k\geq1.
\end{cases}
\] this can be checked easily. In particular, this implies that \[
\mathrm{E}[{N}]=\sum_{k\geq 0} k~p_k =\lambda\sum_{k\geq 1} p_{k-1} = \lambda\sum_{j\geq 0} p_{j} =\lambda.
\] In fact, more generally, using either a generalization of the above
or using Theorem \ref{thm:pgfthm}, we see that \[
\mathrm{E}{\prod\limits_{i=0}^{m-1} (N-i)}=\left.\frac{{\rm d}^m}{{\rm d}s^m} P_N(s)\right\vert_{s=1}=\lambda^m, \quad m\geq 1.
\] This, in particular, implies that \[
\mathrm{Var}[{N}]=\mathrm{E}[{N^2}]-[\mathrm{E}({N})]^2 = \mathrm{E}~[N(N-1)]+\mathrm{E}[N]-(\mathrm{E}[{N]})^2=\lambda^2+\lambda-\lambda^2=\lambda.
\] Note that interestingly for the Poisson distribution
\(\mathrm{Var}[N]=\mathrm{E}[N]\).

\subsubsection{Negative Binomial
Distribution}\label{S:negative-binomial-distribution}

The third important count distribution is the negative binomial
distribution. Recall that the binomial distribution arose as the
distribution of the number of \emph{successes} in \(m\) independent
repetition of an experiment with binary outcomes. If we instead consider
the number of \emph{successes} until we observe the \(r\)-th
\emph{failure} in independent repetitions of an experiment with binary
outcomes, then its distribution is a negative binomial distribution. A
particular case, when \(r=1\), is the geometric distribution. However
when \(r\) in not an integer, the above random experiment would not be
applicable. In the following, we will allow the parameter \(r\) to be
any positive real number to then motivate the distribution more
generally. To explain its name, we recall the binomial series,
\emph{i.e.} \[
(1+x)^s= 1 + s x + \frac{s(s-1)}{2!}x^2 + \ldots..., \quad s\in\mathbb{R}; \vert x \vert<1.
\] If we define \(\binom{s}{k}\), the generalized binomial coefficient,
by \[
\binom{s}{k}=\frac{s(s-1)\cdots(s-k+1)}{k!},
\] then we have \[
(1+x)^s= \sum_{k=0}^{\infty} \binom{s}{k} x^k, \quad s\in\mathbb{R}; \vert x \vert<1.
\] If we let \(s=-r\), then we see that the above yields \[
(1-x)^{-r}= 1 + r x + \frac{(r+1)r}{2!}x^2 + \ldots...= \sum_{k=0}^\infty \binom{r+k-1}{k} x^k, \quad r\in\mathbb{R}; \vert x \vert<1.
\] This implies that if we define \(p_k\) as \[
p_k = \binom{k+r-1}{k} \left(\frac{1}{1+\beta}\right)^r \left(\frac{\beta}{1+\beta}\right)^k, \quad k=0,1,\ldots
\] for \(r>0\) and \(\beta\geq0\), then it defines a valid \emph{pmf}.
Such defined distribution is called the negative binomial distribution
with parameters \((r,\beta)\) with \(r>0\) and \(\beta\geq 0\).
Moreover, the binomial series also implies that the \emph{pgf} of this
distribution is given by \[
\begin{aligned}
  P_N(z) &= (1-\beta(z-1))^{-r}, \quad \vert z \vert < 1+\frac{1}{\beta}, \beta\geq0. 
\end{aligned}
\] The above implies that the \emph{mgf} is given by \[
\begin{aligned}
  M_N(t) &= (1-\beta(e^t-1))^{-r}, \quad t < \log\left(1+\frac{1}{\beta}\right), \beta\geq0. 
\end{aligned}
\] We derive its moments using Theorem \ref{thm:freq-thm1} as follows:

\begin{eqnarray*}
\mathrm{E}[N]&=&M'(0)= \left. r\beta e^t (1-\beta(e^t-1))^{-r-1}\right\vert_{t=0}=r\beta;\\
\mathrm{E}[N^2]&=&M''(0)= \left.\left[ r\beta e^t (1-\beta(e^t-1))^{-r-1} + r(r+1)\beta^2 e^{2t} (1-\beta(e^t-1))^{-r-2}\right]\right\vert_{t=0}\\
&=&r\beta(1+\beta)+r^2\beta^2;\\
\hbox{and }\mathrm{Var}[N]&=&\mathrm{E}{[N^2]}-(\mathrm{E}[{N}])^2=r\beta(1+\beta)+r^2\beta^2-r^2\beta^2=r\beta(1+\beta)
\end{eqnarray*}

We note that when \(\beta>0\), we have
\(\mathrm{Var}[N] >\mathrm{E}[N]\). In other words, this distribution is
overdispersed (relative to the Poisson); similarly, when \(q>0\) the
binomial distribution is said to be underdispersed (relative to the
Poisson).

Finally, we observe that the Poisson distribution also emerges as a
limit of negative binomial distributions. Towards establishing this, let
\(\beta_r\) be such that as \(r\) approaches infinity \(r\beta_r\)
approaches \(\lambda>0\). Then we see that the mgfs of negative binomial
distributions with parameters \((r,\beta_r)\) satisfies \[
\lim_{r\rightarrow 0} (1-\beta_r(e^t-1))^{-r}=\exp\{\lambda(e^t-1)\},
\] with the right hand side of the above equation being the \emph{mgf}
of the Poisson distribution with parameter \(\lambda\).\footnote{For the
  theoretical basis underlying the above argument, see
  \citep{billingsley}.}

\section{The (a, b, 0) Class}\label{S:the-a-b-0-class}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Define the (a,b,0) class of frequency distributions
\item
  Discuss the importance of the recursive relationship underpinning this
  class of distributions
\item
  Identify conditions under which this general class reduces to each of
  the binomial, Poisson, and negative binomial distributions
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In the previous section we studied three distributions, namely the
binomial, the Poisson and the negative binomial distributions. In the
case of the Poisson, to derive its mean we used the the fact that \[
kp_k=\lambda p_{k-1}, \quad k\geq 1,
\] which can be expressed equivalently as \[
\frac{p_k}{p_{k-1}}=\frac{\lambda}{k}, \quad k\geq 1. 
\] Interestingly, we can similarly show that for the binomial
distribution \[
\frac{p_k}{p_{k-1}}=\frac{-q}{1-q}+\left(\frac{(m+1)q}{1-q}\right)\frac{1}{k}, \quad k=1,\ldots,m, 
\] and that for the negative binomial distribution \[
\frac{p_k}{p_{k-1}}=\frac{\beta}{1+\beta}+\left(\frac{(r-1)\beta}{1+\beta}\right)\frac{1}{k}, \quad k\geq 1. 
\] The above relationships are all of the form

\begin{equation}
\frac{p_k}{p_{k-1}}=a+\frac{b}{k}, \quad k\geq 1; 
\label{eq:ab0}
\end{equation}

this raises the question if there are any other distributions which
satisfy this seemingly general recurrence relation. Note that the ratio
on the left, the ratio of two probabilities, is non-negative.

To begin with, let \(a<0\). In this case as \(k\rightarrow \infty\),
\((a+b/k)\rightarrow a<0\). It follows that if \(a<0\) then \(b\) should
satisfy \(b=-ka\), for some \(k\geq 1\). Any such pair \((a,b)\) can be
written as \[
\left(\frac{-q}{1-q},\frac{(m+1)q}{1-q}\right), \quad q\in(0,1), m\geq 1;
\] note that the case \(a<0\) with \(a+b=0\) yields the degenerate at
\(0\) distribution which is the binomial distribution with \(q=0\) and
arbitrary \(m\geq 1\).

In the case of \(a=0\), again by non-negativity of the ratio
\(p_k/p_{k-1}\), we have \(b\geq 0\). If \(b=0\) the distribution is
degenerate at \(0\), which is a binomial with \(q=0\) or a Poisson
distribution with \(\lambda=0\) or a negative binomial distribution with
\(\beta=0\). If \(b>0\), then clearly such a distribution is a Poisson
distribution with mean (\emph{i.e.} \(\lambda\)) equal to \(b\), as
presented at the beginning of this section.

In the case of \(a>0\), again by non-negativity of the ratio
\(p_k/p_{k-1}\), we have \(a+b/k\geq 0\) for all \(k\geq 1\). The most
stringent of these is the inequality \(a+b\geq 0\). Note that \(a+b=0\)
again results in degeneracy at \(0\); excluding this case we have
\(a+b>0\) or equivalently \(b=(r-1)a\) with \(r>0\). Some algebra easily
yields the following expression for \(p_k\): \[
p_k = \binom{k+r-1}{k} p_0 a^k, \quad k=1,2,\ldots. 
\] The above series converges for \(a<1\) when \(r>0\), with the sum
given by \(p_0*((1-a)^{(-r)}-1)\). Hence, equating the latter to
\(1-p_0\) we get \(p_0=(1-a)^{(r)}\). So in this case the pair \((a,b)\)
is of the form \((a,(r-1)a)\), for \(r>0\) and \(0<a<1\); since an
equivalent parametrization is
\((\beta/(1+\beta),(r-1)\beta/(1+\beta))\), for \(r>0\) and \(\beta>0\),
we see from above that such distributions are negative binomial
distributions.

From the above development we see that not only does the recurrence
\eqref{eq:ab0} tie these three distributions together, but also it
characterizes them. For this reason these three distributions are
collectively referred to in the actuarial literature as (a,b,0) class of
distributions, with \(0\) referring to the starting point of the
recurrence. Note that the value of \(p_0\) is implied by \((a,b)\) since
the probabilities have to sum to one. Of course, \eqref{eq:ab0} as a
recurrence relation for \(p_k\) makes the computation of the \emph{pmf}
efficient by removing redundancies. Later, we will see that it does so
even in the case of compound distributions with the frequency
distribution belonging to the \((a,b,0)\) class - this fact is the more
important motivating reason to study these three distributions from this
viewpoint.

\textbf{Example 2.3.1.} A discrete probability distribution has the
following properties \[
\begin{aligned}
p_k&=c\left( 1+\frac{2}{k}\right) p_{k-1} \:\:\: k=1,2,3,\ldots\\
p_1&= \frac{9}{256}
\end{aligned}
\] Determine the expected value of this discrete random variable.

\textbf{Solution:} Since the \emph{pmf} satisfies the \((a,b,0)\)
recurrence relation we know that the underlying distribution is one
among the binomial, Poisson, and negative binomial distributions. Since
the ratio of the parameters (\emph{i.e.} \(b/a\)) equals \(2\), we know
that it is negative binomial and that \(r=3\). Moreover, since for a
negative binomial \(p_1=r(1+\beta)^{-(r+1)}\beta\), we have \[
\begin{aligned}
&&\frac{9}{256}=&3\frac{\beta}{(1+\beta)^4}\\
\implies &&\frac{3}{(1+3)^4}=&\frac{\beta}{(1+\beta)^4}\\
\implies &&\beta=&3.
\end{aligned}
\] Finally, since the mean of a negative binomial is \(r\beta\) we have
the mean of the given distribution equals \(9\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{Estimating Frequency
Distributions}\label{S:estimating-frequency-distributions}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Define a likelihood for a sample of observations from a discrete
  distribution
\item
  Define the maximum likelihood estimator for a random sample of
  observations from a discrete distribution
\item
  Calculate the maximum likelihood estimator for the binomial, Poisson,
  and negative binomial distributions
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Parameter Estimation}\label{S:parameter-estimation}

In Section \ref{S:basic-frequency-distributions} we introduced three
distributions of importance in modeling various types of count data
arising from insurance. Let us now suppose that we have a set of count
data to which we wish to fit a distribution, and that we have determined
that one of these \((a,b,0)\) distributions is more appropriate than the
others. Since each one of these forms a class of distributions if we
allow its parameter(s) to take any permissible value, there remains the
task of determining the \textbf{best} value of the parameter(s) for the
data at hand. This is a statistical point estimation problem, and in
parametric inference problems the statistical inference paradigm of
\emph{maximum likelihood} usually yields efficient estimators. In this
section we will describe this paradigm and derive the maximum likelihood
estimators.

Let us suppose that we observe the independent and identically
distributed, \emph{iid}, random variables \(X_1,X_2,\ldots,X_n\) from a
distribution with \emph{pmf} \(p_\theta\), where \(\theta\) is a
parameter and an unknown value in the parameter space
\(\Theta\subseteq \mathbb{R}^d\). For example, in the case of the
Poisson distribution \[
p_\theta(x)=e^{-\theta}\frac{\theta^x}{x!}, \quad x=0,1,\ldots,
\] with \(\theta=(0,\infty)\). In the case of the binomial distribution
we have \[
p_\theta(x)=\binom{m}{x} q^x(1-q)^{m-x}, \quad x=0,1,\ldots,m,
\] with \(\theta:=(m,q)\in \{0,1,2,\ldots\}\times[0,1]\). Let us suppose
that the observations are \(x_1,\ldots,x_n\), observed values of the
random sample \(X_1,X_2,\ldots,X_n\) presented earlier. In this case,
the probability of observing this sample from \(p_\theta\) equals \[
\prod_{i=1}^n p_\theta(x_i).
\] The above, denoted by \(L(\theta)\), viewed as a function of
\(\theta\) is called the \emph{likelihood}. Note that we suppressed its
dependence on the data, to emphasize that we are viewing it as a
function of the parameter. For example, in the case of the Poisson
distribution we have \[
L(\lambda)=e^{-n\lambda} \lambda^{\sum_{i=1}^n x_i} \left(\prod_{i=1}^n x_i!\right)^{-1};
\] in the case of the binomial distribution we have \[
L(m,q)=\left(\prod_{i=1}^n \binom{m}{x_i}\right) q^{\sum_{i=1}^n x_i} (1-q)^{nm-\sum_{i=1}^n x_i} .
\] The maximum likelihood estimator (MLE) for \(\theta\) is any
maximizer of the likelihood; in a sense the \emph{MLE} chooses the set
of parameter values that best explains the observed observations.
Consider a sample of size \(3\) from a Bernoulli distribution (binomial
with \(m=1\)) with values \(0,1,0\). The likelihood in this case is
easily checked to equal \[
L(q)=q(1-q)^2,
\] and the plot of the likelihood is given in Figure \ref{fig:berlik}.
As shown in the plot, the maximum value of the likelihood equals
\(4/27\) and is attained at \(q=1/3\), and hence the maximum likelihood
estimate for \(q\) is \(1/3\) for the given sample. In this case one can
resort to algebra to show that \[
q(1-q)^2=\left(q-\frac{1}{3}\right)^2\left(q-\frac{4}{3}\right)+\frac{4}{27},
\] and conclude that the maximum equals \(4/27\), and is attained at
\(q=1/3\) (using the fact that the first term is non-positive in the
interval \([0,1]\)). But as is apparent, this way of deriving the
\emph{mle} using algebra does not generalize. In general, one resorts to
calculus to derive the \emph{mle} - note that for some likelihoods one
may have to resort to other optimization methods, especially when the
likelihood has many local extrema. It is customary to equivalently
maximize the logarithm of the likelihood\footnote{The set of maximizers
  of \(L(\cdot)\) are the same as the set of maximizers of any strictly
  increasing function of \(L(\cdot)\), and hence the same as those for
  \(l(\cdot)\).} \(L(\cdot)\), denoted by \(l(\cdot)\), and look at the
set of zeros of its first derivative\footnote{A slight benefit of
  working with \(l(\cdot)\) is that constant terms in \(L(\cdot)\) do
  not appear in \(l'(\cdot)\) whereas they do in \(L'(\cdot)\).}
\(l'(\cdot)\). In the case of the above likelihood,
\(l(q)=\log(q)+2\log(1-q)\), and \[
l'(q):=\frac{\rm d}{{\rm d}q}l(q)=\frac{1}{q}-\frac{2}{1-q}.
\] The unique zero of \(l'(\cdot)\) equals \(1/3\), and since
\(l''(\cdot)\) is negative, we have \(1/3\) is the unique maximizer of
the likelihood and hence its maximum likelihood estimate.

\begin{figure}

{\centering \includegraphics[width=0.45\linewidth]{Figures/figure2.1} 

}

\caption{Likelihood of a $(0,1,0)$ $3$-sample from Bernoulli}\label{fig:berlik}
\end{figure}

\subsection{Frequency Distributions
MLE}\label{S:frequency-distributions-mle}

In the following, we derive the maximum likelihood estimator,
\emph{MLE}, for the three members of the \((a,b,0)\) class. We begin by
summarizing the discussion above. In the setting of observing
\emph{iid}, independent and identically distributed, random variables
\(X_1,X_2,\ldots,X_n\) from a distribution with \emph{pmf} \(p_\theta\),
where \(\theta\) takes an unknown value in
\(\Theta\subseteq \mathbb{R}^d\), the likelihood \(L(\cdot)\), a
function on \(\Theta\) is defined as \[
L(\theta):=\prod_{i=1}^n p_\theta(x_i),
\] where \(x_1,\ldots,x_n\) are the observed values. The \emph{MLE} of
\(\theta\), denoted as \(\hat{\theta}_{\rm MLE}\), is a function which
maps the observations to an element of the set of maximizers of
\(L(\cdot)\), namely \[
\{\theta \vert L(\theta)=\max_{\eta\in\Theta}L(\eta)\}.
\] Note the above set is a function of the observations, even though
this dependence is not made explicit. In the case of the three
distributions that we will study, and quite generally, the above set is
a singleton with probability tending to one (with increasing sample
size). In other words, for many commonly used distributions and when the
sample size is large, the likelihood estimate is uniquely defined with
high probability.

In the following, we will assume that we have observed \(n\) \emph{iid}
random variables \(X_1,X_2,\ldots,X_n\) from the distribution under
consideration, even though the parametric value is unknown. Also,
\(x_1,x_2,\ldots,x_n\) will denote the observed values. We note that in
the case of count data, and data from discrete distributions in general,
the likelihood can alternately be represented as \[
L(\theta):=\prod_{k\geq 0} \left(p_\theta(k)\right)^{m_k},
\] where \[
m_k:= \left\vert \{i\vert x_i=k, 1\leq i \leq n\} \right\vert=\sum_{i= 1}^n I(x_i=k), \quad k\geq 0.
\] Note that this transformation retains all of the data, compiling it
in a streamlined manner. For large \(n\) it leads to compression of the
data in the sense of \emph{sufficiency}. Below, we present expressions
for the \emph{MLE} in terms of \(\{m_k\}_{k\geq 1}\) as well.

\textbf{MLE - Poisson Distribution:} In this case, as noted above, the
likelihood is given by \[
L(\lambda)=\left(\prod_{i=1}^n x_i!\right)^{-1}e^{-n\lambda}\lambda^{\sum_{i=1}^n x_i},
\] which implies that \[
l(\lambda)= -\sum_{i=1}^n \log(x_i!) -n\lambda +\log(\lambda) \cdot \sum_{i=1}^n x_i,
\] and \[
l'(\lambda)= -n +\frac{1}{\lambda}\sum_{i=1}^n x_i.
\] In evaluating \(l''(\lambda)\), when \(\sum_{i=1}^n x_i>0\),
\(l''< 0\). Consequently, the maximum is attained at the sample mean,
\(\overline{x}\), presented below. When \(\sum_{i=1}^n x_i=0\), the
likelihood is a decreasing function and hence the maximum is attained at
the least possible parameter value; this results in the maximum
likelihood estimate being zero. Hence, we have\\
\[
\overline{x} = \hat{\lambda}_{\rm MLE} = \frac{1}{n}\sum_{i=1}^n x_i. 
\] Note that the sample mean can be computed also as \[
\frac{1}{n} \sum_{k\geq 1} km_k.
\] It is noteworthy that in the case of the Poisson, the exact
distribution of \(\hat{\lambda}_{\rm MLE}\) is available in closed form
- it is a scaled Poisson - when the underlying distribution is a
Poisson. This is so as the sum of independent Poisson random variables
is a Poisson as well. Of course, for large sample size one can use the
ordinary Central Limit Theorem (CLT) to derive a normal approximation.
Note that the latter approximation holds even if the underlying
distribution is any distribution with a finite second moment.

\textbf{MLE - Binomial Distribution:} Unlike the case of the Poisson
distribution, the parameter space in the case of the binomial is
\(2\)-dimensional. Hence the optimization problem is a bit more
challenging. We begin by observing that the likelihood is given by \[
L(m,q)= \left(\prod_{i=1}^n \binom{m}{x_i}\right) q^{\sum_{i=1}^n x_i} (1-q)^{nm-\sum_{i=1}^n x_i}, 
\] and the log-likelihood by \[
l(m,q)= \sum_{i=1}^n \log\left(\binom{m}{x_i}\right) + \left({\sum_{i=1}^n x_i}\right)\log(q)+ \left({nm-\sum_{i=1}^n x_i}\right)\log(1-q). 
\] Note that since \(m\) takes only non-negative integer values, we
cannot use multivariate calculus to find the optimal values.
Nevertheless, we can use single variable calculus to show that

\begin{equation}
\hat{q}_{\rm MLE}\times \hat{m}_{\rm MLE}= \frac{1}{n}\sum_{i=1}^n X_i.  
\label{eq:binmle}
\end{equation}

Towards this we note that for a fixed value of \(m\), \[
\frac{\delta}{\delta q} l(m,q) = \left({\sum_{i=1}^n x_i}\right)\frac{1}{q}- \left({nm-\sum_{i=1}^n x_i}\right)\frac{1}{1-q},
\] and that \[
\frac{\delta^2}{\delta q^2} l(m,q) = -\left[\left({\sum_{i=1}^n x_i}\right)\frac{1}{q^2} + \left({nm-\sum_{i=1}^n x_i}\right)\frac{1}{(1-q)^2}\right]\leq 0.
\] The above implies that for any fixed value of \(m\), the maximizing
value of \(q\) satisfies \[
mq=\frac{1}{n}\sum_{i=1}^n X_i,
\] and hence we establish equation \eqref{eq:binmle}. The above reduces
the task to the search for \(\hat{m}_{\rm MLE}\), which is member of the
set of the maximizers of

\begin{equation}
L\left(m,\frac{1}{nm}\sum_{i=1}^n x_i\right).
\label{eq:binlikm}
\end{equation}

Note the likelihood would be zero for values of \(m\) smaller than
\(\max\limits_{1\leq i \leq n}x_i\), and hence \[
\hat{m}_{\rm MLE}\geq \max_{1\leq i \leq n}x_i.
\] Towards specifying an algorithm to compute \(\hat{m}_{\rm MLE}\), we
first point out that for some data sets \(\hat{m}_{\rm MLE}\) could
equal \(\infty\), indicating that a Poisson distribution would render a
better fit than any binomial distribution. This is so as the binomial
distribution with parameters \((m,\overline{x}/m)\) approaches the
Poisson distribution with parameter \(\overline{x}\) with \(m\)
approaching infinity. The fact that some data sets will \textbf{prefer}
a Poisson distribution should not be surprising since in the above sense
the set of Poisson distribution is on the boundary of the set of
binomial distributions.

Interestingly, in \citep{olkin1981} they show that if the sample mean is
less than or equal to the sample variance then
\(\hat{m}_{\rm MLE}=\infty\); otherwise, there exists a finite \(m\)
that maximizes equation \eqref{eq:binlikm}. In Figure \ref{fig:MLEm} below
we display the plot of \(L\left(m,\frac{1}{nm}\sum_{i=1}^n x_i\right)\)
for three different samples of size \(5\); they differ only in the value
of the sample maximum. The first sample of \((2,2,2,4,5)\) has the ratio
of sample mean to sample variance greater than \(1\) (\(1.875\)), the
second sample of \((2,2,2,4,6)\) has the ratio equal to \(1.25\) which
is closer to \(1\), and the third sample of \((2,2,2,4,7)\) has the
ratio less than \(1\) (\(0.885\)). For these three samples, as shown in
Figure \ref{fig:MLEm}, \(\hat{m}_{\rm MLE}\) equals \(7\), \(18\) and
\(\infty\), respectively. Note that the limiting value of
\(L\left(m,\frac{1}{nm}\sum_{i=1}^n x_i\right)\) as \(m\) approaches
infinity equals

\begin{equation}
\left(\prod_{i=1}^n x_i! \right)^{-1} \exp\left\{-\sum_{i=1}^n x_i\right\} \overline{x}^{n\overline{x}}. 
\label{eq:Poilik}
\end{equation}

Also, note that Figure \ref{fig:MLEm} shows that the \emph{MLE} of \(m\)
is non-robust, \emph{i.e.} changes in a small proportion of the data set
can cause large changes in the estimator.

The above discussion suggests the following simple algorithm:

\begin{itemize}
\tightlist
\item
  \emph{Step 1}. If the sample mean is less than or equal to the sample
  variance, \(\hat{m}_{MLE}=\infty\). The \emph{MLE} suggested
  distribution is a Poisson distribution with
  \(\hat{\lambda}=\overline{x}\).
\item
  \emph{Step 2}. If the sample mean is greater than the sample variance,
  then compute \(L(m,\overline{x}/m)\) for \(m\) values greater than or
  equal to the sample maximum until \(L(m,\overline{x}/m)\) is close to
  the value of the Poisson likelihood given in \eqref{eq:Poilik}. The
  value of \(m\) that corresponds to the maximum value of
  \(L(m,\overline{x}/m)\) among those computed equals \(\hat{m}_{MLE}\).
\end{itemize}

We note that if the underlying distribution is the binomial distribution
with parameters \((m,q)\) (with \(q>0\)) then \(\hat{m}_{MLE}\) will
equal \(m\) for large sample sizes. Also, \(\hat{q}_{MLE}\) will have an
asymptotically normal distribution and converge with probability one to
\(q\).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{Figures/figure2.2} 

}

\caption{Plot of $L(m,\overline{x}/m)$ for binomial distribution}\label{fig:MLEm}
\end{figure}

\textbf{MLE - Negative Binomial Distribution:} The case of the negative
binomial distribution is similar to that of the binomial distribution in
the sense that we have two parameters and the \emph{MLE}s are not
available in closed form. A difference between them is that unlike the
binomial parameter \(m\) which takes positive integer values, the
parameter \(r\) of the negative binomial can assume any positive real
value. This makes the optimization problem a tad more complex. We begin
by observing that the likelihood can be expressed in the following form:
\[
L(r,\beta)=\left(\prod_{i=1}^n \binom{r+x_i-1}{x_i}\right) (1+\beta)^{-n(r+\overline{x})} \beta^{n\overline{x}}.  
\] The above implies that log-likelihood is given by \[
l(r,\beta)=\sum_{i=1}^n \log\binom{r+x_i-1}{x_i} -n(r+\overline{x}) \log(1+\beta) +n\overline{x}\log\beta,
\] and hence \[
\frac{\delta}{\delta\beta} l(r,\beta) = -\frac{n(r+\overline{x})}{1+\beta} + \frac{n\overline{x}}{\beta}.
\] Equating the above to zero, we get \[
\hat{r}_{MLE}\times \hat{\beta}_{MLE} = \overline{x}.
\] The above reduces the two dimensional optimization problem to a
one-dimensional problem - we need to maximize \[
l(r,\overline{x}/r)=\sum_{i=1}^n \log\binom{r+x_i-1}{x_i} -n(r+\overline{x}) \log(1+\overline{x}/r) +n\overline{x}\log(\overline{x}/r),
\] with respect to \(r\), with the maximizing \(r\) being its \emph{MLE}
and \(\hat{\beta}_{MLE}=\overline{x}/\hat{r}_{MLE}\). In
\citep{levin1977} it is shown that if the sample variance is greater
than the sample mean then there exists a unique \(r>0\) that maximizes
\(l(r,\overline{x}/r)\) and hence a unique \emph{MLE} for \(r\) and
\(\beta\). Also, they show that if \(\hat{\sigma}^2\leq \overline{x}\),
then the negative binomial likelihood will be dominated by the Poisson
likelihood with \(\hat{\lambda}=\overline{x}\). In other words, a
Poisson distribution offers a better fit to the data. The guarantee in
the case of \(\hat{\sigma}^2>\hat{\mu}\) permits us to use some
algorithm to maximize \(l(r,\overline{x}/r)\). Towards an alternate
method of computing the likelihood, we note that \[
l(r,\overline{x}/r)=\sum_{i=1}^n \sum_{j=1}^{x_i}\log(r-1+j) - \sum_{i=1}^n\log(x_i!) - n(r+\overline{x}) \log(r+\overline{x}) + nr\log(r) + n\overline{x}\log(\overline{x}),
\] which yields \[
\left(\frac{1}{n}\right)\frac{\delta}{\delta r}l(r,\overline{x}/r)=\left(\frac{1}{n}\right)\sum_{i=1}^n \sum_{j=1}^{x_i}\frac{1}{r-1+j} - \log(r+\overline{x}) + \log(r).
\] We note that, in the above expressions for the terms involving a
double summation, the inner sum equals zero if \(x_i=0\). The
\emph{maximum likelihood estimate} for \(r\) is a root of the last
expression and we can use a root finding algorithm to compute it. Also,
we have \[
\left(\frac{1}{n}\right)\frac{\delta^2}{\delta r^2}l(r,\overline{x}/r)=\frac{\overline{x}}{r(r+\overline{x})}-\left(\frac{1}{n}\right)\sum_{i=1}^n \sum_{j=1}^{x_i}\frac{1}{(r-1+j)^2}.
\] A simple but quickly converging iterative root finding algorithm is
the Newton's method, which incidentally the Babylonians are believed to
have used for computing square roots. Under this method, an initial
approximation is selected for the root and new approximations for the
root are successively generated until convergence. Applying the Newton's
method to our problem results in the following algorithm:\\
 \emph{Step i}. Choose an approximate solution, say \(r_0\). Set \(k\)
to \(0\).\\
\emph{Step ii}. Define \(r_{k+1}\) as \[
r_{k+1}:= r_k - \frac{\left(\frac{1}{n}\right)\sum_{i=1}^n \sum_{j=1}^{x_i}\frac{1}{r_k-1+j} - \log(r_k+\overline{x}) + \log(r_k)}{\frac{\overline{x}}{r_k(r_k+\overline{x})}-\left(\frac{1}{n}\right)\sum_{i=1}^n \sum_{j=1}^{x_i}\frac{1}{(r_k-1+j)^2}}
\]\\
\emph{Step iii}. If \(r_{k+1}\sim r_k\), then report \(r_{k+1}\) as
\emph{maximum likelihood estimate}; else increment \(k\) by \(1\) and
repeat \emph{Step ii}.

For example, we simulated a \(5\) observation sample of
\(41, 49, 40, 27, 23\) from the negative binomial with parameters
\(r=10\) and \(\beta=5\). Choosing the starting value of \(r\) such that
\[
r\beta=\hat{\mu} \quad \hbox{and} \quad r\beta(1+\beta)=\hat{\sigma}^2
\] where \(\hat{\mu}\) represents the estimated mean and
\(hat{\sigma}^2\) is the estimated variance. This leads to the starting
value for \(r\) of \(23.14286\). The iterates of \(r\) from the Newton's
method are \[
21.39627, 21.60287, 21.60647, 21.60647;
\] the rapid convergence seen above is typical of the Newton's method.
Hence in this example, \(\hat{r}_{MLE}\sim21.60647\) and
\(\hat{\beta}_{MLE}=8.3308\).

\emph{R Implementation of Newton's Method - Negative Binomial MLE for
\(r\)}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

To summarize our discussion of MLE for the \((a,b,0)\) class of
distributions, in Figure \ref{fig:MLEab0} below we plot the maximum
value of the Poisson likelihood, \(L(m,\overline{x}/m)\) for the
binomial, and \(L(r,\overline{x}/r)\) for the negative binomial, for the
three samples of size \(5\) given in \protect\hyperlink{tab:2.1}{Table
2.1}. The data was constructed to cover the three orderings of the
sample mean and variance. As show in the Figure \ref{fig:MLEab0}, and
supported by theory, if \(\hat{\mu}<\hat{\sigma}^2\) then the negative
binomial will result in a higher maximum likelihood value; if
\(\hat{\mu}=\hat{\sigma}^2\) the Poisson will have the highest
likelihood value; and finally in the case that
\(\hat{\mu}>\hat{\sigma}^2\) the binomial will give a better fit than
the others. So before fitting a frequency data with an \((a,b,0,)\)
distribution, it is best to start with examining the ordering of
\(\hat{\mu}\) and \(\hat{\sigma}^2\). We again emphasize that the
Poisson is on the \textbf{boundary} of the negative binomial and
binomial distributions. So in the case that
\(\hat{\mu}\geq\hat{\sigma}^2\) (\(\hat{\mu}\leq\hat{\sigma}^2\), resp.)
the Poisson will yield a better fit than the negative binomial
(binomial, resp.), which will also be indicated by \(\hat{r}=\infty\)
(\(\hat{m}=\infty\), resp.).

\[\begin{matrix}
\begin{array}{c|c|c}
\hline
\text{Data} & \text{Mean }(\hat{\mu}) & \text{Variance }(\hat{\sigma}^2) \\
\hline
(2,3,6,8,9) & 5.60 & 7.44 \\ 
(2,5,6,8,9) & 6 & 6\\
(4,7,8,10,11) & 8 & 6\\\hline
\end{array}
\end{matrix}\]

\protect\hyperlink{tab:2.1}{Table 2.1} : Three Samples of Size \(5\)

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{Figures/figure2.3} 

}

\caption{Plot of $(a,b,0)$ Partially Maximized Likelihoods}\label{fig:MLEab0}
\end{figure}

\section{Other Frequency
Distributions}\label{S:other-frequency-distributions}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Define the (a,b,1) class of frequency distributions and discuss the
  importance of the recursive relationship underpinning this class of
  distributions
\item
  Interpret zero truncated and modifed versions of the binomial,
  Poisson, and negative binomial distributions
\item
  Compute probabilities using the recursive relationship
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In the previous sections, we discussed three distributions with supports
contained in the set of non-negative integers, which well cater to many
insurance applications. Moreover, typically by allowing the parameters
to be a function of known (to the insurer) explanatory variables such as
age, sex, geographic location (territory), and so forth, these
distributions allow us to explain claim probabilities in terms of these
variables. The field of statistical study that studies such models is
known as regression analysis - it is an important topic of actuarial
interest that we will not pursue in this book; see
\citep{freesregression}.

There are clearly infinitely many other count distributions, and more
importantly the above distributions by themselves do not cater to all
practical needs. In particular, one feature of some insurance data is
that the proportion of zero counts can be out of place with the
proportion of other counts to be explainable by the above distributions.
In the following we modify the above distributions to allow for
arbitrary probability for zero count irrespective of the assignment of
relative probabilities for the other counts. Another feature of a data
set which is naturally comprised of homogeneous subsets is that while
the above distributions may provide good fits to each subset, they may
fail to do so to the whole data set. Later we naturally extend the
\((a,b,0)\) distributions to be able to cater to, in particular, such
data sets.

\subsection{Zero Truncation or
Modification}\label{S:zero-truncation-or-modification}

Let us suppose that we are looking at auto insurance policies which
appear in a database of auto claims made in a certain period. If one is
to study the number of claims that these policies have made during this
period, then clearly the distribution has to assign a probability of
zero to the count variable assuming the value zero. In other words, by
restricting attention to count data from policies in the database of
claims, we have in a sense zero-truncated the count data of all
policies. In personal lines (like auto), policyholders may not want to
report that first claim because of fear that it may increase future
insurance rates - this behavior will inflate the proportion of zero
counts. Examples such as the latter modify the proportion of zero
counts. Interestingly, natural modifications of the three distributions
considered above are able to provide good fits to
zero-modified/truncated data sets arising in insurance.

As presented below, we modify the probability assigned to zero count by
the \((a,b,0)\) class while maintaining the relative probabilities
assigned to non-zero counts - zero modification. Note that since the
\((a,b,0)\) class of distributions satisfies the recurrence
\eqref{eq:ab0}, maintaining relative probabilities of non-zero counts
implies that recurrence \eqref{eq:ab0} is satisfied for \(k\geq 2\). This
leads to the definition of the following class of distributions.

\textbf{Definition}. A count distribution is a member of the
\((a, b, 1)\) class if for some constants \(a\) and \(b\) the
probabilities \(p_k\) satisfy

\begin{equation}
\frac{p_k}{p_{k-1}}=a+\frac{b}{k},\quad k\geq 2.
\label{eq:ab1}
\end{equation}

Note that since the recursion starts with \(p_1\), and not \(p_0\), we
refer to this super-class of \((a,b,0)\) distributions by (a,b,1). To
understand this class, recall that each valid pair of values for \(a\)
and \(b\) of the \((a,b,0)\) class corresponds to a unique vector of
probabilities \(\{p_k\}_{k\geq 0}\). If we now look at the probability
vector \(\{\tilde{p}_k\}_{k\geq 0}\) given by \[
\tilde{p}_k= \frac{1-\tilde{p}_0}{1-p_0}\cdot p_k, \quad k\geq 1,
\] where \(\tilde{p}_0\in[0,1)\) is arbitrarily chosen, then since the
relative probabilities for positive values according to
\(\{p_k\}_{k\geq 0}\) and \(\{\tilde{p}_k\}_{k\geq 0}\) are the same, we
have \(\{\tilde{p}_k\}_{k\geq 0}\) satisfies recurrence \eqref{eq:ab1}.
This, in particular, shows that the class of \((a,b,1)\) distributions
is strictly wider than that of \((a,b,0)\).

In the above, we started with a pair of values for \(a\) and \(b\) that
led to a valid \((a,b,0)\) distribution, and then looked at the
\((a,b,1)\) distributions that corresponded to this \((a,b,0)\)
distribution. We will now argue that the \((a,b,1)\) class allows for a
larger set of permissible distributions for \(a\) and \(b\) than the
\((a,b,0)\) class. Recall from Section \ref{S:the-a-b-0-class} that in
the case of \(a<0\) we did not use the fact that the recurrence
\eqref{eq:ab0} started at \(k=1\), and hence the set of pairs \((a,b)\)
with \(a<0\) that are permissible for the \((a,b,0)\) class is identical
to those that are permissible for the \((a,b,1)\) class. The same
conclusion is easily drawn for pairs with \(a=0\). In the case that
\(a>0\), instead of the constraint \(a+b>0\) for the \((a,b,0)\) class
we now have the weaker constraint of \(a+b/2>0\) for the \((a,b,1)\)
class. With the parametrization \(b=(r-1)a\) as used in Section
\ref{S:the-a-b-0-class}, instead of \(r>0\) we now have the weaker
constraint of \(r>-1\). In particular, we see that while zero modifying
a \((a,b,0)\) distribution leads to a distribution in the \((a,b,1)\)
class, the conclusion does not hold in the other direction.

Zero modification of a count distribution \(F\) such that it assigns
zero probability to zero count is called a zero truncation of \(F\).
Hence, the zero truncated version of probabilities \(\{p_k\}_{k\geq 0}\)
is given by \[
\tilde{p}_k=\begin{cases}
0, & k=0;\\
\frac{p_k}{1-p_0}, & k\geq 1.
\end{cases}
\]

In particular, we have that a zero modification of a count distribution
\(\{p_k^T\}_{k\geq 0}\), denoted by \(\{p^M_k\}_{k\geq 0}\), can be
written as a convex combination of the degenerate distribution at \(0\)
and the zero truncation of \(\{p_k\}_{k\geq 0}\), denoted by
\(\{p^T_k\}_{k\geq 0}\). That is we have \[
p^M_k= p^M_0 \cdot \delta_{0}(k) + (1-p^M_0) \cdot p^T_k, \quad k\geq 0.  
\]

\textbf{Example 2.5.1. Zero Truncated/Modified Poisson}. Consider a
Poisson distribution with parameter \(\lambda=2\). Calculate
\(p_k, k=0,1,2,3\), for the usual (unmodified), truncated and a modified
version with \((p_0^M=0.6)\).

\textbf{Solution.} For the Poisson distribution as a member of the
(\(a,b\),0) class, we have \(a=0\) and \(b=\lambda=2\). Thus, we may use
the recursion \(p_k = \lambda p_{k-1}/k= 2 p_{k-1}/k\) for each type,
after determining starting probabilities. The calculation of
probabilities for \(k\leq 3\) is shown in
\protect\hyperlink{tab:2.2}{Table 2.2}.

\[\begin{matrix}
\begin{array}{c|c|c|c}
\hline
k & p_k & p_k^T & p_k^M\\\hline
0 & p_0=e^{-\lambda}=0.135335 & 0 & 0.6\\\hline
1 & p_1=p_0(0+\frac{\lambda}{1})=0.27067 &
\frac{p_1}{1-p_0}=0.313035 &
\frac{1-p_0^M}{1-p_0}~p_1=0.125214\\\hline
2 & p_2=p_1\left( \frac{\lambda}{2}\right)=0.27067 &
p_2^T=p_1^T\left(\frac{\lambda}{2}\right)=0.313035 &
p_2^M=p_1^M\left(\frac{\lambda}{2}\right)=0.125214\\\hline
3 & p_3=p_2\left(\frac{\lambda}{3}\right)=0.180447 &
p_3^T=p_2^T\left(\frac{\lambda}{3}\right)=0.208690 &
p_3^M=p_2^M\left(\frac{\lambda}{3}\right)=0.083476\\\hline
\end{array}
\end{matrix}\]

\protect\hyperlink{tab:2.2}{Table 2.2} : Calculation of probabilities
for \(k\leq 3\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{Mixture Distributions}\label{S:mixture-distributions}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Define a mixture distribution when the mixing component is based on a
  finite number of sub-groups
\item
  Compute mixture distribution probabilities from mixing proportions and
  knowledge of the distribution of each subgroup
\item
  Define a mixture distribution when the mixing component is continuous
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In many applications, the underlying population consists of naturally
defined sub-groups with some homogeneity within each sub-group. In such
cases it is convenient to model the individual sub-groups, and in a
ground-up manner model the whole population. As we shall see below,
beyond the aesthetic appeal of the approach, it also extends the range
of applications that can be catered to by standard parametric
distributions.

Let \(k\) denote the number of defined sub-groups in a population, and
let \(F_i\) denote the distribution of an observation drawn from the
\(i\)-th subgroup. If we let \(\alpha_i\) denote the proportion of the
population in the \(i\)-th subgroup, with \(\sum_{i=1}^k \alpha_i=1\),
then the distribution of a randomly chosen observation from the
population, denoted by \(F\), is given by

\begin{equation}
F(x)=\sum_{i=1}^k \alpha_i \cdot F_i(x).
\label{eq:mixdefn}
\end{equation}

The above expression can be seen as a direct application of the Law of
Total Probability. As an example, consider a population of drivers split
broadly into two sub-groups, those with at most \(5\)-years of driving
experience and those with more than \(5\)-years experience. Let
\(\alpha\) denote the proportion of drivers with less than \(5\) years
experience, and \(F_{\leq 5}\) and \(F_{> 5}\) denote the distribution
of the count of claims in a year for a driver in each group,
respectively. Then the distribution of claim count of a randomly
selected driver is given by \[
\alpha\cdot F_{\leq 5}(x) + (1-\alpha)F_{> 5}(x).
\]

An alternate definition of a mixture distribution is as follows. Let
\(N_i\) be a random variable with distribution \(F_i\),
\(i=1,\ldots, k\). Let \(I\) be a random variable taking values
\(1,2,\ldots,k\) with probabilities \(\alpha_1,\ldots,\alpha_k\),
respectively. Then the random variable \(N_I\) has a distribution given
by equation \eqref{eq:mixdefn}\footnote{This in particular lays out a way
  to simulate from a mixture distribution that makes use of efficient
  simulation schemes that may exist for the component distributions.}.

In \eqref{eq:mixdefn} we see that the distribution function is a convex
combination of the component distribution functions. This result easily
extends to the density function, the survival function, the raw moments,
and the expectation as these are all linear mappings of the distribution
function. We note that this is not true for central moments like the
variance, and conditional measures like the hazard rate function. In the
case of variance it is easily seen as

\begin{equation}
\mathrm{Var}{[N_I]}=\mathrm{E}[{\mathrm{Var}[{N_I\vert I}]]} + \mathrm{Var}[{\mathrm{E}[{N_I|I}}]]=\sum_{i=1}^k \alpha_i \mathrm{Var}[{N_i}] + \mathrm{Var}[{\mathrm{E}[{N_I|I}}]] .
\label{eq:LawTotalVariation}
\end{equation}

Appendix \ref{C:AppB} provides additonal background about this important
expression.

\textbf{Example 2.6.1. Actuarial Exam Question}. In a certain town the
number of common colds an individual will get in a year follows a
Poisson distribution that depends on the individual's age and smoking
status. The distribution of the population and the mean number of colds
are as follows:

\[\begin{matrix}
\begin{array}{l|c|c}
\hline
 & \text{Proportion of population} &
\text{Mean number of colds}\\\hline
\text{Children} & 0.3 & 3\\
\text{Adult Non-Smokers} & 0.6 & 1\\
\text{Adult Smokers} & 0.1 & 4\\\hline
\end{array}
\end{matrix}\]

\protect\hyperlink{tab:2.3}{Table 2.3} : The distribution of the
population and the mean number of colds

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate the probability that a randomly drawn person has 3 common
  colds in a year.
\item
  Calculate the conditional probability that a person with exactly 3
  common colds in a year is an adult smoker.
\end{enumerate}

\textbf{Solution.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Using Law of Total Probability, we can write the required probability
  as \(\Pr(N_I=3)\), with \(I\) denoting the group of the randomly
  selected individual with \(1,2\) and \(3\) signifying the groups
  \emph{Children}, \emph{Adult Non-Smoker}, and \emph{Adult Smoker},
  respectively. Now by conditioning we get \[
  \Pr(N_I=3)=0.3\cdot\Pr(N_1=3)+0.6\cdot\Pr(N_2=3)+0.1\cdot\Pr(N_3=3),
  \] with \(N_1,N_2\) and \(N_3\) following Poisson distributions with
  means \(3,1\), and \(4\), respectively. Using the above, we get
  \(\Pr(N_I=3)\sim0.1235\)
\item
  The conditional probability of event A given event B,
  \(\Pr(A\vert B) = \frac{\Pr(A,B)}{\Pr(B)}\). The required conditional
  probability in this problem can then be written as
  \(\Pr(I=3\vert N_I=3)\), which equals \[
  \Pr(I=3\vert N_I=3)=\frac{\Pr(I=3,N_3=3)}{\Pr(N_I=3)}\sim\frac{0.1 \times 0.1954}{0.1235}\sim 0.1581.
  \]
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In the above example, the number of subgroups \(k\) was equal to three.
In general, \(k\) can be any natural number, but when \(k\) is large it
is parsimonious from a modeling point of view to take the following
\emph{infinitely many subgroup} approach. To motivate this approach, let
the \(i\)-th subgroup be such that its component distribution \(F_i\) is
given by \(G_{\tilde{\theta_i}}\), where \(G_\cdot\) is a parametric
family of distributions with parameter space
\(\Theta\subseteq \mathbb{R}^d\). With this assumption, the distribution
function \(F\) of a randomly drawn observation from the population is
given by \[
F(x)=\sum_{i=1}^k \alpha_i G_{\tilde{\theta_i}}(x),\quad \forall x\in\mathbb{R}.
\] which can be alternately written as\\
\[
F(x)=\mathrm{E}[{G_{\tilde{\vartheta}}(x)}],\quad \forall x\in\mathbb{R},
\] where \(\tilde{\vartheta}\) takes values \(\tilde{\theta_i}\) with
probability \(\alpha_i\), for \(i=1,\ldots,k\). The above makes it clear
that when \(k\) is large, one could model the above by treating
\(\tilde{\vartheta}\) as continuous random variable.

To illustrate this approach, suppose we have a population of drivers
with the distribution of claims for an individual driver being
distributed as a Poisson. Each person has their own (personal) expected
number of claims \(\lambda\) - smaller values for good drivers, and
larger values for others. There is a distribution of \(\lambda\) in the
population; a popular and convenient choice for modeling this
distribution is a gamma distribution with parameters
\((\alpha, \theta)\). With these specifications it turns out that the
resulting distribution of \(N\), the claims of a randomly chosen driver,
is a negative binomial with parameters \((r=\alpha,\beta=\theta)\). This
can be shown in many ways, but a straightforward argument is as follows:

\begin{align*}
\Pr(N=k)&= \int_0^\infty \frac{e^{-\lambda}\lambda^k}{k!} \frac{\lambda^{\alpha-1}e^{-\lambda/\theta}}{\Gamma{(\alpha)}\theta^{\alpha}} {\rm d}\lambda = 
\frac{1}{k!\Gamma(\alpha)\theta^\alpha}\int_0^\infty \lambda^{\alpha+k-1}e^{-\lambda(1+1/\theta)}{\rm d}\lambda=\frac{\Gamma{(\alpha+k)}}{k!\Gamma(\alpha)\theta^\alpha(1+1/\theta)^{\alpha+k}} \\
&=\binom{\alpha+k-1}{k}\left(\frac{1}{1+\theta}\right)^\alpha\left(\frac{\theta}{1+\theta}\right)^k, \quad k=0,1,\ldots
\end{align*}

Note that the above derivation implicitly uses the following: \[
f_{N\vert\Lambda=\lambda}(N=k)=\frac{e^{-\lambda}\lambda^k}{k!}, \quad k\geq 0; \quad \hbox{and} \quad f_{\Lambda}(\lambda)= \frac{\lambda^{\alpha-1}e^{-\lambda/\theta}}{\Gamma{(\alpha)}\theta^{\alpha}}, \quad \lambda>0.
\]

It is worth mentioning that by considering mixtures of a parametric
class of distributions we increase the richness of the class. This
expansion of distributions results in the mixture class being able to
cater well to more applications that the parametric class we started
with. Mixture modeling is a very important modeling technique in
insurance applications, and later chapters will cover more aspects of
this modeling technique.

\textbf{Example 2.6.2.} Suppose that \(N|\Lambda \sim\)
Poisson\((\Lambda)\) and that \(\Lambda \sim\) gamma with mean of 1 and
variance of 2. Determine the probability that \(N=1\).

\textbf{Solution.} For a gamma distribution with parameters
\((\alpha, \theta)\), we have that the mean is \(\alpha \theta\) and the
variance is \(\alpha \theta^2\). Using these expressions we have \[
\begin{aligned}
\alpha &= \frac{1}{2} \text{   and   } \theta =2.
\end{aligned}
\] Now, one can directly use the above result to conclude that \(N\) is
distributed as a negative binomial with \(r = \alpha = \frac{1}{2}\) and
\(\beta= \theta =2\). Thus \[
\begin{aligned}
\Pr(N=1)  &= \binom{1+r-1}{1}(\frac{1}{(1+\beta)^r})\left(\frac{\beta}{1+\beta}\right)^1 \\
&=                 \binom{1+\frac{1}{2}-1}{1}{\frac{1}{(1+2)^{1/2}}}\left(\frac{2}{1+2}\right)^1\\
&=  \frac{1}{3^{3/2}} = 0.19245 .
\end{aligned}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{Goodness of Fit}\label{S:goodness-of-fit}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Calculate a goodness of fit statistic to compare a hypothesized
  discrete distribution to a sample of discrete observations
\item
  Compare the statistic to a reference distribution to assess the
  adequacy of the fit
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In the above we have discussed three basic frequency distributions,
along with their extensions through zero modification/truncation and by
looking at mixtures of these distributions. Nevertheless, these classes
still remain parametric and hence by their very nature a small subset of
the class of all possible frequency distributions (\emph{i.e.} the set
of distributions on non-negative integers.) Hence, even though we have
talked about methods for estimating the unknown parameters, the
\emph{fitted} distribution will not be a good representation of the
underlying distribution if the latter is \textbf{far} from the class of
distribution used for modeling. In fact, it can be shown that the
\emph{maximum likelihood estimator} will converge to a value such that
the corresponding distribution will be a Kullback-Leibler
\emph{projection} of the underlying distribution on the class of
distributions used for modeling. Below we present one testing method -
Pearson's chi-square statistic - to check for the goodness of fit of the
fitted distribution. For more details on the Pearson's chi-square test,
at an introductory mathematical statistics level, we refer the reader to
Section 9.1 of \citep{zimmerman2015}.

In \(1993\), a portfolio of \(n=7,483\) automobile insurance policies
from a major Singaporean insurance company had the distribution of auto
accidents per policyholder as given in \protect\hyperlink{tab:2.4}{Table
2.4}.

\[\begin{matrix}
\begin{array}{c|c|c|c|c|c|c}
\hline
\text{Count }(k) & 0 & 1 & 2 & 3 & 4 & \text{Total}\\
\hline
\text{No. of Policies with }k\text{ accidents }(m_k) & 6,996 & 455 & 28 & 4 & 0 & 7483\\
\hline
\end{array}
\end{matrix}\]

\protect\hyperlink{tab:2.4}{Table 2.4} : Singaporean Automobile Accident
Data

If we a fit a Poisson distribution, then the \emph{MLE} for \(\lambda\),
the Poisson mean, is the sample mean which is given by \[
\overline{N} = \frac{0\cdot 6996 + 1 \cdot 455 + 2 \cdot 28 + 3 \cdot 4 + 4 \cdot 0}{7483} = 0.06989.
\] Now if we use Poisson (\(\hat{\lambda}_{MLE}\)) as the fitted
distribution, then a tabular comparison of the fitted counts and
observed counts is given by \protect\hyperlink{tab:2.5}{Table 2.5}
below, where \(\hat{p}_k\) represents the estimated probabilities under
the fitted Poisson distribution.

\[\begin{matrix}
\begin{array}{c|c|c}
\hline
\text{Count}  & \text{Observed}  & \text{Fitted Counts}\\
(k) & (m_k) & \text{Using Poisson }(n\hat{p}_k)\\
\hline
0 & 6,996 & 6,977.86 \\
1 & 455 & 487.70 \\
2 & 28 & 17.04 \\
3 & 4 & 0.40 \\
\geq4 & 0 & 0.01\\
\hline
\text{Total} & 7,483 & 7,483.00\\
\hline
\end{array}
\end{matrix}\]

\protect\hyperlink{tab:2.5}{Table 2.5} : Comparison of Observed to
Fitted Counts: Singaporean Auto Data

While the fit seems \emph{reasonable}, a tabular comparison falls short
of a statistical test of the hypothesis that the underlying distribution
is indeed Poisson. The Pearson's chi-square statistic is a goodness of
fit statistical measure that can be used for this purpose. To explain
this statistic let us suppose that a dataset of size \(n\) is grouped
into \(k\) cells with \(m_k/n\) and \(\hat{p}_k\), for \(k=1\ldots,K\)
being the observed and estimated probabilities of an observation
belonging to the \(k\)-th cell, respectively. The Pearson's chi-square
test statistic is then given by \[
\sum_{k=1}^K\frac{\left( m_k-n\widehat{p}_k \right) ^{2}}{n\widehat{p}_k}.
\] The motivation for the above statistic derives from the fact that \[
\sum_{k=1}^K\frac{\left( m_k-n{p}_k \right) ^{2}}{n{p}_k}
\] has a limiting chi-square distribution with \(K-1\) degrees of
freedom if \(p_k\), \(k=1,\ldots,K\) are the true cell probabilities.
Now suppose that only the summarized data represented by \(m_k\),
\(k=1,\ldots,K\) is available. Further, if \(p_k\)'s are functions of
\(s\) parameters, replacing \(p_k\)'s by any \emph{efficiently}
estimated probabilities \(\widehat{p}_k\)'s results in the statistic
continuing to have a limiting chi-square distribution but with degrees
of freedom given by \(K-1-s\). Such efficient estimates can be derived
for example by using the \emph{MLE} method (with a multinomial
likelihood) or by estimating the \(s\) parameters which minimizes the
Pearson's chi-square statistic above. For example, the \texttt{R} code
below does calculate an estimate for \(\lambda\) doing the latter and
results in the estimate \(0.06623153\), close but different from the
\emph{MLE} of \(\lambda\) using the full data:

\begin{verbatim}
m<-c(6996,455,28,4,0);
op<-m/sum(m);
g<-function(lam){sum((op-c(dpois(0:3,lam),1-ppois(3,lam)))^2)};
optim(sum(op*(0:4)),g,method="Brent",lower=0,upper=10)$par
\end{verbatim}

When one uses the full data to estimate the probabilities, the
asymptotic distribution is \emph{in between} chi-square distributions
with parameters \(K-1\) and \(K-1-s\). In practice it is common to
ignore this subtlety and assume the limiting chi-square has \(K-1-s\)
degrees of freedom. Interestingly, this practical shortcut works quite
well in the case of the Poisson distribution.

For the Singaporean auto data the Pearson's chi-square statistic equals
\(41.98\) using the full data \emph{MLE} for \({\lambda}\). Using the
limiting distribution of chi-square with \(5-1-1=3\) degrees of freedom,
we see that the value of \(41.98\) is way out in the tail (\(99\)-th
percentile is below \(12\)). Hence we can conclude that the Poisson
distribution provides an inadequate fit for the data.

In the above, we started with the cells as given in the above tabular
summary. In practice, a relevant question is how to define the cells so
that the chi-square distribution is a good approximation to the finite
sample distribution of the statistic. A rule of thumb is to define the
cells in such a way to have at least \(80\%\), if not all, of the cells
having expected counts greater than \(5\). Also, it is clear that a
larger number of cells results in a higher power of the test, and hence
a simple rule of thumb is to maximize the number of cells such that each
cell has at least 5 observations.

\section{Exercises}\label{S:exercises}

\subsubsection*{Theoretical Exercises}\label{theoretical-exercises}
\addcontentsline{toc}{subsubsection}{Theoretical Exercises}

\textbf{Exercise 2.1.} Derive an expression for \(p_N(\cdot)\) in terms
of \(F_N(\cdot)\) and \(S_N(\cdot)\).

\textbf{Exercise 2.2.} A measure of center of location must be
\textbf{equi-variant} with respect to shifts, or location
transformations. In other words, if \(N_1\) and \(N_2\) are two random
variables such that \(N_1+c\) has the same distribution as \(N_2\), for
some constant \(c\), then the difference between the measures of the
center of location of \(N_2\) and \(N_1\) must equal \(c\). Show that
the mean satisfies this property.

\textbf{Exercise 2.3.} Measures of dispersion should be invariant with
respect to shifts and scale equi-variant. Show that standard deviation
satisfies these properties by doing the following:

\begin{itemize}
\tightlist
\item
  Show that for a random variable \(N\), its standard deviation equals
  that of \(N+c\), for any constant \(c\).
\item
  Show that for a random variable \(N\), its standard deviation equals
  \(1/c\) times that of \(cN\), for any positive constant \(c\).
\end{itemize}

\textbf{Exercise 2.4.} Let \(N\) be a random variable with probability
mass function given by \[
p_N(k):= \begin{cases}
\left(\frac{6}{\pi^2}\right)\left(\frac{1}{k^{2}}\right), & k\geq 1;\\
0, &\hbox{otherwise}.
\end{cases}
\] Show that the mean of \(N\) is \(\infty\).

\textbf{Exercise 2.5.} Let \(N\) be a random variable with a finite
second moment. Show that the function \(\psi(\cdot)\) defined by \[
\psi(x):=\mathrm{E}{(N-x)^2}. \quad x\in\mathbb{R}
\] is minimized at \(\mu_N\) without using calculus. Also, give a proof
of this fact using derivatives. Conclude that the minimum value equals
the variance of \(N\).

\textbf{Exercise 2.6.} Derive the first two central moments of the
\((a,b,0)\) distributions using the methods mentioned below:

\begin{itemize}
\tightlist
\item
  For the binomial distribution, derive the moments using only its
  \emph{pmf}, then its \emph{mgf}, and then its \emph{pgf}.
\item
  For the Poisson distribution, derive the moments using only its
  \emph{mgf}.
\item
  For the negative binomial distribution, derive the moments using only
  its \emph{pmf}, and then its \emph{pgf}.
\end{itemize}

\textbf{Exercise 2.7.} Let \(N_1\) and \(N_2\) be two independent
Poisson random variables with means \(\lambda_1\) and \(\lambda_2\),
respectively. Identify the conditional distribution of \(N_1\) given
\(N_1+N_2\).

\textbf{Exercise 2.8.} (\textbf{Non-Uniqueness of the MLE}) Consider the
following parametric family of densities indexed by the parameter \(p\)
taking values in \([0,1]\): \[
f_p(x)=p\cdot\phi(x+2)+(1-p)\cdot\phi(x-2), \quad x\in\mathbb{R},
\] where \(\phi(\cdot)\) represents the standard normal density.

\begin{itemize}
\tightlist
\item
  Show that for all \(p\in[0,1]\), \(f_p(\cdot)\) above is a valid
  density function.
\item
  Find an expression in \(p\) for the mean and the variance of
  \(f_p(\cdot)\).
\item
  Let us consider a sample of size one consisting of \(x\). Show that
  when \(x\) equals \(0\), the set of \emph{maximum likelihood
  estimates} for \(p\) equals \([0,1]\); also show that the \emph{MLE}
  is unique otherwise.
\end{itemize}

\textbf{Exercise 2.9.} Graph the region of the plane corresponding to
values of \((a,b)\) that give rise to valid \((a,b,0)\) distributions.
Do the same for \((a,b,1)\) distributions.

\textbf{Exercise 2.10.} (\textbf{Computational Complexity}) For the
\((a,b,0)\) class of distributions, count the number of basic
mathematical operations (addition, subtraction, multiplication,
division) needed to compute the \(n\) probabilities
\(p_0\ldots p_{n-1}\) using the recurrence relationship. For the
negative binomial distribution with non-integer \(r\), count the number
of such operations. What do you observe?

\textbf{Exercise 2.11.} (** **) Using the development of Section 2.3
rigorously show that not only does the recurrence \eqref{eq:ab0} tie the
binomial, the Poisson and the negative binomial distributions together,
but that it also characterizes them.

\subsubsection*{Exercises with a Practical
Focus}\label{exercises-with-a-practical-focus}
\addcontentsline{toc}{subsubsection}{Exercises with a Practical Focus}

\textbf{Exercise 2.12. Actuarial Exam Question.} You are given:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(p_k\) denotes the probability that the number of claims equals \(k\)
  for \(k=0,1,2,\ldots\)
\item
  \(\frac{p_n}{p_m}=\frac{m!}{n!}, m\ge 0, n\ge 0\)
\end{enumerate}

Using the corresponding zero-modified claim count distribution with
\(p_0^M=0.1\), calculate \(p_1^M\).

\textbf{Exercise 2.13. Actuarial Exam Question.} During a one-year
period, the number of accidents per day was distributed as follows:

\[
\begin{matrix}
\begin{array}{c|c|c|c|c|c|c}
\hline
\text{No. of Accidents} & 0 & 1 & 2 & 3 & 4 & 5\\
\hline
\text{No. of Days} & 209 & 111 & 33 & 7 & 5 & 2\\
\hline
\end{array}
\end{matrix}
\]

You use a chi-square test to measure the fit of a Poisson distribution
with mean 0.60. The minimum expected number of observations in any group
should be 5. The maximum number of groups should be used. Determine the
value of the chi-square statistic.

A discrete probability distribution has the following properties \[
\begin{aligned}
\Pr(N=k) = \left( \frac{3k+9}{8k}\right) \Pr(N=k-1), \quad k=1,2,3,\ldots
\end{aligned}
\] Determine the value of \(\Pr(N=3)\). (Ans: 0.1609)

\subsubsection*{Additional Exercises}\label{additional-exercises}
\addcontentsline{toc}{subsubsection}{Additional Exercises}

Here are a set of exercises that guide the viewer through some of the
theoretical foundations of \textbf{Loss Data Analytics}. Each tutorial
is based on one or more questions from the professional actuarial
examinations -- typically the Society of Actuaries Exam C.

\href{https://www.ssc.wisc.edu/~jfrees/loss-data-analytics/loss-data-analytics-problems/}{Frequency
Distribution Guided Tutorials}

\section{Further Resources and
Contributors}\label{Freq-further-reading-and-resources}

Appendix Chapter \ref{C:AppA} gives a general introduction to maximum
likelihood theory regarding estimation of parameters from a parametric
family. Appendix Chapter \ref{C:AppC} gives more specific examples and
expands some of the concepts.

\subsubsection*{Contributors}\label{contributors-1}
\addcontentsline{toc}{subsubsection}{Contributors}

\begin{itemize}
\tightlist
\item
  \textbf{N.D. Shyamalkumar}, The University of Iowa, and \textbf{Krupa
  Viswanathan}, Temple University, are the principal authors of the
  initial version of this chapter. Email:
  \href{mailto:shyamal-kumar@uiowa.edu}{\nolinkurl{shyamal-kumar@uiowa.edu}}
  for chapter comments and suggested improvements.
\item
  Chapter reviewers include: Paul Johnson, Hirokazu (Iwahiro) Iwasawa,
  Rajesh Sahasrabuddhe, Michelle Xia.
\end{itemize}

\subsection{TS 2.A. R Code for Plots}\label{S:rcode}

\textbf{Code for Figure \ref{fig:MLEab0}:}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Code for Figure \ref{fig:MLEm}:}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\chapter{Modeling Loss Severity}\label{C:Severity}

\emph{Chapter Preview.} The traditional loss distribution approach to
modeling aggregate losses starts by separately fitting a frequency
distribution to the number of losses and a severity distribution to the
size of losses. The estimated aggregate loss distribution combines the
loss frequency distribution and the loss severity distribution by
convolution. Discrete distributions often referred to as counting or
frequency distributions were used in Chapter \ref{C:Frequency-Modeling}
to describe the number of events such as number of accidents to the
driver or number of claims to the insurer. Lifetimes, asset values,
losses and claim sizes are usually modeled as continuous random
variables and as such are modeled using continuous distributions, often
referred to as loss or severity distributions. A mixture distribution is
a weighted combination of simpler distributions that is used to model
phenomenon investigated in a heterogeneous population, such as modelling
more than one type of claims in liability insurance (small frequent
claims and large relatively rare claims). In this chapter we explore the
use of continuous as well as mixture distributions to model the random
size of loss. We present key attributes that characterize continuous
models and means of creating new distributions from existing ones. We
also explore the effect of coverage modifications, which change the
conditions that trigger a payment, such as applying deductibles, limits,
or adjusting for inflation, on the distribution of individual loss
amounts. The frequency distributions from Chapter
\ref{C:Frequency-Modeling} will be combined with the ideas from this
chapter to describe the aggregate losses over the whole portfolio in
Chapter \ref{C:AggLossModels}.

\section{Basic Distributional Quantities}\label{S:BasicQuantities}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to define some basic distributional
quantities:

\begin{itemize}
\tightlist
\item
  moments,
\item
  percentiles, and
\item
  generating functions.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Moments}\label{S:Chap3Moments}

Let \(X\) be a continuous random variable with probability density
function \(f_{X}\left( x \right)\). The \emph{k}-th raw moment of \(X\),
denoted by \(\mu_{k}^{\prime}\), is the expected value of the
\emph{k}-th power of \(X\), provided it exists. The first raw moment
\(\mu_{1}^{\prime}\) is the mean of \(X\) usually denoted by \(\mu\).
The formula for \(\mu_{k}^{\prime}\) is given as \[
\mu_{k}^{\prime} = \mathrm{E}\left( X^{k} \right) = \int_{0}^{\infty}{x^{k}f_{X}\left( x \right)dx } .
\] The support of the random variable \(X\) is assumed to be nonnegative
since actuarial phenomena are rarely negative. An easy integration by
parts shows that the raw moments for nonnegative variables can also be
computed using \[
\mu_{k}^{\prime} = \int_{0}^{\infty}{k~x^{k-1}\left[1- F_{X}(x) \right]dx },
\] that is based on the survival function, denoted as
\(S_X(x) = 1-F_{X}(x)\). This formula is particularly useful when
\(k=1\).

The \emph{k}-th central moment of \(X\), denoted by \(\mu_{k}\), is the
expected value of the \emph{k}-th power of the deviation of \(X\) from
its mean \(\mu\). The formula for \(\mu_{k}\) is given as \[
\mu_{k} = \mathrm{E}\left\lbrack {(X - \mu)}^{k} \right\rbrack = \int_{0}^{\infty}{\left( x - \mu \right)^{k}f_{X}\left( x \right) dx }.
\] The second central moment \(\mu_{2}\) defines the variance of \(X\),
denoted by \(\sigma^{2}\). The square root of the variance is the
standard deviation \(\sigma\).

From a classical perspective, further characterization of the shape of
the distribution includes its degree of symmetry as well as its flatness
compared to the normal distribution. The ratio of the third central
moment to the cube of the standard deviation
\(\left( \mu_{3} / \sigma^{3} \right)\) defines the coefficient of
skewness which is a measure of symmetry. A positive coefficient of
skewness indicates that the distribution is skewed to the right
(positively skewed). The ratio of the fourth central moment to the
fourth power of the standard deviation
\(\left(\mu_{4} / \sigma^{4} \right)\) defines the coefficient of
kurtosis. The normal distribution has a coefficient of kurtosis of 3.
Distributions with a coefficient of kurtosis greater than 3 have heavier
tails and higher peak than the normal, whereas distributions with a
coefficient of kurtosis less than 3 have lighter tails and are flatter.
Section \ref{S:Tails} describes the tails of distributions from an
insurance and actuarial perspective.

\textbf{Example 3.1.1. Actuarial Exam Question.} Assume that the rv
\(X\) has a gamma distribution with mean 8 and skewness 1. Find the
variance of \(X\). (\emph{Hint}: The gamma distribution is reviewed in
Section \ref{S:Loss:Gamma}.)

\textbf{Solution.} The probability density function of \(X\) is given by
\[
f_{X}\left( x \right) = \frac{\left( x / \theta \right)^{\alpha}}{x ~\Gamma\left( \alpha \right)} e^{- x / \theta}
\] for \(x > 0\). For \(\alpha>0\), the \emph{k}-th raw moment is \[
\mu_{k}^{\prime} = \mathrm{E}\left( X^{k} \right) = \int_{0}^{\infty}{\frac{1}{\Gamma\left( \alpha \right)\theta^{\alpha}}x^{k + \alpha - 1}e^{- x / \theta} dx} = \frac{\Gamma\left( k + \alpha \right)}{\Gamma\left( \alpha \right)}\theta^{k}
\] Given \(\Gamma\left( r + 1 \right) = r\Gamma\left( r \right)\) and
\(\Gamma\left( 1 \right) = 1\), then
\(\mu_{1}^{\prime} = \mathrm{E}\left( X \right) = \alpha\theta\),
\(\mu_{2}^{\prime} = \mathrm{E}\left( X^{2} \right) = \left( \alpha + 1 \right)\alpha\theta^{2}\),
\(\mu_{3}^{\prime} = \mathrm{E}\left( X^{3} \right) = \left( \alpha + 2 \right)\left( \alpha + 1 \right)\alpha\theta^{3}\),
and
\(\mathrm{Var}\left( X \right) = (\alpha + 1)\alpha\theta^2 - (\alpha\theta)^2 = \alpha\theta^{2}\).

\[
\text{Skewness}  = \frac{\mathrm{E}\left\lbrack {(X - \mu_{1}^{\prime})}^{3} \right\rbrack}{{\mathrm{Var}\left( X \right)}^{3/2}} = \frac{\mu_{3}^{\prime} - 3\mu_{2}^{\prime}\mu_{1}^{\prime} + 2{\mu_{1}^{\prime}}^{3}}{{\mathrm{Var}\left( X \right)}^{3/2}} \\
 = \frac{\left( \alpha + 2 \right)\left( \alpha + 1 \right)\alpha\theta^{3} - 3\left( \alpha + 1 \right)\alpha^{2}\theta^{3} + 2\alpha^{3}\theta^{3}}{\left( \alpha\theta^{2} \right)^{3/2}} = \frac{2}{\alpha^{1/2}} = 1.
\]

Hence, \(\alpha = 4\). Since,
\(\mathrm{E}\left( X \right) = \alpha\theta = 8\), then \(\theta = 2\)
and finally, \(\mathrm{Var}\left( X \right) = \alpha\theta^{2} = 16\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Quantiles}\label{quantiles}

Quantiles can also be used to describe the characteristics of the
distribution of \(X\). When the distribution of \(X\) is continuous, for
a given fraction \(0 \leq p \leq 1\) the corresponding quantile is the
solution of the equation \[
F_{X}\left( \pi_{p} \right) = p .
\] For example, the middle point of the distribution, \(\pi_{0.5}\), is
the median. A percentile is a type of quantile; a \(100p\) percentile is
the number such that \(100 \times p\) percent of the data is below it.

\textbf{Example 3.1.1. Actuarial Exam Question.} Let \(X\) be a
continuous random variable with density function
\(f_{X}\left( x \right) = \theta e^{- \theta x}\), for \(x > 0\) and 0
elsewhere. If the median of this distribution is \(\frac{1}{3}\), find
\(\theta\).

\textbf{Solution.}

The distribution function is
\(F_{X}\left( x \right) = 1 - e^{- \theta x}\). So,
\(F_{X}\left( \pi_{0.5} \right) = 1 - e^{- \theta\pi_{0.5}} = 0.5\). As,
\(\pi_{0.5} = \frac{1}{3}\), we have
\(F_X\left(\frac{1}{3}\right) = 1 - e^{-\theta / 3} = 0.5\) and
\(\theta = 3 \ln 2\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Section \ref{S:MS:QuantileEstimator} will extend the definition of
quantiles to include distributions that are discrete, continuous, or a
hybrid combination.

\subsection{Moment Generating
Function}\label{moment-generating-function}

The moment generating function, denoted by \(M_{X}(t)\) uniquely
characterizes the distribution of \(X\). While it is possible for two
different distributions to have the same moments and yet still differ,
this is not the case with the moment generating function. That is, if
two random variables have the same moment generating function, then they
have the same distribution. The moment generating function is given by
\[
M_{X}(t) = \mathrm{E}\left( e^{tX} \right) = \int_{0}^{\infty}{e^{\text{tx}}f_{X}\left( x \right) dx }
\] for all \(t\) for which the expected value exists. The moment
generating is a real function whose \emph{k}-th derivative at zero is
equal to the \emph{k}-th raw moment of \(X\). In symbols, this is \[
\left.\frac{d^k}{dt^k} M_{X}(t)\right|_{t=0} = \mathrm{E}\left( X^{k} \right) .
\]

\textbf{Example 3.1.3. Actuarial Exam Question.} The random variable
\(X\) has an exponential distribution with mean \(\frac{1}{b}\). It is
found that \(M_{X}\left( - b^{2} \right) = 0.2\). Find \(b\).
(\emph{Hint}: The exponential is a special case of the gamma
distribution which is reviewed in Section \ref{S:Loss:Gamma}.)

\textbf{Solution.}

With \(X\) having an exponential distribution with mean \(\frac{1}{b}\),
we have that \[
M_{X}(t) = \mathrm{E}\left( e^{tX} \right) = \int_{0}^{\infty}{e^{\text{tx}}be^{- bx} dx} = \int_{0}^{\infty}{be^{- x\left( b - t \right)} dx} = \frac{b}{\left( b - t \right)}.
\]

Then, \[
M_{X}\left( - b^{2} \right) = \frac{b}{\left( b + b^{2} \right)} = \frac{1}{\left( 1 + b \right)} = 0.2.
\] Thus, \(b = 4\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 3.1.4. Actuarial Exam Question.} Let
\(X_{1}, \ldots, X_{n}\) be independent random variables, where \(X_i\)
has a gamma distribution with parameters \(\alpha_{i}\) and \(\theta\).
Find the distribution of \(S = \sum_{i = 1}^{n}X_{i}\), the mean
\(\mathrm{E}(S)\), and the variance \(\mathrm{Var}(S)\).

\textbf{Solution.}

The moment generating function of \(S\) is \[
M_{S}(t) = \text{E}\left( e^{\text{tS}} \right) = \mathrm{E}\left( e^{t\sum_{i = 1}^{n}X_{i}} \right) 
= \mathrm{E}\left( \prod_{i = 1}^{n}e^{tX_{i}} \right) .
\] Using independence, we get\\
\[
M_{S}(t) = \prod_{i = 1}^{n}{\mathrm{E}\left( e^{tX_{i}} \right) = \prod_{i = 1}^{n}{M_{X_{i}}(t)}} .
\]

The moment generating function of the gamma distribution \(X_i\) is
\(M_{X_i}(t) = (1-\theta)^{\alpha_i}\). . Then, \[
M_{S}(t) = \prod_{i = 1}^{n}\left( 1 - \theta t \right)^{- \alpha_{i}} = \left( 1 - \theta t \right)^{- \sum_{i = 1}^{n}\alpha_{i}} . 
\] This indicates that the distribution of \(S\) is gamma with
parameters \(\sum_{i = 1}^{n}\alpha_{i}\) and \(\theta\).

This is a demonstration of how we can use the uniqueness property of the
moment generating function to determine the probability distribution of
a random variable.

We can find the mean and variance from the properties of the gamma
distribution. Alternatively, by finding the first and second derivatives
of \(M_{S}(t)\) at zero, we can show that
\(\mathrm{E}\left( S \right) = \left. \ \frac{\partial M_{S}(t)}{\partial t} \right|_{t = 0} = \alpha\theta\)
where \(\alpha = \sum_{i = 1}^{n}\alpha_{i}\), and \[
\mathrm{E}\left( S^{2} \right) = \left. \ \frac{\partial^{2}M_{S}(t)}{\partial t^{2}} \right|_{t = 0} = \left( \alpha + 1 \right)\alpha\theta^{2}.
\] Hence, \(\mathrm{Var}\left( S \right) = \alpha\theta^{2}\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

One can also use the moment generating function to compute the
probability generating function

\[
P_{X}(z) = \mathrm{E}\left( z^{X} \right) = M_{X}\left( \ln z \right) . 
\]

As introduced in Section \ref{S:generating-functions}, the probability
generating function is more useful for discrete \emph{rv}s.

\section{Continuous Distributions for Modeling Loss
Severity}\label{S:ContinuousDistn}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to define and apply four fundamental
severity distributions:

\begin{itemize}
\tightlist
\item
  gamma,
\item
  Pareto,
\item
  Weibull, and
\item
  generalized beta distribution of the second kind.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Gamma Distribution}\label{S:Loss:Gamma}

Recall that the traditional approach in modelling losses is to fit
separate models for claim frequency and claim severity. When frequency
and severity are modeled separately it is common for actuaries to use
the Poisson distribution (introduced in Section
\ref{S:poisson-distribution}) for claim count and the gamma distribution
to model severity. An alternative approach for modelling losses that has
recently gained popularity is to create a single model for pure premium
(average claim cost) that will be described in Chapter
\ref{C:ModelSelection}.

The continuous variable \(X\) is said to have the gamma distribution
with shape parameter \(\alpha\) and scale parameter \(\theta\) if its
probability density function is given by \[
f_{X}\left( x \right) = \frac{\left( x/ \theta  \right)^{\alpha}}{x~ \Gamma\left( \alpha \right)}\exp \left( -x/ \theta \right) \ \ \ \text{for } x > 0 .
\] Note that \(\alpha > 0,\ \theta > 0\).

The two panels in Figure \ref{fig:gammapdf} demonstrate the effect of
the scale and shape parameters on the gamma density function.

\begin{figure}

{\centering \includegraphics[width=1.2\linewidth]{LossDataAnalytics_files/figure-latex/gammapdf-1} 

}

\caption{Gamma Densities. The left-hand panel is with shape=2 and Varying Scale. 
 The right-hand panel is with scale=100 and Varying Shape.}\label{fig:gammapdf}
\end{figure}

When \(\alpha = 1\) the gamma reduces to an exponential distribution and
when \(\alpha = \frac{n}{2}\) and \(\theta = 2\) the gamma reduces to a
chi-square distribution with \(n\) degrees of freedom. As we will see in
Section \ref{S:AppA:HT}, the chi-square distribution is used extensively
in statistical hypothesis testing.

The distribution function of the gamma model is the incomplete gamma
function, denoted by \(\Gamma\left(\alpha; \frac{x}{\theta} \right)\),
and defined as \[
F_{X}\left( x \right) = \Gamma\left( \alpha; \frac{x}{\theta} \right) = \frac{1}{\Gamma\left( \alpha \right)}\int_{0}^{x /\theta}t^{\alpha - 1}e^{- t}~dt
\] \(\alpha > 0,\ \theta > 0\). For an integer \(\alpha\), it can be
written as
\(\Gamma\left( \alpha; \frac{x}{\theta} \right) = 1 - e^{-x/\theta}\sum_{k = 0}^{\alpha-1}\frac{(x/\theta)^k}{k!}\).

The \(k\)-th moment of the gamma distributed random variable for any
positive \(k\) is given by \[
\mathrm{E}\left( X^{k} \right) = \theta^{k} \frac{\Gamma\left( \alpha + k \right)}{\Gamma\left( \alpha \right)} .
\] The mean and variance are given by
\(\mathrm{E}\left( X \right) = \alpha\theta\) and
\(\mathrm{Var}\left( X \right) = \alpha\theta^{2}\), respectively.

Since all moments exist for any positive \(k\), the gamma distribution
is considered a light tailed distribution, which may not be suitable for
modeling risky assets as it will not provide a realistic assessment of
the likelihood of severe losses.

\subsection{Pareto Distribution}\label{pareto-distribution}

The Pareto distribution, named after the Italian economist Vilfredo
Pareto (1843-1923), has many economic and financial applications. It is
a positively skewed and heavy-tailed distribution which makes it
suitable for modeling income, high-risk insurance claims and severity of
large casualty losses. The survival function of the Pareto distribution
which decays slowly to zero was first used to describe the distribution
of income where a small percentage of the population holds a large
proportion of the total wealth. For extreme insurance claims, the tail
of the severity distribution (losses in excess of a threshold) can be
modeled using a Generalized Pareto distribution.

The continuous variable \(X\) is said to have the Pareto distribution
with shape parameter \(\alpha\) and scale parameter \(\theta\) if its
pdf is given by \[
f_{X}\left( x \right) = \frac{\alpha\theta^{\alpha}}{\left( x + \theta \right)^{\alpha + 1}} \ \ \  x  >  0,\ \alpha >  0,\ \theta > 0.
\] The two panels in Figure \ref{fig:Paretopdf} demonstrate the effect
of the scale and shape parameters on the Pareto density function.

\begin{figure}

{\centering \includegraphics[width=1.2\linewidth]{LossDataAnalytics_files/figure-latex/Paretopdf-1} 

}

\caption{Pareto Densities. The left-hand panel is with scale=2000 and Varying Shape.  The right-hand panel is with shape=3 and ,Varying Scale}\label{fig:Paretopdf}
\end{figure}

The distribution function of the Pareto distribution is given by \[
F_{X}\left( x \right) = 1 - \left( \frac{\theta}{x + \theta} \right)^{\alpha}  \ \ \ x > 0,\ \alpha > 0,\ \theta > 0.
\] It can be easily seen that the hazard function of the Pareto
distribution is a decreasing function in \(x\), another indication that
the distribution is heavy tailed. When the hazard function decreases
over time the population dies off at a decreasing rate resulting in a
heavier tail for the distribution. The hazard function reveals
information about the tail distribution and is often used to model data
distributions in survival analysis. The hazard function is defined as
the instantaneous potential that the event of interest occurs within a
very narrow time frame.

The \(k\)-th moment of the Pareto distributed random variable exists, if
and only if, \(\alpha > k\). If \(k\) is a positive integer then \[
\mathrm{E}\left( X^{k} \right) = \frac{\theta^{k}~ k!}{\left( \alpha - 1 \right)\cdots\left( \alpha - k \right)} \ \ \ \alpha > k.
\] The mean and variance are given by
\[\mathrm{E}\left( X \right) = \frac{\theta}{\alpha - 1} \ \ \ \text{for } \alpha > 1\]
and
\[\mathrm{Var}\left( X \right) = \frac{\alpha\theta^{2}}{\left( \alpha - 1 \right)^{2}\left( \alpha - 2 \right)} \ \ \ \text{for } \alpha > 2,\]respectively.

\textbf{Example 3.2.1. } The claim size of an insurance portfolio
follows the Pareto distribution with mean and variance of 40 and 1800
respectively. Find

The shape and scale parameters.

The 95-th percentile of this distribution.

\textbf{Solution.}

\textbf{a.} As, \(X\sim Pa(\alpha,\theta)\), we have
\(\mathrm{E}\left( X \right) = \frac{\theta}{\alpha - 1} = 40\) and
\(\mathrm{Var}\left( X \right) = \frac{\alpha\theta^{2}}{\left( \alpha - 1 \right)^{2}\left( \alpha - 2 \right)} = 1800\).
By dividing the square of the first equation by the second we get
\(\frac{\alpha - 2}{\alpha} = \frac{40^{2}}{1800}\). Thus,
\(\alpha = 18.02\) and \(\theta = 680.72\).\\
\textbf{b.} The 95-th percentile, \(\pi_{0.95}\), satisfies the equation
\[
F_{X}\left( \pi_{0.95} \right) = 1 - \left( \frac{680.72}{\pi_{0.95} + 680.72} \right)^{18.02} = 0.95.
\] Thus, \(\pi_{0.95} = 122.96\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Weibull Distribution}\label{S:LS:Weibull}

The Weibull distribution, named after the Swedish physicist Waloddi
Weibull (1887-1979) is widely used in reliability, life data analysis,
weather forecasts and general insurance claims. Truncated data arise
frequently in insurance studies. The Weibull distribution has been used
to model excess of loss treaty over automobile insurance as well as
earthquake inter-arrival times.

The continuous variable \(X\) is said to have the Weibull distribution
with shape parameter \(\alpha\) and scale parameter \(\theta\) if its
probability density function is given by \[
f_{X}\left( x \right) = \frac{\alpha}{\theta}\left( \frac{x}{\theta} \right)^{\alpha - 1} \exp \left(- \left( \frac{x}{\theta} \right)^{\alpha}\right) \ \ \ x > 0,\ \alpha > 0,\ \theta > 0.
\] The two panels Figure \ref{fig:Weibullpdf} demonstrate the effects of
the scale and shape parameters on the Weibull density function.

\begin{figure}

{\centering \includegraphics[width=1.2\linewidth]{LossDataAnalytics_files/figure-latex/Weibullpdf-1} 

}

\caption{Weibull Densities. The left-hand panel is with shape=3 and Varying Scale. The right-hand panel is with scale=100 and Varying Shape.}\label{fig:Weibullpdf}
\end{figure}

The distribution function of the Weibull distribution is given by \[
F_{X}\left( x \right) = 1 - e^{- \left( x / \theta \right)^{\alpha}}  \ \ \ x >  0,\ \alpha >  0,\ \theta > 0.
\]

It can be easily seen that the shape parameter \(\alpha\) describes the
shape of the hazard function of the Weibull distribution. The hazard
function is a decreasing function when \(\alpha < 1\) (heavy tailed
distribution), constant when \(\alpha = 1\) and increasing when
\(\alpha > 1\) (light tailed distribution). This behavior of the hazard
function makes the Weibull distribution a suitable model for a wide
variety of phenomena such as weather forecasting, electrical and
industrial engineering, insurance modeling, and financial risk analysis.

The \(k\)-th moment of the Weibull distributed random variable is given
by \[
\mathrm{E}\left( X^{k} \right) = \theta^{k}~\Gamma\left( 1 + \frac{k}{\alpha} \right) .
\]

The mean and variance are given by \[
\mathrm{E}\left( X \right) = \theta~\Gamma\left( 1 + \frac{1}{\alpha} \right)
\] and \[
\mathrm{Var}(X)= \theta^{2}\left( \Gamma\left( 1 + \frac{2}{\alpha} \right)  - \left\lbrack \Gamma\left( 1 + \frac{1}{\alpha} \right) \right\rbrack  ^{2}\right),
\] respectively.

\textbf{Example 3.2.2.} Suppose that the probability distribution of the
lifetime of AIDS patients (in months) from the time of diagnosis is
described by the Weibull distribution with shape parameter 1.2 and scale
parameter 33.33.

Find the probability that a randomly selected person from this
population survives at least 12 months,

A random sample of 10 patients will be selected from this population.
What is the probability that at most two will die within one year of
diagnosis.

Find the 99-th percentile of the distribution of lifetimes.

\textbf{Solution.}

\textbf{a.} Let \(X\) be the lifetime of AIDS patients (in months)
having a Weibull distribution with parameters
\(\left( 1.2,33.33 \right)\). We have,

\[
\Pr \left( X \geq 12 \right) = S_{X} \left( 12 \right) = e^{- \left( \frac{12}{33.33} \right)^{1.2}} = 0.746.
\]

\textbf{b.} Let \(Y\) be the number of patients who die within one year
of diagnosis. Then, \(Y\sim Bin\left( 10,\ 0.254 \right)\) and
\(\Pr\left( Y \leq 2 \right) = 0.514.\)

\textbf{c.} Let \(\pi_{0.99}\) denote the 99-th percentile of this
distribution. Then, \[
S_{X}\left( \pi_{0.99} \right) = \exp\left\{- \left( \frac{\pi_{0.99}}{33.33} \right)^{1.2}\right\} = 0.01.
\] Solving for \(\pi_{0.99}\), we get \(\pi_{0.99} = 118.99\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{The Generalized Beta Distribution of the Second
Kind}\label{the-generalized-beta-distribution-of-the-second-kind}

The Generalized Beta Distribution of the Second Kind (\emph{GB2}) was
introduced by \citet{venter1983transformed} in the context of insurance
loss modeling and by \citet{mcdonald1984some} as an income and wealth
distribution. It is a four-parameter very flexible distribution that can
model positively as well as negatively skewed distributions.

The continuous variable \(X\) is said to have the \emph{GB2}
distribution with parameters \(\sigma\), \(\theta\), \(\alpha_1\) and
\(\alpha_2\) if its probability density function is given by

\begin{equation}
f_{X}\left( x \right) = \frac{(x/\theta)^{\alpha_2/\sigma}}{x \sigma~\mathrm{B}\left( \alpha_1,\alpha_2\right)\left\lbrack 1 + \left( x/\theta \right)^{1/\sigma} \right\rbrack^{\alpha_1 + \alpha_2}} \ \ \ \text{for } x > 0,
  \label{eq:GB2Distn}
\end{equation}

\(\sigma,\theta,\alpha_1,\alpha_2 > 0\), and where the beta function
\(\mathrm{B}\left( \alpha_1,\alpha_2 \right)\) is defined as \[
\mathrm{B}\left( \alpha_1,\alpha_2\right) = \int_{0}^{1}{t^{\alpha_1 - 1}\left( 1 - t \right)^{\alpha_2 - 1}}~ dt.
\]

The \emph{GB2} provides a model for heavy as well as light tailed data.
It includes the exponential, gamma, Weibull, Burr, Lomax, F, chi-square,
Rayleigh, lognormal and log-logistic as special or limiting cases. For
example, by setting the parameters \(\sigma = \alpha_1 = \alpha_2 = 1\),
then the \emph{GB2} reduces to the log-logistic distribution. When
\(\sigma = 1\) and \(\alpha_2 \rightarrow \infty\), it reduces to the
gamma distribution and when \(\alpha = 1\) and
\(\alpha_2 \rightarrow \infty\), it reduces to the Weibull distribution.

A \emph{GB2} random variable can be defined as follows. Suppose that
\(G_1\) and \(G_2\) are independent random variables where \(G_i\) has a
gamma distribution with parameters \(\alpha_i\) and scale parameter 1.
Then, one can show that the random variable
\(X = \theta \left(\frac{G_1}{G_2}\right)^{\sigma}\) has a \emph{GB2}
distribution with \emph{pdf} summarized in equation \eqref{eq:GB2Distn}.
This theoretical result has several implications. For example, when the
moments exist, one can show that the \(k\)-th moment of the \emph{GB2}
distributed random variable is given by \[
\mathrm{E}\left( X^{k} \right) = \frac{\theta^{k}~\mathrm{B}\left( \alpha_1 +k \sigma,\alpha_2 - k \sigma \right)}{\mathrm{B}\left( \alpha_1,\alpha_2 \right)}, \ \ \ k > 0.
\]

Earlier applications of the \emph{GB2} were on income data and more
recently have been used to model long-tailed claims data (Section
\ref{S:Tails} describes different interpretations of the descriptor
``long-tail''). \emph{GB2} was used to model different types of
automobile insurance claims, severity of fire losses as well as medical
insurance claim data.

\section{Methods of Creating New Distributions}\label{MethodsCreation}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Understand connections among the distributions
\item
  Give insights into when a distribution is preferred when compared to
  alternatives
\item
  Provide foundations for creating new distributions
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Functions of Random Variables and their
Distributions}\label{functions-of-random-variables-and-their-distributions}

In Section \ref{S:ContinuousDistn} we discussed some elementary known
distributions. In this section we discuss means of creating new
parametric probability distributions from existing ones. Specifically,
let \(X\) be a continuous random variable with a known probability
density function \(f_{X}(x)\) and distribution function \(F_{X}(x)\). We
are interested in the distribution of \(Y = g\left( X \right)\), where
\(g(X)\) is a one-to-one transformation defining a new random variable
\(Y\). In this section we apply the following techniques for creating
new families of distributions: (a) multiplication by a constant (b)
raising to a power, (c) exponentiation and (d) mixing.

\subsection{Multiplication by a
Constant}\label{multiplication-by-a-constant}

If claim data show change over time then such transformation can be
useful to adjust for inflation. If the level of inflation is positive
then claim costs are rising, and if it is negative then costs are
falling. To adjust for inflation we multiply the cost \(X\) by 1+
inflation rate (negative inflation is deflation). To account for
currency impact on claim costs we also use a transformation to apply
currency conversion from a base to a counter currency.

Consider the transformation \(Y = cX\), where \(c > 0\), then the
distribution function of \(Y\) is given by \[
F_{Y}\left( y \right) = \Pr\left( Y \leq y \right) = \Pr\left( cX \leq y \right) = \Pr\left( X \leq \frac{y}{c} \right) = F_{X}\left( \frac{y}{c} \right).
\] Hence, the probability density function of interest \(f_{Y}(y)\) can
be written as \[
f_{Y}\left( y \right) = \frac{1}{c}f_{X}\left( \frac{y}{c} \right).
\] Suppose that \(X\) belongs to a certain set of parametric
distributions and define a rescaled version \(Y\  = \ cX\),
\(c\  > \ 0\). If \(Y\) is in the same set of distributions then the
distribution is said to be a scale distribution. When a member of a
scale distribution is multiplied by a constant \(c\) (\(c > 0\)), the
scale parameter for this scale distribution meets two conditions:

The parameter is changed by multiplying by \(c\);

All other parameters remain unchanged.

\textbf{Example 3.3.1. Actuarial Exam Question.} The aggregate losses of
Eiffel Auto Insurance are denoted in Euro currency and follow a
lognormal distribution with \(\mu = 8\) and \(\sigma = 2\). Given that 1
euro \(=\) 1.3 dollars, find the set of lognormal parameters which
describe the distribution of Eiffel's losses in dollars.

\textbf{Solution.}

Let \(X\) and \(Y\) denote the aggregate losses of Eiffel Auto Insurance
in euro currency and dollars respectively. As \(Y = 1.3X\), we have, \[
F_{Y}\left( y \right) = \Pr\left( Y \leq y \right) = \Pr\left( 1.3X \leq y \right) = \Pr\left( X \leq \frac{y}{1.3} \right) = F_{X}\left( \frac{y}{1.3} \right).
\]

\(X\) follows a lognormal distribution with parameters \(\mu = 8\) and
\(\sigma = 2\). The probability density function of \(X\) is given by \[
f_{X}\left( x \right) = \frac{1}{x \sigma \sqrt{2\pi}}\exp \left\{- \frac{1}{2}\left( \frac{\ln x - \mu}{\sigma} \right)^{2}\right\} \ \ \ \text{for } x > 0.
\] As \(\left| \frac{dx}{dy} \right| = \frac{1}{1.3}\), the probability
density function of interest \(f_{Y}(y)\) is \[
f_{Y}\left( y \right) = \frac{1}{1.3}f_{X}\left( \frac{y}{1.3} \right) \\
= \frac{1}{1.3}\frac{1.3}{y \sigma \sqrt{2\pi}}\exp \left\{- \frac{1}{2}\left( \frac{\ln\left( y/1.3 \right) - \mu}{\sigma} \right)^{2}\right\} \\
= \frac{1}{y \sigma\sqrt{2\pi}}\exp \left\{- \frac{1}{2}\left( \frac{\ln y - \left( \ln 1.3 + \mu \right)}{\sigma} \right)^{2}\right\}.
\] Then \(Y\) follows a lognormal distribution with parameters
\(\ln 1.3 + \mu = 8.26\) and \(\sigma = 2.00\). If we let
\(\mu = ln(m)\) then it can be easily seen that \(m\)=\(e^{\mu}\) is the
scale parameter which was multiplied by 1.3 while \(\sigma\) is the
shape parameter that remained unchanged.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 3.3.2. Actuarial Exam Question.} Demonstrate that the
gamma distribution is a scale distribution.

\textbf{Solution.}

Let \(X\sim Ga(\alpha,\theta)\) and \(Y = cX\). As
\(\left| \frac{dx}{dy} \right| = \frac{1}{c}\), then \[
f_{Y}\left( y \right) = \frac{1}{c}f_{X}\left( \frac{y}{c} \right) = \frac{\left( \frac{y}{c\theta} \right)^{\alpha}}{y~\Gamma\left( \alpha \right)}\exp \left( - \frac{y}{c\theta} \right)  .
\] We can see that \(Y\sim Ga(\alpha,c\theta)\) indicating that gamma is
a scale distribution and \(\theta\) is a scale parameter.

Using the same approach you can demonstrate that other distributions
introduced in Section \ref{S:ContinuousDistn} are also scale
distributions. In actuarial modeling, working with a scale distribution
is very convenient because it allows to incorporate the effect of
inflation and to accommodate changes in the currency unit.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Raising to a Power}\label{raising-to-a-power}

In Section \ref{S:LS:Weibull} we talked about the flexibility of the
Weibull distribution in fitting reliability data. Looking to the origins
of the Weibull distribution, we recognize that the Weibull is a power
transformation of the exponential distribution. This is an application
of another type of transformation which involves raising the random
variable to a power.

Consider the transformation \(Y = X^{\tau}\), where \(\tau > 0\), then
the distribution function of \(Y\) is given by \[
F_{Y}\left( y \right) = \Pr\left( Y \leq y \right) = \Pr\left( X^{\tau} \leq y \right) = \Pr\left( X \leq y^{1/ \tau} \right) = F_{X}\left( y^{1/ \tau} \right).
\]

Hence, the probability density function of interest \(f_{Y}(y)\) can be
written as \[
f_{Y}(y) = \frac{1}{\tau} y^{1/ \tau - 1} f_{X}\left( y^{1/ \tau} \right).
\] On the other hand, if \(\tau < 0\), then the distribution function of
\(Y\) is given by \[
F_{Y}\left( y \right) = \Pr\left( Y \leq y \right) = \Pr\left( X^{\tau} \leq y \right) = \Pr\left( X \geq y^{1/ \tau} \right) = 1 - F_{X}\left( y^{1/ \tau} \right), 
\] and \[
f_{Y}(y) = \left| \frac{1}{\tau} \right|{y^{1/ \tau - 1}f}_{X}\left( y^{1/ \tau} \right).
\]

\textbf{Example 3.3.3.} We assume that \(X\) follows the exponential
distribution with mean \(\theta\) and consider the transformed variable
\(Y = X^{\tau}\). Show that \(Y\) follows the Weibull distribution when
\(\tau\) is positive and determine the parameters of the Weibull
distribution.

\textbf{Solution.}

As \(X\) follows the exponential distribution with mean \(\theta\), we
have \[
f_{X}(x) = \frac{1}{\theta}e^{- x/ \theta} \ \ \ \, x > 0.
\] Solving for \emph{x} yields \(x = y^{1/\tau}\). Taking the
derivative, we have \[
\left| \frac{dx}{dy} \right| = \frac{1}{\tau}{y^{\frac{1}{\tau}-1}}.
\] Thus, \[
f_{Y}\left( y \right) = \frac{1}{\tau}{y^{\frac{1}{\tau} - 1}f}_{X}\left( y^{\frac{1}{\tau}} \right) \\
= \frac{1}{\tau \theta }y^{\frac{1}{\tau} - 1}e^{- \frac{y^{\frac{1}{\tau}}}{\theta}} = \frac{\alpha}{\beta}\left( \frac{y}{\beta} \right)^{\alpha - 1}e^{- \left( y/ \beta \right)^{\alpha}}.
\] where \(\alpha = \frac{1}{\tau}\) and \(\beta = \theta^{\tau}\).
Then, \(Y\) follows the Weibull distribution with shape parameter
\(\alpha\) and scale parameter \(\beta\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Exponentiation}\label{exponentiation}

The normal distribution is a very popular model for a wide number of
applications and when the sample size is large, it can serve as an
approximate distribution for other models. If the random variable \(X\)
has a normal distribution with mean \(\mu\) and variance \(\sigma^{2}\),
then \(Y = e^{X}\) has lognormal distribution with parameters \(\mu\)
and \(\sigma^{2}\). The lognormal random variable has a lower bound of
zero, is positively skewed and has a long right tail. A lognormal
distribution is commonly used to describe distributions of financial
assets such as stock prices. It is also used in fitting claim amounts
for automobile as well as health insurance. This is an example of
another type of transformation which involves exponentiation.

In general, consider the transformation \(Y = e^{X}\). Then, the
distribution function of \(Y\) is given by
\[F_{Y}\left( y \right) = \Pr\left( Y \leq y \right) = \Pr\left( e^{X} \leq y \right) = \Pr\left( X \leq \ln y \right) = F_{X}\left( \ln y \right).\]
Taking derivatives, we see that the probability density function of
interest \(f_{Y}(y)\) can be written as \[
f_{Y}(y) = \frac{1}{y}f_{X}\left( \ln y \right).
\] As an important special case, suppose that \(X\) is normally
distributed with mean \(\mu\) and variance \(\sigma^2\). Then, the
distribution of \(Y = e^X\) is

\[
f_{Y}(y) = \frac{1}{y}f_{X}\left( \ln y \right)
= \frac{1}{y\sqrt{2 \pi}} \exp \left\{-\frac{1}{2}\left(\frac{ \ln y - \mu}{\sigma}\right)^2\right\}. 
\] This is known as a \emph{lognormal} distribution.

\textbf{Example 3.3.4. Actuarial Exam Question.} Assume that \(X\) has a
uniform distribution on the interval \((0,\ c)\) and define
\(Y = e^{X}\). Find the distribution of \(Y\).

\textbf{Solution.}

We begin with the cdf of \(Y\), \[
F_{Y}\left( y \right) = \Pr\left( Y \leq y \right) = \Pr\left( e^{X} \leq y \right) = \Pr\left( X \leq \ln y \right) = F_{X}\left( \ln y \right).
\] Taking the derivative, we have, \[
f_{Y}\left( y \right) = \frac{1}{y}f_{X}\left(\ln y \right) = \frac{1}{cy} .
\] Since \(0 < x < c\), then \(1 < y < e^{c}\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Finite Mixtures}\label{finite-mixtures}

Mixture distributions represent a useful way of modelling data that are
drawn from a heterogeneous population. This parent population can be
thought to be divided into multiple subpopulations with distinct
distributions.

\subsubsection{Two-point Mixture}\label{two-point-mixture}

If the underlying phenomenon is diverse and can actually be described as
two phenomena representing two subpopulations with different modes, we
can construct the two point mixture random variable \(X\). Given random
variables \(X_{1}\) and \(X_{2}\), with probability density functions
\(f_{X_{1}}\left( x \right)\) and \(f_{X_{2}}\left( x \right)\)
respectively, the probability density function of \(X\) is the weighted
average of the component probability density function
\(f_{X_{1}}\left( x \right)\) and \(f_{X_{2}}\left( x \right)\). The
probability density function and distribution function of \(X\) are
given by
\[f_{X}\left( x \right) = af_{X_{1}}\left( x \right) + \left( 1 - a \right)f_{X_{2}}\left( x \right),\]
and
\[F_{X}\left( x \right) = aF_{X_{1}}\left( x \right) + \left( 1 - a \right)F_{X_{2}}\left( x \right),\]

for \(0 < a <1\), where the mixing parameters \(a\) and \((1 - a)\)
represent the proportions of data points that fall under each of the two
subpopulations respectively. This weighted average can be applied to a
number of other distribution related quantities. The \emph{k}-th moment
and moment generating function of \(X\) are given by
\(\mathrm{E}\left( X^{k} \right) = a\mathrm{E}\left( X_{1}^{K} \right) + \left( 1 - a \right)\mathrm{E}\left( X_{2}^{k} \right)\),
and \[M_{X}(t) = aM_{X_{1}}(t) + \left( 1 - a \right)M_{X_{2}}(t),\]
respectively.

\textbf{Example 3.3.5. Actuarial Exam Question.} A collection of
insurance policies consists of two types. 25\% of policies are Type 1
and 75\% of policies are Type 2. For a policy of Type 1, the loss amount
per year follows an exponential distribution with mean 200, and for a
policy of Type 2, the loss amount per year follows a Pareto distribution
with parameters \(\alpha=3\) and \(\theta=200\). For a policy chosen at
random from the entire collection of both types of policies, find the
probability that the annual loss will be less than 100, and find the
average loss.

\textbf{Solution.}

The two types of losses are the random variables \(X_1\) and \(X_2\).
\(X_1\) has an exponential distribution with mean 100, so
\(F_{X_1}\left(100\right)=1-e^{-\frac{100}{200}}=0.393\). \(X_2\) has a
Pareto distribution with parameters \(\alpha=3\) and \(\theta=200\), so
\(F_{X_1}\left(100\right)=1-\left(\frac{200}{100+200}\right)^3=0.704\).
Hence,
\(F_X\left(100\right)=\left(0.25\times0.393\right)+\left(0.75\times0.704\right)=0.626\).

The average loss is given by
\[\mathrm{E}\left(X\right)=0.25\mathrm{E}\left(X_1\right)+0.75\mathrm{E}\left(X_2\right)=\left(0.25\times200\right)+\left(0.75\times100\right)=125\].

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{\texorpdfstring{\emph{k}-point
Mixture}{k-point Mixture}}\label{k-point-mixture}

In case of finite mixture distributions, the random variable of interest
\(X\) has a probability \(p_{i}\) of being drawn from homogeneous
subpopulation \(i\), where \(i = 1,2,\ldots,k\) and \(k\) is the
initially specified number of subpopulations in our mixture. The mixing
parameter \(p_{i}\) represents the proportion of observations from
subpopulation \(i\). Consider the random variable \(X\) generated from
\(k\) distinct subpopulations, where subpopulation \(i\) is modeled by
the continuous distribution \(f_{X_{i}}\left( x \right)\). The
probability distribution of \(X\) is given by
\[f_{X}\left( x \right) = \sum_{i = 1}^{k}{p_{i}f_{X_{i}}\left( x \right)},\]
where \(0 < p_{i} < 1\) and \(\sum_{i = 1}^{k} p_{i} = 1\).

This model is often referred to as a finite mixture or a \(k\)-point
mixture. The distribution function, \(r\)-th moment and moment
generating functions of the \(k\)-th point mixture are given as

\[F_{X}\left( x \right) = \sum_{i = 1}^{k}{p_{i}F_{X_{i}}\left( x \right)},\]
\[\mathrm{E}\left( X^{r} \right) = \sum_{i = 1}^{k}{p_{i}\mathrm{E}\left( X_{i}^{r} \right)}, \text{and}\]
\[M_{X}(t) = \sum_{i = 1}^{k}{p_{i}M_{X_{i}}(t)},\] respectively.

\textbf{Example 3.3.6. Actuarial Exam Question.} \(Y_{1}\) is a mixture
of \(X_{1}\) and \(X_{2}\) with mixing weights \(a\) and \((1 - a)\).
\(Y_{2}\) is a mixture of \(X_{3}\) and \(X_{4}\) with mixing weights
\(b\) and \((1 - b)\). \(Z\) is a mixture of \(Y_{1}\) and \(Y_{2}\)
with mixing weights \(c\) and \((1 - c)\).

Show that \(Z\) is a mixture of \(X_{1}\), \(X_{2}\), \(X_{3}\) and
\(X_{4}\), and find the mixing weights.

\textbf{Solution.} Applying the formula for a mixed distribution, we get
\[f_{Y_{1}}\left( x \right) = af_{X_{1}}\left( x \right) + \left( 1 - a \right)f_{X_{2}}\left( x \right)\]

\[f_{Y_{2}}\left( x \right) = bf_{X_{3}}\left( x \right) + \left( 1 - b \right)f_{X_{4}}\left( x \right)\]

\[f_{Z}\left( x \right) = cf_{Y_{1}}\left( x \right) + \left( 1 - c \right)f_{Y_{2}}\left( x \right)\]

Substituting the first two equations into the third, we get

\[f_{Z}\left( x \right) = c\left\lbrack af_{X_{1}}\left( x \right) + \left( 1 - a \right)f_{X_{2}}\left( x \right) \right\rbrack + \left( 1 - c \right)\left\lbrack bf_{X_{3}}\left( x \right) + \left( 1 - b \right)f_{X_{4}}\left( x \right) \right\rbrack\]

\[= caf_{X_{1}}\left( x \right) + c\left( 1 - a \right)f_{X_{2}}\left( x \right) + \left( 1 - c \right)bf_{X_{3}}\left( x \right) + (1 - c)\left( 1 - b \right)f_{X_{4}}\left( x \right)\].

Then, \(Z\) is a mixture of \(X_{1}\), \(X_{2}\), \(X_{3}\) and
\(X_{4}\), with mixing weights \(\text{ca}\), \(c\left( 1 - a \right)\),
\(\left( 1 - c \right)b\) and \((1 - c)\left( 1 - b \right)\),
respectively. It can be easily seen that the mixing weights sum to one.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Continuous Mixtures}\label{continuous-mixtures}

A mixture with a very large number of subpopulations (\(k\) goes to
infinity) is often referred to as a continuous mixture. In a continuous
mixture, subpopulations are not distinguished by a discrete mixing
parameter but by a continuous variable \(\Theta\), where \(\Theta\)
plays the role of \(p_{i}\) in the finite mixture. Consider the random
variable \(X\) with a distribution depending on a parameter \(\Theta\),
where \(\Theta\) itself is a continuous random variable. This
description yields the following model for \(X\) \[
f_{X}\left( x \right) = \int_{-\infty}^{\infty}{f_{X}\left(x \left| \theta \right.  \right)g_{\Theta}( \theta )} d \theta ,
\] where \(f_{X}\left( x | \theta \right)\) is the conditional
distribution of \(X\) at a particular value of \(\Theta=\theta\) and
\(g_{\Theta}\left( \theta \right)\) is the probability statement made
about the unknown parameter \(\theta\). In a Bayesian context (described
in Section \ref{S:MS:BayesInference}), this is known as the prior
distribution of \(\Theta\) (the prior information or expert opinion to
be used in the analysis).

The distribution function, \(k\)-th moment and moment generating
functions of the continuous mixture are given as \[
F_{X}\left( x \right) = \int_{-\infty}^{\infty}{F_{X}\left(x \left| \theta \right.  \right) g_{\Theta}(\theta)} d \theta,
\] \[
\mathrm{E}\left( X^{k} \right) = \int_{-\infty}^{\infty}{\mathrm{E}\left( X^{k}\left| \theta \right.  \right)g_{\Theta}(\theta)}d \theta,
\] \[
M_{X}(t) = \mathrm{E}\left( e^{t X} \right) = \int_{-\infty}^{\infty}{\mathrm{E}\left( e^{ tx}\left| \theta \right.  \right)g_{\Theta}(\theta)}d \theta, 
\] respectively.

The \(k\)-th moment of the mixture distribution can be rewritten as \[
\mathrm{E}\left( X^{k} \right) = \int_{-\infty}^{\infty}{\mathrm{E}\left( X^{k}\left| \theta \right.  \right)g_{\Theta}(\theta)}d\theta ~=~ \mathrm{E}\left\lbrack \mathrm{E}\left( X^{k}\left| \Theta \right.  \right) \right\rbrack .
\]

Using the law of iterated expectations (see Appendix Chapter
\ref{C:AppB}), we can define the mean and variance of \(X\) as \[
\mathrm{E}\left( X \right) = \mathrm{E}\left\lbrack \mathrm{E}\left( X\left| \Theta \right.  \right) \right\rbrack
\] and \[
\mathrm{Var}\left( X \right) = \mathrm{E}\left\lbrack \mathrm{Var}\left( X\left| \Theta \right.  \right) \right\rbrack + \mathrm{Var}\left\lbrack \mathrm{E}\left( X\left| \Theta \right.  \right) \right\rbrack .
\]

\textbf{Example 3.3.7. Actuarial Exam Question.} \(X\) has a normal
distribution with a mean of \(\Lambda\) and variance of 1. \(\Lambda\)
has a normal distribution with a mean of 1 and variance of 1. Find the
mean and variance of \(X\).

\textbf{Solution.}

X is a continuous mixture with mean

\[
\mathrm{E}\left(X\right)=\mathrm{E}\left[\mathrm{E}\left(X\middle|\Lambda\right)\right]=\mathrm{E}\left(\Lambda\right)=1 \text{ and } \mathrm{V}\left(X\right)=\mathrm{V}\left[\mathrm{E}\left(X\middle|\Lambda\right)\right]+\mathrm{E}\left[\mathrm{V}\left(X\middle|\Lambda\right)\right]=\mathrm{V}\left(\Lambda\right)+\mathrm{E}\left(1\right)=1+1=2.
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 3.3.8. Actuarial Exam Question.} Claim sizes, \(X\), are
uniform on the interval \(\left(\Theta,\Theta+10\right)\) for each
policyholder. \(\Theta\) varies by policyholder according to an
exponential distribution with mean 5. Find the unconditional
distribution, mean and variance of \(X\).

\textbf{Solution.}

The conditional distribution of \(X\) is
\(f_{X}\left( \left. \ x \right|\theta \right) = \frac{1}{10}\) for
\(\theta < x < \theta + 10\).

The prior distribution of \(\theta\) is
\(g_{\Theta}(\theta) = \frac{1}{5}e^{- \frac{\theta}{5}}\) for
\(0 < \theta < \infty\).

The conditional mean and variance of \(X\) are given by \[
\mathrm{E}\left( \left. \ X \right|\theta \right) = \frac{\theta + \theta + 10}{2} = \theta + 5
\] and \[
\mathrm{Var}\left( \left. \ X \right|\theta \right) = \frac{\left\lbrack \left( \theta + 10 \right) - \theta \right\rbrack^{2}}{12} = \frac{100}{12}, 
\] respectively.

Hence, the unconditional mean and variance of \(X\) are given by

\[
\mathrm{E}\left( X \right) = \mathrm{E}\left\lbrack \mathrm{E}\left( X\left| \Theta \right.  \right) \right\rbrack = \mathrm{E}\left( \Theta + 5 \right) = \mathrm{E}\left( \Theta \right) + 5 = 5 + 5 = 10,
\]

and

\[
\mathrm{Var}\left( X \right) = \mathrm{E}\left\lbrack V\left( X\left| \Theta \right.  \right) \right\rbrack + \mathrm{Var}\left\lbrack \mathrm{E}\left( X\left| \Theta \right.  \right) \right\rbrack \\
= \mathrm{E}\left( \frac{100}{12} \right) + \mathrm{Var}\left( \Theta + 5 \right) = 8.33 + \mathrm{Var}\left( \Theta \right) = 33.33. \]

The unconditional distribution of \(X\) is

\[
f_{X}\left( x \right) = \int f_{X}\left( x |\theta \right) ~g_{\Theta}(\theta) d \theta .
\]

\[
f_{X}\left( x \right) = \left\{ \begin{matrix}
\int_{0}^{x}{\frac{1}{50}e^{- \frac{\theta}{5}}d\theta = \frac{1}{10}\left( 1 - e^{- \frac{x}{5}} \right)} & 0 \leq x \leq 10, \\
\int_{x - 10}^{x}{\frac{1}{50}e^{- \frac{\theta}{5}} d\theta} = \frac{1}{10}\left( e^{- \frac{\left( x - 10 \right)}{5}} - e^{- \frac{x}{5}} \right) & 10 < x < \infty. \\
\end{matrix} \right.\ 
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{Coverage Modifications}\label{S:CoverageModifications}

In this section we evaluate the impacts of coverage modifications: a)
deductibles, b) policy limit, c) coinsurance and inflation on insurer's
costs.

\subsection{Policy Deductibles}\label{S:PolicyDeduct}

Under an ordinary deductible policy, the insured (policyholder) agrees
to cover a fixed amount of an insurance claim before the insurer starts
to pay. This fixed expense paid out of pocket is called the deductible
and often denoted by \(d\). If the loss exceeds \(d\) then the insurer
is responsible for covering the loss X less the deductible \(d\).
Depending on the agreement, the deductible may apply to each covered
loss or to the total losses during a defined benefit period (month,
year, etc.)

Deductibles eliminate a large number of small claims, reduce costs of
handling and processing these claims, reduce premiums for the
policyholders and reduce moral hazard. Moral hazard occurs when the
insured takes more risks, increasing the chances of loss due to perils
insured against, knowing that the insurer will incur the cost (e.g.~a
policyholder with collision insurance may be encouraged to drive
recklessly). The larger the deductible, the less the insured pays in
premiums for an insurance policy.

Let \(X\) denote the loss incurred to the insured and \(Y\) denote the
amount of paid claim by the insurer. Speaking of the benefit paid to the
policyholder, we differentiate between two variables: The payment per
loss and the payment per payment. The payment per loss variable, denoted
by \(Y^{L}\) or \((X-d)_+\) is left censored because values of \(X\)
that are less than \(d\) are not ignored but are set equal to zero. This
variable includes losses for which a payment is made as well as losses
less than the deductible and hence is defined as \[
Y^{L} = \left( X - d \right)_{+} 
= \left\{ \begin{array}{cc}
0 & X < d, \\
X - d & X > d  
\end{array} \right. .
\] \(Y^{L}\) is often referred to as left censored and shifted variable
because the values below \(d\) are not ignored and all losses are
shifted by a value \(d\).

On the other hand, the payment per payment variable, denoted by
\(Y^{P}\), is defined only when there is a payment. Specifically,
\(Y^P\) equals \(X-d\) on the event \(\{X >d\}\), denoted as
\(Y^P = X-d ||X>d\). Another way of expressing this that is commonly
used is \[
Y^{P} = \left\{ \begin{matrix}
\text{Undefined} & X \le d \\
X - d & X > d 
\end{matrix} . \right. 
\] Here, \(Y^{P}\) is often referred to as left truncated and shifted
variable or excess loss variable because the claims smaller than \(d\)
are not reported and values above \(d\) are shifted by \(d\).

Even when the distribution of \(X\) is continuous, the distribution of
\(Y^{L}\) is a hybrid combination of discrete and continuous components.
The discrete part of the distribution is concentrated at \(Y = 0\) (when
\(X \leq d\)) and the continuous part is spread over the interval
\(Y > 0\) (when \(X > d\)). For the discrete part, the probability that
no payment is made is the probability that losses fall below the
deductible; that is,
\[\Pr\left( Y^{L} = 0 \right) = \Pr\left( X \leq d \right) = F_{X}\left( d \right).\]
Using the transformation \(Y^{L} = X - d\) for the continuous part of
the distribution, we can find the probability density function of
\(Y^{L}\) given by \[f_{Y^{L}}\left( y \right) = \left\{ \begin{matrix}
F_{X}\left( d \right) & y = 0, \\
f_{X}\left( y + d \right) & y > 0 
\end{matrix} \right. \]

We can see that the payment per payment variable is the payment per loss
variable conditioned on the loss exceeding the deductible; that is,
\(Y^{P} = \left. \ Y^{L} \right|X > d\). Hence, the probability density
function of \(Y^{P}\) is given by
\[f_{Y^{P}}\left( y \right) = \frac{f_{X}\left( y + d \right)}{1 - F_{X}\left( d \right)},\]
for \(y > 0\). Accordingly, the distribution functions of \(Y^{L}\)and
\(Y^{P}\) are given by
\[F_{Y^{L}}\left( y \right) = \left\{ \begin{matrix}
F_{X}\left( d \right) & y = 0, \\
F_{X}\left( y + d \right) & y > 0. \\
\end{matrix} \right.\ \] and
\[F_{Y^{P}}\left( y \right) = \frac{F_{X}\left( y + d \right) - F_{X}\left( d \right)}{1 - F_{X}\left( d \right)},\]
for \(y > 0\), respectively.

The raw moments of \(Y^{L}\) and \(Y^{P}\) can be found directly using
the probability density function of \(X\) as follows
\[\mathrm{E}\left\lbrack \left( Y^{L} \right)^{k} \right\rbrack = \int_{d}^{\infty}\left( x - d \right)^{k}f_{X}\left( x \right)dx ,\]
and \[
\mathrm{E}\left\lbrack \left( Y^{P} \right)^{k} \right\rbrack = \frac{\int_{d}^{\infty}\left( x - d \right)^{k}f_{X}\left( x \right) dx }{{1 - F}_{X}\left( d \right)} = \frac{\mathrm{E}\left\lbrack \left( Y^{L} \right)^{k} \right\rbrack}{{1 - F}_{X}\left( d \right)},
\] respectively. For \(k=1\), we can use the survival function to
calculate \(\mathrm{E}(Y^L)\) as \[
\mathrm{E}(Y^L) = \int_d^{\infty} [1-F_X(x)] ~dx .
\] This could be easily proved if we start with the initial definition
of \(\mathrm{E}(Y^L)\) and use integration by parts.

We have seen that the deductible \(d\) imposed on an insurance policy is
the amount of loss that has to be paid out of pocket before the insurer
makes any payment. The deductible \(d\) imposed on an insurance policy
reduces the insurer's payment. The loss elimination ratio (LER) is the
percentage decrease in the expected payment of the insurer as a result
of imposing the deductible. \emph{LER} is defined as
\[LER = \frac{\mathrm{E}\left( X \right) - \mathrm{E}\left( Y^{L} \right)}{\mathrm{E}\left( X \right)}.\]

A little less common type of policy deductible is the franchise
deductible. The franchise deductible will apply to the policy in the
same way as ordinary deductible except that when the loss exceeds the
deductible \(d\), the full loss is covered by the insurer. The payment
per loss and payment per payment variables are defined as
\[Y^{L} = \left\{ \begin{matrix}
0 & X \leq d, \\
X & X > d, \\
\end{matrix} \right.\ \] and \[Y^{P} = \left\{ \begin{matrix}
\text{Undefined} & X \leq d, \\
X & X > d, \\
\end{matrix} \right.\ \] respectively.

\textbf{Example 3.4.1. Actuarial Exam Question.} A claim severity
distribution is exponential with mean 1000. An insurance company will
pay the amount of each claim in excess of a deductible of 100. Calculate
the variance of the amount paid by the insurance company for one claim,
including the possibility that the amount paid is 0.

\textbf{Solution.}

Let \(Y^{L}\) denote the amount paid by the insurance company for one
claim. \[Y^{L} = \left( X - 100 \right)_{+} = \left\{ \begin{matrix}
0 & X \leq 100, \\
X - 100 & X > 100. \\
\end{matrix} \right.\ \] The first and second moments of \(Y^{L}\) are
\[E\left( Y^{L} \right) = \int_{100}^{\infty}\left( x - 100 \right)f_{X}\left( x \right)dx \\
= {\int_{100}^{\infty}{S_{X}\left( x \right)}dx = 1000e}^{- \frac{100}{1000}},\]
and
\[E\left\lbrack \left( Y^{L} \right)^{2} \right\rbrack = \int_{100}^{\infty}\left( x - 100 \right)^{2}f_{X}\left( x \right)dx \\\\
= 2 \times 1000^{2}e^{- \frac{100}{1000}}.\] So,
\[\mathrm{Var}\left( Y^{L} \right) = \left( 2 \times 1000^{2}e^{- \frac{100}{1000}} \right) - \left( {1000e}^{- \frac{100}{1000}} \right)^{2} = 990,944.\]

An arguably simpler path to the solution is to make use of the
relationship between \(X\) and \(Y^{P}\). If \(X\) is exponentially
distributed with mean 1000, then \(Y^{P}\) is also exponentially
distributed with the same mean, because of the memoryless property of
the exponential distribution. Hence, \(E\left( Y^{P} \right)\)=1000 and
\[E\left\lbrack \left( Y^{P} \right)^{2} \right\rbrack = 2 \times 1000^{2}.\]
Using the relationship between \(Y^{L}\) and \(Y^{P}\) we find
\[E\left( Y^{L} \right) = \ E\left( Y^{P} \right)S_{X}\left( 100 \right){= 1000e}^{- \frac{100}{1000}}\]

\[E\left\lbrack \left( Y^{L} \right)^{2} \right\rbrack = E\left\lbrack \left( Y^{P} \right)^{2} \right\rbrack S_{X}\left( 100 \right) = 2 \times 1000^{2}e^{- \frac{100}{1000}}.\]

The relationship between \(X\) and \(Y^P\) can also be used when dealing
with the uniform or the Pareto distributions. You can easily show that
if \(X\) is uniform over the interval \(\left(0,\theta\right)\) then
\(Y^P\) is uniform over the interval \(\left(0,\theta-d\right)\) and if
\(X\) is Pareto with parameters \(\alpha\) and \(\theta\) then \(Y^P\)
is Pareto with parameters \(\alpha\) and \(\theta+d\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 3.4.2. Actuarial Exam Question.} For an insurance:

Losses have a density function
\[f_{X}\left( x \right) = \left\{ \begin{matrix}
    0.02x & 0 < x  < 10, \\
    0 & \text{elsewhere.} \\
    \end{matrix} \right. \]

The insurance has an ordinary deductible of 4 per loss.

\(Y^{P}\) is the claim payment per payment random variable.

Calculate \(\mathrm{E}\left( Y^{P} \right)\).

\textbf{Solution.}

We define \(Y^P\) as follows \[Y^{P} = \left\{ \begin{matrix}
\text{Undefined} & X \leq 4, \\
X - 4 & X > 4. \\
\end{matrix} \right.\ \] So,
\(E\left( Y^{P} \right) = \frac{\int_{4}^{10}\left( x - 4 \right)0.02xdx}{{1 - F}_{X}\left( 4 \right)} = \frac{2.88}{0.84} = 3.43\).

Note that we divide by \(S_X(4)=1-F_X(4)\), as this is the range where
the variable \(Y^P\) is defined.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 3.4.3. Actuarial Exam Question.} You are given:

Losses follow an exponential distribution with the same mean in all
years.

The loss elimination ratio this year is 70\%.

The ordinary deductible for the coming year is 4/3 of the current
deductible.

Compute the loss elimination ratio for the coming year.

\textbf{Solution.}

Let the losses \(X\sim Exp(\theta)\) and the deductible for the coming
year \(d' = \frac{4}{3}d\), the deductible of the current year. The
\emph{LER} for the current year is
\[\frac{E\left( X \right) - E\left( Y^{L} \right)}{E\left( X \right)} = \frac{\theta - \theta e^{- d / \theta}}{\theta} = 1 - e^{- d / \theta} = 0.7.\]
Then, \(e^{- d / \theta} = 0.3\).

The \emph{LER} for the coming year is

\begin{align*}
&\frac{\theta - \theta \exp(- \frac{d'}{\theta})}{\theta}=\frac{\theta - \theta \exp(- \frac{\left( \frac{4}{3}d \right)}{\theta})}{\theta} \\
&= 1 - \exp\left(- \frac{ \frac{4}{3} d }{\theta}\right) = 1 - \left( e^{-d /\theta} \right)^{4/3} = 1 - {0.3}^{4/3} = 0.8 .
\end{align*}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Policy Limits}\label{S:PolicyLimits}

Under a limited policy, the insurer is responsible for covering the
actual loss \(X\) up to the limit of its coverage. This fixed limit of
coverage is called the policy limit and often denoted by \(u\). If the
loss exceeds the policy limit, the difference \(X - u\) has to be paid
by the policyholder. While a higher policy limit means a higher payout
to the insured, it is associated with a higher premium.

Let \(X\) denote the loss incurred to the insured and \(Y\) denote the
amount of paid claim by the insurer. The variable \(Y\) known as the
\emph{limited loss variable} and denoted by \(X \land u\). It is a right
censored variable because values above \(u\) are set equal to \(u\). The
limited loss random variable \(Y\) is defined as \[
Y = X \land u = \left\{ \begin{matrix}
X & X \leq u, \\
u & X > u. \\
\end{matrix} \right.\ 
\] It can be seen that the distinction between \(Y^{L}\) and \(Y^{P}\)
is not needed under limited policy as the insurer will always make a
payment.

Using the definitions of
\(\left(X-d\right)_+ \text{ and } \left(X\land d\right)\), it can be
easily seen that the expected payment without any coverage modification,
\(X\), is equal to the sum of the expected payments with deductible
\(d\) and limit \(d\). That is,
\({X=\left(X-d\right)}_++ \left(X\land d\right)\).

When a loss is subject to a deductible \(d\) and a limit \(u\), the
per-loss variable \(Y^L\) is defined as \[
Y^{L} = \left\{ \begin{matrix}
0 & X \leq d, \\
X - d  & d <  X \leq u, \\
 u - d  & X > u. \\
\end{matrix} \right.\ 
\] Hence, \(Y^L\) can be expressed as
\(Y^L=\left(X\land u\right)-\left(X\land d\right)\).

Even when the distribution of \(X\) is continuous, the distribution of
\(Y\) is a hybrid combination of discrete and continuous components. The
discrete part of the distribution is concentrated at \(Y = u\) (when
\(X > u\)), while the continuous part is spread over the interval
\(Y < u\) (when \(X \leq u\)). For the discrete part, the probability
that the benefit paid is \(u\), is the probability that the loss exceeds
the policy limit \(u\); that is,
\[\Pr \left( Y = u \right) = \Pr \left( X > u \right) = {1 - F}_{X}\left( u \right).\]
For the continuous part of the distribution \(Y = X\), hence the
probability density function of \(Y\) is given by
\[f_{Y}\left( y \right) = \left\{ \begin{matrix}
f_{X}\left( y \right) & 0 < y < u, \\
1 - F_{X}\left( u \right) & y = u. \\
\end{matrix} \right.\ \] Accordingly, the distribution function of \(Y\)
is given by \[F_{Y}\left( y \right) = \left\{ \begin{matrix}
F_{X}\left( x \right) & 0 < y < u, \\
1 & y \geq u. \\
\end{matrix} \right.\ \] The raw moments of \(Y\) can be found directly
using the probability density function of \(X\) as follows \[
\mathrm{E}\left( Y^{k} \right) = \mathrm{E}\left\lbrack \left( X \land u \right)^{k} \right\rbrack = \int_{0}^{u}x^{k}f_{X}\left( x \right)dx + \int_{u}^{\infty}{u^{k}f_{X}\left( x \right)} dx \\ 
= \int_{0}^{u}x^{k}f_{X}\left( x \right)dx + u^{k}\left\lbrack {1 - F}_{X}\left( u \right) \right\rbrack .
\] For \(k=1\), we can use the survival function to calculate
\(\mathrm{E}\left( Y \right)\) as follows \[
\mathrm{E}\left( Y \right) = \mathrm{E}\left( X \land u  \right) 
= \int_{0}^{u} [1-F_{X}(x) ]dx .
\] This could be easily proved if we start with the initial definition
of \(\mathrm{E}\left( Y \right)\) and use integration by parts.

\textbf{Example 3.4.4. Actuarial Exam Question.} Under a group insurance
policy, an insurer agrees to pay 100\% of the medical bills incurred
during the year by employees of a small company, up to a maximum total
of one million dollars. The total amount of bills incurred, \(X\), has
probability density function
\[f_{X}\left( x \right) = \left\{ \begin{matrix}
\frac{x\left( 4 - x \right)}{9} & 0 < x < 3, \\
0 & \text{elsewhere.} \\
\end{matrix} \right.\ \] where \(x\) is measured in millions. Calculate
the total amount, in millions of dollars, the insurer would expect to
pay under this policy.

\textbf{Solution.}

Define the total amount of bills paid by the insurer as
\[Y = X \land 1 = \left\{ \begin{matrix}
X & X \leq 1, \\
1 & X > 1. \\
\end{matrix} \right.\ \] So
\(\mathrm{E}\left( Y \right) = \mathrm{E}\left( X \land 1 \right) = \int_{0}^{1}\frac{x^{2}(4 - x)}{9}dx + 1 * \int_{1}^{3}\frac{x\left( 4 - x \right)}{9}dx = 0.935\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Coinsurance and Inflation}\label{coinsurance-and-inflation}

As we have seen in Section \ref{S:PolicyDeduct}, the amount of loss
retained by the policyholder can be losses up to the deductible \(d\).
The retained loss can also be a percentage of the claim. The percentage
\(\alpha\), often referred to as the coinsurance factor, is the
percentage of claim the insurance company is required to cover. If the
policy is subject to an ordinary deductible and policy limit,
coinsurance refers to the percentage of claim the insurer is required to
cover, after imposing the ordinary deductible and policy limit. The
payment per loss variable, \(Y^{L}\), is defined as \[
Y^{L} = \left\{ \begin{matrix}
0 & X \leq d, \\
\alpha\left( X - d \right) & d <  X \leq u, \\
\alpha\left( u - d \right) & X > u. \\
\end{matrix} \right.\ 
\] The policy limit (the maximum amount paid by the insurer) in this
case is \(\alpha\left( u - d \right)\), while \(u\) is the maximum
covered loss.

We have seen in Section \ref{S:PolicyLimits} that when a loss is subject
to both a deductible \(d\) and a limit \(u\) the per-loss variable
\(Y^L\) can be expressed as
\(Y^L=\left(X\land u\right)-\left(X\land d\right)\). With coinsurance,
this becomes \(Y^L\) can be expressed as
\(Y^L=\alpha\left[(X\land u)-(X\land d)\right]\).

The \(k\)-th moment of \(Y^{L}\) is given by \[
\mathrm{E}\left\lbrack \left( Y^{L} \right)^{k} \right\rbrack 
= \int_{d}^{u}\left\lbrack \alpha\left( x - d \right) \right\rbrack^{k}f_{X}\left( x \right)dx 
+ \left\lbrack \alpha\left( u - d \right) \right\rbrack^{k} [1-F_{X}\left( u \right)] .
\]

A growth factor \(\left( 1 + r \right)\) may be applied to \(X\)
resulting in an inflated loss random variable \(\left( 1 + r \right)X\)
(the prespecified \emph{d} and \emph{u} remain unchanged). The resulting
per loss variable can be written as \[Y^{L} = \left\{ \begin{matrix}
0 & X \leq \frac{d}{1 + r}, \\
\alpha\left\lbrack \left( 1 + r \right)X - d \right\rbrack & \frac{d}{1 + r} <  X \leq \frac{u}{1 + r}, \\
\alpha\left( u - d \right) & X > \frac{u}{1 + r}. \\
\end{matrix} \right.\ \] The first and second moments of \(Y^{L}\) can
be expressed as
\[\mathrm{E}\left( Y^{L} \right) = \alpha\left( 1 + r \right)\left\lbrack \mathrm{E}\left( X \land \frac{u}{1 + r} \right) - \mathrm{E}\left( X \land \frac{d}{1 + r} \right) \right\rbrack,\]
and \[\mathrm{E}\left\lbrack \left( Y^{L} \right)^{2} 
\right\rbrack = \alpha^{2}\left( 1 + r \right)^{2}  \left\{ \mathrm{E}\left\lbrack \left( X \land \frac{u}{1 + r} \right)^{2} \right\rbrack - \mathrm{E}\left\lbrack \left( X \land \frac{d}{1 + r} \right)^{2} \right\rbrack  \right. \\
\left. \ \ \ \ \ - 2\left( \frac{d}{1 + r} \right)\left\lbrack \mathrm{E}\left( X \land \frac{u}{1 + r} \right) - \mathrm{E}\left( X \land \frac{d}{1 + r} \right) \right\rbrack \right\} ,\]
respectively.

The formulas given for the first and second moments of \(Y^{L}\) are
general. Under full coverage, \(\alpha = 1\), \(r = 0\), \(u = \infty\),
\(d = 0\) and \(\mathrm{E}\left( Y^{L} \right)\) reduces to
\(\mathrm{E}\left( X \right)\). If only an ordinary deductible is
imposed, \(\alpha = 1\), \(r = 0\), \(u = \infty\) and
\(\mathrm{E}\left( Y^{L} \right)\) reduces to
\(\mathrm{E}\left( X \right) - \mathrm{E}\left( X \land d \right)\). If
only a policy limit is imposed \(\alpha = 1\), \(r = 0\), \(d = 0\) and
\(\mathrm{E}\left( Y^{L} \right)\) reduces to
\(\mathrm{E}\left( X \land u \right)\).

\textbf{Example 3.4.5. Actuarial Exam Question.} The ground up loss
random variable for a health insurance policy in 2006 is modeled with
\emph{X}, an exponential distribution with mean 1000. An insurance
policy pays the loss above an ordinary deductible of 100, with a maximum
annual payment of 500. The ground up loss random variable is expected to
be 5\% larger in 2007, but the insurance in 2007 has the same deductible
and maximum payment as in 2006. Find the percentage increase in the
expected cost per payment from 2006 to 2007.

\textbf{Solution.}

We define the amount per loss \(Y^L\) in both years as
\[Y_{2006}^{L} = \left\{ \begin{matrix}
0 & X \leq 100, \\
X - 100 & 100 <  X \leq 600, \\
500 & X > 600. \\
\end{matrix} \right.\ \]

\[Y_{2007}^{L} = \left\{ \begin{matrix}
0 & X \leq 95.24, \\
1.05X - 100 & 95.24 <  X \leq 571.43, \\
500 & X > 571.43. \\
\end{matrix} \right.\ \]

So,

\[E\left( Y_{2006}^{L} \right) = E\left( X \land 600 \right) - E\left( X \land 100 \right) = 1000\left( {1 - e}^{- \frac{600}{1000}} \right) - 1000\left( {1 - e}^{- \frac{100}{1000}} \right)\]

\[= 356.026\].

\[E\left( Y_{2007}^{L} \right) = 1.05\left\lbrack E\left( X \land 571.43 \right) - E\left( X \land 95.24 \right) \right\rbrack\]

\[
= 1.05\left\lbrack 1000\left( {1 - e}^{- \frac{571.43}{1000}} \right) - 1000\left( {1 - e}^{- \frac{95.24}{1000}} \right) \right\rbrack
\]

\[=361.659\].

\(E\left( Y_{2006}^{P} \right) = \frac{356.026}{e^{- \frac{100}{1000}}} = 393.469\).

\(E\left( Y_{2007}^{P} \right) = \frac{361.659}{e^{- \frac{95.24}{1000}}} = 397.797\).

Because
\(\frac{E\left( Y_{2007}^{P} \right)}{E\left( Y_{2006}^{P} \right)} -1 = 0.011,\)
there is an increase of 1.1\% from 2006 to 2007. Due to the policy
limit, the cost per payment event grew by only 1.1\% between 2006 and
2007 even though the ground up losses increased by 5\% between the two
years.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Reinsurance}\label{S:Chap3Reinsurance}

In Section \ref{S:PolicyDeduct} we introduced the policy deductible,
which is a contractual arrangement under which an insured transfers part
of the risk by securing coverage from an insurer in return for an
insurance premium. Under that policy, the insured must pay all losses up
to the deductible, and the insurer only pays the amount (if any) above
the deductible. We now introduce reinsurance, a mechanism of insurance
for insurance companies. Reinsurance is a contractual arrangement under
which an insurer transfers part of the underlying insured risk by
securing coverage from another insurer (referred to as a reinsurer) in
return for a reinsurance premium. Although reinsurance involves a
relationship between three parties: the original insured, the insurer
(often referred to as cedent or cedant) and the reinsurer, the parties
of the reinsurance agreement are only the primary insurer and the
reinsurer. There is no contractual agreement between the original
insured and the reinsurer. Though many different types of reinsurance
contracts exist, a common form is excess of loss coverage. In such
contracts, the primary insurer must make all required payments to the
insured until the primary insurer's total payments reach a fixed
reinsurance deducible. The reinsurer is then only responsible for paying
losses above the reinsurance deductible. The maximum amount retained by
the primary insurer in the reinsurance agreement (the reinsurance
deductible) is called retention.

Reinsurance arrangements allow insurers with limited financial resources
to increase the capacity to write insurance and meet client requests for
larger insurance coverage while reducing the impact of potential losses
and protecting the insurance company against catastrophic losses.
Reinsurance also allows the primary insurer to benefit from underwriting
skills, expertise and proficient complex claim file handling of the
larger reinsurance companies.

\textbf{Example 3.4.6. Actuarial Exam Question.} Losses arising in a
certain portfolio have a two-parameter Pareto distribution with
\(\alpha=5\) and \(\theta=3,600\). A reinsurance arrangement has been
made, under which (a) the reinsurer accepts 15\% of losses up to
\(u=5,000\) and all amounts in excess of 5,000 and (b) the insurer pays
for the remaining losses.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Express the random variables for the reinsurer's and the insurer's
  payments as a function of \(X\), the portfolio losses.
\item
  Calculate the mean amount paid on a single claim by the insurer.
\item
  By assuming that the upper limit is \(u = \infty\), calculate an upper
  bound on the standard deviation of the amount paid on a single claim
  by the insurer (retaining the 15\% copayment).
\end{enumerate}

\textbf{Solution.}

a). The reinsurer's portion is

\[
Y_{reinsurer}  
= \left\{ \begin{array}{cc}
0.15 X & X < 5000, \\
0.15(5000) + X-5000 & X \ge 5000  
\end{array} \right. .
\]

and the insurer's portion is

\[
Y_{insurer}  
= \left\{ \begin{array}{cc}
0.85 X & X < 5000, \\
0.85(5000) & X \ge 5000  
\end{array} \right. = 0.85(X \wedge 5000).
\] b) Using the limited expected value tables for the Pareto
distribution, we have

\[
\mathrm{E}~Y_{insurer}  = 0.85~\mathrm{E}~(X \wedge 5000)= 0.85~\frac{\theta}{\alpha-1}\left[
1- \left(\frac{\theta}{5000+\theta}\right)^{\alpha-1}
\right] \\
= 0.85~\frac{3600}{5-1}\left[
1- \left(\frac{3600}{5000+3600}\right)^{5-1}\right] = 741.5103.
\]

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  For the first moment of the unlimited variable, we have
\end{enumerate}

\[
\mathrm{E}~Y_{insurer}(u=\infty)  = 0.85~\mathrm{E}~X = 0.85~\frac{\theta}{\alpha-1} = 0.85~\frac{3600}{5-1}  = 765. 
\] For the second moment of the unlimited variable, we use the table of
distributions to get

\[
\mathrm{E}~Y_{insurer}(u=\infty)^2  = 0.85^2~\mathrm{E}~X^2 = 0.85^2~\frac{\theta^2 \Gamma(2+1)\Gamma(\alpha-2)}{\Gamma(\alpha)} \\
= 0.85^2~\frac{3600^2 *2*2}{24} = 1560600.
\] Thus, the variance is \(1560600-765^2 =975375.\) Alternatively, you
can use the formula

\[
 0.85^2~\mathrm{Var}~X = 0.85^2~\frac{\alpha \theta^2}{(\alpha-1)^2(\alpha-2)} \\
= 0.85^2~\frac{5(3600^2)}{(5-1)^2(5-2)} = 975375 .
\]

Taking square roots, the standard deviation is
\(\sqrt{975375} \approx 987.6108\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Further discussions of reinsurance will be provided in Section
\ref{S:Reinsurance}.

\section{Maximum Likelihood Estimation}\label{S:MaxLikeEstimation}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Define a likelihood for a sample of observations from a continuous
  distribution
\item
  Define the maximum likelihood estimator for a random sample of
  observations from a continuous distribution
\item
  Estimate parametric distributions based on grouped, censored, and
  truncated data
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Maximum Likelihood Estimators for Complete
Data}\label{maximum-likelihood-estimators-for-complete-data}

Up to this point, the chapter has focused on parametric distributions
that are commonly used in insurance applications. However, to be useful
in applied work, these distributions must use ``realistic'' values for
the parameters and for this we turn to data. At a foundational level, we
assume that the analyst has available a random sample
\(X_1, \ldots, X_n\) from a distribution with distribution function
\(F_X\) (for brevity, we sometimes drop the subscript \(X\)). As is
common, we use the vector \(\boldsymbol \theta\) to denote the set of
parameters for \(F\).This basic sample scheme is reviewed in Appendix
Section \ref{S:AppA:BASIC}. Although basic, this sampling scheme
provides the foundations for understanding more complex schemes that are
regularly used in practice, and so it is important to master the basics.

Before a draw from a distribution, we consider potential outcomes
summarized by the random variable \(X_i\) (here, \(i\) is 1, 2,
\ldots{}, \(n\)). After the draw, we observe \(x_i\). Notationally, we
use upper case roman letters for random variables and lower case ones
for realizations. We have seen this set-up already in Section
\ref{S:estimating-frequency-distributions}, where we used
\(\Pr(X_1 =x_1, \ldots, X_n=x_n)\) to quantify the ``likelihood'' of
drawing a sample \(\{x_1, \ldots, x_n\}\). With continuous data, we use
the joint probability density function (\emph{pdf}) instead of joint
probabilities. With the independence assumption, the joint \emph{pdf}
may be written as the product of pdfs. Thus, we define the
\textbf{likelihood} to be

\begin{equation}
L(\boldsymbol \theta) = \prod_{i=1}^n f(x_i) .
  \label{eq:Likelihood}
\end{equation}

From the notation, note that we consider this to be a function of the
parameters in \(\boldsymbol \theta\), with the data
\(\{x_1, \ldots, x_n\}\) held fixed. The maximum likelihood estimator is
that value of the parameters in \(\boldsymbol \theta\) that maximize
\(L(\boldsymbol \theta)\).

From calculus, we know that maximizing a function produces the same
results as maximizing the logarithm of a function (this is because the
logarithm is a monotone, convex function). Because we get the same
results, to ease computation considerations, it is common to consider
the \textbf{logarithmic likelihood}, denoted as

\begin{equation}
l(\boldsymbol \theta) = \ln L(\boldsymbol \theta) = \sum_{i=1}^n \ln f(x_i) .
\label{eq:Loglikelihood}
\end{equation}

\textbf{Example 3.5.1. Actuarial Exam Question.} You are given the
following five observations: 521, 658, 702, 819, 1217. You use the
single-parameter Pareto with cumulative distribution function: \[
F(x) = 1- \left(\frac{500}{x}\right)^{\alpha}, ~~~~ x>500 .
\]

With \(n=5\), the log-likelihood function is \[
l(\alpha|\mathbf{x} ) =  \sum_{i=1}^5 \ln f(x_i;\alpha ) =  5 \alpha \ln 500 + 5 \ln \alpha
-(\alpha+1) \sum_{i=1}^5 \ln x_i.
\] Figure \ref{fig:LoglikeOnePareto} shows the logarithmic likelihood as
a function of the parameter \(\alpha\).

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/LoglikeOnePareto-1} 

}

\caption{Logarithmic Likelihood for a One Parameter Pareto}\label{fig:LoglikeOnePareto}
\end{figure}

We can determine the maximum value of the logarithmic likelihood by
taking derivatives and setting it equal to zero. This yields \[
\frac{ \partial}{\partial \alpha } l(\alpha |\mathbf{x}) =    5  \ln 500 + 5 / \alpha -  \sum_{i=1}^5 \ln x_i
=_{set} 0 \Rightarrow \hat{\alpha}_{MLE} = \frac{5}{\sum_{i=1}^5 \ln x_i - 5  \ln 500 } = 2.453 .
\]

Naturally, there are many problems where it is not practical to use hand
calculations for optimization. Fortunately there are many statistical
routines available such as the \texttt{R} function \texttt{optim}.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

This code confirms our hand calculation result where the maximum
likelihood estimator is \(\alpha_{MLE} =\) 2.453125.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

We present a few additional examples to illustrate how actuaries fit a
parametric distribution model to a set of claim data using maximum
likelihood.

\textbf{Example 3.5.2. Actuarial Exam Question.} Consider a random
sample of claim amounts: 8,000 10,000 12,000 15,000. You assume that
claim amounts follow an inverse exponential distribution, with parameter
\(\theta\). Calculate the maximum likelihood estimator for \(\theta\).

\textbf{Solution.}

The probability density function is
\[f_{X}\left( x \right) = \frac{\theta e^{- \frac{\theta}{x}}}{x^{2}}, \]
where \(x > 0\).

The likelihood function, \(L\left( \theta \right)\), can be viewed as
the probability of the observed data, written as a function of the
model's parameter \(\theta\)
\[L\left( \theta \right) = \prod_{i = 1}^{4}{f_{X_{i}}\left( x_{i} \right)} = \frac{\theta^{4}e^{- \theta\sum_{i = 1}^{4}\frac{1}{x_{i}}}}{\prod_{i = 1}^{4}x_{i}^{2}}.\]

The log-likelihood function, \(\ln L \left( \theta \right)\), is the sum
of the individual logarithms.
\[\ln L \left( \theta \right) = 4 \ln \theta - \theta\sum_{i = 1}^{4}\frac{1}{x_{i}} - 2\sum_{i = 1}^{4}\ln x_{i} .\]

\[
\frac{d \ln L \left( \theta \right)}{d \theta} = \frac{4}{\theta} - \sum_{i = 1}^{4}\frac{1}{x_{i}}.
\]

The maximum likelihood estimator of \(\theta\), denoted by
\(\hat{\theta}\), is the solution to the equation
\[\frac{4}{\hat{\theta}} - \sum_{i = 1}^{4}{\frac{1}{x_{i}} = 0}.\]
Thus,
\(\hat{\theta} = \frac{4}{\sum_{i = 1}^{4}\frac{1}{x_{i}}} = 10,667\)

The second derivative of \(\ln L \left( \theta \right)\) is given by
\[\frac{d^{2}\ln L\left( \theta \right)}{d\theta^{2}} = \frac{- 4}{\theta^{2}}.\]
Evaluating the second derivative of the loglikelihood function at
\(\hat{\theta} = 10,667\) gives a negative value, indicating
\(\hat{\theta}\) as the value that maximizes the loglikelihood function.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 3.5.3. Actuarial Exam Question.} A random sample of size
6 is from a lognormal distribution with parameters \(\mu\) and
\(\sigma\). The sample values are 200, 3,000, 8,000, 60,000, 60,000,
160,000. Calculate the maximum likelihood estimator for \(\mu\) and
\(\sigma\).

\textbf{Solution.}

The probability density function is
\[f_{X}\left( x \right) = \frac{1}{x \sigma \sqrt{2\pi}}\exp - \frac{1}{2}\left( \frac{\ln x - \mu}{\sigma} \right)^{2},\]
where \(x > 0\).

The likelihood function, \(L\left( \mu,\sigma \right)\), is the product
of the \emph{pdf} for each data point.
\[L\left( \mu,\sigma \right) = \prod_{i = 1}^{6}{f_{X_{i}}\left( x_{i} \right)} = \frac{1}{\sigma^{6}\left( 2\pi \right)^{3}\prod_{i = 1}^{6}x_{i}}exp - \frac{1}{2}\sum_{i = 1}^{6}\left( \frac{\ln x_{i} - \mu}{\sigma} \right)^{2}.\]
The loglikelihood function, \(\ln L \left( \mu,\sigma \right)\), is the
sum of the individual logarithms.
\[\ln \left( \mu,\sigma \right) = - 6 \ln \sigma - 3 \ln \left( 2\pi \right) - \sum_{i = 1}^{6}\ln x_{i} - \frac{1}{2}\sum_{i = 1}^{6}\left( \frac{\ln x_{i} - \mu}{\sigma} \right)^{2}.\]
The first partial derivatives are
\[\frac{\partial \ln L\left( \mu,\sigma \right)}{\partial\mu} = \frac{1}{\sigma^{2}}\sum_{i = 1}^{6}\left( \ln x_{i} - \mu \right).\]
\[\frac{\partial \ln L\left( \mu,\sigma \right)}{\partial\sigma} = \frac{- 6}{\sigma} + \frac{1}{\sigma^{3}}\sum_{i = 1}^{6}\left( \ln x_{i} - \mu \right)^{2}.\]
The maximum likelihood estimators of \(\mu\) and \(\sigma\), denoted by
\(\hat{\mu}\) and \(\hat{\sigma}\), are the solutions to the equations
\[\frac{1}{{\hat{\sigma}}^{2}}\sum_{i = 1}^{6}\left( \ln x_{i} - \hat{\mu} \right) = 0.\]
\[\frac{- 6}{\hat{\sigma}} + \frac{1}{{\hat{\sigma}}^{3}}\sum_{i = 1}^{6}\left( \ln x_{i} - \hat{\mu} \right)^{2} = 0.\]
These yield the estimates

\[\hat{\mu} = \frac{\sum_{i = 1}^{6}{\ln x_{i}}}{6} = 9.38 \ \ \ \text{and} \ \ \ 
{\hat{\sigma}}^{2} = \frac{\sum_{i = 1}^{6}\left( \ln x_{i} - \hat{\mu} \right)^{2}}{6} = 5.12 .
\].

The second partial derivatives are

\[
\frac{\partial^{2}\ln L\left( \mu,\sigma \right)}{\partial\mu^{2}} = \frac{- 6}{\sigma^{2}}, \ \ \ \ 
\frac{\partial^{2}\ln L\left( \mu,\sigma \right)}{\partial\mu\partial\sigma} = \frac{- 2}{\sigma^{3}}\sum_{i = 1}^{6}\left( \ln x_{i} - \mu \right)
\]

and

\[
\frac{\partial^{2}\ln L\left( \mu,\sigma \right)}{\partial\sigma^{2}} = \frac{6}{\sigma^{2}} - \frac{3}{\sigma^{4}}\sum_{i = 1}^{6}\left( \ln x_{i} - \mu \right)^{2}
\].

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Two follow-up questions rely on large sample properties that you may
have seen in an earlier course. Appendix Chapter \ref{C:AppC} reviews
the definition of the likelihood function, introduces its properties,
reviews the maximum likelihood estimators, extends their large-sample
properties to the case where there are multiple parameters in the model,
and reviews statistical inference based on maximum likelihood
estimators. In the solutions of these examples we derive the asymptotic
variance of maximum-likelihood estimators of the model parameters. We
use the delta method to derive the asymptotic variances of functions of
these parameters.

\textbf{Example 3.5.2 - Follow - Up.} Refer to \textbf{Example 3.5.2.}

Approximate the variance of the maximum likelihood estimator.

Determine an approximate 95\% confidence interval for \(\theta\).

Determine an approximate 95\% confidence interval for
\(\Pr \left( X \leq 9,000 \right).\)

\textbf{Solution.}

\textbf{a.} Taking reciprocal of negative expectation of the second
derivative of \(\ln L \left( \theta \right)\), we obtain an estimate of
the variance of \(\hat{\theta}\),
\(\widehat{Var}\left( \hat{\theta} \right) = \left. \ \left\lbrack E\left( \frac{d^{2}\ln L \left( \theta \right)}{d\theta^{2}} \right) \right\rbrack^{- 1} \right|_{\theta = \hat{\theta}} = \frac{{\hat{\theta}}^{2}}{4} = 28,446,222\).

It should be noted that as the sample size \(n \rightarrow \infty\), the
distribution of the maximum likelihood estimator \(\hat{\theta}\)
converges to a normal distribution with mean \(\theta\) and variance
\(\hat{V}\left( \hat{\theta} \right)\). The approximate confidence
interval in this example is based on the assumption of normality,
despite the small sample size, only for the purpose of illustration.

\textbf{b.} The 95\% confidence interval for \(\theta\) is given by

\[
10,667 \pm 1.96\sqrt{28,446,222} = \left( 213.34,\ 21,120.66 \right).
\]

\textbf{c.} The distribution function of \(X\) is
\(F\left( x \right) = 1 - e^{- \frac{x}{\theta}}\). Then, the maximum
likelihood estimate of \(g_{\Theta}(\theta) = F\left( 9,000 \right)\) is
\[g\left( \hat{\theta} \right) = 1 - e^{- \frac{9,000}{10,667}} = 0.57.\]
We use the delta method to approximate the variance of
\(g\left( \hat{\theta} \right)\).
\[\frac{\text{dg}\left( \theta \right)}{d \theta} = {- \frac{9,000}{\theta^{2}}e}^{- \frac{9,000}{\theta}}.\]

\(\widehat{Var}\left\lbrack g\left( \hat{\theta} \right) \right\rbrack = \left( - {\frac{9,000}{{\hat{\theta}}^{2}}e}^{- \frac{9,000}{\hat{\theta}}} \right)^{2}\hat{V}\left( \hat{\theta} \right) = 0.0329\).

The 95\% confidence interval for \(F\left( 9,000 \right)\) is given by
\[0.57 \pm 1.96\sqrt{0.0329} = \left( 0.214,\ 0.926 \right).\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 3.5.3 - Follow - Up.} Refer to \textbf{Example 3.5.3.}

Estimate the covariance matrix of the maximum likelihood estimator.

Determine approximate 95\% confidence intervals for \(\mu\) and
\(\sigma\).

Determine an approximate 95\% confidence interval for the mean of the
lognormal distribution.

\textbf{a.} To derive the covariance matrix of the mle we need to find
the expectations of the second derivatives. Since the random variable
\(X\) is from a lognormal distribution with parameters \(\mu\) and
\(\sigma\), then \(\text{lnX}\) is normally distributed with mean
\(\mu\) and variance \(\sigma^{2}\).

\(\mathrm{E}\left( \frac{\partial^{2}\text{lnL}\left( \mu,\sigma \right)}{\partial\mu^{2}} \right) = \mathrm{E}\left( \frac{- 6}{\sigma^{2}} \right) = \frac{- 6}{\sigma^{2}}\),

\(\mathrm{E}\left( \frac{\partial^{2}\text{lnL}\left( \mu,\sigma \right)}{\partial\mu\partial\sigma} \right) = \frac{- 2}{\sigma^{3}}\sum_{i = 1}^{6}{\mathrm{E}\left( \ln x_{i} - \mu \right)} = \frac{- 2}{\sigma^{3}}\sum_{i = 1}^{6}\left\lbrack \mathrm{E}\left( \ln x_{i} \right) - \mu \right\rbrack\)=\(\frac{- 2}{\sigma^{3}}\sum_{i = 1}^{6}\left( \mu - \mu \right) = 0\),

and

\(\mathrm{E}\left( \frac{\partial^{2}\text{lnL}\left( \mu,\sigma \right)}{\partial\sigma^{2}} \right) = \frac{6}{\sigma^{2}} - \frac{3}{\sigma^{4}}\sum_{i = 1}^{6}{\mathrm{E}\left( \ln x_{i} - \mu \right)}^{2} = \frac{6}{\sigma^{2}} - \frac{3}{\sigma^{4}}\sum_{i = 1}^{6}{\mathrm{V}\left( \ln x_{i} \right) = \frac{6}{\sigma^{2}} - \frac{3}{\sigma^{4}}\sum_{i = 1}^{6}{\sigma^{2} = \frac{- 12}{\sigma^{2}}}}\).

Using the negatives of these expectations we obtain the Fisher
information matrix \[\begin{bmatrix}
\frac{6}{\sigma^{2}} & 0 \\
0 & \frac{12}{\sigma^{2}} \\
\end{bmatrix}.\]

The covariance matrix, \(\Sigma\), is the inverse of the Fisher
information matrix \[\Sigma = \begin{bmatrix}
\frac{\sigma^{2}}{6} & 0 \\
0 & \frac{\sigma^{2}}{12} \\
\end{bmatrix}.\]

The estimated matrix is given by \[\hat{\Sigma} = \begin{bmatrix}
0.8533 & 0 \\
0 & 0.4267 \\
\end{bmatrix}.\]

\textbf{b.} The 95\% confidence interval for \(\mu\) is given by
\(9.38 \pm 1.96\sqrt{0.8533} = \left( 7.57,\ 11.19 \right)\).

The 95\% confidence interval for \(\sigma^{2}\) is given by
\(5.12 \pm 1.96\sqrt{0.4267} = \left( 3.84,\ 6.40 \right)\).

\textbf{c.} The mean of \emph{X} is
\(\exp\left( \mu + \frac{\sigma^{2}}{2} \right)\). Then, the maximum
likelihood estimate of
\[g\left( \mu,\sigma \right) = \exp\left( \mu + \frac{\sigma^{2}}{2} \right)\]
is
\[g\left( \hat{\mu},\hat{\sigma} \right) = \exp\left( \hat{\mu} + \frac{{\hat{\sigma}}^{2}}{2} \right) = 153,277.\]

We use the delta method to approximate the variance of the mle
\(g\left( \hat{\mu},\hat{\sigma} \right)\).

\(\frac{\partial g\left( \mu,\sigma \right)}{\partial\mu} = exp\left( \mu + \frac{\sigma^{2}}{2} \right)\)
and
\(\frac{\partial g\left( \mu,\sigma \right)}{\partial\sigma} = \sigma exp\left( \mu + \frac{\sigma^{2}}{2} \right)\).

Using the delta method, the approximate variance of
\(g\left( \hat{\mu},\hat{\sigma} \right)\) is given by

\[\left. \ \hat{V}\left( g\left( \hat{\mu},\hat{\sigma} \right) \right) = \begin{bmatrix}
\frac{\partial g\left( \mu,\sigma \right)}{\partial\mu} & \frac{\partial g\left( \mu,\sigma \right)}{\partial\sigma} \\
\end{bmatrix}\Sigma\begin{bmatrix}
\frac{\partial g\left( \mu,\sigma \right)}{\partial\mu} \\
\frac{\partial g\left( \mu,\sigma \right)}{\partial\sigma} \\
\end{bmatrix} \right|_{\mu = \hat{\mu},\sigma = \hat{\sigma}}\]

\[= \begin{bmatrix}
153,277 & 346,826 \\
\end{bmatrix}\begin{bmatrix}
0.8533 & 0 \\
0 & 0.4267 \\
\end{bmatrix}\begin{bmatrix}
153,277 \\
346,826 \\
\end{bmatrix} =\]71,374,380,000

The 95\% confidence interval for
\(\exp\left( \mu + \frac{\sigma^{2}}{2} \right)\) is given by

\(153,277 \pm 1.96\sqrt{71,374,380,000} = \left( - 370,356,\ 676,910 \right)\).

Since the mean of the lognormal distribution cannot be negative, we
should replace the negative lower limit in the previous interval by a
zero.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 3.5.4. Wisconsin Property Fund.} To see how maximum
likelihood estimators work with real data, we return to the 2010 claims
data introduced in Section \ref{S:LGPIF}.

The following snippet of code shows how to fit the exponential, gamma,
Pareto, lognormal, and GB2 models. For consistency, the code employs the
\texttt{R} package \texttt{VGAM}. The acronym stands for \emph{Vector
Generalized Linear and Additive Models}; as suggested by the name, this
package can do far more than fit these models although it suffices for
our purposes. The one exception is the GB2 density which is not widely
used outside of insurance applications; however, we can code this
density and compute maximum likelihood estimators using the
\texttt{optim} general purpose optimizer.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/MLECompare-1} 

}

\caption{Density Comparisons for the Wisconsin Property Fund}\label{fig:MLECompare}
\end{figure}

Results from the fitting exercise are summarized in Figure
\ref{fig:MLECompare}. Here, the black ``longdash'' curve is a smoothed
histogram of the actual data (that we will introduce in Section
\ref{S:MS:NonParInf}); the other curves are parametric curves where the
parameters are computed via maximum likelihood. We see poor fits in the
red dashed line from the exponential distribution fit and the blue
dotted line from the gamma distribution fit. Fits of the other curves,
Pareto, lognormal, and GB2, all seem to provide reasonably good fits to
the actual data. Chapter \ref{C:ModelSelection} describes in more detail
the principles of model selection.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Maximum Likelihood Estimators using Modified
Data}\label{S:Loss:MLEModified}

In many applications, actuaries and other analysts wish to estimate
model parameters based on individual data that are not limited. However,
there are also important applications when only limited, or
\emph{modified}, data are available. This section introduces maximum
likelihood estimation for grouped, censored, and truncated data. Later,
we will follow up with additional details in Section
\ref{S:MS:ModifiedData}.

\subsubsection{Maximum Likelihood Estimators for Grouped
Data}\label{MLEGrouped}

In the previous section we considered the maximum likelihood estimation
of continuous models from complete (individual) data. Each individual
observation is recorded, and its contribution to the likelihood function
is the density at that value. In this section we consider the problem of
obtaining maximum likelihood estimates of parameters from grouped data.
The observations are only available in grouped form, and the
contribution of each observation to the likelihood function is the
probability of falling in a specific group (interval). Let \(n_{j}\)
represent the number of observations in the interval
\(\left( \left. \ c_{j - 1},c_{j} \right\rbrack \right.\ \) The grouped
data likelihood function is thus given by \[
L\left( \theta \right) = \prod_{j = 1}^{k}\left\lbrack F_X\left( \left. \ c_{j} \right|\theta \right) - F_X\left( \left. \ c_{j - 1} \right|\theta \right) \right\rbrack^{n_{j}},
\] where \(c_{0}\) is the smallest possible observation (often set to
zero) and \(c_{k}\) is the largest possible observation (often set to
infinity).

\textbf{Example 3.5.5. Actuarial Exam Question.} For a group of
policies, you are given that losses follow the distribution function
\(F_X\left( x \right) = 1 - \frac{\theta}{x}\), for
\(\theta < x < \infty.\) Further, a sample of 20 losses resulted in the
following:

\[
{\small
\begin{matrix}\hline
\text{Interval} & \text{Number of Losses}  \\ \hline
(\theta, 10] & 9 \\
(10, 25] & 6 \\
(25, \infty) & 5  \\ \hline
\end{matrix}
}
\]

Calculate the maximum likelihood estimate of \(\theta\).

\textbf{Solution.}

The contribution of each of the 9 observations in the first interval to
the likelihood function is the probability of \(X \leq 10\); that is,
\(\Pr\left( X \leq 10 \right) = F_X\left( 10 \right)\). Similarly, the
contributions of each of 6 and 5 observations in the second and third
intervals are
\(\Pr\left( 10 < X \leq 25 \right) = F_X\left( 25 \right) - F_X(10)\)
and \(P\left( X > 25 \right) = 1 - F_X(25)\), respectively. The
likelihood function is thus given by \[
L\left( \theta \right) = \left\lbrack F_X\left( 10 \right) \right\rbrack^{9}\left\lbrack F_X\left( 25 \right) - F_X(10) \right\rbrack^{6}\left\lbrack 1 - F_X(25) \right\rbrack^{5}
\] \[
= {\left( 1 - \frac{\theta}{10} \right)}^{9}\left( \frac{\theta}{10} - \frac{\theta}{25} \right)^{6}\left( \frac{\theta}{25} \right)^{5}
\] \[
= {\left( \frac{10 - \theta}{10} \right)}^{9}\left( \frac{15\theta}{250} \right)^{6}\left( \frac{\theta}{25} \right)^{5}.
\] Then,
\(\ln L \left( \theta \right) = 9\ln \left( 10 - \theta \right) + 6\ln \theta + 5\ln \theta - 9\ln 10 + 6\ln 15 - 6\ln 250 - 5\ln 25\).
\[
\frac{d \ln L \left( \theta \right)}{d \theta} = \frac{- 9}{\left( 10 - \theta \right)} + \frac{6}{\theta} + \frac{5}{\theta}.
\] The maximum likelihood estimator, \(\hat{\theta}\), is the solution
to the equation \[
\frac{- 9}{\left( 10 - \hat{\theta} \right)} + \frac{11}{\hat{\theta}} = 0
\] and \(\hat{\theta} = 5.5\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Maximum Likelihood Estimators for Censored
Data}\label{maximum-likelihood-estimators-for-censored-data}

Another possible distinguishing feature of a data gathering mechanism is
censoring. While for some events of interest (losses, claims, lifetimes,
etc.) the complete data maybe available, for others only partial
information is available; all that may be known is that the observation
exceeds a specific value. The limited policy introduced in Section
\ref{S:PolicyLimits} is an example of right censoring. Any loss greater
than or equal to the policy limit is recorded at the limit. The
contribution of the censored observation to the likelihood function is
the probability of the random variable exceeding this specific limit.
Note that contributions of both complete and censored data share the
survivor function, for a complete point this survivor function is
multiplied by the hazard function, but for a censored observation it is
not. The likelihood function for censored data is then given by

\[
L(\theta) = \left[ \prod_{i=1}^r f_X(x_i) \right] \left[  S_X(u)  \right]^m ,
\] where \(r\) is the number of known loss amounts below the limit \(u\)
and \(m\) is the number of loss amounts larger than the limit \(u\).

\textbf{Example 3.5.6. Actuarial Exam Question.} The random variable
\(X\) has survival function: \[
S_{X}\left( x \right) = \frac{\theta^{4}}{\left( \theta^{2} + x^{2} \right)^{2}}.
\] Two values of \(X\) are observed to be 2 and 4. One other value
exceeds 4. Calculate the maximum likelihood estimate of \(\theta\).

\textbf{Solution.}

The contributions of the two observations 2 and 4 are
\(f_{X}\left( 2 \right)\) and \(f_{X}\left( 4 \right)\) respectively.
The contribution of the third observation, which is only known to exceed
4 is \(S_{X}\left( 4 \right)\). The likelihood function is thus given by
\[
L\left( \theta \right) = f_{X}\left( 2 \right)f_{X}\left( 4 \right)S_{X}\left( 4 \right).
\] The probability density function of \(X\) is given by \[
f_{X}\left( x \right) = \frac{4x\theta^{4}}{\left( \theta^{2} + x^{2} \right)^{3}}.
\] Thus, \[
L\left( \theta \right) = \frac{8\theta^{4}}{\left( \theta^{2} + 4 \right)^{3}}\frac{16\theta^{4}}{\left( \theta^{2} + 16 \right)^{3}}\frac{\theta^{4}}{\left( \theta^{2} + 16 \right)^{2}} \\
= \frac{128\theta^{12}}{\left( \theta^{2} + 4 \right)^{3}\left( \theta^{2} + 16 \right)^{5}},
\]

So, \[
\ln L\left( \theta \right) = \ln 128 + 12\ln \theta - 3\ln \left( \theta^{2} + 4 \right) - 5\ln \left( \theta^{2} + 16 \right) ,
\]

and

\(\frac{d \ln L\left( \theta \right)}{d \theta} = \frac{12}{\theta} - \frac{6\theta}{\left( \theta^{2} + 4 \right)} - \frac{10\theta}{\left( \theta^{2} + 16 \right)}\).

The maximum likelihood estimator, \(\hat{\theta}\), is the solution to
the equation \[
\frac{12}{\hat{\theta}} - \frac{6\hat{\theta}}{\left( {\hat{\theta}}^{2} + 4 \right)} - \frac{10\hat{\theta}}{\left( {\hat{\theta}}^{2} + 16 \right)} = 0
\] or
\[12\left( {\hat{\theta}}^{2} + 4 \right)\left( {\hat{\theta}}^{2} + 16 \right) - 6{\hat{\theta}}^{2}\left( {\hat{\theta}}^{2} + 16 \right) - 10{\hat{\theta}}^{2}\left( {\hat{\theta}}^{2} + 4 \right) = \\
- 4{\hat{\theta}}^{4} + 104{\hat{\theta}}^{2} + 768 = 0,\] which yields
\({\hat{\theta}}^{2} = 32\) and \(\hat{\theta} = 5.7\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Maximum Likelihood Estimators for Truncated
Data}\label{maximum-likelihood-estimators-for-truncated-data}

This section is concerned with the maximum likelihood estimation of the
continuous distribution of the random variable \(X\) when the data is
incomplete due to truncation. If the values of \(X\) are truncated at
\(d\), then it should be noted that we would not have been aware of the
existence of these values had they not exceeded \(d\). The policy
deductible introduced in Section \ref{S:PolicyDeduct} is an example of
left truncation. Any loss less than or equal to the deductible is not
recorded. The contribution to the likelihood function of an observation
\(x\) truncated at \(d\) will be a conditional probability and the
\(f_{X}\left( x \right)\) will be replaced by
\(\frac{f_{X}\left( x \right)}{S_{X}\left( d \right)}\). The likelihood
function for truncated data is then given by

\[
L(\theta) = \prod_{i=1}^k \frac{f_X(x_i)}{S_X(d)} ,
\] where \(k\) is the number of loss amounts larger than the deductible
\(d\).

\textbf{Example 3.5.7. Actuarial Exam Question.} For the single
parameter Pareto distribution with \(\theta = 2\), maximum likelihood
estimation is applied to estimate the parameter \(\alpha\). Find the
estimated mean of the ground up loss distribution based on the maximum
likelihood estimate of \(\alpha\) for the following data set:

Ordinary policy deductible of 5, maximum covered loss of 25 (policy
limit 20)

8 insurance payment amounts: 2, 4, 5, 5, 8, 10, 12, 15

2 limit payments: 20, 20.

\textbf{Solution.}

The contributions of the different observations can be summarized as
follows:

For the exact loss: \(f_{X}\left( x \right)\)

For censored observations: \(S_{X}\left( 25 \right)\).

For truncated observations:
\(\frac{f_{X}\left( x \right)}{S_{X}\left( 5 \right)}\).

Given that ground up losses smaller than 5 are omitted from the data
set, the contribution of all observations should be conditional on
exceeding 5. The likelihood function becomes \[
L\left( \alpha \right) = \frac{\prod_{i = 1}^{8}{f_{X}\left( x_{i} \right)}}{\left\lbrack S_{X}\left( 5 \right) \right\rbrack^{8}}\left\lbrack \frac{S_{X}\left( 25 \right)}{S_{X}\left( 5 \right)} \right\rbrack^{2}.
\] For the single parameter Pareto the probability density and
distribution functions are given by

\[
f_{X}\left( x \right) = \frac{\alpha\theta^{\alpha}}{x^{\alpha + 1}} \ \ \text{and} \ \ F_{X}\left( x \right) = 1 - \left( \frac{\theta}{x} \right)^{\alpha},
\] for \(x > \theta\), respectively. Then, the likelihood and
loglikelihood functions are given by \[
L\left( \alpha \right) = \frac{\alpha^{8}}{\prod_{i = 1}^{8}x_{i}^{\alpha + 1}}\frac{5^{10\alpha}}{25^{2\alpha}},
\] \[
\ln L \left( \alpha \right) = 8\ln\alpha - \left( \alpha + 1 \right)\sum_{i = 1}^{8}{\ln x_{i}} + 10\alpha \ln 5 - 2\alpha \ln 25.
\]

\(\frac{d \ln L \left( \alpha \right)}{d \theta} = \frac{8}{\alpha} - \sum_{i = 1}^{8}{\ln x_{i}} + 10\ln 5 - 2\ln 25\).

The maximum likelihood estimator, \(\hat{\alpha}\), is the solution to
the equation \[
\frac{8}{\hat{\alpha}} - \sum_{i = 1}^{8}{\ln x_{i}} + 10\ln 5 - 2\ln 25 = 0,
\] which yields \[
\hat{\alpha} = \frac{8}{\sum_{i = 1}^{8}{\ln x_{i}} - 10\ln 5 + 2\ln 25} = \frac{8}{(\ln 7 + \ln 9 + \cdots + \ln 20) - 10\ln 5 + 2\ln 25} = 0.785.
\] The mean of the Pareto only exists for \(\alpha > 1\). Since
\(\hat{\alpha} = 0.785 < 1\). Then, the mean does not exist.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{Further Resources and
Contributors}\label{LM-further-reading-and-resources}

\subsubsection*{Contributors}\label{contributors-2}
\addcontentsline{toc}{subsubsection}{Contributors}

\begin{itemize}
\tightlist
\item
  \textbf{Zeinab Amin}, The American University in Cairo, is the
  principal author of this chapter. Email:
  \href{mailto:zeinabha@aucegypt.edu}{\nolinkurl{zeinabha@aucegypt.edu}}
  for chapter comments and suggested improvements.
\item
  Many helpful comments have been provided by Hirokazu (Iwahiro)
  Iwasawa,
  \href{mailto:iwahiro@bb.mbn.or.jp}{\nolinkurl{iwahiro@bb.mbn.or.jp}} .
\item
  Other chapter reviewers include: Rob Erhardt, Jorge Yslas, Tatjana
  Miljkovic, and Samuel Kolins.
\end{itemize}

\subsubsection*{Exercises}\label{exercises}
\addcontentsline{toc}{subsubsection}{Exercises}

Here are a set of exercises that guide the viewer through some of the
theoretical foundations of \textbf{Loss Data Analytics}. Each tutorial
is based on one or more questions from the professional actuarial
examinations -- typically the Society of Actuaries Exam C.

\href{http://www.ssc.wisc.edu/~jfrees/loss-data-analytics/chapter-3-modeling-loss-severity/loss-data-analytics-severity-problems/}{Severity
Distribution Guided Tutorials}

\subsubsection*{Further Readings and
References}\label{further-readings-and-references}
\addcontentsline{toc}{subsubsection}{Further Readings and References}

Notable contributions include: \citet{cummins1991managing},
\citet{frees2008hierarchical}, \citet{klugman2012},
\citet{kreer2015goodness}, \citet{mcdonald1984some},
\citet{mcdonald1995generalization}, \citet{tevet2016applying}, and
\citet{venter1983transformed}.

\chapter{Model Selection and Estimation}\label{C:ModelSelection}

\emph{Chapter Preview}. Chapters \ref{C:Frequency-Modeling} and
\ref{C:Severity} have described how to fit parametric models to
frequency and severity data, respectively. This chapter begins with the
selection of models. To compare alternative parametric models, it is
helpful to summarize data without reference to a specific parametric
distribution. Section \ref{S:MS:NonParInf} describes nonparametric
estimation, how we can use it for model comparisons and how it can be
used to provide starting values for parametric procedures. The process
of model selection is then summarized in Section
\ref{S:MS:ModelSelection}. Although our focus is on continuous data, the
same process can be used for discrete data or data that come from a
hybrid combination of discrete and continuous data.

Model selection and estimation are fundamental aspects of statistical
modeling. To provide a flavor as to how they can be adapted to
alternative sampling schemes, Section \ref{S:MS:ModifiedData} describes
estimation for grouped, censored and truncated data (following the
Section \ref{S:MaxLikeEstimation} introduction). To see how they can be
adapted to alternative models, the chapter closes with Section
\ref{S:MS:BayesInference} on Bayesian inference, an alternative
procedure where the (typically unknown) parameters are treated as random
variables.

\section{Nonparametric Inference}\label{S:MS:NonParInf}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Estimate moments, quantiles, and distributions without reference to a
  parametric distribution
\item
  Summarize the data graphically without reference to a parametric
  distribution
\item
  Determine measures that summarize deviations of a parametric from a
  nonparametric fit
\item
  Use nonparametric estimators to approximate parameters that can be
  used to start a parametric estimation procedure
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Nonparametric Estimation}\label{nonparametric-estimation}

In Section \ref{S:basic-frequency-distributions} for frequency and
Section \ref{S:BasicQuantities} for severity, we learned how to
summarize a distribution by computing means, variances,
quantiles/percentiles, and so on. To approximate these summary measures
using a dataset, one strategy is to:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  assume a parametric form for a distribution, such as a negative
  binomial for frequency or a gamma distribution for severity,
\item
  estimate the parameters of that distribution, and then
\item
  use the distribution with the estimated parameters to calculate the
  desired summary measure.
\end{enumerate}

This is the parametric approach. Another strategy is to estimate the
desired summary measure directly from the observations \emph{without}
reference to a parametric model. Not surprisingly, this is known as the
nonparametric approach.

Let us start by considering the most basic type of sampling scheme and
assume that observations are realizations from a set of random variables
\(X_1, \ldots, X_n\) that are iid draws from an unknown population
distribution \(F(\cdot)\). An equivalent way of saying this is that
\(X_1, \ldots, X_n\), is a \emph{random sample} (with replacement) from
\(F(\cdot)\). To see how this works, we now describe nonparametric
estimators of many important measures that summarize a distribution.

\subsubsection{Moment Estimators}\label{S:MS:MomentEstimator}

We learned how to define moments in Section \ref{S:generating-functions}
for frequency and Section \ref{S:Chap3Moments} for severity. In
particular, the \(k\)-th moment, \(\mathrm{E~}[X^k] = \mu^{\prime}_k\),
summarizes many aspects of the distribution for different choices of
\emph{k}. Here, \(\mu^{\prime}_k\) is sometimes called the \emph{k}th
\emph{population} moment to distinguish it from the \emph{k}th sample
moment, \[
\frac{1}{n} \sum_{i=1}^n X_i^k ,
\] which is the corresponding nonparametric estimator. In typical
applications, \(k\) is a positive integer, although it need not be.

An important special case is the first moment where \emph{k=1}. In this
case, the prime symbol (\(\prime\)) and the \(1\) subscript are usually
dropped and one uses \(\mu=\mu^{\prime}_1\) to denote the population
mean, or simply the \emph{mean}. The corresponding sample estimator for
\(\mu\) is called the \emph{sample mean}, denoted with a bar on top of
the random variable: \[
\bar{X} =\frac{1}{n} \sum_{i=1}^n X_i .
\] Another type of summary measure of interest is the the \(k\)\emph{-th
central moment}, \(\mathrm{E~} [(X-\mu)^k] = \mu_k\). (Sometimes,
\(\mu^{\prime}_k\) is called the \(k\)-th \emph{raw} moment to
distinguish it from the central moment \(\mu_k\).). A nonparametric, or
sample, estimator of \(\mu_k\) is \[
\frac{1}{n} \sum_{i=1}^n \left(X_i - \bar{X}\right)^k .
\] The second central moment (\(k=2\)) is an important case for which we
typically assign a new symbol, \(\sigma^2 = \mathrm{E~} [(X-\mu)^2]\),
known as the \emph{variance}. Properties of sample moment estimator of
the variance such as \(n^{-1}\sum_{i=1}^n \left(X_i - \bar{X}\right)^2\)
have been studied extensively and so it is natural that many variations
have been proposed. The most widely used variation is one where the
effective sample size is reduced by one, and so we define \[
s^2 = \frac{1}{n-1} \sum_{i=1}^n \left(X_i - \bar{X}\right)^2.
\] Here, the statistic \(s^2\) known as the \emph{sample variance}.
Dividing by \emph{n-1} instead of \emph{n} matters little when you have
a sample size \emph{n} in the thousands as is common in insurance
applications. Still, the resulting estimator is unbiased in the sense
that \(\mathrm{E~} s^2 = \sigma^2\), a desirable property particularly
when interpreting results of an analysis.

\subsubsection{Empirical Distribution
Function}\label{empirical-distribution-function}

We have seen how to compute nonparametric estimators of the \emph{k}th
moment \(\mathrm{E~} X^k\). In the same way, for any known function
\(\mathrm{g}(\cdot)\), we can estimate \(\mathrm{E~} \mathrm{g}(X)\)
using \(n^{-1}\sum_{i=1}^n \mathrm{g}(X_i)\).

Now suppose that we fix a value of \emph{x} and consider the function
\(\mathrm{g}(X) = I(X \le x)\). Here, the notation \(I(\cdot)\) is the
indicator function; it returns 1 if the event \((\cdot)\) is true and 0
otherwise. For this choice of \(\mathrm{g}(\cdot)\), the expected value
is \(\mathrm{E~} I(X \le x) = \Pr(X \le x) = F(x)\), the distribution
function evaluated at a fixed point \emph{x}. Using the analog
principle, we define the nonparametric estimator of the distribution
function

\[
\begin{aligned}
F_n(x)
&=  \frac{1}{n} \sum_{i=1}^n I\left(X_i \le x\right) \\
&=  \frac{\text{number of observations less than or equal to }x}{n} . 
\end{aligned}
\] As a nonparametric estimator, \(F_n(\cdot)\) is based on only
observations and does not assume a parametric family for the
distribution, it is also known as the empirical distribution function.

\textbf{Example 4.1.1. Toy Data Set}. To illustrate, consider a
fictitious, or ``toy,'' data set of \(n=10\) observations. Determine the
empirical distribution function.

\[
{\small
\begin{array}{c|cccccccccc}
\hline
i &1&2&3&4&5&6&7&8&9&10 \\
X_i& 10 &15 &15 &15 &20 &23 &23 &23 &23 &30\\
\hline
\end{array}
}
\]

You should check that the sample mean is \(\bar{X} = 19.7\) and that the
sample variance is \(s^2 =34.45556\). The corresponding empirical
distribution function is

\[
\begin{aligned}
F_n(x) &=
\left\{
\begin{array}{ll}
0 & \text{ for }\ x<10 \\
0.1 & \text{ for }\ 10 \leq x<15 \\
0.4 & \text{ for }\ 15 \leq x<20 \\
0.5 & \text{ for }\ 20 \leq x<23 \\
0.9 & \text{ for }\ 23 \leq x<30 \\
1 & \text{ for }\ x \geq 30,
\end{array}
\right.\end{aligned}
\]

which is shown in the following graph in Figure \ref{fig:EDFToy}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{LossDataAnalytics_files/figure-latex/EDFToy-1} 

}

\caption{Empirical Distribution Function of a Toy Example}\label{fig:EDFToy}
\end{figure}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Quartiles, Percentiles and
Quantiles}\label{S:MS:QuantileEstimator}

We have already seen the median, which is the number such that
approximately half of a data set is below (or above) it. The first
quartile is the number such that approximately 25\% of the data is below
it and the third quartile is the number such that approximately 75\% of
the data is below it. A \(100p\) percentile is the number such that
\(100 \times p\) percent of the data is below it.

To generalize this concept, consider a distribution function
\(F(\cdot)\), which may or may not be continuous, and let \(q\) be a
fraction so that \(0<q<1\). We want to define a quantile, say \(q_F\),
to be a number such that \(F(q_F) \approx q\). Notice that when
\(q = 0.5\), \(q_F\) is the median; when \(q = 0.25\), \(q_F\) is the
first quartile, and so on. So, a quantile generalizes the concepts of
median, quartiles, and percentiles.

To be precise, for a given \(0<q<1\), define the \(q\)\textbf{th
quantile} \(q_F\) to be \emph{any} number that satisfies

\begin{equation}
  F(q_F-) \le q \le F(q_F)
  \label{eq:Quantile}
\end{equation}

Here, the notation \(F(x-)\) means to evaluate the function \(F(\cdot)\)
as a left-hand limit.

To get a better understanding of this definition, let us look at a few
special cases. First, consider the case where \(X\) is a continuous
random variable so that the distribution function \(F(\cdot)\) has no
jump points, as illustrated in Figure \ref{fig:Quantile1}. In this
figure, a few fractions, \(q_1\), \(q_2\), and \(q_3\) are shown with
their corresponding quantiles \(q_{F,1}\), \(q_{F,2}\), and \(q_{F,3}\).
In each case, it can be seen that \(F(q_F-)= F(q_F)\) so that there is a
unique quantile. Because we can find a unique inverse of the
distribution function at any \(0<q<1\), we can write \(q_F= F^{-1}(q)\).

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{LossDataAnalytics_files/figure-latex/Quantile1-1} 

}

\caption{Continuous Quantile Case}\label{fig:Quantile1}
\end{figure}

Figure \ref{fig:Quantile2} shows three cases for distribution functions.
The left panel corresponds to the continuous case just discussed. The
middle panel displays a jump point similar to those we already saw in
the empirical distribution function of Figure \ref{fig:EDFToy}. For the
value of \(q\) shown in this panel, we still have a unique value of the
quantile \(q_F\). Even though there are many values of \(q\) such that
\(F(q_F-) \le q \le F(q_F)\), for a particular value of \(q\), there is
only one solution to equation \eqref{eq:Quantile}. The right panel depicts
a situation in which the quantile cannot be uniquely determined for the
\(q\) shown as there is a range of \(q_F\)'s satisfying equation
\eqref{eq:Quantile}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{LossDataAnalytics_files/figure-latex/Quantile2-1} 

}

\caption{Three Quantile Cases}\label{fig:Quantile2}
\end{figure}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.1.2. Toy Data Set: Continued.} Determine quantiles
corresponding to the 20th, 50th, and 95th percentiles.

\textbf{Solution}. Consider Figure \ref{fig:EDFToy}. The case of
\(q=0.20\) corresponds to the middle panel, so the 20th percentile is
15. The case of \(q=0.50\) corresponds to the right panel, so the median
is any number between 20 and 23 inclusive. Many software packages use
the average 21.5 (e.g. \texttt{R}, as seen below). For the 95th
percentile, the solution is 30. We can see from the graph that 30 also
corresponds to the 99th and the 99.99th percentiles.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{quantile}\NormalTok{(xExample, }\DataTypeTok{probs=}\KeywordTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.95}\NormalTok{), }\DataTypeTok{type=}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  20%  50%  95% 
## 15.0 21.5 30.0
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

By taking a weighted average between data observations, smoothed
empirical quantiles can handle cases such as the right panel in Figure
\ref{fig:Quantile2}. The \(q\)th smoothed empirical quantile is defined
as \[
\hat{\pi}_q = (1-h) X_{(j)} + h X_{(j+1)}
\] where \(j=\lfloor(n+1)q\rfloor\), \(h=(n+1)q-j\), and
\(X_{(1)}, \ldots, X_{(n)}\) are the ordered values (known as the
\emph{order statistics}) corresponding to \(X_1, \ldots, X_n\). Note
that \(\hat{\pi}_q\) is simply a linear interpolation between
\(X_{(j)}\) and \(X_{(j+1)}\).

\textbf{Example 4.1.3. Toy Data Set: Continued.} Determine the 50th and
20th smoothed percentiles.

\textbf{Solution} Take \(n=10\) and \(q=0.5\). Then,
\(j=\lfloor(11)0.5 \rfloor= \lfloor5.5 \rfloor=5\) and
\(h=(11)(0.5)-5=0.5\). Then the 0.5-th smoothed empirical quantile is
\[\hat{\pi}_{0.5} = (1-0.5) X_{(5)} + (0.5) X_{(6)} = 0.5 (20) + (0.5)(23) = 21.5.\]
Now take \(n=10\) and \(q=0.2\). In this case,
\(j=\lfloor(11)0.2\rfloor=\lfloor 2.2 \rfloor=2\) and
\(h=(11)(0.2)-2=0.2\). Then the 0.2-th smoothed empirical quantile is
\[\hat{\pi}_{0.2} = (1-0.2) X_{(2)} + (0.2) X_{(3)} = 0.2 (15) + (0.8)(15) = 15.\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Density Estimators}\label{density-estimators}

\textbf{Discrete Variable.} When the random variable is discrete,
estimating the probability mass function \(f(x) = \Pr(X=x)\) is
straightforward. We simply use the sample average, defined to be

\[f_n(x) = \frac{1}{n} \sum_{i=1}^n I(X_i = x).\]

\textbf{Continuous Variable within a Group.} For a continuous random
variable, consider a discretized formulation in which the domain of
\(F(\cdot)\) is partitioned by constants
\(\{c_0 < c_1 < \cdots < c_k\}\) into intervals of the form
\([c_{j-1}, c_j)\), for \(j=1, \ldots, k\). The data observations are
thus ``grouped'' by the intervals into which they fall. Then, we might
use the basic definition of the empirical mass function, or a variation
such as
\[f_n(x) = \frac{n_j}{n \times (c_j - c_{j-1})}  \ \ \ \ \ \ c_{j-1} \le x < c_j,\]
where \(n_j\) is the number of observations (\(X_i\)) that fall into the
interval \([c_{j-1}, c_j)\).

\textbf{Continuous Variable (not grouped).} Extending this notion to
instances where we observe individual data, note that we can always
create arbitrary groupings and use this formula. More formally, let
\(b>0\) be a small positive constant, known as a bandwidth, and define a
density estimator to be

\begin{equation}
  f_n(x) = \frac{1}{2nb} \sum_{i=1}^n I(x-b < X_i \le x + b)
  \label{eq:KDF}
\end{equation}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The idea is that the estimator \(f_n(x)\) in equation \eqref{eq:KDF} is
the average over \(n\) \emph{iid} realizations of a random variable with
mean

\[
\begin{aligned}
\mathrm{E~ } \frac{1}{2b} I(x-b < X \le x + b) &=  \frac{1}{2b}\left(F(x+b)-F(x-b)\right) \\
&=  \frac{1}{2b} \left( \left\{ F(x) + b F^{\prime}(x) + b^2 C_1\right\}
\left\{ F(x) - b F^{\prime}(x) + b^2 C_2\right\} \right) \\
&=  F^{\prime}(x) + b \frac{C_1-C_2}{2} \rightarrow  F^{\prime}(x) = f(x),
\end{aligned}
\]

as \(b\rightarrow 0\). That is, \(f_n(x)\) is an asymptotically unbiased
estimator of \(f(x)\) (its expectation approaches the true value as
sample size increases to infinity). This development assumes some
smoothness of \(F(\cdot)\), in particular, twice differentiability at
\(x\), but makes no assumptions on the form of the distribution function
\(F\). Because of this, the density estimator \(f_n\) is said to be
\emph{nonparametric}.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

More generally, define the kernel density estimator of the pdf at
\emph{x} as

\begin{equation} 
  f_n(x) = \frac{1}{nb} \sum_{i=1}^n w\left(\frac{x-X_i}{b}\right) ,
  \label{eq:kernelDens}
\end{equation}

where \(w\) is a probability density function centered about 0. Note
that equation \eqref{eq:KDF} simply becomes the kernel density estimator
where \(w(x) = \frac{1}{2}I(-1 < x \le 1)\), also known as the
\emph{uniform kernel}. Other popular choices are shown in
\protect\hyperlink{tab:41}{Table 4.1}.

\[
{\small
\begin{matrix}
\text{Table 4.1: Popular Choices for the Kernel Density Estimator}\\
\begin{array}{l|cc}
\hline
\text{Kernel} &  w(x) \\
\hline
\text{Uniform } &  \frac{1}{2}I(-1 < x \le 1) \\
\text{Triangle} &  (1-|x|)\times I(|x| \le 1) \\
\text{Epanechnikov} & \frac{3}{4}(1-x^2) \times I(|x| \le 1) \\
\text{Gaussian} & \phi(x) \\
\hline
\end{array}\end{matrix}
}
\]

Here, \(\phi(\cdot)\) is the standard normal density function. As we
will see in the following example, the choice of bandwidth \(b\) comes
with a bias-variance tradeoff between matching local distributional
features and reducing the volatility.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.1.4. Property Fund.} Figure \ref{fig:Density2} shows a
histogram (with shaded gray rectangles) of logarithmic property claims
from 2010. The (blue) thick curve represents a Gaussian kernel density
where the bandwidth was selected automatically using an ad hoc rule
based on the sample size and volatility of the data. For this dataset,
the bandwidth turned out to be \(b=0.3255\). For comparison, the (red)
dashed curve represents the density estimator with a bandwidth equal to
0.1 and the green smooth curve uses a bandwidth of 1. As anticipated,
the smaller bandwidth (0.1) indicates taking local averages over less
data so that we get a better idea of the local average, but at the price
of higher volatility. In contrast, the larger bandwidth (1) smooths out
local fluctuations, yielding a smoother curve that may miss
perturbations in the local average. For actuarial applications, we
mainly use the kernel density estimator to get a quick visual impression
of the data. From this perspective, you can simply use the default ad
hoc rule for bandwidth selection, knowing that you have the ability to
change it depending on the situation at hand.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{LossDataAnalytics_files/figure-latex/Density2-1} 

}

\caption{Histogram of Logarithmic Property Claims with Superimposed Kernel Density Estimators}\label{fig:Density2}
\end{figure}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Nonparametric density estimators, such as the kernel estimator, are
regularly used in practice. The concept can also be extended to give
smooth versions of an empirical distribution function. Given the
definition of the kernel density estimator, the \emph{kernel estimator
of the distribution function} can be found as

\[
\begin{aligned}
\hat{F}_n(x) = \frac{1}{n} \sum_{i=1}^n W\left(\frac{x-X_i}{b}\right).\end{aligned}
\]

where \(W\) is the distribution function associated with the kernel
density \(w\). To illustrate, for the uniform kernel, we have
\(w(y) = \frac{1}{2}I(-1 < y \le 1)\), so

\[
\begin{aligned}
W(y) =
\begin{cases}
0 &            y<-1\\
\frac{y+1}{2}& -1 \le y < 1 \\
1 & y \ge 1 \\
\end{cases}\end{aligned} .
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.1.5. Actuarial Exam Question.}

You study five lives to estimate the time from the onset of a disease to
death. The times to death are:

\[
\begin{array}{ccccc}
2 & 3 & 3 & 3 & 7  \\
\end{array}
\]

Using a triangular kernel with bandwith \(2\), calculate the density
function estimate at 2.5.

\textbf{Solution.} For the kernel density estimate, we have
\[f_n(x) = \frac{1}{nb} \sum_{i=1}^n w\left(\frac{x-X_i}{b}\right),\]
where \(n=5\), \(b=2\), and \(x=2.5\). For the triangular kernel,
\(w(x) = (1-|x|)\times I(|x| \le 1)\). Thus,

\[
\begin{array}{c|c|c}
\hline
X_i & \frac{x-X_i}{b} & w\left(\frac{x-X_i}{b} \right) \\
\hline
2 & \frac{2.5-2}{2}=\frac{1}{4} &  (1-\frac{1}{4})(1) = \frac{3}{4} \\
\hline
3 & & \\
3 & \frac{2.5-3}{2}=\frac{-1}{4} & \left(1-\left| \frac{-1}{4} \right| \right)(1) = \frac{3}{4} \\
3 & & \\
\hline
7 & \frac{2.5-7}{2}=-2.25 & (1-|-2.25|)(0) = 0\\
\hline
\end{array}
\]

Then the kernel density estimate is
\[f_n(x) = \frac{1}{5(2)}\left( \frac{3}{4} + (3) \frac{3}{4} + 0 \right) = \frac{3}{10}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Plug-in Principle}\label{plug-in-principle}

One way to create a nonparametric estimator is to use the \emph{analog}
or plug-in principle where one replaces the unknown cdf \(F\) with a
known estimate such as the empirical \emph{cdf} \(F_n\). So, if we are
trying to estimate
\(\mathrm{E}~\mathrm{g}(X)=\mathrm{E}_F~\mathrm{g}(X)\) for a generic
function \emph{g}, then we define a nonparametric estimator to be
\(\mathrm{E}_{F_n}~\mathrm{g}(X)=n^{-1}\sum_{i=1}^n \mathrm{g}(X_i)\).

To see how this works, as a special case of \emph{g} we consider the
\emph{loss elimination ratio} introduced in Section 3.4.1, \[
LER(d) = \frac{\mathrm{E~}(\min(X,d) )}{\mathrm{E~}(X)}
\] for a fixed deductible \(d\).

\textbf{Example. 4.1.11. Bodily Injury Claims and Loss Elimination
Ratios}

We use a sample of 432 closed auto claims from Boston from
\citet{derrig2001applications}. Losses are recorded for payments due to
bodily injuries in auto accidents. Losses are not subject to deductibles
but are subject to various policy limits also available in the data. It
turns out that only 17 out of 432 (\(\approx\) 4\%) were subject to
policy limit and so we ignore these data for this illustration.

The average loss paid is 6906. Figure \ref{fig:BIClaims} shows other
aspects of the distribution. Specifically, the left-hand panel shows the
empirical distribution function, the right-hand panel gives a
nonparametric density plot.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/BIClaims-1} 

}

\caption{Bodily Injury Claims. The left-hand panel gives the empirical distribution function. The right-hand panel presents a nonparametric density plot.}\label{fig:BIClaims}
\end{figure}

The impact of bodily injury losses can be mitigated by the imposition of
limits or purchasing reinsurance policies (see Section 10.3). To
quantify the impact of these risk mitigation tools, it is common to
compute the \emph{loss elimination ratio (LER)} as introduced in Section
3.4.1. The distribution function is not available and so much be
estimated in some way. Using the plug-in principle, a nonparametric
estimator can be defined as

\[
LER_n(d) = \frac{n^{-1} \sum_{i=1}^n \min(X_i,d)}{n^{-1} \sum_{i=1}^n X_i} = \frac{\sum_{i=1}^n \min(X_i,d)}{\sum_{i=1}^n X_i} .
\]

Figure \ref{fig:BIClaims} shows the estimator \(LER_n(d)\) for various
choices of \emph{d}. For example, if \(d=14,000\), then it turns out
that \(LER_n(14000) \approx\) 0.9768. Imposing a limit of 14,000 means
that we expect to retain 97.68 percent of claims.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/LER-1} 

}

\caption{LER for Bodily Injury Claims. The figure presents the loss elimination ratio (LER) as a function of deductible d.}\label{fig:LER}
\end{figure}

\subsection{Tools for Model Selection and
Diagnostics}\label{S:MS:ToolsModelSelection}

The previous section introduced nonparametric estimators in which there
was no parametric form assumed about the underlying distributions.
However, in many actuarial applications, analysts seek to employ a
parametric fit of a distribution for ease of explanation and the ability
to readily extend it to more complex situations such as including
explanatory variables in a regression setting. When fitting a parametric
distribution, one analyst might try to use a gamma distribution to
represent a set of loss data. However, another analyst may prefer to use
a Pareto distribution. How does one know which model to \textbf{select}?

Nonparametric tools can be used to corroborate the selection of
parametric models. Essentially, the approach is to compute selected
summary measures under a fitted parametric model and to compare it to
the corresponding quantity under the nonparametric model. As the
nonparametric does not assume a specific distribution and is merely a
function of the data, it is used as a benchmark to assess how well the
parametric distribution/model represents the data. This comparison may
alert the analyst to deficiencies in the parametric model and sometimes
point ways to improving the parametric specification. Procedures geared
towards assessing the validity of a model are known as model
diagnostics.

\subsubsection{Graphical Comparison of
Distributions}\label{S:MS:GraphComparison}

We have already seen the technique of overlaying graphs for comparison
purposes. To reinforce the application of this technique, Figure
\ref{fig:ComparisonCDFPDF} compares the empirical distribution to two
parametric fitted distributions. The left panel shows the distribution
functions of claims distributions. The dots forming an ``S-shaped''
curve represent the empirical distribution function at each observation.
The thick blue curve gives corresponding values for the fitted gamma
distribution and the light purple is for the fitted Pareto distribution.
Because the Pareto is much closer to the empirical distribution function
than the gamma, this provides evidence that the Pareto is the better
model for this data set. The right panel gives similar information for
the density function and provides a consistent message. Based (only) on
these figures, the Pareto distribution is the clear choice for the
analyst.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{LossDataAnalytics_files/figure-latex/ComparisonCDFPDF-1} 

}

\caption{Nonparametric Versus Fitted Parametric Distribution and Density Functions. The left-hand panel compares distribution functions, with the dots corresponding to the empirical distribution, the thick blue curve corresponding to the fitted gamma and the light purple curve corresponding to the fitted Pareto. The right hand panel compares these three distributions summarized using probability density functions.}\label{fig:ComparisonCDFPDF}
\end{figure}

For another way to compare the appropriateness of two fitted models,
consider the probability-probability (pp) plot. A \(pp\) plot compares
cumulative probabilities under two models. For our purposes, these two
models are the nonparametric empirical distribution function and the
parametric fitted model. Figure \ref{fig:PPPlot} shows \(pp\) plots for
the Property Fund data. The fitted gamma is on the left and the fitted
Pareto is on the right, compared to the same empirical distribution
function of the data. The straight line represents equality between the
two distributions being compared, so points close to the line are
desirable. As seen in earlier demonstrations, the Pareto is much closer
to the empirical distribution than the gamma, providing additional
evidence that the Pareto is the better model.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{LossDataAnalytics_files/figure-latex/PPPlot-1} 

}

\caption{Probability-Probability ($pp$) Plots. The horizontal axes gives the empirical distribution function at each observation. In the left-hand panel, the corresponding distribution function for the gamma is shown in the vertical axis. The right-hand panel shows the fitted Pareto distribution. Lines of $y=x$ are superimposed.}\label{fig:PPPlot}
\end{figure}

A \(pp\) plot is useful in part because no artificial scaling is
required, such as with the overlaying of densities in Figure
\ref{fig:ComparisonCDFPDF}, in which we switched to the log scale to
better visualize the data. The Chapter 4 \emph{Technical Supplement A.1}
introduces a variation of the \(pp\) plot known as a Lorenz curve; this
is an important tool for assessing income inequality. Furthermore,
\(pp\) plots are available in multivariate settings where more than one
outcome variable is available. However, a limitation of the \(pp\) plot
is that, because it is a plot \emph{cumulative} distribution functions,
it can sometimes be difficult to detect \emph{where} a fitted parametric
distribution is deficient. As an alternative, it is common to use a
quantile-quantile (qq) plot, as demonstrated in Figure \ref{fig:QQPlot}.

The \(qq\) plot compares two fitted models through their quantiles. As
with \(pp\) plots, we compare the nonparametric to a parametric fitted
model. Quantiles may be evaluated at each point of the data set, or on a
grid (e.g., at \(0, 0.001, 0.002, \ldots, 0.999, 1.000\)), depending on
the application. In Figure \ref{fig:QQPlot}, for each point on the
aforementioned grid, the horizontal axis displays the empirical quantile
and the vertical axis displays the corresponding fitted parametric
quantile (gamma for the upper two panels, Pareto for the lower two).
Quantiles are plotted on the original scale in the left panels and on
the log scale in the right panels to allow us to see where a fitted
distribution is deficient. The straight line represents equality between
the empirical distribution and fitted distribution. From these plots, we
again see that the Pareto is an overall better fit than the gamma.
Furthermore, the lower-right panel suggests that the Pareto distribution
does a good job with large observations, but provides a poorer fit for
small observations.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{LossDataAnalytics_files/figure-latex/QQPlot-1} 

}

\caption{Quantile-Quantile ($qq$) Plots. The horizontal axes gives the empirical quantiles at each observation. The right-hand panels they are graphed on a logarithmic basis. The vertical axis gives the quantiles from the fitted distributions; gamma quantiles are in the upper panels, Pareto quantiles are in the lower panels.}\label{fig:QQPlot}
\end{figure}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.1.6. Actuarial Exam Question.} The graph below shows a
\(pp\) plot of a fitted distribution compared to a sample.

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{LossDataAnalytics_files/figure-latex/unnamed-chunk-41-1} 

}

\end{figure}

Comment on the two distributions with respect to left tail, right tail,
and median probabilities.

\textbf{Solution.} The tail of the fitted distribution is too thick on
the left, too thin on the right, and the fitted distribution has less
probability around the median than the sample. To see this, recall that
the \(pp\) plot graphs the cumulative distribution of two distributions
on its axes (empirical on the x-axis and fitted on the y-axis in this
case). For small values of \(x\), the fitted model assigns greater
probability to being below that value than occurred in the sample (i.e.
\(F(x) > F_n(x)\)). This indicates that the model has a heavier left
tail than the data. For large values of \(x\), the model again assigns
greater probability to being below that value and thus less probability
to being above that value (i.e. \(S(x) < S_n(x)\). This indicates that
the model has a lighter right tail than the data. In addition, as we go
from 0.4 to 0.6 on the horizontal axis (thus looking at the middle 20\%
of the data), the \(pp\) plot increases from about 0.3 to 0.4. This
indicates that the model puts only about 10\% of the probability in this
range.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Statistical Comparison of
Distributions}\label{S:MS:Tools:Stats}

When selecting a model, it is helpful to make the graphical displays
presented. However, for reporting results, it can be effective to
supplement the graphical displays with selected statistics that
summarize model goodness of fit. \protect\hyperlink{tab:42}{Table 4.2}
provides three commonly used goodness of fit statistics. In this table,
\(F_n\) is the empirical distribution, \(F\) is the fitted or
hypothesized distribution, and \(F_i = F(x_i)\).

\[
{\small
\begin{matrix}
\text{Table 4.2: Three Goodness of Fit Statistics} \\
\begin{array}{l|cc}
\hline
\text{Statistic} & \text{Definition} & \text{Computational Expression} \\
\hline
\text{Kolmogorov-} & \max_x |F_n(x) - F(x)| & \max(D^+, D^-) \text{ where } \\
~~~\text{Smirnov} && D^+ = \max_{i=1, \ldots, n} \left|\frac{i}{n} - F_i\right| \\
&& D^- = \max_{i=1, \ldots, n} \left| F_i - \frac{i-1}{n} \right| \\
\text{Cramer-von Mises} & n \int (F_n(x) - F(x))^2 f(x) dx & \frac{1}{12n} + \sum_{i=1}^n \left(F_i - (2i-1)/n\right)^2 \\
\text{Anderson-Darling} & n \int \frac{(F_n(x) - F(x))^2}{F(x)(1-F(x))} f(x) dx & -n-\frac{1}{n} \sum_{i=1}^n (2i-1) \log\left(F_i(1-F_{n+1-i})\right)^2 \\
\hline
\end{array} \\
\end{matrix}
}
\]

The \emph{Kolmogorov-Smirnov statistic} is the maximum absolute
difference between the fitted distribution function and the empirical
distribution function. Instead of comparing differences between single
points, the \emph{Cramer-von Mises statistic} integrates the difference
between the empirical and fitted distribution functions over the entire
range of values. The \emph{Anderson-Darling statistic} also integrates
this difference over the range of values, although weighted by the
inverse of the variance. It therefore places greater emphasis on the
tails of the distribution (i.e when \(F(x)\) or \(1-F(x)=S(x)\) is
small).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Exaxmple 4.1.7. Actuarial Exam Question (modified).} A sample of
claim payments is:

\[
\begin{array}{ccccc}
29 & 64 & 90 & 135 & 182  \\
\end{array}
\]

Compare the empirical claims distribution to an exponential distribution
with mean \(100\) by calculating the value of the Kolmogorov-Smirnov
test statistic.

\textbf{Solution.} For an exponential distribution with mean \(100\),
the cumulative distribution function is \(F(x)=1-e^{-x/100}\). Thus,

\[
\begin{array}{ccccc}
\hline
x & F(x) & F_n(x) & F_n(x-) & \max(|F(x)-F_n(x)|,|F(x)-F_n(x-)|) \\
\hline
29  & 0.2517 & 0.2 & 0   & \max(0.0517, 0.2517) = 0.2517 \\
64  & 0.4727 & 0.4 & 0.2 & \max(0.0727, 0.2727) = 0.2727 \\
90  & 0.5934 & 0.6 & 0.4 & \max(0.0066, 0.1934) = 0.1934 \\
135 & 0.7408 & 0.8 & 0.6 & \max(0.0592, 0.1408) = 0.1408 \\
182 & 0.8380 & 1   & 0.8 & \max(0.1620, 0.0380) = 0.1620 \\
\hline
\end{array}
\]

The Kolmogorov-Smirnov test statistic is therefore
\(KS = \max(0.2517, 0.2727, 0.1934, 0.1408, 0.1620) = 0.2727\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Starting Values}\label{starting-values}

The method of moments and percentile matching are nonparametric
estimation methods that provide alternatives to maximum likelihood.
Generally, maximum likelihood is the preferred technique because it
employs data more efficiently. (See Appendix Chapter \ref{C:AppC} for
precise definitions of efficiency.) However, methods of moments and
percentile matching are useful because they are easier to interpret and
therefore allow the actuary or analyst to explain procedures to others.
Additionally, the numerical estimation procedure (e.g.~if performed in
\texttt{R}) for the maximum likelihood is iterative and requires
starting values to begin the recursive process. Although many problems
are robust to the choice of the starting values, for some complex
situations, it can be important to have a starting value that is close
to the (unknown) optimal value. Method of moments and percentile
matching are techniques that can produce desirable estimates without a
serious computational investment and can thus be used as a
\emph{starting value} for computing maximum likelihood.

\subsubsection{Method of Moments}\label{method-of-moments}

Under the method of moments, we approximate the moments of the
parametric distribution using the empirical (nonparametric) moments
described in Section \ref{S:MS:MomentEstimator}. We can then
algebraically solve for the parameter estimates.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.1.8. Property Fund.} For the 2010 property fund, there
are \(n=1,377\) individual claims (in thousands of dollars) with

\[m_1 = \frac{1}{n} \sum_{i=1}^n X_i = 26.62259 \ \ \ \
\text{and} \ \ \ \
 m_2 = \frac{1}{n} \sum_{i=1}^n X_i^2 = 136154.6 .\] Fit the parameters
of the gamma and Pareto distributions using the method of moments.

\textbf{Solution.} To fit a gamma distribution, we have
\(\mu_1 = \alpha \theta\) and
\(\mu_2^{\prime} = \alpha(\alpha+1) \theta^2\). Equating the two yields
the method of moments estimators, easy algebra shows that

\[\alpha = \frac{\mu_1^2}{\mu_2^{\prime}-\mu_1^2}  \ \ \ \text{and} \ \ \  \theta = \frac{\mu_2^{\prime}-\mu_1^2}{\mu_1}.\]

Thus, the method of moment estimators are

\[
\begin{aligned}
\hat{\alpha} &=  \frac{26.62259^2}{136154.6-26.62259^2} = 0.005232809 \\
\hat{\theta} &=  \frac{136154.6-26.62259^2}{26.62259} = 5,087.629.
\end{aligned}
\]

For comparison, the maximum likelihood values turn out to be
\(\hat{\alpha}_{MLE} = 0.2905959\) and
\(\hat{\theta}_{MLE} = 91.61378\), so there are big discrepancies
between the two estimation procedures. This is one indication, as we
have seen before, that the gamma model fits poorly.

In contrast, now assume a Pareto distribution so that
\(\mu_1 = \theta/(\alpha -1)\) and
\(\mu_2^{\prime} = 2\theta^2/((\alpha-1)(\alpha-2) )\). Easy algebra
shows

\[\alpha = 1+ \frac{\mu_2^{\prime}}{\mu_2^{\prime}-\mu_1^2} \ \ \ \
\text{and} \ \ \ \ \
 \theta = (\alpha-1)\mu_1.\]

Thus, the method of moment estimators are

\[
\begin{aligned}
\hat{\alpha} &=  1+ \frac{136154.6}{136154.6-26,62259^2} = 2.005233 \\
\hat{\theta} &=  (2.005233-1) \cdot 26.62259 = 26.7619
\end{aligned}
\]

The maximum likelihood values turn out to be
\(\hat{\alpha}_{MLE} = 0.9990936\) and
\(\hat{\theta}_{MLE} = 2.2821147\). It is interesting that
\(\hat{\alpha}_{MLE}<1\); for the Pareto distribution, recall that
\(\alpha <1\) means that the mean is infinite. This is another
indication that the property claims data set is a long tail
distribution.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

As the above example suggests, there is flexibility with the method of
moments. For example, we could have matched the second and third moments
instead of the first and second, yielding different estimators.
Furthermore, there is no guarantee that a solution will exist for each
problem. You will also find that matching moments is possible for a few
problems where the data are censored or truncated, but in general, this
is a more difficult scenario. Finally, for distributions where the
moments do not exist or are infinite, method of moments is not
available. As an alternative, one can use the percentile matching
technique.

\subsubsection{Percentile Matching}\label{percentile-matching}

Under percentile matching, we approximate the quantiles or percentiles
of the parametric distribution using the empirical (nonparametric)
quantiles or percentiles described in Section
\ref{S:MS:QuantileEstimator}.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.1.9. Property Fund.} For the 2010 property fund, we
illustrate matching on quantiles. In particular, the Pareto distribution
is intuitively pleasing because of the closed-form solution for the
quantiles. Recall that the distribution function for the Pareto
distribution is
\[F(x) = 1 - \left(\frac{\theta}{x+\theta}\right)^{\alpha}.\] Easy
algebra shows that we can express the quantile as

\[F^{-1}(q) = \theta \left( (1-q)^{-1/\alpha} -1 \right).\] for a
fraction \(q\), \(0<q<1\).

Determine estimates of the Pareto distribution parameters using the 25th
and 95th empirical quantiles.

\textbf{Solution.}

The 25th percentile (the first quartile) turns out to be \(0.78853\) and
the 95th percentile is \(50.98293\) (both in thousands of dollars). With
two equations
\[0.78853 = \theta \left( 1- (1-.25)^{-1/\alpha} \right) \ \ \ \ \text{and} \ \ \ \ 50.98293 = \theta \left( 1- (1-.75)^{-1/\alpha} \right)\]
and two unknowns, the solution is
\[\hat{\alpha} = 0.9412076 \ \ \ \ \ \text{and} \ \ \ \
\hat{\theta} = 2.205617 .\] We remark here that a numerical routine is
required for these solutions as no analytic solution is available.
Furthermore, recall that the maximum likelihood estimates are
\(\hat{\alpha}_{MLE} = 0.9990936\) and
\(\hat{\theta}_{MLE} = 2.2821147\), so the percentile matching provides
a better approximation for the Pareto distribution than the method of
moments.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.1.10. Actuarial Exam Question.} You are given:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  Losses follow a loglogistic distribution with cumulative distribution
  function:
  \[F(x) = \frac{\left(x/\theta\right)^{\gamma}}{1+\left(x/\theta\right)^{\gamma}}\]
\item
  The sample of losses is:
\end{enumerate}

\[
\begin{array}{ccccccccccc}
10 &35 &80 &86 &90 &120 &158 &180 &200 &210 &1500 \\
\end{array}
\]

Calculate the estimate of \(\theta\) by percentile matching, using the
40th and 80th empirically smoothed percentile estimates.

\textbf{Solution.} With 11 observations, we have
\(j=\lfloor(n+1)q\rfloor = \lfloor 12(0.4) \rfloor = \lfloor 4.8\rfloor=4\)
and \(h=(n+1)q-j = 12(0.4)-4=0.8\). By interpolation, the 40th
empirically smoothed percentile estimate is
\(\hat{\pi}_{0.4} = (1-h) X_{(j)} + h X_{(j+1)} = 0.2(86)+0.8(90)=89.2\).

Similarly, for the 80th empirically smoothed percentile estimate, we
have \(12(0.8)=9.6\) so the estimate is
\(\hat{\pi}_{0.8} = 0.4(200)+0.6(210)=206\).

Using the loglogistic cumulative distribution, we need to solve the
following two equations for parameters \(\theta\) and \(\gamma\):
\[0.4=\frac{(89.2/\theta)^\gamma}{1+(89.2/\theta)^\gamma} \ \ \ \text{and} \ \ \ \   0.8=\frac{(206/\theta)^\gamma}{1+(206+\theta)^\gamma}\]

Solving for each parenthetical expression gives
\(\frac{2}{3}=(89.2/\theta)^\gamma\) and \(4=(206/\theta)^\gamma\).
Taking the ratio of the second equation to the first gives
\(6=(206/89.2)^\gamma \Rightarrow \gamma=\frac{\ln(6)}{\ln(206/89.2)} = 2.1407\).
Then \(4^{1/2.1407}=206/\theta \Rightarrow \theta=107.8\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Like the method of moments, percentile matching is almost too flexible
in the sense that many estimators can be based on percentile matches;
for example, one actuary can base estimation on the 25th and 95th
percentiles whereas another actuary uses the 20th and 80th percentiles.
In general these estimators will differ and there is no compelling
reason to prefer one over the other. Also as with the method of moments,
percentile matching is appealing because it provides a technique that
can be readily applied in selected situations and has an intuitive
basis. Although most actuarial applications use maximum likelihood
estimators, it can be convenient to have alternative approaches such as
method of moments and percentile matching available.

\section{Model Selection}\label{S:MS:ModelSelection}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Describe the iterative model selection specification process
\item
  Outline steps needed to select a parametric model
\item
  Describe pitfalls of model selection based purely on insample data
  when compared to the advantages of out-of-sample model validation
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

This section underscores the idea that model selection is an iterative
process in which models are cyclically (re)formulated and tested for
appropriateness before using them for inference. After an overview, we
describe the model selection process based on:

\begin{itemize}
\tightlist
\item
  an in-sample or training dataset,
\item
  an out-of-sample or test dataset, and
\item
  a method that combines these approaches known as cross-validation.
\end{itemize}

\subsection{Iterative Model Selection}\label{iterative-model-selection}

In our development, we examine the data graphically, hypothesize a model
structure, and compare the data to a candidate model in order to
formulate an improved model. \citet{box1980sampling} describes this as
an \emph{iterative process} which is shown in Figure
\ref{fig:Iterative}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{Figures/F5Iterative} 

}

\caption{The iterative model specification process.}\label{fig:Iterative}
\end{figure}

This iterative process provides a useful recipe for structuring the task
of specifying a model to represent a set of data.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The first step, the model formulation stage, is accomplished by
  examining the data graphically and using prior knowledge of
  relationships, such as from economic theory or industry practice.
\item
  The second step in the iteration is fitting based on the assumptions
  of the specified model. These assumptions must be consistent with the
  data to make valid use of the model.
\item
  The third step is \emph{diagnostic checking}; the data and model must
  be consistent with one another before additional inferences can be
  made. Diagnostic checking is an important part of the model
  formulation; it can reveal mistakes made in previous steps and provide
  ways to correct these mistakes.
\end{enumerate}

The iterative process also emphasizes the skills you need to make
analytics work. First, you need a willingness to summarize information
numerically and portray this information graphically. Second, it is
important to develop an understanding of model properties. You should
understand how a probabilistic model behaves in order to match a set of
data to it. Third, theoretical properties of the model are also
important for inferring general relationships based on the behavior of
the data.

\subsection{Model Selection Based on a Training
Dataset}\label{model-selection-based-on-a-training-dataset}

It is common to refer to a dataset used for analysis as an
\emph{in-sample} or \emph{training} dataset. Techniques available for
selecting a model depend upon whether the outcomes \(X\) are discrete,
continuous, or a hybrid of the two, although the principles are the
same.

\textbf{Graphical and other Basic Summary Measures.} Begin by
summarizing the data graphically and with statistics that do not rely on
a specific parametric form, as summarized in Section
\ref{S:MS:NonParInf}. Specifically, you will want to graph both the
empirical distribution and density functions. Particularly for loss data
that contain many zeros and that can be skewed, deciding on the
appropriate scale (e.g., logarithmic) may present some difficulties. For
discrete data, tables are often preferred. Determine sample moments,
such as the mean and variance, as well as selected quantiles, including
the minimum, maximum, and the median. For discrete data, the mode (or
most frequently occurring value) is usually helpful.

These summaries, as well as your familiarity of industry practice, will
suggest one or more candidate parametric models. Generally, start with
the simpler parametric models (for example, one parameter exponential
before a two parameter gamma), gradually introducing more complexity
into the modeling process.

Critique the candidate parametric model numerically and graphically. For
the graphs, utilize the tools introduced in Section
\ref{S:MS:ToolsModelSelection} such as \(pp\) and \(qq\) plots. For the
numerical assessments, examine the statistical significance of
parameters and try to eliminate parameters that do not provide
additional information.

\textbf{Likelihood Ratio Tests.} For comparing model fits, if one model
is a subset of another, then a likelihood ratio test may be employed;
the general approach to likelihood ratio testing is described in
Sections \ref{S:AppA:HT:LRT} and \ref{S:AppC:MLEModelVal}.

\textbf{Goodness of Fit Statistics.} Generally, models are not proper
subsets of one another so overall goodness of fit statistics are helpful
for comparing models. \emph{Information criteria} are one type of
goodness of statistic. The most widely used examples are Akaike's
Information Criterion (\emph{AIC}) and the (Schwarz) Bayesian
Information Criterion (\emph{BIC}); they are widely cited because they
can be readily generalized to multivariate settings. Section
\ref{S:AppA:HT:IC} provides a summary of these statistics.

For selecting the appropriate distribution, statistics that compare a
parametric fit to a nonparametric alternative, summarized in Section
\ref{S:MS:Tools:Stats}, are useful for model comparison. For discrete
data, a \emph{goodness of fit} statistic (as described in Section
\ref{S:goodness-of-fit}) is generally preferred as it is more intuitive
and simpler to explain.

\subsection{Model Selection Based on a Test
Dataset}\label{model-selection-based-on-a-test-dataset}

Model validation is the process of confirming that the proposed model is
appropriate, especially in light of the purposes of the investigation.
An important limitation of the model selection process based only on
insample data is that it can be susceptible to data-snooping, that is,
fitting a great number of models to a single set of data. By looking at
a large number of models, we may overfit the data and understate the
natural variation in our representation.

Selecting a model based only on insample data also does not support the
goal of predictive inference. Particularly in actuarial applications,
our goal is to make statements about \emph{new} experience rather than a
dataset at hand. For example, we use claims experience from one year to
develop a model that can be used to price insurance contracts for the
following year. As an analogy, we can think about the training data set
as experience from one year that is used to predict the behavior of the
next year's test data set.

We can respond to these criticisms by using a technique sometimes known
as \textbf{out-of-sample validation}. The ideal situation is to have
available two sets of data, one for training, or model development, and
one for testing, or model validation. We initially develop one or
several models on the first data set that we call our \emph{candidate}
models. Then, the relative performance of the candidate models can be
measured on the second set of data. In this way, the data used to
validate the model is unaffected by the procedures used to formulate the
model.

\textbf{Random Split of the Data.} Unfortunately, rarely will two sets
of data be available to the investigator. However, we can implement the
validation process by splitting the data set into \textbf{training} and
\textbf{test} subsamples, respectively. Figure \ref{fig:ModelValidation}
illustrates this splitting of the data.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{LossDataAnalytics_files/figure-latex/ModelValidation-1} 

}

\caption{Model Validation. A data set is randomly split into two subsamples.}\label{fig:ModelValidation}
\end{figure}

Various researchers recommend different proportions for the allocation.
\citet{snee1977validation} suggests that data-splitting not be done
unless the sample size is moderately large. The guidelines of
\citet{picard1990data} show that the greater the number of parameters to
be estimated, the greater the proportion of observations needed for the
model development subsample.

\textbf{Model Validation Statistics.} Much of the literature supporting
the establishment of a model validation process is based on regression
and classification models that you can think of as an
\emph{input-output} problem (\citet{james2013introduction}). That is, we
have several inputs \(x_1, \ldots, x_k\) that are related to an output
\(y\) through a function such as
\[y = \mathrm{g}\left(x_1, \ldots, x_k\right).\] One uses the training
sample to develop an estimate of \(\mathrm{g}\), say,
\(\hat{\mathrm{g}}\), and then calibrate the distance from the observed
outcomes to the predictions using a criterion of the form

\begin{equation}
\sum_i \mathrm{d}(y_i,\hat{\mathrm{g}}\left(x_{i1}, \ldots, x_{ik}\right) ) .
\label{eq:OutSampleCriter}
\end{equation}

Here, the sum \emph{i} is over the test data. In many regression
applications, it is common to use squared Euclidean distance of the form
\(\mathrm{d}(y_i,\mathrm{g}) = (y_i-\mathrm{g})^2\). In actuarial
applications, Euclidean distance
\(\mathrm{d}(y_i,\mathrm{g}) = |y_i-\mathrm{g}|\) is often preferred
because of the skewed nature of the data (large outlying values of \(y\)
can have a large effect on the measure). Chapter
\ref{C:PremiumFoundations} describes another measure, the Gini index,
that is useful in actuarial applications particularly when there is a
large proportion of zeros in claims data (corresponding to no claims).

\textbf{Selecting a Distribution.} Still, our focus so far has been to
select a distribution for a data set that can be used for actuarial
modeling without additional inputs \(x_1, \ldots, x_k\). Even in this
more fundamental problem, the model validation approach is valuable. If
we base all inference on only in-sample data, then there is a tendency
to select more complicated models then needed. For example, we might
select a four parameter GB2, generalized beta of the second kind,
distribution when only a two parameter Pareto is needed. Information
criteria such as AIC and BIC included penalties for model complexity and
so provide some protection but using a test sample is the best guarantee
to achieve parsimonious models. From a quote often attributed to Albert
Einstein, we want to ``use the simplest model as possible but no
simpler.''

\subsection{Model Selection Based on
Cross-Validation}\label{model-selection-based-on-cross-validation}

Although out-of-sample validation is the gold standard in predictive
modeling, it is not always practical to do so. The main reason is that
we have limited sample sizes and the out-of-sample model selection
criterion in equation \eqref{eq:OutSampleCriter} depends on a
\emph{random} split of the data. This means that different analysts,
even when working the same data set and same approach to modeling, may
select different models. This is likely in actuarial applications
because we work with skewed data sets where there is a large chance of
getting some very large outcomes and large outcomes may have a great
influence on the parameter estimates.

\textbf{Cross-Validation Procedure.} Alternatively, one may use
\textbf{cross-validation}, as follows.

\begin{itemize}
\tightlist
\item
  The procedure begins by using a random mechanism to split the data
  into \emph{K} subsets known as \emph{folds}, where analysts typically
  use 5 to 10.
\item
  Next, one uses the first \emph{K}-1 subsamples to estimate model
  parameters. Then, ``predict'' the outcomes for the \emph{K}th
  subsample and use a measure such as in equation
  \eqref{eq:OutSampleCriter} to summarize the fit.
\item
  Now, repeat this by holding out each of the \emph{K} sub-samples,
  summarizing with a cumulative out-of-sample statistic.
\end{itemize}

Repeat these steps for several candidate models and choose the model
with the lowest cumulative out-of-sample statistic.

Cross-validation is widely used because it retains the predictive flavor
of the out-of-sample model validation process but, due to the re-use of
the data, is more stable over random samples.

\section{Estimation using Modified Data}\label{S:MS:ModifiedData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Describe grouped, censored, and truncated data
\item
  Estimate parametric distributions based on grouped, censored, and
  truncated data
\item
  Estimate distributions nonparametrically based on grouped, censored,
  and truncated data
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Parametric Estimation using Modified
Data}\label{parametric-estimation-using-modified-data}

Basic theory and many applications are based on \emph{individual}
observations that are ``\emph{complete}'' and ``\emph{unmodified},'' as
we have seen in the previous section. Section \ref{S:MaxLikeEstimation}
introduced the concept of observations that are ``\emph{modified}'' due
to two common types of limitations: \textbf{censoring} and
\textbf{truncation}. For example, it is common to think about an
insurance deductible as producing data that are truncated (from the
left) or policy limits as yielding data that are censored (from the
right). This viewpoint is from the primary insurer (the seller of the
insurance). However, as we will see in Chapter \ref{C:PortMgt}, a
reinsurer (an insurer of an insurance company) may not observe claims
smaller than an amount, only that a claim exists, an example of
censoring from the left. So, in this section, we cover the full gamut of
alternatives. Specifically, this section will address parametric
estimation methods for three alternatives to individual, complete, and
unmodified data: \textbf{interval-censored} data available only in
groups, data that are limited or \textbf{censored}, and data that may
not be observed due to \textbf{truncation}.

\subsubsection{Parametric Estimation using Grouped
Data}\label{S:MS:GroupedData}

Consider a sample of size \(n\) observed from the distribution
\(F(\cdot)\), but in groups so that we only know the group into which
each observation fell, not the exact value. This is referred to as
\textbf{grouped} or \textbf{interval-censored} data. For example, we may
be looking at two successive years of annual employee records. People
employed in the first year but not the second have left sometime during
the year. With an exact departure date (individual data), we could
compute the amount of time that they were with the firm. Without the
departure date (grouped data), we only know that they departed sometime
during a year-long interval.

Formalizing this idea, suppose there are \(k\) groups or intervals
delimited by boundaries \(c_0 < c_1< \cdots < c_k\). For each
observation, we only observe the interval into which it fell (e.g.
\((c_{j-1}, c_j)\)), not the exact value. Thus, we only know the number
of observations in each interval. The constants
\(\{c_0 < c_1 < \cdots < c_k\}\) form some partition of the domain of
\(F(\cdot)\). Then the probability of an observation \(X_i\) falling in
the \(j\)th interval is \[\Pr\left(X
_i \in (c_{j-1}, c_j]\right) = F(c_j) - F(c_{j-1}).\]

The corresponding probability mass function for an observation is

\[
\begin{aligned}
f(x) &=
\begin{cases}
F(c_1) - F(c_{0}) &   \text{if }\ x \in (c_{0}, c_1]\\
\vdots & \vdots \\
F(c_k) - F(c_{k-1}) &   \text{if }\ x \in (c_{k-1}, c_k]\\
\end{cases} \\
&= \prod_{j=1}^k \left\{F(c_j) - F(c_{j-1})\right\}^{I(x \in (c_{j-1}, c_j])}
\end{aligned}
\]

Now, define \(n_j\) to be the number of observations that fall in the
\(j\)th interval, \((c_{j-1}, c_j]\). Thus, the likelihood function
(with respect to the parameter(s) \(\theta\)) is \[
\begin{aligned}
\mathcal{L}(\theta) = \prod_{j=1}^n f(x_i) = \prod_{j=1}^k \left\{F(c_j) - F(c_{j-1})\right\}^{n_j}
\end{aligned}
\]

And the log-likelihood function is \[
\begin{aligned}
L(\theta) = \ln \mathcal{L}(\theta) = \ln \prod_{j=1}^n f(x_i) = \sum_{j=1}^k n_j \ln \left\{F(c_j) - F(c_{j-1})\right\}
\end{aligned}
\]

Maximizing the likelihood function (or equivalently, maximizing the
log-likelihood function) would then produce the maximum likelihood
estimates for grouped data.

\textbf{Example 4.3.1. Actuarial Exam Question.}

You are given:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  Losses follow an exponential distribution with mean \(\theta\).
\item
  A random sample of 20 losses is distributed as follows:
\end{enumerate}

\[
{\small
\begin{array}{l|c}
\hline
\text{Loss Range} & \text{Frequency} \\
\hline
[0,1000] & 7 \\
(1000,2000] & 6 \\
(2000,\infty) & 7 \\
\hline
\end{array}
}
\]

Calculate the maximum likelihood estimate of \(\theta\).

\textbf{Solution.} \[
\begin{aligned}
\mathcal{L}(\theta) &= F(1000)^7[F(2000)-F(1000)]^6[1-F(2000)]^7 \\
&= (1-e^{-1000/\theta})^7(e^{-1000/\theta} - e^{-2000/\theta})^6(e^{-2000/\theta})^7 \\
&= (1-p)^7(p-p^2)^6(p^2)^7 \\
&= p^{20}(1-p)^{13}
\end{aligned}
\]

where \(p=e^{-1000/\theta}\). Maximizing this expression with respect to
\(p\) is equivalent to maximizing the likelihood with respect to
\(\theta\). The maximum occurs at \(p=\frac{20}{33}\) and so
\(\hat{\theta}=\frac{-1000}{\ln(20/33)}= 1996.90\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Censored Data}\label{censored-data}

\textbf{Censoring} occurs when we record only a limited value of an
observation. The most common form is \textbf{right-censoring}, in which
we record the \emph{smaller} of the ``true'' dependent variable and a
censoring variable. Using notation, let \(X\) represent an outcome of
interest, such as the loss due to an insured event or time until an
event. Let \(C_U\) denote the censoring amount. With right-censored
observations, we record \(X_U^{\ast}= \min(X, C_U) = X \wedge C_U\). We
also record whether or not censoring has occurred. Let
\(\delta_U= I(X \leq C_U)\) be a binary variable that is 0 if censoring
occurs and 1 if it does not.

For an example that we saw in Section \ref{S:PolicyLimits}, \(C_U\) may
represent the upper limit of coverage of an insurance policy (we used
\(u\) for the upper limit in that section). The loss may exceed the
amount \(C_U\), but the insurer only has \(C_U\) in its records as the
amount paid out and does not have the amount of the actual loss \(X\) in
its records.

Similarly, with \textbf{left-censoring}, we record the \emph{larger} of
a variable of interest and a censoring variable. If \(C_L\) is used to
represent the censoring amount, we record \(X_L^{\ast}= \max(X, C_L)\)
along with the censoring indicator \(\delta_L= I(X \geq C_L)\).

As an example, you got a brief introduction to reinsurance, insurance
for insurers, in Section \ref{S:Chap3Reinsurance} and will see more in
Chapter \ref{C:PortMgt}. Suppose a reinsurer will cover insurer losses
greater than \(C_L\); this means that the reinsurer is responsible for
the excess of \(X_L^{\ast}\) over \(C_L\). Using notation, this is
\(Y = X_L^{\ast} - C_L\). To see this, first consider the case where the
policyholder loss \(X < C_L\). Then, the insurer will pay the entire
claim and \(Y =C_L- C_L=0\), no loss for the reinsurer. For the second
case, if the loss \(X \ge C_L\), then \(Y = X-C_L\) represents the
reinsurer's retained claims. Put another way, if a loss occurs, the
reinsurer records the actual amount if it exceeds the limit \(C_L\) and
otherwise it only records that it had a loss of \(0\).

\subsubsection{Truncated Data}\label{truncated-data}

Censored observations are recorded for study, although in a limited
form. In contrast, \textbf{truncated} outcomes are a type of missing
data. An outcome is potentially truncated when the availability of an
observation depends on the outcome.

In insurance, it is common for observations to be
\textbf{left-truncated} at \(C_L\) when the amount is \[
\begin{aligned}
Y &=
\left\{
\begin{array}{cl}
\text{we do not observe }X & X < C_L \\
X & X \geq C_L
\end{array}
\right.\end{aligned} .
\]

In other words, if \(X\) is less than the threshold \(C_L\), then it is
not observed.

For an example we saw in Section \ref{S:PolicyDeduct}, \(C_L\) may
represent the deductible of an insurance policy (we used \(d\) for the
deductible in that section). If the insured loss is less than the
deductible, then the insurer may not observe or record the loss at all.
If the loss exceeds the deductible, then the excess \(X-C_L\) is the
claim that the insurer covers. In Section \ref{S:PolicyDeduct}, we
defined the per payment loss to be \[
Y^{P} = \left\{ \begin{matrix}
\text{Undefined} & X \le d \\
X - d & X > d 
\end{matrix} \right. ,
\] so that if a loss exceeds a deductible, we record the excess amount
\(X-d\). This is very important when considering amounts that the
insurer will pay. However, for estimation purposes of this section, it
matters little if we substract a known constant such as \(C_L=d\). So,
for our truncated variable \(Y\), we use the simpler convention and do
not subract \(d\).

Similarly for \textbf{right-truncated} data, if \(X\) exceeds a
threshold \(C_U\), then it is not observed. In this case, the amount is
\[
\begin{aligned}
Y &=
\left\{
\begin{array}{cl}
X & X \leq C_U \\
\text{we do not observe }X & X > C_U.
\end{array}
\right.\end{aligned}
\]

Classic examples of truncation from the right include \(X\) as a measure
of distance to a star. When the distance exceeds a certain level
\(C_U\), the star is no longer observable.

Figure \ref{fig:CensorTrunc} compares truncated and censored
observations. Values of \(X\) that are greater than the ``upper''
censoring limit \(C_U\) are not observed at all (right-censored), while
values of \(X\) that are smaller than the ``lower'' truncation limit
\(C_L\) are observed, but observed as \(C_L\) rather than the actual
value of \(X\) (left-truncated).

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{LossDataAnalytics_files/figure-latex/CensorTrunc-1} 

}

\caption{Censoring and Truncation}\label{fig:CensorTrunc}
\end{figure}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example -- Mortality Study.} Suppose that you are conducting a
two-year study of mortality of high-risk subjects, beginning January 1,
2010 and finishing January 1, 2012. Figure \ref{fig:Mortality}
graphically portrays the six types of subjects recruited. For each
subject, the beginning of the arrow represents that the the subject was
recruited and the arrow end represents the event time. Thus, the arrow
represents exposure time.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{LossDataAnalytics_files/figure-latex/Mortality-1} 

}

\caption{Timeline for Several Subjects on Test in a Mortality Study}\label{fig:Mortality}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{Type A - Right-censored.} This subject is alive at the
  beginning and the end of the study. Because the time of death is not
  known by the end of the study, it is right-censored. Most subjects are
  Type A.
\item
  \textbf{Type B - Complete} information is available for a type B
  subject. The subject is alive at the beginning of the study and the
  death occurs within the observation period.
\item
  \textbf{Type C - Right-censored and left-truncated.} A type C subject
  is right-censored, in that death occurs after the observation period.
  However, the subject entered after the start of the study and is said
  to have a \emph{delayed entry time}. Because the subject would not
  have been observed had death occurred before entry, it is
  left-truncated.
\item
  \textbf{Type D - Left-truncated.} A type D subject also has delayed
  entry. Because death occurs within the observation period, this
  subject is not right censored.
\item
  \textbf{Type E - Left-truncated.} A type E subject is not included in
  the study because death occurs prior to the observation period.
\item
  \textbf{Type F - Right-truncated.} Similarly, a type F subject is not
  included because the entry time occurs after the observation period.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

To summarize, for outcome \(X\) and constants \(C_L\) and \(C_U\),

\begin{longtable}[]{@{}ccc@{}}
\toprule
Limitation Type & Limited Variable & Recording
Information\tabularnewline
\midrule
\endhead
right censoring & \(X_U^{\ast}= \min(X, C_U)\) &
\(\delta_U= I(X \leq C_U)\)\tabularnewline
left censoring & \(X_L^{\ast}= \max(X, C_L)\) &
\(\delta_L= I(X \geq C_L)\)\tabularnewline
interval censoring & &\tabularnewline
right truncation & \(X\) & observe \(X\) if
\(X \leq C_U\)\tabularnewline
left truncation & \(X\) & observe \(X\) if \(X \geq C_L\)\tabularnewline
\bottomrule
\end{longtable}

\subsubsection{Parametric Estimation using Censored and Truncated
Data}\label{parametric-estimation-using-censored-and-truncated-data}

For simplicity, we assume non-random censoring amounts and a continuous
outcome \(X\). To begin, consider the case of right-censored data where
we record \(X_U^{\ast}= \min(X, C_U)\) and censoring indicator
\(\delta= I(X \leq C_U)\). If censoring occurs so that \(\delta=0\),
then \(X \geq C_U\) and the likelihood is
\(\Pr(X \geq C_U) = 1-F(C_U)\). If censoring does not occur so that
\(\delta=1\), then \(X < C_U\) and the likelihood is \(f(x)\).
Summarizing, we have the likelihood of a single observation as

\[
\begin{aligned}
\left\{
\begin{array}{ll}
1-F(C_U) & \text{if }\delta=0 \\
f(x) & \text{if } \delta = 1 
\end{array}
\right. = \left\{ f(x)\right\}^{\delta} \left\{1-F(C_U)\right\}^{1-\delta} .
\end{aligned}
\]

The right-hand expression allows us to present the likelihood more
compactly. Now, for an \emph{iid} sample of size \(n\), the likelihood
is

\[
\mathcal{L} = 
\prod_{i=1}^n \left\{ f(x_i)\right\}^{\delta_i} \left\{1-F(C_{Ui})\right\}^{1-\delta_i} = \prod_{\delta_i=1} f(x_i) \prod_{\delta_i=0} \{1-F(C_{Ui})\},
\]

with potential censoring times \(\{ C_{U1}, \ldots,C_{Un} \}\). Here,
the notation ``\(\prod_{\delta_i=1}\)'' means to take the product over
uncensored observations, and similarly for ``\(\prod_{\delta_i=0}\).''

On the other hand, truncated data are handled in likelihood inference
via conditional probabilities. Specifically, we adjust the likelihood
contribution by dividing by the probability that the variable was
observed. To summarize, we have the following contributions to the
likelihood function for six types of outcomes:

\[
{\small
\begin{array}{lc}
\hline
\text{Outcome} & \text{Likelihood Contribution} \\
\hline
\text{exact value} & f(x) \\
\text{right-censoring} & 1-F(C_U) \\
\text{left-censoring} & F(C_L) \\
\text{right-truncation} & f(x)/F(C_U) \\
\text{left-truncation} & f(x)/(1-F(C_L)) \\
\text{interval-censoring} & F(C_U)-F(C_L) \\
\hline
\end{array}
}
\]

For known outcomes and censored data, the likelihood is
\[\mathcal{L}(\theta) = \prod_{E} f(x_i) \prod_{R} \{1-F(C_{Ui})\} \prod_{L}
F(C_{Li}) \prod_{I} (F(C_{Ui})-F(C_{Li})),\] where ``\(\prod_{E}\)'' is
the product over observations with \emph{E}xact values, and similarly
for \emph{R}ight-, \emph{L}eft- and \emph{I}nterval-censoring.

For right-censored and left-truncated data, the likelihood is
\[\mathcal{L} = \prod_{E} \frac{f(x_i)}{1-F(C_{Li})} \prod_{R} \frac{1-F(C_{Ui})}{1-F(C_{Li})},\]
and similarly for other combinations. To get further insights, consider
the following.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Special Case: Exponential Distribution.} Consider data that are
right-censored and left-truncated, with random variables \(X_i\) that
are exponentially distributed with mean \(\theta\). With these
specifications, recall that \(f(x) = \theta^{-1} \exp(-x/\theta)\) and
\(F(x) = 1-\exp(-x/\theta)\).

For this special case, the log-likelihood is

\[
\begin{aligned}
L(\theta) &= \sum_{E} \left\{ \ln f(x_i) - \ln (1-F(C_{Li})) \right\} + \sum_{R}\left\{ \ln (1-F(C_{Ui}))- \ln (1-\mathrm{F}(C_{Li})) \right\}\\
&= \sum_{E} (-\ln \theta -(x_i-C_{Li})/\theta ) -\sum_{R} (C_{Ui}-C_{Li})/\theta .
\end{aligned}
\]

To simplify the notation, define \(\delta_i = I(X_i \geq C_{Ui})\) to be
a binary variable that indicates right-censoring. Let
\(X_i^{\ast \ast} = \min(X_i, C_{Ui}) - C_{Li}\) be the amount that the
observed variable exceeds the lower truncation limit. With this, the
log-likelihood is

\begin{equation}
  L(\theta) =  - \sum_{i=1}^n ((1-\delta_i) \ln \theta + \frac{x_i^{\ast \ast}}{\theta})
  \label{eq:EXPloglik}
\end{equation}

Taking derivatives with respect to the parameter \(\theta\) and setting
it equal to zero yields the maximum likelihood estimator

\[\widehat{\theta}  = \frac{1}{n_u} \sum_{i=1}^n  x_i^{\ast \ast},\]

where \(n_u = \sum_i (1-\delta_i)\) is the number of uncensored
observations.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.3.2. Actuarial Exam Question.} You are given:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  A sample of losses is: 600 700 900
\item
  No information is available about losses of 500 or less.
\item
  Losses are assumed to follow an exponential distribution with mean
  \(\theta\).
\end{enumerate}

Calculate the maximum likelihood estimate of \(\theta\).

\textbf{Solution.} These observations are truncated at 500. The
contribution of each observation to the likelihood function is
\[\frac{f(x)}{1-F(500)} = \frac{\theta^{-1}e^{-x/\theta}}{e^{-500/\theta}}\]

Then the likelihood function is

\[\mathcal{L}(\theta)= \frac{\theta^{-1} e^{-600/\theta} \theta^{-1} e^{-700/\theta} \theta^{-1} e^{-900/\theta}}{(e^{-500/\theta})^3} = \theta^{-3}e^{-700/\theta}\]

The log-likelihood is

\[L(\theta) = \ln\mathcal{L}(\theta) = -3\ln \theta - 700\theta^{-1}\]

Maximizing this expression by setting the derivative with respect to
\(\theta\) equal to 0, we have

\[L'(\theta) = -3\theta^{-1} + 700\theta^{-2} = 0 \ \Rightarrow \ \hat{\theta} = \frac{700}{3} = 233.33\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.3.3. Actuarial Exam Question.} You are given the
following information about a random sample:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  The sample size equals five.
\item
  The sample is from a Weibull distribution with \(\tau=2\).
\item
  Two of the sample observations are known to exceed 50, and the
  remaining three observations are 20, 30, and 45.
\end{enumerate}

Calculate the maximum likelihood estimate of \(\theta\).

\textbf{Solution.} The likelihood function is

\[
\begin{aligned} 
\mathcal{L}(\theta) &= f(20) f(30) f(45) [1-F(50)]^2 \\
&= \frac{2(20/\theta)^2 e^{-(20/\theta)^2}}{20} \frac{2(30/\theta)^2 e^{-(30/\theta)^2}}{30} \frac{2(45/\theta)^2 e^{-(45/\theta)^2}}{45}(e^{-(50/\theta)^2})^2 \\
&\propto \frac{1}{\theta^6} e^{-8325/\theta^2}
\end{aligned}
\]

The natural logarithm of the above expression is
\(-6\ln\theta - \frac{8325}{\theta^2}\). Maximizing this expression by
setting its derivative to 0, we get

\[\frac{-6}{\theta} + \frac{16650}{\theta^3} = 0 \ \Rightarrow \ \hat{\theta} = \left(\frac{16650}{6}\right)^{\frac{1}{2}} = 52.6783\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Nonparametric Estimation using Modified
Data}\label{nonparametric-estimation-using-modified-data}

Nonparametric estimators provide useful benchmarks, so it is helpful to
understand the estimation procedures for grouped, censored, and
truncated data.

\subsubsection{Grouped Data}\label{grouped-data}

As we have seen in Section \ref{S:MS:GroupedData}, observations may be
grouped (also referred to as interval censored) in the sense that we
only observe them as belonging in one of \(k\) intervals of the form
\((c_{j-1}, c_j]\), for \(j=1, \ldots, k\). At the boundaries, the
empirical distribution function is defined in the usual way: \[
F_n(c_j) = \frac{\text{number of observations } \le c_j}{n}.
\]

For other values of \(x \in (c_{j-1}, c_j)\), we can estimate the
distribution function with the ogive estimator, which linearly
interpolates between \(F_n(c_{j-1})\) and \(F_n(c_j)\), i.e.~the values
of the boundaries \(F_n(c_{j-1})\) and \(F_n(c_j)\) are connected with a
straight line. This can formally be expressed as
\[F_n(x) = \frac{c_j-x}{c_j-c_{j-1}} F_n(c_{j-1}) + \frac{x-c_{j-1}}{c_j-c_{j-1}} F_n(c_j) \ \ \ \text{for } c_{j-1} \le x < c_j\]

The corresponding density is
\[f_n(x) = F^{\prime}_n(x) = \frac{F_n(c_j)-F_n(c_{j-1})}{c_j - c_{j-1}} \ \ \  \text{for } c_{j-1} \le x < c_j .\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.3.4. Actuarial Exam Question.}

You are given the following information regarding claim sizes for 100
claims:

\[
{\small
\begin{array}{r|c}
\hline
\text{Claim Size} &  \text{Number of Claims} \\
\hline
0 - 1,000 & 16 \\
1,000 - 3,000 & 22 \\
3,000 - 5,000 & 25 \\
5,000 - 10,000 & 18 \\
10,000 - 25,000 & 10 \\
25,000 - 50,000 & 5 \\
50,000 - 100,000 & 3 \\
\text{over  } 100,000 & 1 \\
\hline
\end{array}
}
\]

Using the ogive, calculate the estimate of the probability that a
randomly chosen claim is between 2000 and 6000.

\textbf{Solution.} At the boundaries, the empirical distribution
function is defined in the usual way, so we have
\[F_{100}(1000) = 0.16, \ F_{100}(3000)=0.38, \ F_{100}(5000)=0.63, \ F_{100}(10000)=0.81\]
For other claim sizes, the ogive estimator linearly interpolates between
these values:
\[F_{100}(2000) = 0.5F_{100}(1000) + 0.5F_{100}(3000) = 0.5(0.16)+0.5(0.38)=0.27\]
\[F_{100}(6000)=0.8F_{100}(5000)+0.2F_{100}(10000) = 0.8(0.63)+0.2(0.81)=0.666\]
Thus, the probability that a claim is between 2000 and 6000 is
\(F_{100}(6000) - F_{100}(2000) = 0.666-0.27 = 0.396\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Right-Censored Empirical Distribution
Function}\label{right-censored-empirical-distribution-function}

It can be useful to calibrate parametric estimators with nonparametric
methods that do not rely on a parametric form of the distribution. The
product-limit estimator due to \citep{kaplan1958} is a well-known
estimator of the distribution function in the presence of censoring.

\textbf{Motivation for the Kaplan-Meier Product Limit Estimator.} To
explain why the product-limit works so well with censored observations,
let us first return to the ``usual'' case without censoring. Here, the
empirical distribution function \(F_n(x)\) is an \emph{unbiased}
estimator of the distribution function \(F(x)\). This is because
\(F_n(x)\) is the average of indicator variables each of which are
unbiased, that is, \(\mathrm{E~} I(X_i \le x) = \Pr(X_i \le x) = F(x)\).

Now suppose the the random outcome is censored on the right by a
limiting amount, say, \(C_U\), so that we record the smaller of the two,
\(X^* = \min(X, C_U)\). For values of \(x\) that are smaller than
\(C_U\), the indicator variable still provides an unbiased estimator of
the distribution function before we reach the censoring limit. That is,
\(\mathrm{E~} I(X^* \le x) = F(x)\) because
\(I(X^* \le x) = I(X \le x)\) for \(x < C_U\). In the same way,
\(\mathrm{E~} I(X^* > x) = 1 -F(x) = S(x)\). But, for \(x>C_U\),
\(I(X^* \le x)\) is in general not an unbiased estimator of \(F(x)\).

As an alternative, consider \emph{two} random variables that have
different censoring limits. For illustration, suppose that we observe
\(X_1^* = \min(X_1, 5)\) and \(X_2^* = \min(X_2, 10)\) where \(X_1\) and
\(X_2\) are independent draws from the same distribution. For
\(x \le 5\), the empirical distribution function \(F_2(x)\) is an
unbiased estimator of \(F(x)\). However, for \(5 < x \le 10\), the first
observation cannot be used for the distribution function because of the
censoring limitation. Instead, the strategy developed by
\citep{kaplan1958} is to use \(S_2(5)\) as an estimator of \(S(5)\) and
then to use the second observation to estimate the survival function
conditional on survival to time 5,
\(\Pr(X > x | X >5) = \frac{S(x)}{S(5)}\). Specifically, for
\(5 < x \le 10\), the estimator of the survival function is

\[
\hat{S}(x) = S_2(5) \times I(X_2^* > x ) .
\]

\textbf{Kaplan-Meier Product Limit Estimator.} Extending this idea, for
each observation \(i\), let \(u_i\) be the upper censoring limit
(\(=\infty\) if no censoring). Thus, the recorded value is \(x_i\) in
the case of no censoring and \(u_i\) if there is censoring. Let
\(t_{1} <\cdots< t_{k}\) be \(k\) distinct points at which an uncensored
loss occurs, and let \(s_j\) be the number of uncensored losses
\(x_i\)'s at \(t_{j}\). The corresponding risk set is the number of
observations that are active (not censored) at a value \emph{less than}
\(t_{j}\), denoted as
\(R_j = \sum_{i=1}^n I(x_i \geq t_{j}) + \sum_{i=1}^n I(u_i \geq t_{j})\).

With this notation, the \textbf{product-limit estimator} of the
distribution function is

\begin{equation}
\hat{F}(x) =
\left\{
\begin{array}{ll}
0 & x<t_{1} \\
1-\prod_{j:t_{j} \leq x}\left( 1-\frac{s_j}{R_{j}}\right) & x \geq t_{1} 
\end{array}
\right. .\label{eq:KaplanMeier}
\end{equation}

As usual, the corresponding estimate of the survival function is
\(\hat{S}(x) = 1 - \hat{F}(x)\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.3.5. Actuarial Exam Question.} The following is a
sample of 10 payments:

\[
\begin{array}{cccccccccc}
4 &4 &5+ &5+ &5+ &8 &10+ &10+ &12 &15 \\
\end{array}
\]

where \(+\) indicates that a loss has exceeded the policy limit.

Using the Kaplan-Meier product-limit estimator, calculate the
probability that the loss on a policy exceeds 11, \(\hat{S}(11)\).

\textbf{Solution.} There are four event times (non-censored
observations). For each time \(t_j\), we can calculate the number of
events \(s_j\) and the risk set \(R_j\) as the following:

\[
\begin{array}{cccc}
\hline
j & t_j & s_j & R_j \\
\hline
1 & 4 & 2 & 10 \\
2 & 8 & 1 & 5 \\
3 & 12 & 1 & 2 \\
4 & 15 & 1 & 1 \\
\hline
\end{array}
\]

Thus, the Kaplan-Meier estimate of \(S(11)\) is \[
\begin{aligned}
\hat{S}(11) &= \prod_{j:t_j\leq 11} \left( 1- \frac{s_j}{R_j} \right) =  \prod_{j=1}^{2} \left( 1- \frac{s_j}{R_j} \right)\\
&= \left(1-\frac{2}{10} \right) \left(1-\frac{1}{5} \right) = (0.8)(0.8)= 0.64. \\
\end{aligned}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example. 4.3.6. Bodily Injury Claims.} We consider again the
Boston auto bodily injury claims data from
\citet{derrig2001applications} that was introduced in Example 4.1.11. In
that example, we omitted the 17 claims that were censored by policy
limits. Now, we include the full dataset and use the Kaplan-Meier
product limit to estimate the survival function. This is given in Figure
\ref{fig:KMSurvival}.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/KMSurvival-1} 

}

\caption{Kaplan-Meier Estimate of the Survival Function for Bodily Injury Claims}\label{fig:KMSurvival}
\end{figure}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Right-Censored, Left-Truncated Empirical Distribution
Function}\label{right-censored-left-truncated-empirical-distribution-function}

In addition to right-censoring, we now extend the framework to allow for
left-truncated data. As before, for each observation \(i\), let \(u_i\)
be the upper censoring limit (\(=\infty\) if no censoring). Further, let
\(d_i\) be the lower truncation limit (0 if no truncation). Thus, the
recorded value (if it is greater than \(d_i\)) is \(x_i\) in the case of
no censoring and \(u_i\) if there is censoring. Let
\(t_{1} <\cdots< t_{k}\) be \(k\) distinct points at which an event of
interest occurs, and let \(s_j\) be the number of recorded events
\(x_i\)'s at time point \(t_{j}\). The corresponding risk set is
\[R_j = \sum_{i=1}^n I(x_i \geq t_{j}) + \sum_{i=1}^n I(u_i \geq t_{j}) - \sum_{i=1}^n I(d_i \geq t_{j}).\]

With this new definition of the risk set, the product-limit estimator of
the distribution function is as in equation \eqref{eq:KaplanMeier}.

\textbf{Greenwood's Formula}. \citep{greenwood1926} derived the formula
for the estimated variance of the product-limit estimator to be

\[\widehat{Var}(\hat{F}(x)) = (1-\hat{F}(x))^{2} \sum _{j:t_{j} \leq x} \dfrac{s_j}{R_{j}(R_{j}-s_j)}.\]

\texttt{R}`s \texttt{survfit} method takes a survival data object and
creates a new object containing the Kaplan-Meier estimate of the
survival function along with confidence intervals. The Kaplan-Meier
method (\texttt{type=\textquotesingle{}kaplan-meier\textquotesingle{}})
is used by default to construct an estimate of the survival curve. The
resulting discrete survival function has point masses at the observed
event times (discharge dates) \(t_j\), where the probability of an event
given survival to that duration is estimated as the number of observed
events at the duration \(s_j\) divided by the number of subjects exposed
or 'at-risk' just prior to the event duration \(R_j\).

Two alternate types of estimation are also available for the
\texttt{survfit} method. The alternative
(\texttt{type=\textquotesingle{}fh2\textquotesingle{}}) handles ties, in
essence, by assuming that multiple events at the same duration occur in
some arbitrary order. Another alternative
(\texttt{type=\textquotesingle{}fleming-harrington\textquotesingle{}})
uses the Nelson-Aalen (see \citep{aalen1978}) estimate of the
\textbf{cumulative hazard function} to obtain an estimate of the
survival function. The estimated cumulative hazard \(\hat{H}(x)\) starts
at zero and is incremented at each observed event duration \(t_j\) by
the number of events \(s_j\) divided by the number at risk \(R_j\). With
the same notation as above, the \textbf{Nelson-Äalen} estimator of the
distribution function is

\[
\begin{aligned}
\hat{F}_{NA}(x) &=
\left\{
\begin{array}{ll}
0 & x<t_{1} \\
1- \exp \left(-\sum_{j:t_{j} \leq x}\frac{s_j}{R_j} \right) & x \geq t_{1} 
\end{array}
\right. .\end{aligned}
\]

Note that the above expression is a result of the Nelson-Äalen estimator
of the cumulative hazard function
\[\hat{H}(x)=\sum_{j:t_j\leq x}  \frac{s_j}{R_j} \] and the relationship
between the survival function and cumulative hazard function,
\(\hat{S}_{NA}(x)=e^{-\hat{H}(x)}\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.3.7. Actuarial Exam Question.}

For observation \(i\) of a survival study:

\begin{itemize}
\tightlist
\item
  \(d_i\) is the left truncation point
\item
  \(x_i\) is the observed value if not right censored
\item
  \(u_i\) is the observed value if right censored
\end{itemize}

You are given:

\[
{\small
\begin{array}{c|cccccccccc}
\hline
\text{Observation } (i) & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\ \hline
d_i & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1.3 & 1.5 & 1.6\\
x_i & 0.9 & - & 1.5 & - & - & 1.7 & - & 2.1 & 2.1 & - \\
u_i & - & 1.2 & - & 1.5 & 1.6 & - & 1.7 & - & - & 2.3 \\
\hline
\end{array}
}
\]

Calculate the Kaplan-Meier product-limit estimate, \(\hat{S}(1.6)\)

\textbf{Solution.} Recall the risk set
\(R_j = \sum_{i=1}^n \left\{ I(x_i \geq t_{j}) + I(u_i \geq t_{j}) - I(d_i \geq t_{j}) \right\}\).
Then

\[
\begin{array}{ccccc}
\hline
j & t_j & s_j & R_j & \hat{S}(t_j) \\
\hline
1  & 0.9   & 1   & 10-3 = 7 & 1-\frac{1}{7} = \frac{6}{7} \\
2  & 1.5   & 1   & 8-2 = 6  & \frac{6}{7}\left( 1 - \frac{1}{6} \right) = \frac{5}{7}\\
3  & 1.7   & 1   & 5-0 = 5  & \frac{5}{7}\left( 1 - \frac{1}{5} \right) = \frac{4}{7}\\
4  & 2.1   & 2   & 3        & \frac{4}{7}\left( 1 - \frac{2}{3}\right) = \frac{4}{21}\\
\hline
\end{array}
\]

The Kaplan-Meier estimate is therefore \(\hat{S}(1.6) = \frac{5}{7}\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.3.8. Actuarial Exam Question. - Continued.}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Using the Nelson-Äalen estimator, calculate the probability that the
  loss on a policy exceeds 11, \(\hat{S}_{NA}(11)\).
\item
  Calculate Greenwood's approximation to the variance of the
  product-limit estimate \(\hat{S}(11)\).
\end{enumerate}

\textbf{Solution.} As before, there are four event times (non-censored
observations). For each time \(t_j\), we can calculate the number of
events \(s_j\) and the risk set \(R_j\) as the following:

\[
\begin{array}{cccc}
\hline
j & t_j & s_j & R_j \\
\hline
1 & 4 & 2 & 10 \\
2 & 8 & 1 & 5 \\
3 & 12 & 1 & 2 \\
4 & 15 & 1 & 1 \\
\hline
\end{array}
\]

The Nelson-Äalen estimate of \(S(11)\) is
\(\hat{S}_{NA}(11)=e^{-\hat{H}(11)} = e^{-0.4} = 0.67\), since \[
\begin{aligned}
\hat{H}(11) &= \sum_{j:t_j\leq 11} \frac{s_j}{R_j}  = \sum_{j=1}^{2} \frac{s_j}{R_j}  \\
&= \frac{2}{10} + \frac{1}{5}  = 0.2 + 0.2 = 0.4 .\\
\end{aligned}
\]

From earlier work, the Kaplan-Meier estimate of \(S(11)\) is
\(\hat{S}(11) = 0.64\). Then Greenwood's estimate of the variance of the
product-limit estimate of \(S(11)\) is \[
\begin{aligned}
\widehat{Var}(\hat{S}(11)) &= (\hat{S}(11))^2 \sum_{j:t_j\leq 11} \frac{s_j}{R_j(R_j-s_j)}
&= (0.64)^2 \left(\frac{2}{10(8)} + \frac{1}{5(4)} \right)  = 0.0307. \\
\end{aligned}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{Bayesian Inference}\label{S:MS:BayesInference}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Describe the Bayesian model as an alternative to the frequentist
  approach and summarize the five components of this modeling approach.
\item
  Summarize posterior distributions of parameters and use these
  posterior distributions to predict new outcomes.
\item
  Use conjugate distributions to determine posterior distributions of
  parameters.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Introduction to Bayesian Inference}\label{S:IntroBayes}

Up to this point, our inferential methods have focused on the
frequentist setting, in which samples are repeatedly drawn from a
population. The vector of parameters \(\boldsymbol \theta\) is fixed yet
unknown, whereas the outcomes \(X\) are realizations of random
variables.

In contrast, under the Bayesian framework, we view both the model
parameters and the data as random variables. We are uncertain about the
parameters \(\boldsymbol \theta\) and use probability tools to reflect
this uncertainty.

To get a sense of the Bayesian framework, begin by recalling Bayes' rule

\[
\Pr(parameters|data) = \frac{\Pr(data|parameters) \times \Pr(parameters)}{\Pr(data)}
\]

where

\begin{itemize}
\tightlist
\item
  \(\Pr(parameters)\) is the distribution of the parameters, known as
  the \emph{prior} distribution.
\item
  \(\Pr(data | parameters)\) is the sampling distribution. In a
  frequentist context, it is used for making inferences about the
  parameters and is known as the \emph{likelihood}.
\item
  \(\Pr(parameters | data)\) is the distribution of the parameters
  having observed the data, known as the \emph{posterior} distribution.
\item
  \(\Pr(data)\) is the marginal distribution of the data. It is
  generally obtained by integrating (or summing) the joint distribution
  of data and parameters over parameter values.
\end{itemize}

\textbf{Why Bayes?} There are several advantages of the Bayesian
approach. First, we can describe the entire distribution of parameters
conditional on the data. This allows us, for example, to provide
probability statements regarding the likelihood of parameters. Second,
the Bayesian approach provides a unified approach for estimating
parameters. Some non-Bayesian methods, such as least squares, require a
separate approach to estimate variance components. In contrast, in
Bayesian methods, all parameters can be treated in a similar fashion.
This is convenient for explaining results to consumers of the data
analysis. Third, this approach allows analysts to blend prior
information known from other sources with the data in a coherent manner.
This topic is developed in detail in the credibility chapter. Fourth,
Bayesian analysis is particularly useful for forecasting future
responses.

\textbf{Poisson - Gamma Special Case.} To develop intuition, we consider
the Poisson-Gamma case that holds a prominent position in actuarial
applications. The idea is to consider a set of random variables
\(X_1, \ldots, X_n\) where each \(X_i\) could represent the number of
claims for the \emph{i}th policyholder. Assume that \(X_i\) has a
Poisson distribution with parameter \(\lambda\), analogous to the
likelihood that we first saw in Chapter \ref{C:Frequency-Modeling}. In a
non-Bayesian (or frequentist) context, the parameter \(\lambda\) is
viewed as an unknown quantity that is not random (it is said to be
``fixed''). In the Bayesian context, the unknown parameter \(\lambda\)
is viewed as uncertain and is modeled as a random variable. In this
special case, we use the gamma distribution to reflect this uncertainty,
the prior distribution.

Think of the following two-stage sampling scheme to motivate our
probabilistic set-up.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In the first stage, the parameter \(\lambda\) is drawn from a gamma
  distribution.
\item
  In the second stage, for that value of \(\lambda\), there are \(n\)
  draws from the same (identical) Poisson distribution that are
  independent, conditional on \(\lambda\).
\end{enumerate}

From this simple set-up, some important conclusions emerge.

\begin{itemize}
\tightlist
\item
  The distribution of \(X_i\) is no longer Poisson. For a special case,
  it turns out to be a negative binomial distribution (see the following
  ``Snippet of Theory'').
\item
  The random variables \(X_1, \ldots, X_n\) are not independent. This is
  because they share the common random variable \(\lambda\).
\item
  As in the frequentist context, the goal is to make statments about
  likely values of parameters such as \(\lambda\) given the observed
  data \(X_1, \ldots, X_n\). However, because now both the parameter and
  the data are random variables, we can use the language of conditional
  probability to make such statements. As we will see in Section
  \ref{S:ConjugateDistributions}, it turns out that the distribution of
  \(\lambda\) given the data \(X_1, \ldots, X_n\) is also gamma (with
  updated parameters), a result that simplifies the task of inferring
  likely values of the parameter \(\lambda\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Let us demonstrate that the distribution of \(X\) is negative binomial.
We assume that the distribution of \(X\) given \(\lambda\) is Poisson,
so that \[
\Pr(X = x|\lambda) = \frac{\lambda^x}{\Gamma(x+1)} e^{-\lambda} ,
\] using notation \(\Gamma(x+1) = x!\) for integer \(x\). Assume that
\(\lambda\) is a draw from a gamma distribution with fixed parameters,
say, \(\alpha\) and \(\theta\), so this has \emph{pdf} \[
f(\lambda) = \frac{\lambda^{\alpha-1}}{\theta^{\alpha}\Gamma(\alpha)}\exp(-\lambda/\theta).
\] We know that a \emph{pdf} integrates to one. To make the development
easier, define the reciprocal parameter \(\theta_r = 1/\theta\) and so
we have \[
\int_0^{\infty} f(\lambda) ~d\lambda =1 ~~~ ==> ~~~ \theta_r^{-\alpha} \Gamma(\alpha) = \int_0^{\infty} \lambda^{\alpha-1} \exp\left(-\lambda\theta_r\right) ~
d\lambda .
\] From Appendix Chapter \ref{C:AppB} on iterated expectations, we have
that the pmf of \(X\) can be computed in an iterated fashion as

\[
\begin{aligned}
\Pr(X = x) 
&=  \mathrm{E} \left\{\Pr(X = x|\lambda)\right\}\\
&=  \int_0^{\infty} \Pr(X = x|\lambda) f(\lambda) ~d\lambda \\
&=  \int_0^{\infty} \frac{\lambda^x}{\Gamma(x+1)} e^{-\lambda} \frac{\lambda^{\alpha-1}}{\theta^{\alpha}\Gamma(\alpha)}\exp(-\lambda/\theta) ~d\lambda\\
&=  \frac{1}{\theta^{\alpha}\Gamma(x+1)\Gamma(\alpha)} \int_0^{\infty} \lambda^{x+\alpha-1} \exp\left(-\lambda(1+\frac{1}{\theta})\right) ~d\lambda \\
&=  \frac{1}{\theta^{\alpha}\Gamma(x+1)\Gamma(\alpha)} \Gamma(x+\alpha)\left(1+\frac{1}{\theta}\right)^{-(x+\alpha)} \\
&=  \frac{\Gamma(x+\alpha)}{\Gamma(x+1)\Gamma(\alpha)}\left(\frac{1}{1+\theta}\right)^{\alpha} \left(\frac{\theta}{1+\theta}\right)^{x} .\\
\end{aligned} 
\] Here, we used the gamma distribution equality with the substitution
\(\theta_r = 1 + 1/\theta\). As can be seen from Section
\ref{S:negative-binomial-distribution}, this is a negative binomial
distribution with parameter \(r = \alpha\) and \(\beta = \theta\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this chapter, we use small examples that can be done by hand in order
to focus on the foundations. For practical implementation, analysts rely
heavily on simulation methods using modern computational methods such as
Markov Chain Monte Carlo (MCMC) simulation. We will get an exposure to
simulation techniques in Chapter \ref{C:Simulation} but more intensive
techniques such as \emph{MCMC} requires yet more background. See
\citet{hartman2016} for an introduction to computational Bayesian
methods from an actuarial perspective.

\subsection{Bayesian Model}\label{bayesian-model}

With the intuition developed in the preceding Section
\ref{S:IntroBayes}, we now restate the Bayesian model with a bit more
precision using mathematical notation. For simplicity, this summary
assumes both the outcomes and parameters are continuous random
variables. In the examples, we sometimes ask the viewer to apply these
same principles to discrete versions. Conceptually both the continuous
and discrete cases are the same; mechanically, one replaces a \emph{pdf}
by a \emph{pmf} and an integral by a sum.

As stated earlier, under the Bayesian perspective, the model parameters
and data are both viewed as random. Our uncertainty about the parameters
of the underlying data generating process is reflected in the use of
probability tools.

\textbf{Prior Distribution.} Specifically, think about parameters
\(\boldsymbol \theta\) as a random vector and let
\(\pi(\boldsymbol \theta)\) denote the distribution of possible
outcomes. This is knowledge that we have before outcomes are observed
and is called the prior distribution. Typically, the prior distribution
is a regular distribution and so integrates or sums to one, depending on
whether \(\boldsymbol \theta\) is continuous or discrete. However, we
may be very uncertain (or have no clue) about the distribution of
\(\boldsymbol \theta\); the Bayesian machinery allows the following
situation \[\int \pi(\theta) d\theta = \infty,\] in which case
\(\pi(\cdot)\) is called an improper prior.

\textbf{Model Distribution.} The distribution of outcomes given an
assumed value of \(\boldsymbol \theta\) is known as the model
distribution and denoted as
\(f(x | \boldsymbol \theta) = f_{X|\boldsymbol \theta} (x|\boldsymbol \theta )\).
This is the usual frequentist mass or density function. This is simply
the likelihood in the frequentist context and so it is also convenient
to use this as a descriptor for the model distribution.

\textbf{Joint Distribution.} The distribution of outcomes and model
parameters is, unsurprisingly, known as the joint distribution and
denoted as
\(f(x , \boldsymbol \theta) = f(x|\boldsymbol \theta )\pi(\boldsymbol \theta)\).

\textbf{Marginal Outcome Distribution.} The distribution of outcomes can
be expressed as
\[f(x) = \int f(x | \boldsymbol \theta)\pi(\boldsymbol \theta) ~d \boldsymbol \theta.\]
This is analogous to a frequentist mixture distribution. In the mixture
distribution, we combined (or ``mixed'') different subpopulations. In
the Bayesian context, the marginal distribution is a combination of
different realizations of parameters (in some literatures, you can think
about this as combining different ``states of nature'').

\textbf{Posterior Distribution of Parameters.} After outcomes have been
observed (hence the terminology ``posterior''), one can use Bayes
theorem to write the distribution as
\[\pi(\boldsymbol \theta | x) =\frac{f(x , \boldsymbol \theta)}{f(x)} =\frac{f(x|\boldsymbol \theta )\pi(\boldsymbol \theta)}{f(x)}\]
The idea is to update your knowledge of the distribution of
\(\boldsymbol \theta\) (\(\pi(\boldsymbol \theta)\)) with the data
\(x\). Making statements about potential values of parameters is an
important aspect of statistical inference.

\subsection{Bayesian Inference}\label{bayesian-inference}

\subsubsection{Summarizing the Posterior Distribution of
Parameters}\label{summarizing-the-posterior-distribution-of-parameters}

One way to summarize a distribution is to use a confidence interval type
statement. To summarize the \emph{posterior} distribution of parameters,
the interval \([a,b]\) is said to be a \(100(1-\alpha)\%\) credibility
interval for \(\boldsymbol \theta\) if
\[\Pr (a \le \theta \le b | \mathbf{x}) \ge 1- \alpha.\]

For another approach to summarization, we can look to classical decision
analysis. In this set-up, the loss function \(l(\hat{\theta}, \theta)\)
determines the penalty paid for using the estimate \(\hat{\theta}\)
instead of the true \(\theta\). The \textbf{Bayes estimate} is the value
that minimizes the expected loss
\(\mathrm{E~}[ l(\hat{\theta}, \theta)]\). Some important special cases
include:

\[
{\small
\begin{array}{cll}
\hline
\text{Loss function } l(\hat{\theta}, \theta) & \text{Descriptor} & \text{Bayes Estimate} \\
\hline 
(\hat{\theta}- \theta)^2 & \text{squared error loss} & \mathrm{E}(\theta|X) \\
|\hat{\theta}- \theta| & \text{absolute deviation loss} & \text{median of } \pi(\theta|x) \\
I(\hat{\theta} =\theta) & \text{zero-one loss (for discrete probabilities)} & \text{mode of } \pi(\theta|x) \\
\hline
\end{array}
}
\]

Minimizing expected loss is a rigorous method for providing a single
``best guess'' about a likely value of a parameter, comparable to a
frequentist estimator of the unknown (fixed) parameter.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.4.1. Actuarial Exam Question.} You are given:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  In a portfolio of risks, each policyholder can have at most one claim
  per year.
\item
  The probability of a claim for a policyholder during a year is \(q\).
\item
  The prior density is \[\pi(q) = q^3/0.07, \ \ \ 0.6 < q < 0.8\]
\end{enumerate}

A randomly selected policyholder has one claim in Year 1 and zero claims
in Year 2. For this policyholder, calculate the posterior probability
that \(0.7 < q < 0.8\).

\textbf{Solution.} The posterior density is proportional to the product
of the likelihood function and prior density. Thus,
\[\pi(q|1,0) \propto f(1|q)\ f(0|q)\ \pi(q) \propto q(1-q)q^3 = q^4-q^5\]

To get the exact posterior density, we integrate the above function over
its range \((0.6, 0.8)\)

\[\int_{0.6}^{0.8} q^4-q^5 dq = \frac{q^5}{5} - \left. \frac{q^6}{6} \right|_{0.6}^{0.8} = 0.014069 \ \Rightarrow \ \pi(q|1,0)=\frac{q^4-q^5}{0.014069}\]

Then
\[\Pr(0.7<q<0.8|1,0)= \int_{0.7}^{0.8} \frac{q^4-q^5}{0.014069}dq = 0.5572\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.4.2. Actuarial Exam Question.} You are given:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  The prior distribution of the parameter \(\Theta\) has probability
  density function:
  \[\pi(\theta) = \frac{1}{\theta^2}, \ \ 1 < \theta < \infty\]
\item
  Given \(\Theta = \theta\), claim sizes follow a Pareto distribution
  with parameters \(\alpha=2\) and \(\theta\).
\end{enumerate}

A claim of 3 is observed. Calculate the posterior probability that
\(\Theta\) exceeds 2.

\emph{Solution:} The posterior density, given an observation of 3 is

\[\pi(\theta|3) =  \frac{f(3|\theta)\pi(\theta)}{\int_1^\infty f(3|\theta)\pi(\theta)d\theta} =
\frac{\frac{2\theta^2}{(3+\theta)^3}\frac{1}{\theta^2}}{\int_1^\infty 2(3+\theta)^{-3} d\theta} =
\frac{2(3+\theta)^{-3}}{\left. -(3+\theta)^{-2}\right|_1^\infty} = 32(3+\theta)^{-3}, \ \ \theta > 1\]

Then

\[\Pr(\Theta>2|3) = \int_2^\infty 32(3+\theta)^{-3}d\theta = \left. -16(3+\theta)^{-2} \right|_2^\infty = \frac{16}{25} = 0.64\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Bayesian Predictive
Distribution}\label{bayesian-predictive-distribution}

For another type of statistical inference, it is often of interest to
``predict'' the value of a random outcome that is yet to be observed.
Specifically, for new data \(y\), the predictive distribution is
\[f(y|x) = \int f(y|\theta) \pi(\theta|x) d\theta .\] It is also
sometimes called a ``posterior'' distribution as the distribution of the
new data is conditional on a base set of data.

Using squared error loss for the loss function, the \textbf{Bayesian
prediction} of \(Y\) is

\[
\begin{aligned}
\mathrm{E}(Y|X) &=  \int ~y f(y|X) dy = \int y \left(\int f(y|\theta) \pi(\theta|X) d\theta \right) dy \\
&=  \int  \mathrm{E}(Y|\theta) \pi(\theta|X) ~d\theta .
\end{aligned}
\] As noted earlier, for some situations the distribution of parameters
is discrete, not continuous. Having a discrete set of possible
parameters allow us to think of them as alternative ``states of
nature,'' a helpful interpretation.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.4.3. Actuarial Exam Question.} For a particular
policy, the conditional probability of the annual number of claims given
\(\Theta = \theta\), and the probability distribution of \(\Theta\) are
as follows:

\[
{\small
\begin{array}{l|ccc}
\hline
\text{Number of Claims} & 0 & 1 & 2 \\
\text{Probability} & 2\theta & \theta & 1-3\theta \\
\hline
\end{array}
}
\]

\[
{\small
\begin{array}{l|cc}
\hline
\theta & 0.05 & 0.30 \\
\text{Probability} & 0.80 & 0.20 \\
\hline
\end{array}
}
\]

Two claims are observed in Year 1. Calculate the Bayesian prediction of
the number of claims in Year 2.

\textbf{Solution.} Start with the posterior distribution of the
parameter \[
\Pr(\theta|X) = \frac{\Pr(X|\theta)\Pr(\theta)}{\sum_{\theta}\Pr(X|\theta)\Pr(\theta)}
\] so \[
\begin{aligned} 
\Pr(\theta=0.05|X=2) &= \frac{\Pr(X=2|\theta=0.05)\Pr(\theta=0.05)}
{\Pr(X=2|\theta=0.05)\Pr(\theta=0.05)+\Pr(X=2|\theta=0.3)\Pr(\theta=0.3)}\\
&=\frac{(1-3\times 0.05)(0.8)}{(1-3\times 0.05)(0.8)+(1-3\times 0.3)(0.2)}= \frac{68}{70}.
\end{aligned} 
\]

Thus, \(\Pr(\theta=0.3|X=1)= 1 - \Pr(\theta=0.05|X=1) = \frac{2}{70}\).

From the model distribution, we have \[
E(X|\theta) = 0 \times 2\theta + 1 \times \theta + 2 \times (1-3\theta) = 2 - 5 \theta.
\] Thus,

\[
\begin{aligned}
E(Y|X)
&=   \sum_{\theta}  \mathrm{E}(Y|\theta) \pi(\theta|X) \\
&= \mathrm{E}(Y|\theta=0.05) \pi(\theta=0.05|X)+\mathrm{E}(Y|\theta=0.3) \pi(\theta=0.3|X)\\
&= ( 2 - 5 (0.05))\frac{68}{70} + ( 2 - 5 (0.3))\frac{2}{70} = 1.714.
\end{aligned}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.4.4. Actuarial Exam Question.}

You are given:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  Losses on a company's insurance policies follow a Pareto distribution
  with probability density function: \[
  f(x|\theta) = \frac{\theta}{(x+\theta)^2}, \ \ 0 < x < \infty
  \]
\item
  For half of the company's policies \(\theta=1\) , while for the other
  half \(\theta=3\).
\end{enumerate}

For a randomly selected policy, losses in Year 1 were 5. Calculate the
posterior probability that losses for this policy in Year 2 will exceed
8.

\textbf{Solution.} We are given the prior distribution of \(\theta\) as
\(\Pr(\theta=1)=\Pr(\theta=3)=\frac{1}{2}\), the conditional
distribution \(f(x|\theta)\), and the fact that we observed \(X_1=5\).
The goal is to find the predictive probability \(\Pr(X_2>8|X_1=5)\).

The posterior probabilities are

\[
\begin{aligned}
\Pr(\theta=1|X_1=5) &= \frac{f(5|\theta=1)\Pr(\theta=1)}{f(5|\theta=1)\Pr(\theta=1) + f(5|\theta=3)\Pr(\theta=3)} \\
&= \frac{\frac{1}{36}(\frac{1}{2})}{\frac{1}{36}(\frac{1}{2})+\frac{3}{64}(\frac{1}{2})} = \frac{\frac{1}{72}}{\frac{1}{72} + \frac{3}{128}} = \frac{16}{43}
\end{aligned}
\]

\[
\begin{aligned}
\Pr(\theta=3|X_1=5) &= \frac{f(5|\theta=3)\Pr(\theta=3)}{f(5|\theta=1)\Pr(\theta=1) + f(5|\theta=3)\Pr(\theta=3)} \\
&= 1-\Pr(\theta=1|X_1=5) = \frac{27}{43}
\end{aligned}
\]

Note that the conditional probability that losses exceed 8 is

\[
\begin{aligned}
\Pr(X_2>8|\theta) &= \int_8^\infty f(x|\theta)dx \\
&= \int_8^\infty \frac{\theta}{(x+\theta)^2}dx = \left. -\frac{\theta}{x+\theta} \right|_8^\infty = \frac{\theta}{8 + \theta}
\end{aligned}
\]

The predictive probability is therefore

\[
\begin{aligned}
\Pr(X_2>8|X_1=5) &= \Pr(X_2>8|\theta=1) \Pr(\theta=1|X_1=5) + \Pr(X_2>8|\theta=3) \Pr(\theta=3 | X_1=5) \\
&= \frac{1}{8+1}\left( \frac{16}{43}\right) + \frac{3}{8+3} \left( \frac{27}{43}\right) = 0.2126
\end{aligned}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.4.5. Actuarial Exam Question.}

You are given:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  The probability that an insured will have at least one loss during any
  year is \(p\).
\item
  The prior distribution for \(p\) is uniform on \([0, 0.5]\).
\item
  An insured is observed for 8 years and has at least one loss every
  year.
\end{enumerate}

Calculate the posterior probability that the insured will have at least
one loss during Year 9.

\textbf{Solution.} The posterior probability density is \[
\begin{aligned}
\pi(p|1,1,1,1,1,1,1,1) &\propto \Pr(1,1,1,1,1,1,1,1|p)\ \pi(p) = p^8(2) \propto p^8 \\ 
\Rightarrow \pi(p|1,1,1,1,1,1,1,1) &= \frac{p^8}{\int_0^5 p^8 dp} = \frac{p^8}{(0.5^9)/9} = 9(0.5^{-9})p^8
\end{aligned}
\]

Thus, the posterior probability that the insured will have at least one
loss during Year 9 is

\[
\begin{aligned}
\Pr(X_9=1|1,1,1,1,1,1,1,1) &= \int_0^5 \Pr(X_9=1|p) \pi(p|1,1,1,1,1,1,1,1) dp \\
&= \int_0^5 p(9)(0.5^{-9})p^8 dp = 9(0.5^{-9})(0.5^{10})/10 = 0.45
\end{aligned}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.4.6. Actuarial Exam Question.} You are given:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  Each risk has at most one claim each year. \[
  {\small
  \begin{array}{ccc}
  \hline
  \text{Type of Risk} & \text{Prior Probability} & \text{Annual Claim Probability} \\
  \hline
  \text{I} & 0.7 & 0.1 \\
  \text{II} & 0.2 & 0.2 \\
  \text{III} & 0.1 & 0.4 \\
  \hline
  \end{array}
  }
  \]
\end{enumerate}

One randomly chosen risk has three claims during Years 1-6. Calculate
the posterior probability of a claim for this risk in Year 7.

\textbf{Solution.} The probabilities are from a binomial distribution
with 6 trials in which 3 successes were observed.

\[
\begin{aligned} 
\Pr(3|\text{I}) &= {6 \choose 3} (0.1^3)(0.9^3) = 0.01458 \\
\Pr(3|\text{II}) &= {6 \choose 3} (0.2^3)(0.8^3) = 0.08192 \\
\Pr(3|\text{III}) &= {6 \choose 3} (0.4^3)(0.6^3) = 0.27648
\end{aligned}
\]

The probability of observing three successes is \[
\begin{aligned} \Pr(3) &= \Pr(3|\text{I})\Pr(\text{I}) + \Pr(3|\text{II})\Pr(\text{II}) + \Pr(3|\text{III})\Pr(\text{III}) \\
&=  0.7(0.01458) + 0.2(0.08192) + 0.1(0.27648) = 0.054238
\end{aligned}
\]

The three posterior probabilities are \[
\begin{aligned}
\Pr(\text{I}|3) &= \frac{\Pr(3|\text{I})\Pr(\text{I})}{\Pr(3)} = \frac{0.7(0.01458)}{0.054238} = 0.18817 \\
\Pr(\text{II}|3) &= \frac{\Pr(3|\text{II})\Pr(\text{II})}{\Pr(3)} = \frac{0.2(0.08192)}{0.054238} = 0.30208 \\
\Pr(\text{III}|3) &= \frac{\Pr(3|\text{III})\Pr(\text{III})}{\Pr(3)} = \frac{0.1(0.27648)}{0.054238} = 0.50975 
\end{aligned}
\]

The posterior probability of a claim is then \[
\begin{aligned} 
\Pr(\text{claim} | 3) &= \Pr(\text{claim}|\text{I})\Pr(\text{I} | 3) + \Pr(\text{claim} | \text{II})\Pr(\text{II} | 3) + \Pr(\text{claim} | \text{III}) \Pr(\text{III} | 3) \\ 
&= 0.1(0.18817) + 0.2(0.30208) + 0.4(0.50975) = 0.28313
\end{aligned}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Conjugate Distributions}\label{S:ConjugateDistributions}

In the Bayesian framework, the key to statistical inference is
understanding the posterior distribution of the parameters. As described
in Section \ref{S:IntroBayes}, modern data analysis using Bayesian
methods utilize computationally intensive techniques such as MCMC
simulation. Another approach for computing posterior distributions are
based on conjugate distributions. Although this approach is available
only for a limited number of distributions, it has the appeal that it
provides closed-form expressions for the distributions, allowing for
easy interpretations of results.

To relate the prior and posterior distributions of the parameters, we
have the relationship

\[
\begin{array}{ccc}
\pi(\boldsymbol \theta | x) & = & \frac{f(x|\boldsymbol \theta )\pi(\boldsymbol \theta)}{f(x)}  \\
 & \propto  & f(x|\boldsymbol \theta ) \pi(\boldsymbol \theta) \\
\text{Posterior} & \text{is proportional to} & \text{likelihood} \times \text{prior} .
\end{array}
\]

For conjugate distributions, the posterior and the prior come from the
same family of distributions. The following illustration looks at the
Poisson-gamma special case, the most well-known in actuarial
applications.

\textbf{Special Case -- Poisson-Gamma - Continued.} Assume a
Poisson(\(\lambda\)) model distribution and that \(\lambda\) follows a
gamma(\(\alpha, \theta\)) prior distribution. Then, the posterior
distribution of \(\lambda\) given the data follows a gamma distribution
with new parameters \(\alpha_{post} = \sum_i x_i + \alpha\) and
\(\theta_{post} = 1/(n + 1/\theta)\).

\textbf{Special Case -- Poisson-Gamma - Continued.} The model
distribution is
\[f(\mathbf{x} | \lambda) = \prod_{i=1}^n \frac{\lambda^{x_i} e^{-\lambda}}{x_i!} .\]
The prior distribution is
\[\pi(\lambda) = \frac{\left(\lambda/\theta\right)^{\alpha} \exp(-\lambda/\theta)}{\lambda \Gamma(\alpha)}.\]
Thus, the posterior distribution is proportional to \[
\begin{aligned}
\pi(\lambda | \mathbf{x}) &\propto f(\mathbf{x}|\theta ) \pi(\lambda) \\
&= C \lambda^{\sum_i x_i + \alpha -1} \exp(-\lambda(n+1/\theta))
\end{aligned}
\]

where \(C\) is a constant. We recognize this to be a gamma distribution
with new parameters \(\alpha_{post} = \sum_i x_i + \alpha\) and
\(\theta_{post} = 1/(n + 1/\theta)\). Thus, the gamma distribution is a
conjugate prior for the Poisson model distribution.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 4.4.7. Actuarial Exam Question.}

You are given:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  The conditional distribution of the number of claims per policyholder
  is Poisson with mean \(\lambda\).
\item
  The variable \(\lambda\) has a gamma distribution with parameters
  \(\alpha\) and \(\theta\).
\item
  For policyholders with 1 claim in Year 1, the Bayes prediction for the
  number of claims in Year 2 is 0.15.
\item
  For policyholders with an average of 2 claims per year in Year 1 and
  Year 2, the Bayes prediction for the number of claims in Year 3 is
  0.20.
\end{enumerate}

Calculate \(\theta\).

\textbf{Solution.}

Since the conditional distribution of the number of claims per
policyholder, \(E(X|\lambda)=Var(X|\lambda)=\lambda\), the Bayes
prediction is

\[
\begin{aligned}
\mathrm{E}(X_2|X_1)
&= \int \mathrm{E}(X_2|\lambda) \pi(\lambda|X_1) d\lambda = \alpha_{new} \theta_{new}
\end{aligned}
\] because the posterior distribution is gamma with parameters
\(\alpha_{new}\) and \(\theta_{new}\).

For year 1, we have \[
0.15 = (X_1 + \alpha) \times \frac{1}{n+1/\theta} = (1 + \alpha) \times \frac{1}{1+1/\theta},
\] so \(0.15(1+1/\theta)= 1 + \alpha.\) For year 2, we have \[
0.2 = (X_1+X_2 + \alpha) \times \frac{1}{n+1/\theta} = (4 + \alpha) \times \frac{1}{2+1/\theta},
\] so \(0.2(2+1/\theta)= 4 + \alpha.\) Equating these yields \[
0.2(2+1/\theta)=3 + 0.15(1+1/\theta)
\] resulting in \(\theta = 1/55 = 0.018182\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Closed-form expressions means that results can be readily interpreted
and easily computed; hence, conjugate distributions are useful in
actuarial practice. Two other special cases used extensively are:

\begin{itemize}
\tightlist
\item
  The uncertainty of parameters is summarized using a beta distribution
  and the outcomes have a (conditional on the parameter) binomial
  distribution.
\item
  The uncertainty of parameters is summarized using a normal
  distribution and the outcomes are conditionally normally distributed.
\end{itemize}

Additional results on conjugate distributions are summarized in the
Appendix Section \ref{S:AppConjugateDistributions}.

\section{Further Resources and
Contributors}\label{MS:further-reading-and-resources}

\subsubsection*{Exercises}\label{exercises-1}
\addcontentsline{toc}{subsubsection}{Exercises}

Here are a set of exercises that guide the viewer through some of the
theoretical foundations of \textbf{Loss Data Analytics}. Each tutorial
is based on one or more questions from the professional actuarial
examinations, typically the Society of Actuaries Exam C.

\href{http://www.ssc.wisc.edu/~jfrees/loss-data-analytics/loss-data-analytics-model-selection/}{Model
Selection Guided Tutorials}

\subsubsection*{Contributors}\label{contributors-3}
\addcontentsline{toc}{subsubsection}{Contributors}

\begin{itemize}
\tightlist
\item
  \textbf{Edward W. (Jed) Frees} and \textbf{Lisa Gao}, University of
  Wisconsin-Madison, are the principal authors of the initial version of
  this chapter. Email:
  \href{mailto:jfrees@bus.wisc.edu}{\nolinkurl{jfrees@bus.wisc.edu}} for
  chapter comments and suggested improvements.
\item
  Chapter reviewers include: Andrew Kwon-Nakamura, Hirokazu (Iwahiro)
  Iwasawa, Eren Dodd.
\end{itemize}

\chapter{Aggregate Loss Models}\label{C:AggLossModels}

\emph{Chapter Preview}. This chapter introduces probability models for
describing the aggregate (total) claims that arise from a portfolio of
insurance contracts. We present two standard modeling approaches, the
individual risk model and the collective risk model. Further, we discuss
strategies for computing the distribution of the aggregate claims,
including exact methods for special cases, recursion, and simulation.
Finally, we examine the effects of individual policy modifications such
as deductibles, coinsurance, and inflation, on the frequency and
severity distributions, and thus the aggregate loss distribution.

\section{Introduction}\label{introduction}

The objective of this chapter is to build a probability model to
describe the aggregate claims by an insurance system occurring in a
fixed time period. The insurance system could be a single policy, a
group insurance contract, a business line, or an entire book of an
insurer's business. In this chapter, aggregate claims refer to either
the number or the amount of claims from a portfolio of insurance
contracts. However, the modeling framework can be readily applied in the
more general setup.

Consider an insurance portfolio of \(n\) individual contracts, and let
\(S\) denote the aggregate losses of the portfolio in a given time
period. There are two approaches to modeling the aggregate losses \(S\),
the individual risk model and the collective risk model. The individual
risk model emphasizes the loss from each individual contract and
represents the aggregate losses as: \[\begin{aligned}
S_n=X_1 +X_2 +\cdots+X_n,
\end{aligned}\] where \(X_i~(i=1,\ldots,n)\) is interpreted as the loss
amount from the \(i\)th contract. It is worth stressing that \(n\)
denotes the number of contracts in the portfolio and thus is a fixed
number rather than a random variable. For the individual risk model, one
usually assumes \(X_{i}\)'s are independent. Because of different
contract features such as coverage and exposure, \(X_{i}\)'s are not
necessarily identically distributed. A notable feature of the
distribution of each \(X_i\) is the probability mass at zero
corresponding to the event of no claims.

The collective risk model represents the aggregate losses in terms of a
frequency distribution and a severity distribution: \[\begin{aligned}
S_N=X_1 +X_2 +\cdots+X_N.
\end{aligned}\] Here, one thinks of a random number of claims \(N\) that
may represent either the number of losses or the number of payments. In
contrast, in the individual risk model, we use a fixed number of
contracts \(n\). We think of \(X_1, X_2, \ldots, X_N\) as representing
the amount of each loss. Each loss may or may not correspond to a unique
contract. For instance, there may be multiple claims arising from a
single contract. It is natural to think about \(X_i>0\) because if
\(X_i=0\) then no claim has occurred. Typically we assume that
conditional on \(N=n\), \(X_{1},X_{2},\ldots ,X_{n}\) are iid random
variables. The distribution of \(N\) is known as the frequency
distribution, and the common distribution of \(X\) is known as the
severity distribution. We further assume \(N\) and \(X\) are
independent. With the collective risk model, we may decompose the
aggregate losses into the frequency (\(N\)) process and the severity
(\(X\)) model. This flexibility allows the analyst to comment on these
two separate components. For example, sales growth due to lower
underwriting standards could lead to higher frequency of losses but
might not affect severity. Similarly, inflation or other economic forces
could have an impact on severity but not on frequency.

\section{Individual Risk Model}\label{individual-risk-model}

As noted earlier, for the \emph{individual risk model}, we think of
\(X_i\) as the loss from \(i\)th contract and interpret

\begin{eqnarray*}
S_n=X_1 +X_2 +\cdots+X_n
\end{eqnarray*}

to be the aggregate loss from all contracts in a portfolio or group of
contracts. Here, the \(X_i\)'s are not necessarily identically
distributed and we have \[\begin{aligned}
    {\rm E}(S_n) &= \sum_{i=1}^{n} {\rm E}(X_i)~.
\end{aligned}\]

Under the independence assumption on \(X_i\)'s (which implies
\(\mathrm{Cov}\left( X_i, X_j \right) = 0\) for all \(i \neq j\)), it
can further be shown that \[\begin{aligned}
    {\rm Var}(S_n) &= \sum_{i=1}^{n} {\rm Var}(X_i) \\
    P_{S_n}(z) &= \prod_{i=1}^{n}P_{X_i}(z) \\
    M_{S_n}(t) &= \prod_{i=1}^{n}M_{X_i}(t), 
 \end{aligned}\] where \(P_{S_n}(\cdot)\) and \(M_{S_n}(\cdot)\) are the
probability generating function (\emph{pgf}) and the moment generating
function (\emph{mgf}) of \(S_n\), respectively. The distribution of each
\(X_i\) contains a probability mass at zero, corresponding to the event
of no claims from the \(i\)th contract. One strategy to incorporate the
zero mass in the distribution is to use the two-part framework:
\[\begin{aligned}
X_i = I_i\times B_i = \left\{\begin{array}{ll}
                               0~, & \text{if }~ I_i=0 \\
                               B_i~, & \text{if }~ I_i=1
                             \end{array}
             \right.
\end{aligned}\] Here, \(I_i\) is a Bernoulli variable indicating whether
or not a loss occurs for the \(i\)th contract, and \(B_i\) is a random
variable with nonnegative support representing the amount of losses of
the contract given loss occurrence. Assume that
\(I_1 ,\ldots,I_n ,B_1 ,\ldots,B_n\) are mutually independent. Denote
\({\rm Pr} (I_i =1)=q_i\), \(\mu_i={\rm E}(B_i)\), and
\(\sigma_i^2={\rm Var}(B_i)\). It can be shown (see \emph{Technical
Supplement 5.A.1} for details) that \[\begin{aligned}
\mathrm{E}(S_n)& =\sum_{i=1}^n ~q_i  ~\mu _i \\
\mathrm{Var}(S_n) & =\sum_{i=1}^n \left( q_i \sigma _i^2+q_i (1-q_i)\mu_i^2 \right)\\
P_{S_n}(z) & =\prod_{i=1}^n \left( 1-q_i+q_i P_{B_i}(z) \right)\\
M_{S_n}(t) & =\prod_{i=1}^n \left( 1-q_i+q_i M_{B_i}(t) \right)
\end{aligned}\] A special case of the above model is when \(B_i\)
follows a degenerate distribution with \(\mu_i=b_i\) and
\(\sigma^2_i=0\). One example is term life insurance or a pure endowment
insurance where \(b_i\) represents the insurance benefit amount of the
\(i\)th contract.

Another strategy to accommodate the zero mass in the loss from each
contract is to consider them in aggregate on the portfolio level, as in
the \emph{collective risk model}. Here, the aggregate loss is
\(S_{N} = X_1 + \cdots X_N\), where \(N\) is a random variable
representing the number of non-zero claims that occurred out of the
entire group of contracts. Thus, not every contract in the portfolio may
be represented in this sum, and \(S_N=0\) when \(N=0\). The collective
risk model will be discussed in detail in the next section.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.2.1. Actuarial Exam Question.} An insurance company
sold 300 fire insurance policies as follows:

\[
{\small 
\begin{matrix}
    \begin{array}{c c c} \hline
        \text{Number of} & \text{Policy} & \text{Probability of}\\
        \text{Policies} &  \text{Maximum} &  \text{Claim Per Policy}\\ 
        & (M_i) & (q_i) \\ \hline
        100 & 400 & 0.05\\
        200 & 300 & 0.06\\ \hline
    \end{array}
\end{matrix}
}
\]

You are given:\\
(i) The claim amount for each policy, \(X_i\), is uniformly distributed
between \(0\) and the policy maximum \(M_i\).\\
(ii) The probability of more than one claim per policy is \(0\).\\
(iii) Claim occurrences are independent.

Calculate the mean, \(\mathrm{E~}(S_{300})\), and variance,
\(\mathrm{Var~}(S_{300})\), of the aggregate claims. How would these
results change if every claim is equal to the policy maximum?

\textbf{Solution.} The aggregate claims are
\(S_{300} = X_1+\cdots+X_{300}\), where \(X_1, \ldots, X_{300}\) are
independent but not identically distributed. Policy claims amounts are
uniformly distributed on \((0,M_i)\), so the mean claim amount is
\(M_i/2\) and the variance is\(M_i^2/12\). Thus, for policy
\(i=1,\ldots,300\), we have \[
{\small 
\begin{matrix}
    \begin{array}{ccccc} \hline
        \text{Number of} & \text{Policy} & \text{Probability of} & \text{Mean} & \text{Variance}\\
        \text{Policies} &  \text{Maximum} &  \text{Claim Per Policy} & \text{Amount} & \text{Amount}\\
        & (M_i) & (q_{i}) & (\mu_{i}) & (\sigma_{i}^2) \\ \hline
        100 & 400 & 0.05 & 200 & 400^2/12\\
        200 & 300 & 0.06 & 150 & 300^2/12 \\ \hline
    \end{array}
\end{matrix}
}
\]

The mean of the aggregate claims is
\[\mathrm{E~} (S_{300}) = \sum_{i=1}^{300} q_i \mu_i = 100\left\{0.05(200)\right\} + 200\left\{0.06 (150) \right\} = 2,800\]

The variance of the aggregate claims is

\begin{eqnarray*}
    \mathrm{Var~}(S_{300}) &=& \sum_{i=1}^{300} \left( q_i \sigma _i^2+q_i (1-q_i)\mu_i^2 \right) ~~~~ \text{since } X_i \text{'s are independent} \\
    &=& 100\left\{ 0.05 \left(\frac{400^2}{12}\right) +0.05 (1-0.05 )200^2 \right\}+
    200\left\{
    0.06 \left(\frac{300^2}{12}\right) +0.06 (1-0.06 )150^2 \right\}\\
    &=& 600,467.
\end{eqnarray*}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\emph{Follow-Up.} Now suppose everybody receives the policy maximum
\(M_i\) if a claim occurs. What is the expected aggregate loss
\(\mathrm{E~}(\tilde{S})\) and variance of the aggregate loss
\(\mathrm{Var~}(\tilde{S})\)?

Each policy claim amount \(X_i\) is now deterministic and fixed at
\(M_i\) instead of a randomly distributed amount, so
\(\sigma_i^2 = \mathrm{Var~} (X_i) = 0\) and \(\mu_i = M_i\). Again, the
probability of a claim occurring for each policy is \(q_i\). Under these
circumstances, the expected aggregate loss is \[\begin{aligned}
\mathrm{E~}(\tilde{S}) &= \sum_{i=1}^{300} q_i \mu_i = 100 \left\{0.05(400) \right\} + 200 \left\{ 0.06(300) \right\} = 5,600
\end{aligned}\]

The variance of the aggregate loss is \[\begin{aligned}
\mathrm{Var~}(\tilde{S}) &= \sum_{i=1}^{300} \left( q_i \sigma _i^2+q_i (1-q_i
)\mu_i^2 \right) = \sum_{i=1}^{300} \left( q_i (1-q_i) \mu_i^2 \right) \\
&= 100 \left\{(0.05) (1-0.05) 400^2\right\} +
200 \left\{(0.06) (1-0.06)300^2\right\} \\
&= 1,775,200
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The individual risk model can also be used for claim frequency. If
\(X_i\) denotes the number of claims from the \(i\)th contract, then
\(S_n\) is interpreted as the total number of claims from the portfolio.
In this case, the above two-part framework still applies since there is
a probability mass at zero for contracts that do not experience any
claims. Assume \(X_i\) belongs to the \((a,b,0)\) class with pmf denoted
by \(p_{ik} = \Pr(X_i=k)\) for \(k=0,1,\ldots\) (see Section
\ref{S:the-a-b-0-class}). Let \(X_i^{T}\) denote the associated
zero-truncated distribution in the \((a,b,1)\) class with \emph{pmf}
\(p_{ik}^T=p_{ik}/(1-p_{i0})\) for \(k=1,2,\ldots\) (see Section
\ref{S:zero-truncation-or-modification}). Using the relationship between
their probability generating functions (see \emph{Technical Supplement
5.A.2} for details): \[\begin{aligned}
P_{X_i}(z) = p_{i0} +(1-p_{i0}) P_{X_i^{T}}(z),
\end{aligned}\] we can write \(X_i=I_i\times B_i\) with
\(q_i={\rm Pr}(I_i=1)={\rm Pr}(X_i>0)=1-p_{i0}\) and \(B_i=X_i^T\).
Notice that in this case, we have a zero-modified distribution since the
\(I_i\) variable covers the modified probability mass at zero with
\(q_i = \Pr(I_i=1)\), while the \(B_i=X_i^T\) covers the discrete
non-zero frequency portion. See Section
\ref{S:zero-truncation-or-modification} for the relationship between
zero-truncated and zero-modified distributions.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.2.2.} An insurance company sold a portfolio of 100
independent homeowners insurance policies, each of which has claim
frequency following a zero-modified Poisson distribution, as follows:

\[
{\small 
\begin{matrix}
    \begin{array}{cccc} \hline
        \text{Type of} & \text{Number of}  & \text{Probability of} & \lambda \\
        \text{Policy} & \text{Policies}  &  \text{At Least 1 Claim} &  \\ \hline
        \text{Low-risk} & 40 & 0.03 & 1 \\
        \text{High-risk} & 60 & 0.05 & 2 \\ \hline
    \end{array}
\end{matrix}
}
\] Find the expected value and variance of the claim frequency for the
entire portfolio.

\textbf{Solution.} For each policy, we can write the zero-modified
Poisson claim frequency \(N_i\) as \(N_i = I_i \times B_i\), where
\[q_i = \Pr(I_i = 1) = \Pr(N_i > 0) = 1-p_{i0}\] For the low-risk
policies, we have \(q_i = 0.03\) and for the high-risk policies, we have
\(q_i=0.05\). Further, \(B_i = N_i^T\), the zero-truncated version of
\(N_i\). Thus, we have \[\begin{aligned}
\mu_i &={\rm E}(B_i) = {\rm E}(N_i^T) = \frac{\lambda}{1-e^{-\lambda}} \\
\sigma_i^2 &={\rm Var}(B_i) = {\rm Var}(N_i^T) = \frac{\lambda [1-(\lambda+1)e^{-\lambda}]}{(1-e^{-\lambda})^2}
\end{aligned}\] Let the portfolio claim frequency be
\(S_n = \sum_{i=1}^n N_i\). Using the formulas above, the expected claim
frequency of the portfolio is \[\begin{aligned}
    \mathrm{E~} (S_n) &= \sum_{i=1}^{100} q_i \mu_i \\
    & = 40\left[0.03 \left(\frac{1}{1-e^{-1}} \right) \right] + 60 \left[0.05 \left( \frac{2}{1-e^{-2}} \right) \right] \\
    &= 40(0.03)(1.5820) + 60(0.05)(2.3130) = 8.8375
\end{aligned}\] The variance of the claim frequency of the portfolio is
\[\begin{aligned}
    \mathrm{Var~}(S_n) &= \sum_{i=1}^{100} \left( q_i \sigma _i^2+q_i (1-q_i
    )\mu_i^2 \right) \\
    &= 40 \left[ 0.03 \left(\frac{1-2e^{-1}}{(1-e^{-1})^2} \right) + 0.03(0.97)(1.5820^2) \right] + 60 \left[0.05 \left( \frac{2[1-3e^{-2}]}{ (1-e^{-2})^2} \right) + 0.05(0.95)(2.3130^2) \right] \\
    &= 23.7214
\end{aligned}\] Note that equivalently, we could have calculated the
mean and variance of an individual policy directly using the
relationship between the zero-modified and zero-truncated Poisson
distributions (see Section \ref{S:the-a-b-0-class}).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

To understand the distribution of the aggregate loss, one could use the
central limit theorem to approximate the distribution of \(S_n\) for
large \(n\). Denote \(\mu_{S_n}={\rm E}(S_n)\) and
\(\sigma^2_{S_n}={\rm Var}(S_n)\) and let \(Z\sim N(0,1)\), a standard
normal random variable with cdf \(\Phi\). Then the \emph{cdf} of \(S_n\)
can be approximated as follows: \[\begin{aligned}
 F_{S_n}(s) &= {\rm Pr}({S_n}\leq s) = \Pr \left( \frac{S_n - \mu_{S_n}}{\sigma_{S_n}} \leq \frac{s-\mu_{S_n}}{\sigma_{S_n}} \right) \\
 &\approx \Pr\left( Z \leq \frac{s-\mu_{S_n}}{\sigma_{S_n}} \right) = \Phi \left(\frac{s-\mu_S}{\sigma_S}\right).
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.2.3. Actuarial Exam Question - Follow-Up.} As in the
Example 5.2.1 earlier, an insurance company sold 300 fire insurance
policies, with claim amounts \(X_i\) uniformly distributed between 0 and
the policy maximum \(M_i\). Using the normal approximation, calculate
the probability that the aggregate claim amount \(S_{300}\) exceeds
\(\$3,500\).

\textbf{Solution.} We have seen earlier that
\(\mathrm{E}(S_{300})=2,800\) and \(\mathrm{Var}(S_{300}) = 600,467\).
Then \[\begin{aligned}
{\rm Pr}(S_{300} > 3,500) &= 1 - {\rm Pr}(S_{300} \leq 3,500) \\
&\approx 1- \Phi \left( \frac{3,500-2,800}{\sqrt{600,467}} \right) = 1 - \Phi \left( 0.90334 \right) \\
&= 1 - 0.8168 = 0.1832
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

For small \(n\), the distribution of \(S_n\) is likely skewed, and the
normal approximation would be a poor choice. To examine the aggregate
loss distribution, we go back to the basics and first principles.
Specifically, the distribution can be derived recursively. Define
\(S_k=X_1 + \cdots + X_k, k=1,\ldots,n\).

For \(k=1\):
\[F_{S_1}(s) = {\rm Pr}(S_1\leq s) = {\rm Pr}(X_1\leq s) = F_{X_1}(s).\]
For \(k=2,\ldots,n\): \[\begin{aligned}
    F_{S_k}(s)&={\Pr}(X_1+\cdots+X_k\leq s) ={\Pr}(S_{k-1}+X_k\leq s) \\
    &={\rm E}_{X_k}\left[{\rm Pr}(S_{k-1}\leq s-X_k|X_k)\right]= {\rm E}_{X_k}\left[F_{S_{k-1}}(s-X_k)\right].
   \end{aligned}\]

A special case is when \(X_i\)'s are identically distributed. Let
\(F_X(x)={\Pr}(X\leq x)\) be the common distribution of
\(X_i, ~i=1,\ldots,n\). We define
\[F^{*n}_X(x)={\Pr}(X_1+\cdots+X_n\leq x)\] the \(n\)-fold convolution
of \(F_X\). More generally, we can compute \(F_X^{\ast n}\) recursively.
Begin the recursion at \(k=1\) using
\(F_X^{\ast 1} \left(x \right) =F_X(x)\). Next, for \(k=2\), we have

\begin{eqnarray*}
F_X^{\ast 2} \left(x \right) &=& \Pr(X_1 + X_2 \le x) = \mathrm{E}_{X_2} \left[ \Pr(X_1 \le x - X_2|X_2) \right] \\
&=& \mathrm{E}_{X_2} \left[ F(x - X_2) \right] \\
&=&\left\{\begin{array}{ll}
\int_{0}^{x} F(x-y) f(y) dy & \text{for continuous } X_i \text{'s} \\
\sum_{y \le x} F(x-y) f(y) & \text{for discrete } X_i \text{'s} \\
\end{array}\right.
\end{eqnarray*}

Recall \(F(0) = 0\).

Similarly for \(k=n\), we have \(S_n = X_1 + X_2 + \cdots + X_n\) and

\begin{eqnarray*}
F^{\ast n}\left(x\right) &=& \Pr(S_n \le x) = \Pr(S_{n-1} + X_n \le x)\\
&=&\mathrm{E}_{X_n} \left[ \Pr(S_{n-1} \le x - X_n|X_n) \right] \\
&=&\mathrm{E}_X \left[ F^{\ast(n-1)}(x - X) \right] \\
&=& 
\left\{\begin{array}{ll}
\int_{0}^{x} F^{\ast(n-1)}(x-y)f(y)dy & \text{for continuous } X_i \text{'s} \\
\sum_{y \le x} F^{\ast(n-1)}(x-y)f(y) & \text{for discrete } X_i \text{'s} \\
\end{array}\right.
\end{eqnarray*}

When \(X_i\)'s are independent and belong to the same family of
distributions, there are some simple cases where \(S_n\) has a closed
form. This makes it easy to compute \(\Pr(S_n \le x)\). This property is
known as \emph{closed under convolution}, meaning the distribution of
the sum of independent random variables belongs to the same family of
distributions as that of the component variables, just with different
parameters. Examples include:

\[
{\small
\begin{matrix}
\text{Table of Closed Form Partial Sum Distributions}\\
    \begin{array}{l|l|l} \hline
        \text{Distribution of } X_i & \text{Abbreviation} & \text{Distribution of } S_n   \\ \hline
\text{Normal with mean } \mu_i \text{ and variance } \sigma_i^2 & N(\mu_i,\sigma_i^2) & N\left(\sum_{i=1}^{n}\mu_i,~\sum_{i=1}^{n}\sigma_i^2\right) \\
\text{Exponential with mean } \theta & Exp(\theta) & Gam(n,\theta)\\  
\text{Gamma with shape } \alpha_i \text{ and scale } \theta & Gam(\alpha_i,\theta) & Gam\left(\sum_{i=1}^n\alpha_i,\theta\right) \\  
\text{Poisson with mean (and variance) } \lambda_i  & Poi(\lambda_i)& Poi\left(\sum_{i=1}^{n}\lambda_i\right)\\  
\text{Binomial with } m_i \text{ trials and } q \text{ success probability} & Bin(m_i, q)& Bin\left(\sum_{i=1}^n m_i, q\right)\\  
\text{Geometric with mean } \beta & Geo(\beta) & NB(\beta,n)\\  
\text{Negative binomial with mean } r_i \beta~ \text{ and variance } ~r_i \beta (1+\beta) & NB(\beta,r_i)& NB\left(\beta,\sum_{i=1}^n r_i\right)\\  
\hline
    \end{array}
\end{matrix}
}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.2.4. Gamma Distribution.} Assume that
\(X_1,\ldots,X_n\) are independent random variables with
\(X_i \sim Gam(\alpha_i, \theta)\). The \emph{mgf} of \(X_i\) is
\(M_{X_i}(t) = (1 - \theta t)^{- \alpha_i}\). Thus, the \emph{mgf} of
the sum \(S_n = X_1 + \cdots + X_n\) is \[\begin{aligned}
M_{S_n}(t) &= \prod_{i=1}^n M_{X_i}(t) ~~~~ \text{from the independence of } X_i \text{'s} \\
&= \prod_{i=1}^n (1 - \theta t)^{- \alpha_i} = (1-\theta t)^{-\sum_{i=1}^n \alpha_i }~ ,
\end{aligned}\]

which is the \emph{mgf} of a gamma random variable with parameters
\((\sum_{i=1}^n \alpha_i, \theta)\). Thus,
\(S_n \sim Gam(\sum_{i=1}^n \alpha_i, \theta)\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.2.5. Negative Binomial Distribution.} Assume that
\(X_1,\ldots, X_n\) are independent random variables with
\(X_i \sim NB(\beta, r_i)\). The \emph{pgf} of \(X_i\) is
\(P_{X_i}(z) = \left[1-\beta(z-1) \right]^{-r_i}\). Thus, the \emph{pgf}
of the sum \(S_n =X_1+\cdots+X_n\) is

\[\begin{aligned}
P_{S_n}(z) &= \mathrm{E~}\left[ z^{S_n} \right] = \mathrm{E~}\left[ z^{X_1+\cdots+X_n} \right] = \mathrm{E~}\left[ z^{X_1} z^{X_2} \cdots z^{X_n} \right] \\
&= \mathrm{E~}\left[z^{X_1}\right] \cdots \mathrm{E~}\left[z^{X_n}\right] ~~~~ \text{under the independence of } X_i \text{'s} \\
&= \prod_{i=1}^n P_{X_i}(z) = \prod_{i=1}^n \left[1-\beta(z-1) \right]^{-r_i} = \left[1-\beta(z-1) \right]^{-\sum_{i=1}^n r_i} ~,
\end{aligned}\]

which is the \emph{pgf} of a negative binomial random variable with
parameters \((\beta, \sum_{i=1}^n r_i)\). Thus,
\(S_n \sim NB(\beta, \sum_{i=1}^n r_i)\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.2.6. Actuarial Exam Question (modified).} The annual
number of doctor visits for each individual in a family of 4 has
geometric distribution with mean 1.5. The annual numbers of visits for
the family members are mutually independent. An insurance pays 100 per
doctor visit beginning with the 4th visit per family. Calculate the
probability that the family will receive an insurance payment this year.

\textbf{Solution.} Let \(X_i \sim Geo(\beta=1.5)\) be the number of
doctor visits for one individual in the family and
\(S_4 = X_1 + X_2 + X_3 + X_4\) be the number of doctor visits for the
family. The sum of 4 independent geometric random variables each with
mean \(\beta=1.5\) follows a negative binomial distribution, i.e.
\(S_4 \sim NB(\beta=1.5, r=4)\).

If the insurance pays 100 per visit beginning with the 4th visit for the
family, then the family will not receive an insurance payment if they
have less than 4 claims. This probability is \[\begin{aligned}
    \Pr(S_4 < 4) &= \Pr(S_4 = 0) + \Pr(S_4 = 1) + \Pr(S_4 = 2) +\Pr(S_4 = 3) \\
    &= (1+1.5)^{-4} + \frac{4(1.5)}{(1+1.5)^5} + \frac{4(5)(1.5^2)}{2(1+1.5)^6} + \frac{4(5)(6)(1.5^3)}{3!(1+1.5)^7}\\
    &= 0.0256 + 0.0614 + 0.0922 + 0.1106 = 0.2898
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{Collective Risk Model}\label{collective-risk-model}

\subsection{Moments and Distribution}\label{moments-and-distribution}

Under the collective risk model \(S_N=X_1+\cdots+X_N\), \(\{X_i\}\) are
\emph{iid}, and independent of \(N\). Let
\(\mu = {\rm E}\left( X_{i}\right)\) and
\(\sigma ^{2} = {\rm Var}\left(X_{i}\right)\) for all \(i\). Using the
law of iterated expectations, the mean of the aggregate loss is

\begin{eqnarray*}
{\rm E}(S_N)={\rm E}_N[{\rm E}_S(S|N)] = {\rm E}_N(N\mu) = \mu {\rm E}(N).
\end{eqnarray*}

Using the law of total variance, the variance of the aggregate loss is
\[\begin{aligned}
{\rm Var}(S_N) &= {\rm E}_N[{\rm Var}(S_N|N)] + {\rm Var}_N[{\rm E}(S_N|N)] \\
&= \mathrm{E}_N \left[ \mathrm{Var}(X_1+\cdots+X_N) \right] + \mathrm{Var}_N\left[ \mathrm{E}(X_1+\cdots+X_N) \right] \\
&= \mathrm{E}_N \left[ \mathrm{Var}(X_1)+\cdots+ \mathrm{Var}(X_N) + 2\mathrm{Cov}(X_1, X_2) + \cdots + \mathrm{Cov}(X_{N-1}, X_N) \right] + \mathrm{Var}_N\left[ \mathrm{E}(X_1) + \cdots + \mathrm{E}(X_N) \right]  \\
&={\rm E}_N[N\sigma^2] + {\rm Var}_N[N\mu] ~~~~ \text{since } \mathrm{Cov}(X_i, X_j)=0 \text{ for all } i\neq j \text{ by independence} \\
&=\sigma^2{\rm E}[N] + \mu^2{\rm Var}[N]
\end{aligned}\]

\textbf{Special Case: Poisson Distributed Frequency.} If
\(N \sim Poi (\lambda)\), then \[\begin{aligned}
\mathrm{E}(N) &= \mathrm{Var}(N) = \lambda\\
\mathrm{E}(S) &= \lambda \mathrm{E}(X)\\
\mathrm{Var}(S) &= \lambda (\sigma^2 + \mu^2) = \lambda ~\mathrm{E} (X^2) .
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.3.1. Actuarial Exam Question.} The number of accidents
follows a Poisson distribution with mean 12. Each accident generates 1,
2, or 3 claimants with probabilities 1/2, 1/3, and 1/6 respectively.

Calculate the variance in the total number of claimants.

\textbf{Solution.} \[\begin{aligned} 
& \mathrm{E}(X^2) = 1^2 \left( \frac{1}{2}\right) + 2^2\left(\frac{1}{3} \right) + 3^2\left(\frac{1}{6}\right)
= \frac{10}{3} \\
\Rightarrow &\mathrm{Var}(S_N) = \lambda \ \mathrm{E}(X^2) = 12\left(\frac{10}{3}\right) = 40
\end{aligned}\]

Alternatively, using the general approach,
\(\mathrm{Var}(S_N) = \sigma^2 \mathrm{E}(N) + \mu^2 \mathrm{Var}(N)\),
where

\[\begin{aligned}
\mathrm{E}(N) &= \mathrm{Var}(N) = 12 \\
\mu &= \mathrm{E}(X) = 1\left(\frac{1}{2}\right) + 2\left(\frac{1}{3}\right) + 3\left(\frac{1}{6}\right)
= \frac{5}{3} \\
\sigma^2 &= \mathrm{E}(X^2) - [\mathrm{E}(X)]^2 = \frac{10}{3} - \frac{25}{9}
= \frac{5}{9} \\
\Rightarrow \ \mathrm{Var}(S) &= \left(\frac{5}{9}\right)\left(12\right) + \left(\frac{5}{3}\right)^2\left(12\right) = 40 .
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In general, the moments of \(S_N\) can be derived from its moment
generating function (\emph{mgf}). Because \(X_i\)'s are \emph{iid}, we
denote the \emph{mgf} of \(X\) as \(M_{X}(t) = \mathrm{E~}(e^{tX})\).
Using the law of iterated expectations, the \emph{mgf} of \(S_N\) is

\[\begin{aligned}
M_{S_N}(t) &= \mathrm{E}(e^{t S_N})=\mathrm{E}_N[~\mathrm{E}(e^{tS_N}|N)~]\\
&= \mathrm{E}_N \left[ ~\mathrm{E}\left( e^{t(X_1+\cdots+X_N)}\right) ~\right] = 
\mathrm{E}_N \left[ \mathrm{E}(e^{tX_1})\cdots\mathrm{E}(e^{tX_N}) \right] ~~~~ \text{since } X_i \text{'s are independent} \\
&= \mathrm{E}_N[~(M_{X}(t))^N~]
\end{aligned}\]

Now, recall that the probability generating function (\emph{pgf}) of
\(N\) is \(P_N(z) = \mathrm{E}(z^N)\). Denote \(M_{X}(t)=z\).
Substituting into the expression for the \emph{mgf} of \(S_N\) above, it
is shown

\[\begin{aligned}
M_{S_N}(t) = \mathrm{E~}(z^N)  = P_{N}(z) = P_{N}[M_{X}(t)].
\end{aligned}\]

Similarly, if \(S_N\) is discrete, one can show the \emph{pgf} of
\(S_N\) is: \[\begin{aligned}
P_{S_N}(z) = P_{N}[P_{X}(z)].
\end{aligned}\]

To get \(\mathrm{E}(S_N) = M_{S_N}'(0)\), we use the chain rule \[
M_{S_N}'(t) = \frac{\partial}{\partial t} P_{N}(M_{X}(t)) = P_{N}'(M_{X}(t)) M_{X}'(t)\\
\] and recall
\(M_{X}(0) = 1, M_{X}'(0) = \mathrm{E}(X) = \mu, P_{N}'(1) = \mathrm{E}(N)\).
So, \[\begin{aligned}
\mathrm{E}(S_N) = M_{S_N}'(0) = P_{N}'(M_{X}(0)) M_{X}'(0) = \mu {\rm E}(N)
\end{aligned}\]

Similarly, one could use relation \(\mathrm{E}(S_N^2) = M_{S_N}''(0)\)
to get
\[\mathrm{Var}(S_N) = \sigma^2 \mathrm{E}(N) + \mu^2 \mathrm{Var}(N).\]

\textbf{Special Case. Poisson Frequency.} Let \(N \sim Poi (\lambda)\).
Thus, the \emph{pgf} of \(N\) is \(P_N (z) = e^{\lambda(z-1)}\) and the
\emph{mgf} of \(S_N\) is \[\begin{aligned}
M_{S_N}(t) &= P_N[M_X(t)] = e^{\lambda(M_{X}(t) - 1)}.
\end{aligned}\]

Taking derivatives yields \[\begin{aligned}
M_{S_N}'(t) &= e^{\lambda(M_{X}(t) - 1)}~ \lambda~ M_{X}'(t) = M_{S_N}(t) ~\lambda ~M_{X}'(t)\\
M_{S_N}''(t) &= M_{S}(t) ~\lambda~ M_{X}''(t) + [~M_{S}(t)~\lambda~ M_{X}'(t)~] ~\lambda~ M_{X}'(t)
\end{aligned}\]

Evaluating these at \(t=0\) yields \[\begin{aligned}
\mathrm{E}(S_N) &= M_{S_N}'(0) =  \lambda \mathrm{E}(X) = \lambda \mu
\end{aligned}\] and

\[\begin{aligned} M_{S_N}''(0) &= \lambda \mathrm{E}(X^2) + \lambda^2 \mu^2\\
\Rightarrow \mathrm{Var}(S_N) &= \lambda \mathrm{E}(X^2) + \lambda^2 \mu^2 - (\lambda \mu)^2 = \lambda~ \mathrm{E}(X^2).
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.3.2. Actuarial Exam Question.} You are the producer of
a television quiz show that gives cash prizes. The number of prizes,
\(N\), and prize amount, \(X\), have the following distributions:

\[
{\small 
\begin{matrix}
\begin{array}{ccccc}\hline
    n & \Pr(N=n) & & x & \Pr(X=x)\\ \hline
    1 & 0.8 & & 0 & 0.2 \\
    2 & 0.2 & & 100 & 0.7 \\
       &       & & 1000 & 0.1\\\hline
  \end{array}
\end{matrix}
}
\]

Your budget for prizes equals the expected aggregate cash prizes plus
the standard deviation of aggregate cash prizes. Calculate your budget.

\textbf{Solution.} We need to calculate the mean and standard deviation
of the aggregate (sum) of cash prizes. The moments of the frequency
distribution \(N\) are \[\begin{aligned}
\mathrm{E}(N) &= 1 (0.8) + 2 (0.2) =1.2\\
\mathrm{E}(N^2) &=  1^2 (0.8) + 2^2 (0.2) =1.6\\
\mathrm{Var}(N) &= \mathrm{E}(N^2) - \left[ \mathrm{E}(N) \right]^2= 0.16
\end{aligned}\]

The moments of the severity distribution \(X\) are \[\begin{aligned}
\mathrm{E}(X) &= 0 (0.2) + 100 (0.7) + 1000 (0.1) = 170 = \mu\\
\mathrm{E}(X^2) &= 0^2 (0.2) + 100^2 (0.7) + 1000^2 (0.1) = 107,000\\
\mathrm{Var}(X) &= \mathrm{E}(X^2) - \left[ \mathrm{E}(X) \right]^2=78,100 = \sigma^2
\end{aligned}\]

Thus, the mean and variance of the aggregate cash prize are
\[\begin{aligned}
\mathrm{E}(S_N)  &= \mu \mathrm{E}(N) = 170 (1.2) = 204 \\
\mathrm{Var}(S_N) &= \sigma^2 \mathrm{E}(N) + \mu^2 \mathrm{Var}(N)\\
&= 78,100 (1.2) + 170^2 (0.16) = 98,344
\end{aligned}\]

This gives the following required budget \[\begin{aligned}
Budget &= \mathrm{E}(S_N) + \sqrt{\mathrm{Var}(S_N)} \\
&= 204 + \sqrt{98,344} = 517.60 .
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The distribution of \(S_N\) is called a compound distribution, and it
can be derived based on the convolution of \(F_X\) as follows:
\[\begin{aligned}
F_{S_N}(s) &= \Pr \left(X_1 + \cdots + X_N \le s \right) \\
&=  \mathrm{E} \left[ \Pr \left(X_1 + \cdots + X_N  \le s|N=n \right) \right]\\
&=  \mathrm{E} \left[ F_{X}^{\ast N}(s) \right] \\
&=  p_0 + \sum_{n=1}^{\infty }p_n F_{X}^{\ast n}(s)
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.3.3. Actuarial Exam Question.} The number of claims in
a period has a geometric distribution with mean \(4\). The amount of
each claim \(X\) follows \(\Pr(X=x) = 0.25, \ x=1,2,3,4\), i.e.~a
discrete uniform distribution on \(\{1,2,3,4\}\). The number of claims
and the claim amounts are independent. Let \(S_N\) denote the aggregate
claim amount in the period. Calculate \(F_{S_N}(3)\).

\textbf{Solution.} By definition, we have \[\begin{aligned}
F_{S_N}\left(3 \right) &= {\rm Pr}\left(\sum_{i=1}^N X_i \leq 3\right) = \sum_{n=0}^\infty {\rm Pr}\left(\sum_{i=1}^n X_i\leq 3|N=n\right){\rm Pr}(N=n) \\
&= \sum_n F^{\ast n} \left(3 \right) p_n = \sum_{n=0}^3 F^{\ast n}(3) p_n \\
&= p_0 + F^{\ast 1}(3) \ p_1 + F^{\ast 2}(3) \ p_2 + F^{\ast 3}(3) \ p_3
\end{aligned}\] Because \(N \sim Geo(\beta=4)\), we know that
\[\begin{aligned}
p_n &= \frac{1}{1+\beta}
\left(\frac{\beta}{1+ \beta} \right)^n = \frac{1}{5} \left(\frac{4}{5} \right)^n
\end{aligned}\] For the claim severity distribution, recursively, we
have \[\begin{aligned}
F^{\ast 1}(3) &= \Pr(X \le 3) = \frac{3}{4} \\
F^{\ast 2}(3) &= \sum_{y \le 3} F^{\ast 1} (3-y) f(y) = F^{\ast 1}(2)f(1) + F^{\ast 1}(1)f(2) \\
&= \frac{1}{4}\left[F^{\ast 1} (2) + F^{\ast 1}(1)\right] = \frac{1}{4}\left[{\rm Pr}(X\leq 2) + {\rm Pr}(X \leq 1) \right] \\
&= \frac{1}{4} \left(\frac{2}{4} + \frac{1}{4} \right) = \frac{3}{16}\\
F^{\ast 3}(3) &= \Pr(X_1+X_2 + X_3 \le 3) = \Pr(X_1=X_2=X_3=1) = \left(\frac{1}{4} \right)^3
\end{aligned}\] Notice that we did not need to recursively calculate
\(F^{\ast 3}(3)\) by recognizing that each \(X \in \{1,2,3,4\}\), so the
only way of obtaining \(X_1+X_2+X_3 \leq 3\) is to have
\(X_1=X_2=X_3=1\). Additionally, for \(n \geq 4\), \(F^{\ast n} (3)=0\)
since it is impossible for the sum of 4 or more \(X\)'s to be less than
3. For \(n=0\), \(F^{\ast 0}(3) = 1\) since the sum of 0 \(X\)'s is 0,
which is always less than 3. Laying out the probabilities
systematically,

\[\begin {matrix}
\begin{array}{c c c c}\hline
    x & F^{\ast 1}(x) & F^{\ast 2}(x) & F^{\ast 3}(x)\\ \hline
    0 & & & \\
    1 & \frac{1}{4} & 0 & \\
    2 & \frac{2}{4} & \left( \frac{1}{4} \right)^2 & \\
    3 & \frac{3}{4} & \frac{3}{16} & \left( \frac{1}{4} \right)^3 \\ \hline
\end{array}
\end{matrix}\]

Finally, \[\begin{aligned}
F_{S_N}(3) &= p_0 + F^{\ast 1}(3) \ p_1 + F^{\ast 2}(3) \ p_2 + F^{\ast 3}(3) \ p_3 \\
&= \frac{1}{5} + \frac{3}{4}\left(\frac{4}{25} \right) + \frac{3}{16} \left( \frac{16}{125} \right) + \frac{1}{64} \left( \frac{64}{625}\right) = 0.3456\\
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

When \(\mathrm{E}(N)\) and \(\mathrm{Var}(N)\) are known, one may also
use the central limit theorem to approximate the distribution of \(S_N\)
as in the individual risk model. That is,
\(\frac{S_N - \mathrm{E}(S_N)}{\sqrt{\mathrm{Var}(S_N)}}\) approximately
follows the standard normal distribution \(N(0,1)\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.3.4. Actuarial Exam Question..} You are given:

\[
{\small 
\begin{matrix}
  \begin{array}{ c | c  c }
    \hline
      & \text{Mean} & \text{Standard Deviation}\\ \hline
    \text{Number of Claims} & 8 & 3\\
    \text{Individual Losses} & 10,000 & 3,937\\
    \hline
  \end{array}
\end{matrix}
}
\] Using the normal approximation, determine the probability that the
aggregate loss will exceed 150\(\%\) of the expected loss.

\textbf{Solution.} To use the normal approximation, we must first find
the mean and variance of the aggregate loss \(S\) \[\begin{aligned}
\mathrm{E}(S_N) &= \mu \ \mathrm{E}(N) = 10,000(8) = 80,000\\
\mathrm{Var}(S_N) &= \sigma^2 \ \mathrm{E}(N) + \mu^2 \ \mathrm{Var}(N)\\
&= 3937^2(8) + 10000^2 (3^2) = 1,023,999,752\\
\sqrt{\mathrm{Var}(S_N)} &= 31,999.996 \approx 32,000
\end{aligned}\]

Then under the normal approximation, aggregate loss \(S_N\) is
approximately normal with mean 80,000 and standard deviation 32,000. The
probability that \(S_N\) will exceed 150\(\%\) of the expected aggregate
loss is therefore \[\begin{aligned}
\Pr(S_N>1.5 \mathrm{E}(S_N)~) &= \Pr \left( \frac{S_N - \mathrm{E} (S_N)}{\sqrt{\mathrm{Var}(S_N)}} > \frac{1.5 ~\mathrm{E}(S_N) - \mathrm{E} (S_N)}{\sqrt{\mathrm{Var}(S_N)}} \right) \\
&\approx \Pr \left( Z > \frac{0.5~ \mathrm{E}(S_N)}{\sqrt{\mathrm{Var}(S_N)} } \right), ~~~~ \text{where } Z\sim N(0,1) \\
&= \Pr \left( Z > \frac{0.5(80,000)}{32,000} \right) = \Pr( Z > 1.25) \\
&= 1-\Phi(1.25) = 0.1056
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.3.5. Actuarial Exam Question.} For an individual over
\(65\):\\
(i) The number of pharmacy claims is a Poisson random variable with mean
\(25\).\\
(ii) The amount of each pharmacy claim is uniformly distributed between
\(5\) and \(95\).\\
(iii) The amounts of the claims and the number of claims are mutually
independent.\\
Estimate the probability that aggregate claims for this individual will
exceed \(2000\) using the normal approximation.

\textbf{Solution.} We have claim frequency \(N \sim Poi(\lambda = 25)\)
and claim severity \(X \sim U \left(5, 95 \right)\). To use the normal
approximation, we need to find the mean and variance of the aggregate
claims \(S_N\). Note

\[\begin{matrix}
\begin{array}{lll}
\mathrm{E} (N) = 25 & & \mathrm{Var} (N) = 25\\
\mathrm{E}(X) = \frac{5+95}{2} = 50 = \mu & & \mathrm{Var}(X) = \frac{(95-5)^2}{12} = 675 = \sigma^2\\
\end{array}
\end{matrix}\]

Then for \(S_N\), \[\begin{aligned}
\mathrm{E}(S_N) &= \mu \ \mathrm{E} (N) = 50(25) = 1,250\\
\mathrm{Var}(S_N) &= \sigma^2 \ \mathrm{E}(N) + \mu^2 \ \mathrm{Var}(N)\\
&= 675 (25) + 50^2 (25) = 79,375
\end{aligned}\]

Using the normal approximation, \(S_N\) is approximately normal with
mean 1,250 and variance 79,375. The probability that \(S_N\) exceeds
2,000 is \[\begin{aligned}
\Pr(S_N>2,000) &= \Pr \left(\frac{S_N - \mathrm{E} (S_N)}{\sqrt{\mathrm{Var} (S_N)}} > \frac{2,000- \mathrm{E} (S_N)}{\sqrt{\mathrm{Var} (S_N)}} \right) \\
&= \Pr\left( Z > \frac{2,000-1,250}{\sqrt{79,375}} \right), ~~~~ \text{where } Z\sim N(0,1) \\
&= \Pr (Z > 2.662) = 1-\Phi(2.662) = 0.003884
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Stop-loss Insurance}\label{stop-loss-insurance}

Recall the coverage modifications on the individual policy level in
Section \ref{S:CoverageModifications}. Insurance on the aggregate loss
\(S_N\), subjected to a deductible \(d\), is called
\textit{net stop-loss insurance}. The expected value of the amount of
the aggregate loss in excess of the deductible,

\begin{eqnarray*}
\mathrm{E}[(S-d)_+]
\end{eqnarray*}

is known as the \emph{net stop-loss premium}.

To calculate the net stop-loss premium, we have

\begin{eqnarray*}
\mathrm{E}(S_N-d)_+ 
&=&
\left\{\begin{array}{ll}
\int_{d}^{\infty}(s-d) f_{S_N}(s) ds& \text{for continuous } S_N\\
 \sum_{s>d}(s-d) f_{S_N}(s) & \text{for discrete } S_N\\
 \end{array}\right.\\
&=& \mathrm{E}(S_N) - \mathrm{E}(S_N\wedge d)\\
\end{eqnarray*}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.3.6. Actuarial Exam Question.} In a given week, the
number of projects that require you to work overtime has a geometric
distribution with \(\beta=2\). For each project, the distribution of the
number of overtime hours in the week, \(X\), is as follows:

\[
{\small 
\begin{matrix}
\begin{array}{ccc} \hline
    x &  & f(x)\\ \hline
    5 &  & 0.2 \\
    10 & & 0.3 \\
    20 & & 0.5\\ \hline
  \end{array}
\end{matrix}
}
\]

The number of projects and the number of overtime hours are independent.
You will get paid for overtime hours in excess of 15 hours in the week.
Calculate the expected number of overtime hours for which you will get
paid in the week.

\textbf{Solution.} The number of projects in a week requiring overtime
work has distribution \(N \sim Geo(\beta=2)\), while the number of
overtime hours worked per project has distribution \(X\) as described
above. The aggregate number of overtime hours in a week is \(S_N\) and
we are therefore looking for
\[\mathrm{E}(S_N-15)_+ = \mathrm{E}(S_N) - \mathrm{E}(S_N \wedge 15).\]

To find \(\mathrm{E}(S_N) = \mathrm{E}(X) ~\mathrm{E}(N)\), we have
\[\begin{aligned}
&\mathrm{E}(X) = 5(0.2) + 10(0.3)+ 20(0.5)= 14 \\
&\mathrm{E}(N) = 2 \\
\Rightarrow \ &\mathrm{E}(S) = \mathrm{E}(X) ~ \mathrm{E}(N) = 14(2) = 28
\end{aligned}\]

To find
\(\mathrm{E} (S_N \wedge 15) = 0 \Pr (S_N=0) + 5 \Pr(S_N=5) + 10 \Pr(S_N=10) + 15 \Pr(S_N \geq 15)\),
we have \[\begin{aligned}
\Pr(S_N=0) &= \Pr(N=0) = \frac{1}{1+\beta} = \frac{1}{3} \\
\Pr(S_N=5) &= \Pr(X=5, \ N=1) = 0.2 \left(\frac{2}{9} \right)= \frac{0.4}{9}\\
\Pr(S_N=10) &= \Pr(X=10, \ N=1) + \Pr(X_1=X_2=5, N=2) \\
&= 0.3 \left(\frac{2}{9} \right) + (0.2)(0.2) \left( \frac{4}{27} \right)= 0.0726 \\
\Pr(S_N \geq 15) &= 1 - \left(\frac{1}{3} + \frac{0.4}{9} + 0.0726 \right) = 0.5496\\
\Rightarrow \mathrm{E}(S_N \wedge 15) &= 0 \Pr (S_N=0) + 5 \Pr(S_N=5) + 10 \Pr(S_N=10) + 15 \Pr(S_N \geq 15) \\
&= 0 \left( \frac{1}{3} \right) + 5
\left( \frac{0.4}{9} \right) + 10 (0.0726) + 15 (0.5496) = 9.193\\
\end{aligned}\] Therefore, \[\begin{aligned}
\mathrm{E}(S_N-15)_+ &= \mathrm{E}(S_N) - \mathrm{E}(S_N \wedge 15) \\
&= 28 - 9.193 = 18.807
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Recursive Net Stop-Loss Premium Calculation}. For the discrete
case, this can be computed recursively as \[\begin{aligned}
\mathrm{E}\left[ \left( S_N-(j+1)h \right) _{+} \right]=\mathrm{E}\left[ ( S_N-jh )_{+} \right] -h \left( 1-F_{S_N}(jh)
\right) .
\end{aligned}\] This assumes that the support of \(S_N\) is equally
spaced over units of \(h\).

To establish this, we assume that \(h=1\). We have \[\begin{aligned}
\mathrm{E}\left[ \left( S_N-(j+1) \right) _{+} \right] &=\mathrm{E}(S_N) - \mathrm{E}[S_N\wedge (j+1)] \ ,\ \text{ and } \\
\mathrm{E}\left[ \left( S_N - j \right)_+ \right] &=\mathrm{E}(S_N) - \mathrm{E}[S_N\wedge j]
\end{aligned}\]

Thus, \[\begin{aligned}
\mathrm{E}\left[ \left(S_N-(j+1) \right) _{+}\right] - \mathrm{E}\left[ ( S_N-j )_{+} \right] 
&= \left\{\mathrm{E}(S_N) - \mathrm{E}(S_N\wedge (j+1)) \right\}   -  \left\{\mathrm{E}(S_N) - \mathrm{E}(S_N\wedge j) \right\} \\
&= \mathrm{E}\left(S_N \wedge j \right) - \mathrm{E}\left[ S \wedge (j+1) \right]
\end{aligned}\]

We can write \[\begin{aligned}
\mathrm{E}\left[S_N\wedge (j+1)\right] &= \sum_{x=0}^{j}xf_{S_N}(x) + (j+1)~\Pr(S_N \ge j+1) \\
&= \sum_{x=0}^{j-1}x f_{S_N}(x) + j~\Pr(S_N=j) + (j+1)~\Pr(S_N \ge j+1)
\end{aligned}\]

Similarly, \[\begin{aligned}
\mathrm{E}(S_N\wedge j) = \sum_{x=0}^{j-1}xf_{S_N}(x) + j~\Pr(S_N\ge j)
\end{aligned}\]

With these, expressions, we have \[\begin{aligned}
\mathrm{E}\left[ \left( S_N-(j+1) \right) _{+} \right] - \mathrm{E~}\left[ ( S_N-j )_{+} \right] 
&= \mathrm{E}\left(S_N \wedge j \right) - \mathrm{E}\left[ S \wedge (j+1) \right] \\
&= \left\{ \sum_{x=0}^{j-1}xf_{S_N}(x) + j~\Pr(S_N\ge j) \right\}
- \left\{ \sum_{x=0}^{j-1}x f_{S_N}(x) + j~\Pr(S_N=j) + (j+1)~\Pr(S_N \ge j+1) \right\} \\
&= j~\left[\Pr(S_N \geq j) - \Pr(S_N=j) \right]- (j+1)~\Pr(S_N \ge j+1) \\
&= j~\Pr( S_N > j) - (j+1)~\Pr(S_N \ge j+1) ~~~~ \text{ (note } \Pr(S_N > j) = \Pr(S_N \geq j+1) \text{)} \\
&= -\Pr(S_N\ge j+1) = -\left[1 - F_{S_N}(j)\right],
\end{aligned}\] as required.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.3.7. Actuarial Exam Question - Continued.} Recall that
the goal of this question was to calculate \(\mathrm{E~}(S_N-15)_+\).
Note that the support of \(S_N\) is equally spaced over units of 5, so
this question can also be done recursively, using the expression above
with steps of \(h=5\):

\begin{itemize}
\item
  Step 1:\\
  \[\begin{aligned}
  \mathrm{E~}(S_N-5)_+ &= \mathrm{E}(S_N) - 5 [1-\Pr(S_N \leq 0) ]\\ %\Pr (S_N\geq 5) \\
  &= 28 - 5 \left(1 - \frac{1}{3}\right) = \frac{74}{3}=24.6667
  \end{aligned}\]
\item
  Step 2:\\
  \[\begin{aligned}
  \mathrm{E~}(S_N-10)_+ &= \mathrm{E~}(S_N-5)_+ - 5 [1-\Pr(S_N \leq 5)]\\ %\Pr (S_N\ge 10) \\
  &= \frac{74}{3} - 5\left( 1 - \frac{1}{3} - \frac{0.4}{9}\right) = 21.555
  \end{aligned}\]
\item
  Step 3: \[\begin{aligned}
  \mathrm{E~}(S_N-15)_+ &= \mathrm{E~}(S_N-10)_+ - 5 [1-\Pr(S_N \leq 10)] \\ %\Pr (S_N\ge 15) \\
  &= \mathrm{E~}(S_N-10)_+ - 5\Pr (S_N\ge 15) \\
  &= 21.555 - 5 (0.5496) = 18.807
  \end{aligned}\]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Analytic Results}\label{analytic-results}

There are a few combinations of claim frequency and severity
distributions that result in an easy-to-compute distribution for
aggregate losses. This section provides some simple examples. Although
these examples are computationally convenient, they are generally too
simple to be used in practice.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.3.8.} One has a closed-form expression for the
aggregate loss distribution by assuming a geometric frequency
distribution and an exponential severity distribution.

Assume that claim count \(N\) is geometric with mean
\(\mathrm{E}(N)=\beta\), and that claim amount \(X\) is exponential with
\(\mathrm{E}(X)=\theta\). Recall that the \emph{pgf} of \(N\) and the
\emph{mgf} of \(X\) are: \[\begin{aligned}
P_N (z) &=\frac{1}{1- \beta (z-1)}\\
M_{X}(t) &=\frac{1}{1-\theta t}
\end{aligned}\] Thus, the \emph{mgf} of aggregate loss \(S_N\) can be
expressed two ways (for details, see \emph{Technical Supplement 5.A.3})

\begin{eqnarray}
M_{S_N}(t) &=& P_N [M_{X}(t)] = \frac{1}{1 - \beta \left( \frac{1}{1-\theta t} - 1\right)} \nonumber\\
&=& 1+ \frac{\beta}{1+\beta} \left([1-\theta(1+\beta)t]^{-1}-1 \right)\\
&=& \frac{1}{1+\beta}(1) +\frac{\beta}{1+\beta}
\left( \frac{1}{1-\theta (1+\beta)t}\right)
\end{eqnarray}

From (5.1), we note that \(S_N\) is equivalent to the compound
distribution of \(S_N=X^{*}_1+\cdots+X^{*}_{N^{*}}\), where \(N^{*}\) is
a Bernoulli with mean \(\beta/(1+\beta)\) and \(X^{*}\) is an
exponential with mean \(\theta(1+\beta)\). To see this, we examine the
\emph{mgf} of \(S\): \[\begin{aligned}
M_{S_N}(t) = P_N [M_{X}(t)] = P_{N^{*}} [M_{X^{*}}(t)],
\end{aligned}\] where \[\begin{aligned}
P_{N^*} (z) &=1+ \frac{\beta}{1+ \beta} (z-1),\\
M_{X^*} (t) &=\frac{1}{1- {{\theta(1+\beta)}} t}.
\end{aligned}\]

From (5.2), we note that \(S_N\) is also equivalent to a 2-point mixture
of 0 and \(X^{*}\). Specifically,

\begin{eqnarray*}
S_N &=&
\left\{
\begin{array}{cl}
0 & {\rm with~ probability ~Pr}(N^*=0) = 1/(1+\beta) \\
Y^{*} & {\rm with~ probability ~Pr}(N^*=1) = \beta/(1+\beta)
\end{array}
\right..
\end{eqnarray*}

The distribution function of \(S_N\) is:

\begin{eqnarray*}
\Pr(S_N=0) &=& \frac{1}{1+\beta}\\
\Pr(S_N>s) &=& \Pr(X^*>s) =\frac{\beta}{1+\beta} \exp\left( -\frac{s}{
\theta (1+\beta)}\right)
\end{eqnarray*}

with pdf

\begin{eqnarray*}
f_{S_N}(s) = \frac{\beta}{\theta (1+\beta)^2}\exp\left( -\frac{s}{
\theta (1+\beta)}\right).
\end{eqnarray*}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.3.9.} Consider a collective risk model with an
exponential severity and an arbitrary frequency distribution. Recall
that if \(X_i\sim Exp(\theta)\), then the sum of \emph{iid} exponential,
\(S_n=X_1+\cdots+X_n\), has a gamma distribution, i.e.
\(S_n\sim Gam(n,\theta)\). This has cdf:

\begin{eqnarray*}
F_{X}^{\ast n}(s) &=& \Pr (S_n \le s) = \int_{0}^{s} \frac{1}{\Gamma(n)\theta^n}s^{n-1}\exp\left(-\frac{s}{\theta}\right) ds\\
&=& 1-\sum_{j=0}^{n-1}\frac{1}{j!}\left( \frac{s}{\theta}\right)^j e^{-s/\theta } .
\end{eqnarray*}

The last equality is derived by applying integration by parts \(n-1\)
times.

For the aggregate loss distribution, we can interchange the order of
summations in the second line below to get

\begin{eqnarray*}
F_{S}\left(s\right) &=& p_{0}+\sum_{n=1}^{\infty }p_n F_{X}^{\ast n}\left(s\right)\\
&=& 1 - \sum_{n=1}^{\infty }p_n \sum_{j=0}^{n-1}\frac{1}{j!}
\left( \frac{s}{\theta}\right)^j e^{-s/\theta }\\
&=& 1-e^{-s/\theta}\sum_{j=0}^{\infty} \frac{1}{j!}
\left( \frac{s}{\theta} \right)^j \overline{P}_j
\end{eqnarray*}

where \(\overline{P}_j =p_{j+1}+p_{j+2}+\cdots = \Pr (N>j)\) is the
``survival function'' of the claims count distribution.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Tweedie Distribution}\label{tweedie-distribution}

In this section, we examine a particular compound distribution where the
number of claims has a Poisson distribution and the amount of claims has
a gamma distribution. This specification leads to what is known as a
Tweedie distribution. The Tweedie distribution has a mass probability at
zero and a continuous component for positive values. Because of this
feature, it is widely used in insurance claims modeling, where the zero
mass is interpreted as no claims and the positive component as the
amount of claims.

Specifically, consider the collective risk model \(S_N=X_1+\cdots+X_N\).
Suppose that \(N\) has a Poisson distribution with mean \(\lambda\), and
each \(X_i\) has a gamma distribution shape parameter \(\alpha\) and
scale parameter \(\gamma\). The Tweedie distribution is derived as the
Poisson sum of gamma variables. To understand the distribution of
\(S_N\), we first examine the mass probability at zero. The aggregate
loss is zero when no claims occurred, i.e.
\[{\rm Pr}(S_N=0)= {\rm Pr}(N=0)=e^{-\lambda}.\] In addition, note that
\(S_N\) conditional on \(N=n\), denoted by \(S_n=X_1+\cdots+X_n\),
follows a gamma distribution with shape \(n\alpha\) and scale
\(\gamma\). Thus, for \(s>0\), the density of a Tweedie distribution can
be calculated as \[\begin{aligned}
f_{S_n}(s)&=\sum_{n=1}^{\infty} p_n f_{S_n}(s)\\
&=\sum_{n=1}^{\infty}e^{-\lambda}\frac{(\lambda)^n}{n!}\frac{\gamma^{na}}{\Gamma(n\alpha)}s^{n\alpha-1}e^{-s\gamma}
\end{aligned}\] Thus, the Tweedie distribution can be thought of a
mixture of zero and a positive valued distribution, which makes it a
convenient tool for modeling insurance claims and for calculating pure
premiums. The mean and variance of the Tweedie compound Poisson model
are:
\[{\rm E} (S_N)=\lambda\frac{\alpha}{\gamma}~~~~{\rm and}~~~~{\rm Var} (S)=\lambda\frac{\alpha(1+\alpha)}{\gamma^2}.\]

As another important feature, the Tweedie distribution is a special case
of exponential dispersion models, a class of models used to describe the
random component in generalized linear models. To see this, we consider
the following reparameterization:

\begin{equation*}
\lambda=\frac{\mu^{2-p}}{\phi(2-p)},~~~~\alpha=\frac{2-p}{p-1},~~~~\frac{1}{\gamma}=\phi(p-1)\mu^{p-1}
\end{equation*}

With the above relationships, one can show that the distribution of
\(S_N\) is
\[f_{S_N}(s)=\exp\left[\frac{1}{\phi}\left(\frac{-s}{(p-1)\mu^{p-1}}-\frac{\mu^{2-p}}{2-p}\right)+C(s;\phi)\right]\]
where

\begin{equation*}
C(s;\phi/\omega_i)=\left\{\begin{array}{ll}
                    \displaystyle 0 & {\rm if}~ y=0 \\
                   \displaystyle \ln \sum\limits_{n\ge 1} \left\{\frac{(1/\phi)^{1/(p-1)}y^{(2-p)/(p-1)}}{(2-p)(p-1)^{(2-p)/(p-1)}}\right\}^{n}\frac{1}{n!\Gamma(n(2-p)/(p-1))s} & {\rm if}~ y>0
                  \end{array}\right.
\end{equation*}

Hence, the distribution of \(S_N\) belongs to the exponential family
with parameters \(\mu\), \(\phi\), and \(1 < p < 2\), and we have
\[{\rm E} (S_N)=\mu~~~~{\rm and}~~~~{\rm Var} (S_N)=\phi\mu^{p}.\] This
allows us to use the Tweedie distribution with generalized linear models
to model claims. It is also worth mentioning the two limiting cases of
the Tweedie model: \(p\rightarrow 1\) results in the Poisson
distribution and \(p\rightarrow 2\) results in the gamma distribution.
Thus, the Tweedie model accommodates the situations in between the gamma
and Poisson distributions, which makes intuitive sense as it is the
Poisson sum of gamma random variables.

\section{Computing the Aggregate Claims
Distribution}\label{computing-the-aggregate-claims-distribution}

Computing the distribution of aggregate losses is a difficult, yet
important, problem. As we have seen, for both individual risk model and
collective risk model, computing the distribution frequently involves
the evaluation of a \(n\)-fold convolution. To make the problem
tractable, one strategy is to use a distribution that is easy to
evaluate to approximate the aggregate loss distribution. For instance,
normal distribution is a natural choice based on central limit theorem
where parameters of the normal distribution can be estimated by matching
the moments. This approach has its strength and limitations. The main
advantage is the ease of computation. The disadvantage are: first, the
size and direction of approximation error are unknown; second, the
approximation may fail to capture some special features of the aggregate
loss such as mass point at zero.

This section discusses two practical approaches to computing the
distribution of aggregate loss, the recursive method and the simulation.

\subsection{Recursive Method}\label{recursive-method}

The recursive method applies to compound models where the frequency
component \(N\) belongs to either \((a,b,0)\) or \((a,b,1)\) class (see
Sections \ref{S:the-a-b-0-class} and
\ref{S:zero-truncation-or-modification}) and the severity component
\(X\) has a discrete distribution. For continuous \(X\), a common
practice is to first discretize the severity distribution, after which
the recursive method is ready to apply.

Assume that \(N\) is in the \((a,b,1)\) class so that
\(p_{k}=\left( a+\frac{b}{k} \right) p_{k-1}, k = 2,3,\ldots\). Further
assume that the support of \(X\) is \(\{0,1,\ldots,m\}\), discrete and
finite. Then, the probability function of \(S_N\) is: \[\begin{aligned}
f_{S_N}(s)&=\Pr (S=s) \\
&=\frac{1}{1-af_{X}(0)}\left\{ \left[ p_1 -(a+b)p_{0}\right]
f_X (s)+\sum_{x=1}^{s\wedge m}\left( a+\frac{bx}{s} \right) f_X (x)f_{S_N}(s-x)\right\}.
\end{aligned}\] If \(N\) is in the \((a,b,0)\) class, then
\(p_1=(a+b)p_0\) and so \[
f_{S_N}(s)=\frac{1}{1-af_X (0)}\left\{ \sum_{x=1}^{s\wedge m}\left( a+\frac{bx
}{s}\right) f_X (x)f_{S_N}(s-x)\right\}.
\] \textbf{Special Case: Poisson Frequency.} If \(N \sim Poi(\lambda)\),
then \(a=0\) and \(b=\lambda\), and thus \[
f_{S_N}(s)=\frac{\lambda }{s}\left\{ \sum_{x=1}^{s \wedge
m} x f_X (x) f_{S_N} (s-x)\right\} .
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.4.1. Actuarial Exam Question.} The number of claims in
a period \(N\) has a geometric distribution with mean 4. The amount of
each claim \(X\) follows \({\rm Pr} (X = x) = 0.25\), for
\(x = 1,2,3,4\). The number of claims and the claim amount are
independent. \(S_N\) is the aggregate claim amount in the period.
Calculate \(F_{S_N}(3)\).

\textbf{Solution.} The severity distribution \(X\) follows
\[f_X (x) = \frac{1}{4}, \ \ x=1, 2, 3, 4.\] The frequency distribution
\(N\) is geometric with mean 4, which is a member of the \((a,b,0)\)
class with \(b=0\), \(a=\frac{\beta}{1+\beta} = \frac{4}{5}\), and
\(p_0 = \frac{1}{1+\beta} = \frac{1}{5}\). The support of severity
component \(X\) is \(\{1,\ldots,m=4 \}\), discrete and finite. Thus, we
can use the recursive method \[\begin{aligned}
f_{S_N} (x) &= 1 \sum_{y=1}^{x\wedge m} (a+0) f_X (y) f_{S_N} (x-y) \\
&= \frac{4}{5} \sum_{y=1}^{x\wedge m} f_X (y) f_{S_N} (x-y)
\end{aligned}\] Specifically, we have \[\begin{aligned}
f_{S_N} (0) &= \Pr(N=0) = p_0=\frac{1}{5}\\
f_{S_N} (1) &= \frac{4}{5}\sum_{y=1}^{1} f_X (y) f_{S_N} (1-y) = \frac{4}{5} f_X(1) f_{S_N}(0)\\
&= \frac{4}{5}\left( \frac{1}{4}\right)\left(\frac{1}{5} \right) = \frac{1}{25}\\
f_{S_N} (2) &=  \frac{4}{5}\sum_{y=1}^{2} f_X (y) f_{S_N} (2-y) = \frac{4}{5} \left[ f_X(1)f_{S_N}(1) + f_X(2) f_{S_N}(0) \right] \\
&= \frac{4}{5}\left[ \frac{1}{4} \left( \frac{1}{25} + \frac{1}{5}\right) \right] =
\frac{4}{5}\left( \frac{6}{100}\right) = \frac{6}{125}\\
f_{S_N} (3) &= \frac{4}{5} \left[ f_X(1) f_{S_N}(2) + f_X(2)f_{S_N}(1) + f_X(3) f_{S_N}(0) \right]\\
&= \frac{4}{5}\left[ \frac{1}{4} \left( \frac{1}{25} + \frac{1}{5} +
\frac{6}{125}\right) \right] = \frac{1}{5}\left( \frac{5+25+6}{125}\right) = 0.0576\\
\Rightarrow \ F_{S_N} (3) &= f_{S_N} (0)+f_{S_N} (1)+f_{S_N} (2) +f_{S_N} (3) = 0.3456
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Simulation}\label{simulation}

The distribution of aggregate loss can be evaluated using Monte Carlo
simulation. The idea is that one can calculate the empirical
distribution of \(S_N\) using a random sample. The expected value and
variance of the aggregate loss can also be estimated using the sample
mean and sample variance of the simulated values. Below we summarize the
simulation procedures for the aggregate loss models. Let \(m\) be the
size of the generated random sample of aggregate losses.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Individual Risk Model \(S_n=X_1+\cdots+X_n\)

  \begin{itemize}
  \tightlist
  \item
    Let \(j=1,\ldots,m\) be a counter. Start by setting \(j=1\).
  \item
    Generate each individual loss realization \(x_{ij}\) for
    \(i=1,\ldots,n\). For example, this can be done using the inverse
    transformation method (Section 6.2).
  \item
    Calculate the aggregate loss \(s_j = x_{1j} + \cdots + x_{nj}\).
  \item
    Repeat the above two steps for \(j=2,\ldots,m\) to obtain a
    size-\(m\) sample of \(S_n\), i.e. \(\{s_1,\ldots,s_m\}\).
  \end{itemize}
\item
  Collective Risk Model \(S_N=X_1+\cdots+X_N\)

  \begin{itemize}
  \tightlist
  \item
    Let \(j=1, \ldots, m\) be a counter. Start by setting \(j=1\).
  \item
    Generate the number of claims \(n_j\) from the frequency
    distribution \(N\).
  \item
    Given \(n_j\), generate the amount of each claim independently from
    severity distribution \(X\), denoted by \(x_{1j},\ldots,x_{n_j j}\).
  \item
    Calculate the aggregate loss \(s_j = x_{1j} + \cdots + x_{n_j j}\).
  \item
    Repeat the above three steps for \(j=2,\ldots,m\) to obtain a
    size-\(m\) sample of \(S_N\), i.e. \(\{s_1,\ldots, s_m\}\).
  \end{itemize}
\end{enumerate}

Given the random sample of \(S\), the empirical distribution can be
calculated as \[\hat{F}_S(s)=\frac{1}{m}\sum_{i=1}^{m}I(s_i\leq s),\]
where \(I(\cdot)\) is an indicator function. The empirical distribution
\(\hat{F}_S(s)\) will converge to \({F}_S(s)\) almost surely as the
sample size \(m\rightarrow \infty\).

The above procedure assumes that the probability distributions,
including the parameter values, of the frequency and severity
distributions are known. In practice, one would need to first assume
these distributions, estimate their parameters from the data, and then
assess the quality of model fit using various model validation tools
(see Chapter \ref{C:ModelSelection}). For instance, the assumptions in
the collective risk model suggest a two-stage estimation where one model
is developed for the number of claims \(N\) from the data on claim
counts, and another model is developed for the severity of claims \(X\)
from the data on the amount of claims.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.4.2.} Recall Example 5.3.5 with an individual's claim
frequency \(N \sim Poi(\lambda=25)\) and claim severity
\(X \sim U(5,95)\). Using a simulated sample of 10000 observations,
estimate the mean and variance of the aggregate loss \(S_N\). In
addition, use the simulated sample to estimate the probability that
aggregate claims for this individual will exceed 2000 and compare with
the normal approximation estimates from Example 5.3.5.

\textbf{Solution.} We follow the algorithm for the collective risk
model, where we first simulate frequencies \(n_1,\ldots,n_{10000}\), and
conditional on \(n_j,~j=1,\ldots,10000\), simulate each individual loss
\(x_{ij},~i=1,\ldots n_j\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{4321}\NormalTok{)  }\CommentTok{# For reproducibility of results}
\NormalTok{m <-}\StringTok{ }\DecValTok{10000}      \CommentTok{# Number of observations to simulate}
\NormalTok{lambda <-}\StringTok{ }\DecValTok{25}    \CommentTok{# Parameter for frequency distribution N}
\NormalTok{a <-}\StringTok{ }\DecValTok{5}\NormalTok{; b <-}\StringTok{ }\DecValTok{95} \CommentTok{# Parameters for severity distribution X}
\NormalTok{S <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, m) }\CommentTok{# Initalize an empty vector to store S observations}

\NormalTok{n <-}\StringTok{ }\KeywordTok{rpois}\NormalTok{(m, lambda) }\CommentTok{# Generate m=10000 observations of N from Poisson}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{m)\{ }
\NormalTok{  n_j <-}\StringTok{ }\NormalTok{n[j] }\CommentTok{# Given each n_j (j=1,...,m), generate n_j observations of X from uniform}
\NormalTok{  x_j <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(n_j, }\DataTypeTok{min=}\NormalTok{a, }\DataTypeTok{max=}\NormalTok{b)}
\NormalTok{  s_j <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(x_j) }\CommentTok{# Calculate the aggregate loss s_j}
\NormalTok{  S[j] <-}\StringTok{ }\NormalTok{s_j }\CommentTok{# Store s_j in the vector of observations}
\NormalTok{\}}
\KeywordTok{mean}\NormalTok{(S) }\CommentTok{# Compare to theoretical value of 1,250}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1248.09
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{var}\NormalTok{(S)  }\CommentTok{# Compare to theoretical value of 79,375}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 77441.22
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(S}\OperatorTok{>}\DecValTok{2000}\NormalTok{) }\CommentTok{# Proportion of simulated observations s_j that are > 2000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0062
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Compare to normal approximation method of 0.003884}
\end{Highlighting}
\end{Shaded}

Using simulation, we estimate the mean and variance of the aggregate
claims to be approximately 1248 and 77441 respectively, compared to the
theoretical values of 1,250 and 79,375. In addition, we estimate the
probability that aggregate losses exceed 2000 to be 0.0062, compared to
the normal approximation estimate of 0.003884.

We can assess the appropriateness of the normal approximation by
comparing the empirical distribution of the simulated aggregate losses
to the density of the normal distribution used for the normal
approximation, \(N(\mu=1,250~, ~\sigma^2=79,375)\):

\begin{center}\includegraphics[width=0.7\linewidth]{LossDataAnalytics_files/figure-latex/unnamed-chunk-51-1} \end{center}

The simulated losses are slightly more right-skewed than the normal
distribution, with a longer right tail. This explains why the normal
approximation estimate of \(\Pr(S_N > 2000)\) is lower than the
simulated estimate.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{Effects of Coverage
Modifications}\label{effects-of-coverage-modifications}

\subsection{Impact of Exposure on
Frequency}\label{impact-of-exposure-on-frequency}

This section focuses on an individual risk model for claim counts.
Recall the individual risk model involves a fixed \(n\) number of
contracts and independent loss random variables \(X_i\). Consider the
number of claims from a group of \(n\) policies: \[S=X_1+\cdots+X_n\]
where we assume \(X_i\) are \emph{iid} representing the number of claims
from policy \(i\). In this case, the exposure for the portfolio is
\(n\), using policy as exposure base. The \emph{pgf} of \(S\) is
\[\begin{aligned}
P_{S}(z)&={\rm E}(z^S)={\rm E}\left(z^{\sum_{i=1}^nX_i}\right)\\
&=\prod_{i=1}^n{\rm E}(z^{X_i})=[P_X(z)]^n
\end{aligned}\]

\textbf{Special Case: Poisson.} If \(X_i\sim Poi(\lambda)\), its
\emph{pgf} is \(P_X(z)=e^{\lambda(z-1)}\). Then the \emph{pgf} of \(S\)
is \[P_{S}(z)=[e^{\lambda(z-1)}]^n=e^{n\lambda(z-1)}.\] So
\(S\sim Poi(n\lambda)\). That is, the sum of \(n\) independent Poisson
random variables each with mean \(\lambda\) has a Poisson distribution
with mean \(n\lambda\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Special Case: Negative Binomial.} If \(X_i\sim NB(\beta,r)\),
its \emph{pgf} is \(P_X(z)=[1-\beta(z-1)]^{-r}\). Then the \emph{pgf} of
\(S\) is \[P_{S}(z)=[[1-\beta(z-1)]^{-r}]^n=[1-\beta(z-1)]^{-nr}.\] So
\(S\sim NB(\beta,nr)\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.5.1.} Assume that the number of claims for each
vehicle is Poisson with mean \(\lambda\). Given the following data on
the observed number of claims for each household, calculate the MLE of
\(\lambda\).

\[
{\small 
\begin{matrix}
\begin{array}{c|c|c}
  \hline
  \text{Household ID} & \text{Number of vehicles} & \text{Number of claims} \\
  \hline
  1 & 2 & 0 \\
  2 & 1 & 2 \\
  3 & 3 & 2 \\
  4 & 1 & 0 \\
  5 & 1 & 1 \\
  \hline
\end{array}
\end{matrix}
}
\]

\textbf{Solution.} Each of the 5 households has number of exposures
\(n_j\) (number of vehicles) and number of claims \(S_j\),
\(j=1,...,5\). Note for each household, the number of claims
\(S_j \sim Poi (n_j \lambda)\). The likelihood function is\\
\[\begin{aligned}
L(\lambda) &= \prod_{j=1}^5 \Pr(S_j=s_j) = \prod_{j=1}^5 \frac{e^{-n_j\lambda} (n_j \lambda)^{s_j}}{s_j!} \\
&= \left(\frac{e^{-2\lambda} (2 \lambda)^{0}}{0!} \right)
\left(\frac{e^{-1\lambda} (1 \lambda)^{2}}{2!} \right)
\left(\frac{e^{-3\lambda} (3 \lambda)^{2}}{2!} \right)
\left(\frac{e^{-1\lambda} (1 \lambda)^{0}}{0!} \right)
\left(\frac{e^{-1\lambda} (1 \lambda)^{1}}{1!} \right) \\
&\propto e^{-8\lambda} \lambda^5
\end{aligned}\] Taking the log-likehood, we have \[\begin{aligned}
l(\lambda) = \log L(\lambda) = -8\lambda + 5\log(\lambda)
\end{aligned}\] Setting the first derivative of the log-likehood to 0,
we get \(\hat{\lambda} = \frac{5}{8}\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

If the exposure of the portfolio changes from \(n_1\) to \(n_2\), we can
establish the following relation between the aggregate claim counts:
\[P_{S_{n_2}}(z)=[P_X(z)]^{n_2}=[P_X(z)^{n_1}]^{n_2/n_1}=P_{S_{n_1}}(z)^{n_2/n_1}.\]

\subsection{Impact of Deductibles on Claim
Frequency}\label{S:MS:DedImpactClmFreq}

This section examines the effect of deductibles on claim frequency.
Intuitively, there will be fewer claims filed when a policy deductible
is imposed because a loss below the deductible level may not result in a
claim. Even if an insured does file a claim, this may not result in a
payment by the policy, since the claim may be denied or the loss amount
may ultimately be determined to be below deductible. Let \(N^L\) denote
the number of losses (i.e.~the number of claims with no deductible), and
\(N^P\) denote the number of payments when a deductible \(d\) is
imposed. Our goal is to identify the distribution of \(N^P\) given the
distribution of \(N^L\). We show below that the relationship between
\(N^L\) and \(N^P\) can be established within an aggregate risk model
framework.

Note that sometimes changes in deductibles will affect policyholder
claim behavior. We assume that this is not the case, i.e.~the underlying
distributions of losses for both frequency and severity remain unchanged
when the deductible changes.

Given there are \(N^L\) losses, let \(X_1,X_2\ldots,X_{N^L}\) be the
associated amount of losses. For \(j=1,\ldots,N^L\), define

\begin{eqnarray*}
I_j&=&
\left \{
\begin{array}{cc}
1 & \text{if} ~X_j>d\\
0 & \text{otherwise}\\
\end{array}
\right..
\end{eqnarray*}

Then we establish \[N^P=I_1+I_2+\cdots+I_{N_L},\]

that is, the total number of payments is equal to the number of losses
above the deductible level. Given that \(I_j\)'s are independent
Bernoulli random variables with probability of success \(v=\Pr(X>d)\),
the sum of a \emph{fixed number} of such variables is then a binomial
random variable. Thus, conditioning on \(N^L\), \(N^P\) has a binomial
distribution, i.e. \(N^P | N^L \sim Bin(N^L, v)\), where \(v=\Pr(X>d)\).
This implies that \[\begin{aligned}
\mathrm{E}\left(z^{N^P}|N^L\right)&= \left[ 1+v(z-1)\right]^{N^L}
\end{aligned}\]

So the \emph{pgf} of \(N^P\) is \[\begin{aligned}
P_{N^P}(z)&= \mathrm{E}_{N^P}\left(z^{N^P}\right)=\mathrm{E}_{N^L}\left[\mathrm{E}_{N^P}\left(z^{N^P}|N^L\right)\right]\\
&= \mathrm{E}_{N^L}\left[(1+v(z-1))^{N^L}\right]\\
&= P_{N^L}\left(1+v(z-1)\right)
\end{aligned}\]

Thus, we can write the \emph{pgf} of \(N^P\) as the \emph{pgf} of
\(N^L\), evaluated at a new argument \(z^* = 1+v(z-1)\). That is,
\(P_{N^P}(z)=P_{N^L}(z^*)\).

\textbf{Special Cases:}

\begin{itemize}
\item
  \(N^L\sim Poi (\lambda)\). The \emph{pgf} of \(N^L\) is
  \(P_{N^L}=e^{\lambda(z-1)}\). Thus the \emph{pgf} of \(N^P\) is
  \[\begin{aligned}
  P_{N^P}(z) &= e^{ \lambda(1+v(z-1)-1)} \\
  &= e^{\lambda v(z-1)} ,
  \end{aligned}\]\\
  So \(N^P \sim Poi(\lambda v)\). This means the number of payments has
  the same distribution as the number of losses, but with the expected
  number of payments equal to \(\lambda v = \lambda \Pr(X>d)\).
\item
  \(N^L \sim NB(\beta, r)\). The \emph{pgf} of \(N^L\) is
  \(P_{N^{L}}\left( z\right) =\left[ 1-\beta \left( z-1\right)\right]^{-r}\).
  Thus the \emph{pgf} of \(N^P\) is \[\begin{aligned}
  P_{N^P}(z)&= \left( 1-\beta (1+v(z-1)-1)\right)^{-r}\\
  &= \left( 1-\beta v(z-1)\right)^{-r},
  \end{aligned}\] So \(N^P \sim NB(\beta v, r)\). This means the number
  of payments has the same distribution as the number of losses, but
  with parameters \(\beta v\) and \(r\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.5.2.} Suppose that loss amounts
\(X_i\sim Pareto(\alpha=4,\ \theta=150)\). You are given that the loss
frequency is \(N^L\sim Poi(\lambda)\) and the payment frequency
distribution is \(N^{P}_1\sim Poi(0.4)\) at deductible level \(d_1=30\).
Find the distribution of the payment frequency \(N^{P}_2\) when the
deductible level is \(d_2=100\).

\textbf{Solution.} Because the loss frequency \(N^L\) is Poisson, we can
relate the means of the loss distribution \(N^L\) and the first payment
distribution \(N^{P}_1\) (under deductible \(d_1=30\)) through
\(0.4 = \lambda v_1\), where \[\begin{aligned}
&v_1 = \Pr(X > 30) = \left( \frac{150}{30+150}\right)^4=\left( \frac{5}{6}\right)^4 \\
\Rightarrow \ & \lambda = 0.4 \left( \frac{6}{5} \right)^4
\end{aligned}\] With this, we can assess the second payment distribution
\(N^{P}_2\) (under deductible \(d_2=100\)) as being Poisson with mean
\(\lambda_2 = \lambda v_2\), where \[\begin{aligned}
& v_2 = \Pr(X>100)=\left( \frac{150}{100+150}\right)^4=\left( \frac{3}{5}\right)^4 \\
\Rightarrow \ & \lambda_2 = \lambda v_2 = 0.4\left( \frac{6}{5} \right)^4 \left( \frac{3}{5} \right)^4 = 0.1075
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.5.3. Follow-Up.} Now suppose instead that the loss
frequency is \(N^L \sim NB(\beta,\ r)\) and for deductible \(d_1=30\),
the payment frequency \(N^{P}_1\) is negative binomial with mean
\(0.4\). Find the mean of the payment frequency \(N^{P}_2\) for
deductible \(d_2=100\).

\textbf{Solution.} Because the loss frequency \(N^L\) is negative
binomial, we can relate the parameter \(\beta\) of the \(N^L\)
distribution and the parameter \(\beta_1\) of the first payment
distribution \(N^{P}_1\) using \(\beta_1 = \beta v_1\), where
\[v_1 = \Pr(X > 30) = \left( \frac{5}{6} \right)^4\] Thus, the mean of
\(N^{P}_1\) and the mean of \(N^L\) are related via \[\begin{aligned}
&0.4 =  r \beta_1 = r \left(\beta v_1\right) \\
\Rightarrow \ & r\beta = \frac{0.4}{v_1} = 0.4 \left(\frac{6}{5} \right)^4
\end{aligned}\] Note that
\(v_2 = \Pr(X > 100) = \left( \frac{3}{5}\right)^4\) as in the original
example. Then the second payment frequency distribution under deductible
\(d_2=100\) is \(N^{P}_2 \sim NegBin(\beta v_2, \ r)\) with mean
\[\begin{aligned}
r (\beta v_2) = (r \beta) v_2 = 0.4 \left( \frac{6}{5}\right)^4 \left( \frac{3}{5} \right)^4 = 0.1075
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Next, we examine the more general case where \(N^L\) is a zero-modified
distribution. Recall that a zero-modified distribution can be defined in
terms of an unmodified one (as was shown in Section
\ref{S:zero-truncation-or-modification}). That is, \[\begin{aligned}
p_k^M = c~p_k^0, {~\rm for~} k=1,2,3,\ldots,  {~\rm with~}c = \frac{1-p_0^M}{1-p_0^0},
\end{aligned}\] where \(p^0_k\) is the \emph{pmf} of the unmodified
distribution. In the case that \(p_0^M=0\), we call this a
\emph{zero-truncated} distribution, or \(ZT\). For other arbitrary
values of \(p_0^M\), this is a zero-modified, or \(ZM\), distribution.
The \emph{pgf} for the modified distribution is shown as
\[\begin{aligned}
P^M(z) &= 1-c+c~P^0(z),
\end{aligned}\] expressed in terms of the \emph{pgf} of the unmodified
distribution, \(P^0(z)\). When \(N^L\) follows a zero-modified
distribution, the distribution of \(N^P\) is established using the same
relation from earlier, \(P_{N^P}(z)=P_{N^L}\left(1+v(z-1)\right)\).

\textbf{Special Cases:}

\begin{itemize}
\item
  \(N^{L}\) is a ZM-Poisson random variable with parameters \(\lambda\)
  and \(p_0^{M}\). The \emph{pgf} of \(N^L\) is
  \[P_{N^{L}}(z)=1-\cfrac{1-p_0^{M}}{1-e^{-\lambda}}+\cfrac{1-p_0^{M}}{1-e^{-\lambda}}\left( e^{\lambda(z-1)} \right).\]
  Thus the \emph{pgf} of \(N^P\) is
  \[P_{N^{P}}(z)=1-\cfrac{1-p_0^{M}}{1-e^{-\lambda}}+\cfrac{1-p_0^{M}}{1-e^{-\lambda}}\left( e^{\lambda v(z-1)} \right).\]
  So the number of payments is also a ZM-Poisson distribution with
  parameters \(\lambda v\) and \(p_0^{M}\). The probability at zero can
  be evaluated using \({\rm Pr}(N^P=0) = P_{N^P}(0)\).
\item
  \(N^{L}\) is a ZM-negative binomial random variable with parameters
  \(\beta\), \(r\), and \(p_0^{M}\). The \emph{pgf} of \(N^L\) is
  \[P_{N^{L}}(z)=1-\cfrac{1-p_0^{M}}{1-(1+\beta)^{-r}}+\cfrac{1-p_0^{M}}{1-(1+\beta)^{-r}}\left[ 1-\beta \left( z-1\right)\right]^{-r}.\]
  Thus the \emph{pgf} of \(N^P\) is
  \[P_{N^{P}}(z)=1-\cfrac{1-p_0^{M}}{1-(1+\beta)^{-r}}+\cfrac{1-p_0^{M}}{1-(1+\beta)^{-r}}\left[ 1-\beta v\left( z-1\right)\right]^{-r}.\]
  So the number of payments is also a ZM-negative binomial distribution
  with parameters \(\beta v\), \(r\), and \(p_0^{M}\). Similarly, the
  probability at zero can be evaluated using
  \({\rm Pr}(N^P=0) = P_{N^P}(0)\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.5.4.} Aggregate losses are modeled as follows:\\
(i) The number of losses follows a zero-modified Poisson distribution
with \(\lambda=3\) and \(p_0^M = 0.5\).\\
(ii) The amount of each loss has a Burr distribution with
\(\alpha=3, \theta=50, \gamma=1\).\\
(iii) There is a deductible of \(d=30\) on each loss.\\
(iv) The number of losses and the amounts of the losses are mutually
independent.

Calculate \(\mathrm{E}(N^P)\) and \(\mathrm{Var}(N^P)\).

\textbf{Solution.} Since \(N^L\) follows a ZM-Poisson distribution with
parameters \(\lambda\) and \(p_0^M\), we know that \(N^P\) also follows
a ZM-Poisson distribution, but with parameters \(\lambda v\) and
\(p_0^M\), where

\[v = \Pr(X>30) = \left( \frac{1}{1+(30/50)} \right)^3 = 0.2441\]

Thus, \(N^P\) follows a ZM-Poisson distribution with parameters
\(\lambda^\ast = \lambda v= 0.7324\) and \(p_0^M = 0.5\). Finally,
\[\begin{aligned}
\mathrm{E} (N^P) &= (1-p_0^M) \frac{\lambda^\ast}{1-e^{-\lambda^\ast}} = 0.5 \left( \frac{0.7324}{1-e^{-0.7324}} \right) \\
&= 0.7053 \\
\mathrm{Var} (N^P) &= (1-p_0^M) \left( \frac{\lambda^\ast[1-(\lambda^\ast + 1) e^{-\lambda^\ast}]}{(1-e^{-\lambda^\ast})^2} \right) + p_0^M(1-p_0^M) \left(\frac{\lambda^\ast}{1-e^{-\lambda^\ast}} \right)^2 \\
&= 0.5 \left( \frac{0.7324(1-1.7324 e^{-0.7324})}{(1-e^{-0.7324})^2} \right) + 0.5^2 \left( \frac{0.7324}{1-e^{-0.7324}} \right)^2 \\
&= 0.7244
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Impact of Policy Modifications on Aggregate
Claims}\label{impact-of-policy-modifications-on-aggregate-claims}

In this section, we examine how a change in the deductible affects the
aggregate payments from an insurance portfolio. We assume that the
presence of policy limits (\(u\)), coinsurance (\(\alpha\)), and
inflation (\(r\)) have no effect on the underlying distribution of
frequency of payments made by an insurer. As in the previous section, we
further assume that deductible changes do not impact the underlying
distributions of losses for both frequency and severity.

Recall the notation \(N^L\) for the number of losses. With ground-up
loss amount \(X\) and policy deductible \(d\), we use \(N^P\) for the
number of payments (as defined in the previous section
\ref{S:MS:DedImpactClmFreq}). Also, define the amount of payment on a
per-loss basis as

\begin{eqnarray*}
    X^{L}&=\left\{
      \begin{array}{ll}
        0 ~, & \text{if } ~X<\cfrac{d}{1+r} \\
        \alpha[(1+r)X-d]~, & \text{if } ~\cfrac{d}{1+r}\leq X<\cfrac{u}{1+r} \\
        \alpha(u-d)~, &  \text{if } ~X \ge \cfrac{u}{1+r}\\
      \end{array}
\right.,
\end{eqnarray*}

and the the amount of payment on a per-payment basis as

\begin{eqnarray*}
    X^{P}&=\left\{
      \begin{array}{ll}
        {\rm undefined} ~, & \text{if }~ X<\cfrac{d}{1+r} \\
        \alpha[(1+r)X-d]~, & \text{if }~ \cfrac{d}{1+r}\leq X<\cfrac{u}{1+r} \\
        \alpha(u-d)~, &  \text{if } ~ X \ge \cfrac{u}{1+r}\\
      \end{array}
\right..
\end{eqnarray*}

In the above, \(r\), \(u\), and \(\alpha\) represent the inflation rate,
policy limit, and coinsurance, respectively. Hence, aggregate costs
(payment amounts) can be expressed either on a per loss or per payment
basis: \[\begin{aligned}
S &= X^L_1 + \cdots + X^L_{N^L} \\
&=X^P_1 + \cdots + X^P_{N^P} ~.
\end{aligned}\]

The fundamentals regarding collective risk models are ready to apply.
For instance, we have: \[\begin{aligned}
  {\rm E}(S) &= {\rm E}\left(N^L\right) {\rm E}\left(X^L\right) = {\rm E}\left(N^P\right) {\rm E}\left(X^P\right)\\
  {\rm Var}(S) &= {\rm E}\left(N^L\right) {\rm Var}\left(X^L\right) + \left[{\rm E}\left(X^L\right)\right]^2 {\rm Var}(N^L) \\
  &= {\rm E}\left(N^P\right) {\rm Var}\left(X^P\right) + \left[{\rm E}\left(X^P\right)\right]^2 {\rm Var}(N^P)\\
  M_S(z)&=P_{N^L}\left[M_{X^L}(z)\right]=P_{N^P}\left[M_{X^P}(z)\right]
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.5.5. Actuarial Exam Question.} A group dental policy
has a negative binomial claim count distribution with mean 300 and
variance 800. Ground-up severity is given by the following table:

\[
{\small 
\begin{matrix}
  \begin{array}{ c | c }
    \hline
      \text{Severity} & \text{Probability}\\ \hline
    40 & 0.25\\
    80 & 0.25\\
    120 & 0.25\\
    200 & 0.25\\
    \hline
  \end{array}
\end{matrix}
}
\]

You expect severity to increase 50\% with no change in frequency. You
decide to impose a per claim deductible of 100. Calculate the expected
total claim payment \(S\) after these changes.

\textbf{Solution.} The cost per loss with a 50\% increase in severity
and a 100 deductible per claim is

\begin{eqnarray*}
X^L &=&
\left\{
\begin{array}{cc}
0 & 1.5x<100 \\
1.5x-100 & 1.5x\ge 100\\
\end{array}
\right.
\end{eqnarray*}

This has expectation \[\begin{aligned}
\mathrm{E}(X^L) &= \frac{1}{4} \left[ \left(1.5(40)-100\right)_+ +
\left(1.5(80)-100\right)_+ + \left(1.5(120)-100\right)_+ +
\left(1.5(200)-100\right)_+ \right]  \\
&= \frac{1}{4}\left[ (60-100)_+ + (120-100)_+ + (180-100)_+ + (300-100)_+\right] \\
&= \frac{1}{4}\left[ 0 + 20 + 80 + 200 \right] = 75
\end{aligned}\] Thus, the expected aggregate loss is
\[\mathrm{E}(S)=\mathrm{E}(N) ~ \mathrm{E} \left(X^L \right)= 300 (75) = 22,500
.\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.5.6. Follow-Up.} What is the variance of the total
claim payment, \(\mathrm{Var~}S\)?

\textbf{Solution.} On a per loss basis, we have \[\begin{aligned}
\mathrm{Var}(S) &= \mathrm{E}(N) ~ \mathrm{Var}\left(  X^L \right) + \left[ \mathrm{E} \left(X^L\right) \right]^2 ~ \mathrm{Var} (N)
\end{aligned}\] where \(\mathrm{E}(N) = 300\) and
\(\mathrm{Var}(N) = 800\). We find \[\begin{aligned}
&\mathrm{E} \left[ (X^L)^2 \right] = \frac{1}{4} \left[ 0^2 + 20^2 + 80^2 + 200^2 \right] = 11,700 \\
\Rightarrow \ & \mathrm{Var}(X^L) = \mathrm{E} \left[ (X^L)^2 \right] - \left[ \mathrm{E}(X^L) \right]^2 = 11,700 - 75^2 = 6,075
\end{aligned}\] Thus, the variance of the aggregate claim payment is
\[\begin{aligned}
\mathrm{Var}(S) &= 300(6,075) + 75^2 (800) = 6,322,500
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\emph{Alternative Method: Using the Per Payment Basis.} Previously, we
calculated the expected total claim payment by multiplying the expected
number of losses by the expected payment \emph{per loss}. Recall that we
can also multiply the expected number of payments by the expected
payment \emph{per payment}. In this case, we have
\[S=X_1^P + \cdots + X_{N_P}^P \] The probability of a payment is
\[\Pr(1.5X \ge 100)=\Pr(X \ge 66.\bar{6})=\frac{3}{4} .\] Thus, the
number of payments, \(N^P\) has a negative binomial distribution (see
negative binomial special case in Section \ref{S:MS:DedImpactClmFreq})
with mean
\[\mathrm{E}(N^P) =  \mathrm{E}(N^L)~\Pr(1.5X \geq 100) = 300 \left(\frac{3}{4} \right)=225\]
The cost per payment is

\begin{eqnarray*}
X^P &=&
\left\{
\begin{array}{ll}
\text{undefined}~, & \text{if }~ 1.5x<100 \\
1.5x-100~, & \text{if } ~ 1.5x\ge 100\\
\end{array}
\right.
\end{eqnarray*}

This has expectation
\[\mathrm{E}(X^P)=\frac{\mathrm{E}(X^L)}{\Pr(1.5X > 100)}=\frac{75}{(3/4)}=100\]
Thus, as before, the expected aggregate loss is
\[\mathrm{E}(S)=\mathrm{E}(X^P) ~ \mathrm{E}(N^P) =
100(225)=22,500\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 5.5.7. Actuarial Exam Question.} A company insures a
fleet of vehicles. Aggregate losses have a compound Poisson
distribution. The expected number of losses is 20. Loss amounts,
regardless of vehicle type, have exponential distribution with
\(\theta=200\). To reduce the cost of the insurance, two modifications
are to be made:\\
(i) A certain type of vehicle will not be insured. It is estimated that
this will reduce loss frequency by 20\(\%\).\\
(ii) A deductible of 100 per loss will be imposed.

Calculate the expected aggregate amount paid by the insurer after the
modifications.

\textbf{Solution.} On a per loss basis, we have a 100 deductible. Thus,
the expectation per loss is \[\begin{aligned}
\mathrm{E}( X^L) &= E[(X-100)_+] = E(X) - E(X\wedge 100) \\
&= 200 - 200(1-e^{-100/200}) = 121.31
\end{aligned}\] Loss frequency has been reduced by 20\(\%\), resulting
in an expected number of losses \[\mathrm{E}(N^L) = 0.8(20) = 16\] Thus,
the expected aggregate amount paid after the modifications is
\[\mathrm{E}(S) = \mathrm{E}(X^L)~ \mathrm{E} (N^L) = 121.31(16) = 1,941\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\emph{Alternative Method: Using the Per Payment Basis.} We can also use
the per payment basis to find the expected aggregate amount paid after
the modifications. With the deductible of 100, the probability that a
payment occurs is \(\Pr(X > 100) = e^{-100/200}\). For the per payment
severity, plugging in the expression for \(\mathrm{E}(X^L)\) from the
original example, we have \[\begin{aligned}
\mathrm{E} (X^P) = \frac{\mathrm{E} (X^L)}{\Pr(X > 100)} = \frac{200 - 200(1-e^{-100/200})}{e^{-100/200}} = 200
\end{aligned}\] This is not surprising -- recall that the exponential
distribution is memoryless, so the expected claim amount paid in excess
of 100 is still exponential with mean 200.

Now we look at the payment frequency
\[\mathrm{E} (N^P) = \mathrm{E}(N^L)~\Pr(X>100) = 16 ~e^{-100/200} = 9.7\]
Putting this together, we produce the same answer using the per payment
basis as the per loss basis from earlier
\[\mathrm{E}(S) = \mathrm{E} (X^P)~ \mathrm{E} (N^P)= 200(9.7) = 1,941\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{Further Resources and
Contributors}\label{AL-further-reading-and-resources}

\subsubsection{Exercises}\label{exercises-2}

Here are a set of exercises that guide the viewer through some of the
theoretical foundations of \textbf{Loss Data Analytics}. Each tutorial
is based on one or more questions from the professional actuarial
examinations, typically the Society of Actuaries Exam C.

\href{https://www.ssc.wisc.edu/~jfrees/loss-data-analytics/aggregate-loss-guided-tutorials/}{Aggregate
Loss Guided Tutorials}

\subsubsection*{Contributors}\label{contributors-4}
\addcontentsline{toc}{subsubsection}{Contributors}

\begin{itemize}
\tightlist
\item
  \textbf{Peng Shi} and \textbf{Lisa Gao}, University of
  Wisconsin-Madison, are the principal authors of the initial version of
  this chapter. Email:
  \href{mailto:pshi@bus.wisc.edu}{\nolinkurl{pshi@bus.wisc.edu}} for
  chapter comments and suggested improvements.
\item
  Chapter reviewers include: Vytaras Brazauska, Mark Maxwell, Jiadong
  Ren, Di (Cindy) Xu.
\end{itemize}

\subsection*{TS 5.A.1. Individual Risk Model
Properties}\label{ts-5.a.1.-individual-risk-model-properties}
\addcontentsline{toc}{subsection}{TS 5.A.1. Individual Risk Model
Properties}

For the expected value of the aggregate loss under the individual risk
model,

\[\begin{aligned}
\mathrm{E}(S_n) &=\sum_{i=1}^n ~ \mathrm{E}(X_i) = \sum_{i=1}^n ~ \mathrm{E}(I_i \times B_i) = \sum_{i=1}^n ~ \mathrm{E}(I_i) ~~ \mathrm{E}(B_i) ~~~~ \text{from the independence of } I_i \text{'s and } B_i \text{'s} \\
&= \sum_{i=1}^n \Pr(I_i=1) ~ \mu_i ~~~~ \text{since the expectation of an indicator variable is the probability it equals } 1 \\
&= \sum_{i=1}^n ~ q_i ~ \mu_i
\end{aligned}\]

For the variance of the aggregate loss under the individual risk model,

\[\begin{aligned}
\mathrm{Var}(S_n) &= \sum_{i=1}^n \mathrm{Var}(X_i) ~~~~ \text{from the independence of } X_i \text{'s} \\
&= \sum_{i=1}^n ~ \left( ~ \mathrm{E}\left[ \mathrm{Var}(X_i | I_i) \right] + \mathrm{Var}\left[ \mathrm{E}(X_i|I_i) \right] ~ \right) ~~~~ \text{from the conditional variance formulas}  \\
&= \sum_{i=1}^n \left( q_i ~ \sigma_i^2 ~ + ~ q_i ~ (1-q_i) ~ \mu_i^2 \right)
\end{aligned}\]

To see this, note that \[\begin{aligned}
\mathrm{E}\left[ \mathrm{Var}(X_i | I_i) \right] &= \mathrm{Var}(X_i|I_i=0) ~ \Pr(I_i=0) + \mathrm{Var}(X_i|I_i=1) ~ \Pr(I_i=1) \\
&= q_i ~ \sigma_i^2 + (1-q_i) ~ (0) = q_i ~ \sigma_i^2, 
\end{aligned}\]

and \[\begin{aligned}
\mathrm{Var}\left[ \mathrm{E}(X_i|I_i) \right] &= q_i ~ (1-q_i) ~ \mu_i^2~,
\end{aligned}\]

using the Bernoulli variance shortcut since \(\mathrm{E}(X_i|I_i) = 0\)
when \(I_i=0\) (probability \(\Pr(I_i=0) = 1-q_i\)) and
\(\mathrm{E}(X_i|I_i) = \mu_i\) when \(I_i=1\) (probability
\(\Pr(I_i=1)= q_i\)).

For the probability generating function of the aggregate loss under the
individual risk model,

\[\begin{aligned}
P_{S_n}(z) &= \prod_{i=1}^n ~ P_{X_i}(z) ~~~~ \text{from the independence of } X_i \text{'s} \\
&= \prod_{i=1}^n ~ \mathrm{E}(z^{~X_i}) = \prod_{i=1}^n ~ \mathrm{E}(z^{~I_i \times B_i}) = \mathrm{E} \left[ \mathrm{E}(z^{~I_i \times B_i} | I_i) \right] ~~~~ \text{from the law of iterated expectations} \\
&= \prod_{i=1}^n \left[ ~ E\left(z^{~I_i \times B_i} | I_i=0\right) ~ \Pr(I_i=0) + E\left(z^{~I_i \times B_i} | I_i=1\right) ~ \Pr(I_i=1) ~ \right] \\
&= \prod_{i=1}^n ~ \left[ ~ (1) ~ (1-q_i) + P_{B_i}(z) ~ q_i ~ \right] = \prod_{i=1}^n \left(~ 1-q_i + q_i ~ P_{B_i}(z) ~\right)
\end{aligned}\]

Lastly, for the moment generating function of the aggregate loss under
the individual risk model,

\[\begin{aligned}
M_{S_n}(t) &= \prod_{i=1}^n ~ M_{X_i}(t) ~~~~ \text{from the independence of } X_i \text{'s} \\
&= \prod_{i=1}^n ~ \mathrm{E}(e^{t~X_i}) = \prod_{i=1}^n ~ \mathrm{E}\left(e^{~t~(I_i \times B_i)} \right) = \prod_{i=1}^n ~ \mathrm{E} \left[ \mathrm{E} \left( e^{~t~(I_i \times B_i)} | I_i \right) \right] ~~~~ \text{from the law of iterated expectations} \\
&= \prod_{i=1}^n ~ \left[~ \mathrm{E}\left(e^{~t~(I_i \times B_i)} | I_i=0 \right) ~ \Pr(I_i=0) + \mathrm{E}\left( e^{~t~(I_i \times B_i)} | I_i=1 \right) ~ \Pr(I_i=1) ~\right] \\
&= \prod_{i=1}^n ~ \left[ ~ (1) ~ (1-q_i) + M_{B_i}(t) ~ q_i ~ \right] = \prod_{i=1}^n \left(~ 1-q_i + q_i ~ M_{B_i}(t) ~\right)
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection*{\texorpdfstring{TS 5.A.2. Relationship Between Probability
Generating Functions of \(X_i\) and
\(X_i^T\)}{TS 5.A.2. Relationship Between Probability Generating Functions of X\_i and X\_i\^{}T}}\label{ts-5.a.2.-relationship-between-probability-generating-functions-of-x_i-and-x_it}
\addcontentsline{toc}{subsection}{TS 5.A.2. Relationship Between
Probability Generating Functions of \(X_i\) and \(X_i^T\)}

Let \(X_i\) belong to the \((a,b,0)\) class with \emph{pmf}
\(p_{ik} = \Pr(X_i = k)\) for \(k=0,1,\ldots\) and \(X_i^T\) be the
associated zero-truncated distribution in the \((a,b,1)\) class with
\emph{pmf} \(p_{ik}^T = p_{ik}/(1-p_{i0})\) for \(k=1,2,\ldots\). Then
the relationship between the \emph{pgf} of \(X_i\) and the \emph{pgf} of
\(X_i^T\) is shown by

\[\begin{aligned}
P_{X_i}(z) &= \mathrm{E~}(z^{X_i}) = \mathrm{E}\left[  \mathrm{E}\left( z^{X_i} | X_i \right) \right] ~~~~ \text{from the law of iterated expectations} \\ 
&= \mathrm{E}\left( z^{X_i} | X_i=0 \right)~ \Pr(X_i=0) + \mathrm{E}\left( z^{X_i} | X_i>0 \right) ~ \Pr(X_i>0) \\
&= (1)~ p_{i0} + \mathrm{E}(z^{X_i^T}) ~ (1-p_{i0}) ~~~~ \text{since } (X_i | X_i>0) \text{ is the zero-truncated random variable } X_i^T \\
&= p_{i0} +(1-p_{i0}) P_{X_i^{T}}(z)
\end{aligned}\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection*{\texorpdfstring{TS 5.A.3. Example 5.3.8 Moment Generating
Function of Aggregate Loss
\(S_N\)}{TS 5.A.3. Example 5.3.8 Moment Generating Function of Aggregate Loss S\_N}}\label{ts-5.a.3.-example-5.3.8-moment-generating-function-of-aggregate-loss-s_n}
\addcontentsline{toc}{subsection}{TS 5.A.3. Example 5.3.8 Moment
Generating Function of Aggregate Loss \(S_N\)}

For \(N\sim Geo(\beta)\) and \(X\sim Exp(\theta)\), we have

\[\begin{aligned}
P_N (z) &=\frac{1}{1- \beta (z-1)}\\
M_{X}(t) &=\frac{1}{1-\theta t}
\end{aligned}\]

Thus, the \emph{mgf} of aggregate loss \(S_N\) is \[\begin{aligned}
M_{S_N}(t) &= P_N [M_{X}(t)] = \frac{1}{1 - \beta \left( \frac{1}{1-\theta t} - 1\right)} \\
&= \frac{1}{1 - \beta \left( \frac{\theta t}{1-\theta t} \right)} + 1 - 1 
= 1+ \frac{\beta \left( \frac{\theta t}{1-\theta t} \right)}{1 - \beta \left( \frac{\theta t}{1-\theta t} \right)} \\
&= 1 + \frac{\beta \theta t}{(1-\theta t) - \beta \theta t} = 1+ \frac{\beta \theta t}{1-\theta t (1+\beta)} \cdot \frac{1+\beta}{1+\beta} \\
&= 1 + \frac{\beta}{1+\beta} \left[ \frac{\theta (1+\beta) t}{1-\theta(1+\beta)t} \right] \\
&= 1 + \frac{\beta}{1+\beta} \left[ \frac{1}{1-\theta(1+\beta)t} - 1 \right], \\
\end{aligned}\] which gives the expression (5.1). For the alternate
expression of the \emph{mgf} (5.2), we continue from where we just left
off:

\[\begin{aligned}
M_{S_N}(t) &=  1 + \frac{\beta}{1+\beta} \left[ \frac{\theta (1+\beta) t}{1-\theta(1+\beta)t} \right] \\
&= \frac{1+\beta}{1+\beta} +  \frac{\beta}{1+\beta} \left[ \frac{\theta (1+\beta) t}{1-\theta(1+\beta)t} \right] \\
&= \frac{1}{1+\beta} + \frac{\beta}{1+\beta} + \frac{\beta}{1+\beta} \left[ \frac{\theta (1+\beta) t}{1-\theta(1+\beta)t} \right]  \\
&= \frac{1}{1+\beta} + \frac{\beta}{1+\beta}\left[1 + \frac{\theta (1+\beta) t}{1-\theta (1+\beta)t} \right] \\
&= \frac{1}{1+\beta} +\frac{\beta}{1+\beta} \left[ \frac{1}{1-\theta (1+\beta)t}\right]
\end{aligned}\]

\chapter{Simulation and Resampling}\label{C:Simulation}

\emph{Chapter Preview.} Simulation is a computationally intensive method
used to solve difficult problems. Instead of creating physical processes
and experimenting with them in order to understand their operational
characteristics, a simulation study is based on a computer
representation - it considers various hypothetical conditions as inputs
and summarizes the results. Through simulation, a vast number of
hypothetical conditions can be quickly and inexpensively examined.
Section \ref{S:SimulationFundamentals} introduces simulation, a
wonderful computational tool that is especially useful in complex,
multivariate settings.

We can also use simulation to draw from an empirical distribution - this
process is known as resampling. Resampling allows us to assess the
uncertainty of estimates in complex models. Section \ref{S:Bootstrap}
introduces resampling in the context of bootstrapping to determine the
precision of estimators.

Subsequent sections introduce other topics in resampling. Section
\ref{S:CrossValidation} on cross-validation shows how to use it for
model selection and validation. Section \ref{S:ImportanceSampling} on
importance sampling describes resampling in specific regions of
interest, such as long-tailed actuarial applications. Section
\ref{S:MCMC} on Monte Carlo Markov Chain (MCMC) introduces the
simulation and resampling engine underpinning much of modern Bayesian
analysis.

\section{Simulation Fundamentals}\label{S:SimulationFundamentals}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Generate approximately independent realizations that are uniformly
  distributed
\item
  Transform the uniformly distributed realizations to observations from
  a probability distribution of interest
\item
  Calculate quantities of interest and determining the precision of the
  calculated quantities
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Generating Independent Uniform
Observations}\label{generating-independent-uniform-observations}

The simulations that we consider are generated by computers. A major
strength of this approach is that they can be replicated, allowing us to
check and improve our work. Naturally, this also means that they are not
really random. Nonetheless, algorithms have been produced so that
results appear to be random for all practical purposes. Specifically,
they pass sophisticated tests of independence and can be designed so
that they come from a single distribution - our iid assumption,
identically and independently distributed.

To get a sense as to what these algorithms do, we consider a
historically prominent method.

\textbf{Linear Congruential Generator.} To generate a sequence of random
numbers, start with \(B_0\), a starting value that is known as a
\emph{seed}. This value is updated using the recursive relationship
\[B_{n+1} = a B_n + c  \text{ modulo }m, ~~ n=0, 1, 2, \ldots .\] This
algorithm is called a linear congruential generator. The case of \(c=0\)
is called a \emph{multiplicative} congruential generator; it is
particularly useful for really fast computations.

For illustrative values of \(a\) and \(m\), Microsoft's Visual Basic
uses \(m=2^{24}\), \(a=1,140,671,485\), and \(c = 12,820,163\) (see
\url{https://en.wikipedia.org/wiki/Linear_congruential_generator}). This
is the engine underlying the random number generation in Microsoft's
Excel program.

The sequence used by the analyst is defined as \(U_n=B_n/m.\) The
analyst may interpret the sequence \{\(U_{i}\)\} to be (approximately)
identically and independently uniformly distributed on the interval
(0,1). To illustrate the algorithm, consider the following.

\textbf{Example 6.1.1. Illustrative Sequence.} Take \(m=15\), \(a=3\),
\(c=2\) and \(B_0=1\). Then we have:

\begin{longtable}[]{@{}clc@{}}
\toprule
step \(n\) & \(B_n\) & \(U_n\)\tabularnewline
\midrule
\endhead
0 & \(B_0=1\) &\tabularnewline
1 & \(B_1 =\mod(3 \times 1 +2) = 5\) &
\(U_1 = \frac{5}{15}\)\tabularnewline
2 & \(B_2 =\mod(3 \times 5 +2) = 2\) &
\(U_2 = \frac{2}{15}\)\tabularnewline
3 & \(B_3 =\mod(3 \times 2 +2) = 8\) &
\(U_3 = \frac{8}{15}\)\tabularnewline
4 & \(B_4 =\mod(3 \times 8 +2) = 11\) &
\(U_4 = \frac{11}{15}\)\tabularnewline
\bottomrule
\end{longtable}

Sometimes computer generated random results are known as pseudo-random
numbers to reflect the fact that they are machine generated and can be
replicated. That is, despite the fact that \{\(U_{i}\)\} appears to be
i.i.d, it can be reproduced by using the same seed number (and the same
algorithm).

\textbf{Example 6.1.2. Generating uniform random numbers in \texttt{R}.}
The following code shows how to generate three uniform (0,1) numbers in
\texttt{R} using the \texttt{runif} command. The \texttt{set.seed()}
function sets the initial seed. In many computer packages, the initial
seed is set using the system clock unless specified otherwise.

\paragraph{Three Uniform Random
Variates}\label{three-uniform-random-variates}
\addcontentsline{toc}{paragraph}{Three Uniform Random Variates}

\begin{tabular}{c}
\hline
Uniform\\
\hline
0.92424\\
\hline
0.53718\\
\hline
0.46920\\
\hline
\end{tabular}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The linear congruential generator is just one method of producing
pseudo-random outcomes. It is easy to understand and is (still) widely
used. The linear congruential generator does have limitations, including
the fact that it is possible to detect long-run patterns over time in
the sequences generated (recall that we can interpret
\emph{independence} to mean a total lack of functional patterns). Not
surprisingly, advanced techniques have been developed that address some
of this method's drawbacks.

\subsection{Inverse Transform Method}\label{S:InverseTransform}

With the sequence of uniform random numbers, we next transform them to a
distribution of interest, say \(F\). A prominent technique is the
inverse transform method, defined as

\[
X_i=F^{-1}\left( U_i \right) .
\]

Here, recall from Section 4.1.1 that we introduced the inverse of the
distribution function, \(F^{-1}\), and referred to it also as the
quantile function. Specifically, it is defined to be

\[
F^{-1}(y) = \inf_x ~ \{ F(x) \ge y \} .
\]

Recall that \(\inf\) stands for \emph{infimum} or the greatest lower
bound. It is essentially the smallest value of \emph{x} that satisfies
the inequality \(\{F(x) \ge y\}\). The result is that the sequence
\{\(X_{i}\)\} is approximately \emph{iid} with distribution function
\(F\).

The inverse transform result is available when the underlying random
variable is continuous, discrete or a hybrid combination of the two. We
now present a series of examples to illustrate its scope of
applications.

\textbf{Example 6.1.3. Generating exponential random numbers.} Suppose
that we would like to generate observations from an exponential
distribution with scale parameter \(\theta\) so that
\(F(x) = 1 - e^{-x/\theta}\). To compute the inverse transform, we can
use the following steps: \[
\begin{aligned}
 y = F(x) &\Leftrightarrow  y = 1-e^{-x/\theta} \\
  &\Leftrightarrow -\theta \ln(1-y) = x = F^{-1}(y) .
\end{aligned}
\]

Thus, if \(U\) has a uniform (0,1) distribution, then
\(X = -\theta \ln(1-U)\) has an exponential distribution with parameter
\(\theta\).

The following \texttt{R} code shows how we can start with the same three
uniform random numbers as in \emph{Example 6.1.2} and transform them to
independent exponentially distributed random variables with a mean of
10. Alternatively, you can directly use the \texttt{rexp} function in
\texttt{R} to generate random numbers from the exponential distribution.
The algorithm built into this routine is different so even with the same
starting seed number, individual realizations will differ.

\paragraph{Three Uniform Random
Variates}\label{three-uniform-random-variates-1}
\addcontentsline{toc}{paragraph}{Three Uniform Random Variates}

\begin{tabular}{r|r|r}
\hline
Uniform & Exponential 1 & Exponential 2\\
\hline
0.92424 & 25.80219 & 3.25222\\
\hline
0.53718 & 7.70409 & 8.47652\\
\hline
0.46920 & 6.33362 & 5.40176\\
\hline
\end{tabular}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 6.1.4. Generating Pareto random numbers.} Suppose that
we would like to generate observations from a Pareto distribution with
parameters \(\alpha\) and \(\theta\) so that
\(F(x) = 1 - \left(\frac{\theta}{x+\theta} \right)^{\alpha}\). To
compute the inverse transform, we can use the following steps:

\[
\begin{aligned}
 y = F(x) &\Leftrightarrow 1-y = \left(\frac{\theta}{x+\theta} \right)^{\alpha} \\
  &\Leftrightarrow \left(1-y\right)^{-1/\alpha} = \frac{x+\theta}{\theta} = \frac{x}{\theta} +1 \\
    &\Leftrightarrow \theta \left((1-y)^{-1/\alpha} - 1\right) = x = F^{-1}(y) .\end{aligned}
\]

Thus, \(X = \theta \left((1-U)^{-1/\alpha} - 1\right)\) has a Pareto
distribution with parameters \(\alpha\) and \(\theta\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Inverse Transform Justification.} Why does the random variable
\(X = F^{-1}(U)\) have a distribution function \(F\)?

Show A Snippet of Theory

\hypertarget{ShowTheory.1}{}
\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

This is easy to establish in the continuous case. Because \(U\) is a
Uniform random variable on (0,1), we know that \(\Pr(U \le y) = y\), for
\(0 \le y \le 1\). Thus,

\[
\begin{aligned}
\Pr(X \le x) &= \Pr(F^{-1}(U) \le x) \\
 &= \Pr(F(F^{-1}(U)) \le F(x)) \\
&= \Pr(U \le F(x)) = F(x)
\end{aligned}
\]

as required. The key step is that \(F(F^{-1}(u)) = u\) for each \(u\),
which is clearly true when \(F\) is strictly increasing.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

We now consider some discrete examples.

\textbf{Example 6.1.5. Generating Bernoulli random numbers.} Suppose
that we wish to simulate random variables from a Bernoulli distribution
with parameter \(p=0.85\).

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{LossDataAnalytics_files/figure-latex/BinaryDF-1} 

}

\caption{Distribution Function of a Binary Random Variable}\label{fig:BinaryDF}
\end{figure}

A graph of the cumulative distribution function in Figure
\ref{fig:BinaryDF} shows that the quantile function can be written as \[
\begin{aligned}
F^{-1}(y) = \left\{ \begin{array}{cc}
              0 & 0<y \leq 0.85 \\
              1 & 0.85 < y  \leq  1.0 .
            \end{array} \right.
\end{aligned}
\]

Thus, with the inverse transform we may define \[
\begin{aligned}
X = \left\{ \begin{array}{cc}
              0 & 0<U \leq 0.85  \\
              1 &  0.85 < U  \leq  1.0
            \end{array} \right.
\end{aligned}
\] For illustration, we generate three random numbers to get

\subsubsection*{Three Random Variates}\label{three-random-variates}
\addcontentsline{toc}{subsubsection}{Three Random Variates}

\begin{tabular}{r|r}
\hline
Uniform & Binary X\\
\hline
0.92424 & 1\\
\hline
0.53718 & 0\\
\hline
0.46920 & 0\\
\hline
\end{tabular}

\textbf{Example 6.1.6. Generating random numbers from a discrete
distribution.} Consider the time of a machine failure in the first five
years. The distribution of failure times is given as:

\paragraph{Discrete Distribution}\label{discrete-distribution}
\addcontentsline{toc}{paragraph}{Discrete Distribution}

\begin{tabular}{l|rrrrr|rrrrr|rrrrr|rrrrr|rrrrr}
\hline
  & \$\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\$ & \$\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\$ & \$\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\$ & \$\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\$ & \$\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\$\\
\hline
Time & 1.0 & 2.0 & 3.0 & 4.0 & 5.0\\
\hline
Probability & 0.1 & 0.2 & 0.1 & 0.4 & 0.2\\
\hline
Distribution Function \$F(x)\$ & 0.1 & 0.3 & 0.4 & 0.8 & 1.0\\
\hline
\end{tabular}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{LossDataAnalytics_files/figure-latex/DiscreteDF-1} 

}

\caption{Distribution Function of a Discrete Random Variable}\label{fig:DiscreteDF}
\end{figure}

Using the graph of the distribution function in Figure
\ref{fig:DiscreteDF}, with the inverse transform we may define

\[
\small{
\begin{aligned}
X = \left\{ \begin{array}{cc}
              1 &   0<U  \leq 0.1  \\
              2 &  0.1 < U  \leq  0.3\\
              3 &  0.3 < U  \leq  0.4\\
              4 &  0.4 < U  \leq  0.8  \\
              5 &  0.8 < U  \leq  1.0     .
            \end{array} \right.
\end{aligned}
}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

For general discrete random variables there may not be an ordering of
outcomes. For example, a person could own one of five types of life
insurance products and we might use the following algorithm to generate
random outcomes:

\[
{\small
\begin{aligned}
X = \left\{ \begin{array}{cc}
  \textrm{whole life} &   0<U  \leq 0.1  \\
 \textrm{endowment} &  0.1 < U  \leq  0.3\\
\textrm{term life} &  0.3 < U  \leq  0.4\\
  \textrm{universal life} &  0.4 < U  \leq  0.8  \\
  \textrm{variable life} &  0.8 < U  \leq  1.0 .
            \end{array} \right.
\end{aligned}
}
\]

Another analyst may use an alternative procedure such as:

\[
{\small
\begin{aligned}
X = \left\{ \begin{array}{cc}
  \textrm{whole life} &   0.9<U<1.0  \\
 \textrm{endowment} &  0.7 \leq U < 0.9\\
\textrm{term life} &  0.6 \leq U < 0.7\\
  \textrm{universal life} &  0.2 \leq U < 0.6  \\
  \textrm{variable life} &  0 \leq U < 0.2 .
            \end{array} \right.
\end{aligned}
}
\]

Both algorithms produce (in the long-run) the same probabilities, e.g.,
\(\Pr(\textrm{whole life})=0.1\), and so forth. So, neither is
incorrect. You should be aware that ``there is more than one way to skin
a cat.'' (What an old expression!) Similarly, you could use an
alternative algorithm for ordered outcomes (such as failure times 1, 2,
3, 4, or 5, above).

\textbf{Example 6.1.7. Generating random numbers from a hybrid
distribution.} Consider a random variable that is 0 with probability
70\% and is exponentially distributed with parameter \(\theta= 10,000\)
with probability 30\%. In an insurance application, this might
correspond to a 70\% chance of having no insurance claims and a 30\%
chance of a claim - if a claim occurs, then it is exponentially
distributed. The distribution function, depicted in Figure
\ref{fig:MixedDF}, is given as

\[
\begin{aligned}
F(y) = \left\{ \begin{array}{cc}
              0 &  x<0  \\
              1 - 0.3 \exp(-x/10000) & x \ge 0 .
            \end{array} \right.
\end{aligned}
\]

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{LossDataAnalytics_files/figure-latex/MixedDF-1} 

}

\caption{Distribution Function of a Hybrid Random Variable}\label{fig:MixedDF}
\end{figure}

From Figure \ref{fig:MixedDF}, we can see that the inverse transform for
generating random variables with this distribution function is

\[
\begin{aligned}
X = F^{-1}(U) = \left\{ \begin{array}{cc}
              0 &  0< U  \leq  0.7  \\
              -1000 \ln (\frac{1-U}{0.3}) & 0.7 < U < 1 .
            \end{array} \right.
\end{aligned}
\]

For discrete and hybrid random variables, the key is to draw a graph of
the distribution function that allows you to visualize potential values
of the inverse function.

\subsection{Simulation Precision}\label{simulation-precision}

From the prior subsections, we now know how to generate independent
simulated realizations from a distribution of interest. With these
realizations, we can construct an empirical distribution and approximate
the underlying distribution as precisely as needed. As we introduce more
actuarial applications in this book, you will see that simulation can be
applied in a wide variety of contexts.

Many of these applications can be reduced to the problem of
approximating \(\mathrm{E~}h(X)\), where \(h(\cdot)\) is some known
function. Based on \(R\) simulations (replications), we get
\(X_1,\ldots,X_R\). From this simulated sample, we calculate an average

\[
\overline{h}_R=\frac{1}{R}\sum_{i=1}^{R} h(X_i)
\] that we use as our simulated approximate (estimate) of
\(\mathrm{E~}h(X)\). To estimate the precision of this approximation, we
use the simulation variance

\[
s_{h,R}^2 = \frac{1}{R-1} \sum_{i=1}^{R}\left( h(X_i) -\overline{h}_R
\right) ^2.
\]

From the independence, the standard error of the estimate is
\(s_{h,R}/\sqrt{R}\). This can be made as small as we like by increasing
the number of replications \(R\).

\textbf{Example. 6.1.8. Portfolio management.} In Section 3.4, we
learned how to calculate the expected value of policies with
deductibles. For an example of something that cannot be done with closed
form expressions, we now consider two risks. This is a variation of a
more complex example that will be covered as \emph{Example 10.3.6}.

We consider two property risks of a telecommunications firm:

\begin{itemize}
\tightlist
\item
  \(X_1\) - buildings, modeled using a gamma distribution with mean 200
  and scale parameter 100.
\item
  \(X_2\) - motor vehicles, modeled using a gamma distribution with mean
  400 and scale parameter 200.
\end{itemize}

Denote the total risk as \(X = X_1 + X_2.\) For simplicity, you assume
that these risks are independent.

To manage the risk, you seek some insurance protection. You wish to
manage internally small building and motor vehicles amounts, up to
\(M\), say. Your retained risk is \(Y_{retained}=\)
\(\min(X_1 + X_2,M)\). The insurer's portion is
\(Y_{insurer} = X- Y_{retained}\).

To be specific, we use \(M=\) 400 as well as \(R=\) 1000000 simulations.

\textbf{a.} With the settings, we wish to determine the expected claim
amount and the associated standard deviation of (i) that retained, (ii)
that accepted by the insurer, and (iii) the total overall amount.

Here is the code for the expected claim amounts.

\begin{verbatim}
                   Retained Insurer  Total
Mean                 365.17  235.01 600.18
Standard Deviation    69.51  280.86 316.36
\end{verbatim}

The results of these calculations are:

\begin{verbatim}
                   Retained Insurer  Total
Mean                 365.17  235.01 600.18
Standard Deviation    69.51  280.86 316.36
\end{verbatim}

\textbf{b.} For insured claims, the standard error of the simulation
approximation is \(s_{h,R}/\sqrt{1000000} =\) 280.86
\(/\sqrt{1000000} =\) 0.281. For this example, simulation is quick and
so a large value such as 1000000 is an easy choice. However, for more
complex problems, the simulation size may be an issue.

Figure \ref{fig:PortfolioDF} allows us to visualize the development of
the approximation as the number of simulations increases.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/PortfolioDF-1} 

}

\caption{Estimated Expected Insurer Claims versus Number of Simulations.}\label{fig:PortfolioDF}
\end{figure}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Determination of Number of Simulations}

How many simulated values are recommended? 100? 1,000,000? We can use
the central limit theorem to respond to this question.

As one criterion for your confidence in the result, suppose that you
wish to be within 1\% of the mean with 95\% certainty. That is, you want
\(\Pr \left( |\overline{h}_R - \mathrm{E~}h(X)| \le 0.01 \mathrm{E~}h(X) \right) \le 0.95\).
According to the central limit theorem, your estimate should be
approximately normally distributed and so we want to have \(R\) large
enough to satisfy
\(0.01 \mathrm{E~}h(X)/\sqrt{\mathrm{Var~}h(X)/R}) \ge 1.96\). (Recall
that 1.96 is the 97.5th percentile from the standard normal
distribution.) Replacing \(\mathrm{E~}h(X)\) and \(\mathrm{Var~}h(X)\)
with estimates, you continue your simulation until

\[
\frac{.01\overline{h}_R}{s_{h,R}/\sqrt{R}}\geq 1.96
\] or equivalently

\begin{equation}
R \geq 38,416\frac{s_{h,R}^2}{\overline{h}_R^2}.
\label{eq:NumSimulations}
\end{equation}

This criterion is a direct application of the approximate normality.
Note that \(\overline{h}_R\) and \(s_{h,R}\) are not known in advance,
so you will have to come up with estimates, either by doing a small
pilot study in advance or by interrupting your procedure intermittently
to see if the criterion is satisfied.

\textbf{Example. 6.1.8. Portfolio management - continued}

For our example, the average insurance claim is 235.011 and the
corresponding standard deviation is 280.862. Using equation
\eqref{eq:NumSimulations}, to be within 10\% of the mean, we would only
require at least 54.87 thousand simulations. However, to be within 1\%
we would want at least 5.49 million simulations.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example. 6.1.9. Approximation choices.} An important application
of simulation is the approximation of \(\mathrm{E~}h(X)\). In this
example, we show that the choice of the \(h(\cdot)\) function and the
distribution of \(X\) can play a role.

Consider the following question : what is \(\Pr[X>2]\) when \(X\) has a
Cauchy distribution, with density
\(f(x) =\left(\pi(1+x^2)\right)^{-1}\), on the real line? The true value
is

\[
\Pr\left[X>2\right] = \int_2^\infty \frac{dx}{\pi(1+x^2)} .
\] One can use an \texttt{R} numerical integration function (which works
usually well on improper integrals)

which is equal to 0.14758.

Alternatively, one can use simulation techniques to approximate that
quantity. From calculus, you can check that the quantile function of the
Cauchy distribution is \(F^{-1}(y) = \tan \left( \pi(y-0.5) \right)\).
Then, with simulated uniform (0,1) variates, \(U_1, \ldots, U_R\), we
can construct the estimator

\[
p_1 = \frac{1}{R}\sum_{i=1}^R \mathrm{I}(F^{-1}(U_i)>2) = \frac{1}{R}\sum_{i=1}^R \mathrm{I}(\tan \left( \pi(U_i-0.5) \right)>2) .
\]

\begin{verbatim}
[1] 0.147439
\end{verbatim}

\begin{verbatim}
[1] 0.0003545432
\end{verbatim}

With one million simulations, we obtain an estimate of 0.14744 with
standard error 0.355 (divided by 1000). One can prove that the variance
of \(p_1\) is of order \(0.127/R\).

With other choices of \(h(\cdot)\) and \(F(\cdot)\), it is actually
possible to reduce uncertainty even using the same number of simulations
\texttt{R}. To begin, one can use the symmetry of the Cauchy
distribution to write \(\Pr[X>2]=0.5\cdot\Pr[|X|>2]\). With this, can
construct a new estimator

\[
p_2 = \frac{1}{2R}\sum_{i=1}^R \mathrm{I}(|F^{-1}(U_i)|>2) .
\]

With one million simulations, we obtain an estimate of 0.14748 with
standard error 0.228 (divided by 1000). One can prove that the variance
of \(p_2\) is of order \(0.052/R\).

But one can go one step further. The improper integral can be written as
a proper one by a simple symmetry property (since the function is
symmetry and the integral on the real line is equal to \(1\)) \[
\int_2^\infty \frac{dx}{\pi(1+x^2)}=\frac{1}{2}-\int_0^2\frac{dx}{\pi(1+x^2)} .
\] From this expression, a natural approximation would be \[
p_3 = \frac{1}{2}-\frac{1}{R}\sum_{i=1}^R h_3(2U_i), ~~~~~~\text{where}~h_3(x)=\frac{2}{\pi(1+x^2)} .
\]

With one million simulations, we obtain an estimate of 0.14756 with
standard error 0.169 (divided by 1000). One can prove that the variance
of \(p_3\) is of order \(0.0285/R\).

Finally, one can also consider some change of variable in the integral
\[
\int_2^\infty \frac{dx}{\pi(1+x^2)}=\int_0^{1/2}\frac{y^{-2}dy}{\pi(1-y^{-2})} .
\] From this expression, a natural approximation would be \[
p_4 = \frac{1}{R}\sum_{i=1}^R h_4(U_i/2),~~~~~\text{where}~h_4(x)=\frac{1}{2\pi(1+x^2)} .
\] The expression seems rather similar to the previous one,

With one million simulations, we obtain an estimate of 0.14759 with
standard error 0.01 (divided by 1000). One can prove that the variance
of \(p_4\) is of order \(0.00009/R\), which is much smaller than what we
had so far !

\protect\hyperlink{tab:61}{Table 6.1} summarizes the four choices of
\(h(\cdot)\) and \(F(\cdot)\) to approximate \(\Pr[X>2] =\) 0.14758. The
standard error varies dramatically. Thus, if we have a desired degree of
accuracy, then the \emph{number of simulations} depends strongly on how
we write the integrals we try to approximate.

Table 6.1. Summary of Four Choices to Approximate \(\Pr[X>2]\).

Estimator

Definition

Support Function

Estimate

Standard Error

\(p_1\)

\(\frac{1}{R}\sum_{i=1}^R \mathrm{I}(F^{-1}(U_i)>2)\)

\(~~~~~~F^{-1}(u)=\tan \left( \pi(u-0.5) \right)~~~~~~~\)

0.147439

0.000355

\(p_2\)

\(\frac{1}{2R}\sum_{i=1}^R \mathrm{I}(|F^{-1}(U_i)|>2)\)

\(F^{-1}(u)=\tan \left( \pi(u-0.5) \right)\)

0.147477

0.000228

\(p_3\)

\(\frac{1}{2}-\frac{1}{R}\sum_{i=1}^R h_3(2U_i)\)

\(h_3(x)=\frac{2}{\pi(1+x^2)}\)

0.147558

0.000169

\(p_4\)

\(\frac{1}{R}\sum_{i=1}^R h_4(U_i/2)\)

\(h_4(x)=\frac{1}{2\pi(1+x^2)}\)

0.147587

0.000010

\subsection{Simulation and Statistical
Inference}\label{S:SimulationStatInference}

Simulations not only help us approximate expected values but are also
useful in calculating other aspects of distribution functions. In
particular, they are very useful when distributions of test statistics
are too complicated to derive; in this case, one can use simulations to
approximate the reference distribution. We now illustrate this with the
Kolmogorov-Smirnov test that we learned about in Section 4.1.2.2.

\textbf{Example. 6.1.10. Kolmogorov-Smirnov Test of Distribution.}
Suppose that we have available \(n=100\) observations
\(\{x_1,\cdots,x_n\}\) that, unknown to the analyst, was generated from
a gamma distribution with parameters \(\alpha = 6\) and \(\theta=2\).
The analyst believes that the data come from a lognormal distribution
with parameters 1 and 0.4 and would like to test this assumption.

The first step is to visualize the data.

With this set-up, Figure \ref{fig:KSTestData} provides a graph of a
histogram and empirical distribution. For reference, superimposed are
red dashed lines from the lognormal distribution.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/KSTestData-1} 

}

\caption{Histogram and Empirical Distribution Function of Data used in Kolmogorov-Smirnov Test. The red dashed lines are fits based on (incorrectly) hypothesized lognormal distribution.}\label{fig:KSTestData}
\end{figure}

Recall that the Kolmogorov-Smirnov statistic equals the largest
discrepancy between the empirical and the hypothesized distribution.
This is \(\max_x |F_n(x)-F_0(x)|\), where \(F_0\) is the hypothesized
lognormal distribution. We can calculate this directly as:

\begin{verbatim}
[1] 0.09703627
\end{verbatim}

Fortunately, for the lognormal distribution, \texttt{R} has built-in
tests that allow us to determine this without complex programming:

\begin{verbatim}

    One-sample Kolmogorov-Smirnov test

data:  x
D = 0.097037, p-value = 0.3031
alternative hypothesis: two-sided
\end{verbatim}

However, for many distributions of actuarial interest, pre-built
programs are not available. We can use simulation to test the relevance
of the test statistic. Specifically, to compute the \(p\)-value, let us
generate thousands of random samples from a \(LN(1,0.4)\) distribution
(with the same size), and compute empirically the distribution of the
statistic,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ns <-}\StringTok{ }\FloatTok{1e4}
\NormalTok{d_KS <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{,ns)}
\CommentTok{# compute the test statistics for a large (ns) number of simulated samples}
\ControlFlowTok{for}\NormalTok{(s }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{ns) d_KS[s] <-}\StringTok{ }\KeywordTok{D}\NormalTok{(}\KeywordTok{rlnorm}\NormalTok{(n,}\DecValTok{1}\NormalTok{,.}\DecValTok{4}\NormalTok{),}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{plnorm}\NormalTok{(x,}\DecValTok{1}\NormalTok{,.}\DecValTok{4}\NormalTok{))}

\KeywordTok{mean}\NormalTok{(d_KS}\OperatorTok{>}\KeywordTok{D}\NormalTok{(x,}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{plnorm}\NormalTok{(x,}\DecValTok{1}\NormalTok{,.}\DecValTok{4}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.2843
\end{verbatim}

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/KSSimulatedDistribution-1} 

}

\caption{Simulated Distribution of the Kolmogorov-Smirnov Test Statistic. The vertical red dashed line marks the test statistic for the sample of 100.}\label{fig:KSSimulatedDistribution}
\end{figure}

The simulated distribution based on 10,000 random samples is summarized
in Figure \ref{fig:KSSimulatedDistribution}. Here, the statistic
exceeded the empirical value (0.09704) in 28.43\% of the scenarios,
while the \emph{theoretical} \(p\)-value is 0.3031. For both the
simulation and the theoretical \(p\)-values, the conclusions are the
same; the data do not provide sufficient evidence to reject the
hypothesis of a lognormal distribution.

Although only an approximation, the simulation approach works in a
variety of distributions and test statistics without needing to develop
the nuances of the underpinning theory for each situation. We summarize
the procedure for developing simulated distributions and \emph{p}-values
as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Draw a sample of size \emph{n}, say, \(X_1, \ldots, X_n\), from a
  known distribution function \(F\). Compute a statistic of interest,
  denoted as \(\hat{\theta}(X_1, \ldots, X_n)\). Call this
  \(\hat{\theta}^r\) for the \emph{r}th replication.
\item
  Repeat this \(r=1, \ldots, R\) times to get a sample of statistics,
  \(\hat{\theta}^1, \ldots,\hat{\theta}^R\).
\item
  From the sample of statistics in Step 2,
  \(\{\hat{\theta}^1, \ldots,\hat{\theta}^R\}\), compute a summary
  measure of interest, such as a \emph{p}-value.
\end{enumerate}

\section{Bootstrapping and Resampling}\label{S:Bootstrap}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Generate a nonparametric bootstrap distribution for a statistic of
  interest
\item
  Use the bootstrap distribution to generate estimates of precision for
  the statistic of interest, including bias, standard deviations, and
  confidence intervals
\item
  Perform bootstrap analyses for parametric distributions
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Bootstrap Foundations}\label{bootstrap-foundations}

Simulation presented up to now is based on sampling from a
\textbf{known} distribution. Section \ref{S:SimulationFundamentals}
showed how to use simulation techniques to sample and compute quantities
from known distributions. However, statistical science is dedicated to
providing inferences about distributions that are \emph{unknown}. We
gather summary statistics based on this unknown population distribution.
But how do we sample from an unknown distribution?

Naturally, we cannot simulate draws from an unknown distribution but we
can draw from a sample of observations. If the sample is a good
representation from the population, then our simulated draws from the
sample should well approximate the simulated draws from a population.
The process of sampling from a sample is called \emph{resampling} or
\emph{bootstrapping}. The term bootstrap comes from the phrase ``pulling
oneself up by one's bootstraps'' (Efron, 1979). With resampling, the
original sample plays the role of the population and estimates from the
sample play the role of true population parameters.

The resampling algorithm is the same as introduced in Section
\ref{S:SimulationStatInference} except that now we use simulated draws
from a sample. It is common to use \(\{X_1, \ldots, X_n\}\) to denote
the original sample and let \(\{X_1^*, \ldots, X_n^*\}\) denote the
simulated draws. We draw them with replacement so that the simulated
draws will be independent from one another, the same assumption as with
the original sample. For each sample, we also use \emph{n} simulated
draws, the same number as the original sample size. To distinguish this
procedure from the simulation, it is common to use \emph{B} (for
bootstrap) to be the number of simulated samples. We could also write
\(\{X_1^{(b)}, \ldots, X_n^{(b)}\}\), \(b=1,\ldots, B\) to clarify this.

There are two basic resampling methods, \emph{model-free} and
\emph{model-based}, which are, respectively, as \emph{nonparametric} and
\emph{parametric}. In the nonparametric approach, no assumption is made
about the distribution of the parent population. The simulated draws
come from the empirical distribution function \(F_n(\cdot)\), so each
draw comes from \(\{X_1, \ldots, X_n\}\) with probability 1/\emph{n}.

In contrast, for the parametric approach, we assume that we have
knowledge of the distribution family \emph{F}. The original sample
\(X_1, \ldots, X_n\) is used to estimate parameters of that family, say,
\(\hat{\theta}\). Then, simulated draws are taken from the
\(F(\hat{\theta})\). Section \ref{S:ParametricBootStrap} discusses this
approach in further detail.

\subsubsection*{Nonparametric Bootstrap}\label{nonparametric-bootstrap}
\addcontentsline{toc}{subsubsection}{Nonparametric Bootstrap}

The idea of the nonparametric bootstrap is to use the inverse method on
\(F_n\), the empirical cumulative distribution function, depicted in
Figure \ref{fig:InverseDFboot}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{LossDataAnalytics_files/figure-latex/InverseDFboot-1} 

}

\caption{Inverse of an Empirical Distribution Function}\label{fig:InverseDFboot}
\end{figure}

Because \(F_n\) is a step-function, \(F_n^{-1}\) takes values in
\(\{x_1,\cdots,x_n\}\). More precisely, as illustrated in Figure
\ref{fig:InverseDFboot2}.

\begin{itemize}
\tightlist
\item
  if \(y\in(0,1/n)\) (with probability \(1/n\)) we draw the smallest
  value (\(\min\{x_i\}\))
\item
  if \(y\in(1/n,2/n)\) (with probability \(1/n\)) we draw the second
  smallest value,
\item
  \ldots{}
\item
  if \(y\in((n-1)/n,1)\) (with probability \(1/n\)) we draw the largest
  value (\(\max\{x_i\}\)).
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{LossDataAnalytics_files/figure-latex/InverseDFboot2-1} 

}

\caption{Inverse of an Empirical Distribution Function}\label{fig:InverseDFboot2}
\end{figure}

Using the inverse method with \(F_n\) means sampling from
\(\{x_1,\cdots,x_n\}\), with probability \(1/n\). Generating a bootstrap
sample of size \(B\) means sampling from \(\{x_1,\cdots,x_n\}\), with
probability \(1/n\), with replacement. See the following illustrative
\texttt{R} code.

\begin{verbatim}
[1] 7.0899 0.8742 0.8388 4.5311 0.8388 5.7394 0.8388 2.6164
\end{verbatim}

Observe that value 0.8388 was obtained three times.

\subsection{Bootstrap Precision: Bias, Standard Deviation, and
MSE}\label{bootstrap-precision-bias-standard-deviation-and-mse}

We summarize the nonparametric bootstrap procedure as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  From the sample \(\{X_1, \ldots, X_n\}\), draw a sample of size
  \emph{n} (with replacement), say, \(X_1^*, \ldots, X_n^*\). From the
  simulated draws compute a statistic of interest, denoted as
  \(\hat{\theta}(X_1^*, \ldots, X_n^*)\). Call this \(\hat{\theta}_b^*\)
  for the \emph{b}th replicate.
\item
  Repeat this \(b=1, \ldots, B\) times to get a sample of statistics,
  \(\hat{\theta}_1^*, \ldots,\hat{\theta}_B^*\).
\item
  From the sample of statistics in Step 2,
  \(\{\hat{\theta}_1^*, \ldots, \hat{\theta}_B^*\}\), compute a summary
  measure of interest.
\end{enumerate}

In this section, we focus on three summary measures, the bias, the
standard deviation, and the mean square error (\emph{MSE}).
\protect\hyperlink{tab:62}{Table 6.2} summarizes these three measures.
Here, \(\overline{\hat{\theta^*}}\) is the average of
\(\{\hat{\theta}_1^*, \ldots,\hat{\theta}_B^*\}\).

\[
{\small
\begin{matrix}
\text{Table 6.2. Bootstrap Summary Measures}\\
\begin{array}{l|c|c|c}
\hline
\text{Population Measure}& \text{Population Definition}&\text{Bootstrap Approximation}&\text{Bootstrap Symbol}\\
\hline
\text{Bias} & \mathrm{E}(\hat{\theta})-\theta&\overline{\hat{\theta^*}}-\hat{\theta}& Bias_{boot}(\hat{\theta})  \\\hline
\text{Standard Deviation} &   \sqrt{\mathrm{Var}(\hat{\theta})}
& \sqrt{\frac{1}{B-1} \sum_{b=1}^{B}\left(\hat{\theta}_b^* -\overline{\hat{\theta^*}} \right) ^2}&s_{boot}(\hat{\theta})  \\\hline
\text{Mean Square Error} &\mathrm{E}(\hat{\theta}-\theta)^2 & \frac{1}{B} \sum_{b=1}^{B}\left(\hat{\theta}_b^* -\hat{\theta}
\right)^2&MSE_{boot}(\hat{\theta})\\
\hline
\end{array}\end{matrix}
}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 6.2.1. Bodily Injury Claims and Loss Elimination
Ratios.} To show how the bootstrap can be used to quantify the precision
of estimators, we return to the Example 4.1.11 bodily injury claims data
where we introduced a nonparametric estimator of the loss elimination
ratio.

\protect\hyperlink{tab:63}{Table 6.3} summarizes the results of the
bootstrap estimation. For example, at \(d=14000\), we saw in Example
4.1.11 that the nonparametric estimate of \emph{LER} is 0.97678. This
has an estimated bias of 0.00018 with a standard deviation of 0.00701.
For some applications, you may wish to apply the estimated bias to the
original estimate to give a bias-corrected estimator. This is the focus
of the next example. For this illustration, the bias is small and so
such a correction is not relevant.

\paragraph{Table 6.3. Bootstrap Estimates of LER at Selected
Deductibles}\label{table-6.3.-bootstrap-estimates-of-ler-at-selected-deductibles}
\addcontentsline{toc}{paragraph}{Table 6.3. Bootstrap Estimates of LER
at Selected Deductibles}

\begin{tabular}{r|r|r|r|r|r}
\hline
d & NP Estimate & Bootstrap Bias & Bootstrap SD & Lower Normal 95\% CI & Upper Normal 95\% CI\\
\hline
4000 & 0.54113 & -0.00012 & 0.01228 & 0.51719 & 0.56531\\
\hline
5000 & 0.64960 & -0.00038 & 0.01400 & 0.62254 & 0.67741\\
\hline
10500 & 0.93563 & 0.00018 & 0.01065 & 0.91458 & 0.95632\\
\hline
11500 & 0.95281 & -0.00016 & 0.00918 & 0.93497 & 0.97097\\
\hline
14000 & 0.97678 & 0.00018 & 0.00701 & 0.96286 & 0.99035\\
\hline
18500 & 0.99382 & 0.00006 & 0.00342 & 0.98706 & 1.00047\\
\hline
\end{tabular}

The bootstrap standard deviation gives a measure of precision. For one
application of standard deviations, we can use the normal approximation
to create a confidence interval. For example, the \texttt{R} function
\texttt{boot.ci} produces the normal confidence intervals at 95\%. These
are produced by creating an interval of twice the length of 1.95994
bootstrap standard deviations, centered about the bias-corrected
estimator (1.95994 is the 97.5th quantile of the standard normal
distribution). For example, the lower normal 95\% CI at \(d=14000\) is
\((0.97678-0.00018)- 1.95994*0.00701\) \(= 0.96286\). We further discuss
bootstrap confidence intervals in the next section.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 6.2.2. Estimating \(\exp(\mu)\).} The bootstrap can be
used to quantify the bias of an estimator, for instance. Consider here a
sample \(\mathbf{x}=\{x_1,\cdots,x_n\}\) that is iid with mean \(\mu\).

Suppose that the quantity of interest is \(\theta=\exp(\mu)\). A natural
estimator would be \(\widehat{\theta}_1=\exp(\overline{x})\). This
estimator is biased (due to the Jensen inequality) but is asymptotically
unbiased. For our sample, the estimate is as follows.

\begin{verbatim}
[1] 19.13463
\end{verbatim}

One can use the central limit theorem to get a correction using \[
\overline{X}\approx\mathcal{N}\left(\mu,\frac{\sigma^2}{n}\right)\text{ where }\sigma^2=\text{Var}[X_i] ,
\] so that, with the normal moment generating function, we have \[
\mathrm{E}~\exp[\overline{X}] \approx \exp\left(\mu+\frac{\sigma^2}{2n}\right) .
\] Hence, one can consider naturally \[
\widehat{\theta}_2=\exp\left(\overline{x}-\frac{\widehat{\sigma}^2}{2n}\right) .
\] For our data, this turns out to be as follows.

\begin{verbatim}
[1] 18.73334
\end{verbatim}

As another strategy (that we do not pursue here), one can also use
Taylor's approximation to get a more accurate estimator (as in the delta
method), \[
g(\overline{x})=g(\mu)+(\overline{x}-\mu)g'(\mu)+(\overline{x}-\mu)^2\frac{g''(\mu)}{2}+\cdots
\] The alternative we do explore is to use a bootstrap strategy: given a
bootstrap sample, \(\mathbf{x}^{\ast}_{b}\), let
\(\overline{x}^{\ast}_{b}\) denotes its mean, and set \[
\widehat{\theta}_3=\frac{1}{B}\sum_{b=1}^B\exp(\overline{x}^{\ast}_{b}) .
\] To implement this, we have the following code.

Then, you can \texttt{plot(results)} and \texttt{print(results)} to see
the following.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/BootstrapDistn-1} 

}

\caption{Distribution of Bootstrap Replicates. The left-hand panel is a histogram of replicates. The right-hand panel is a quantile-quantile plot, comparing the bootstrap distribution to the standard normal distribution.}\label{fig:BootstrapDistn}
\end{figure}

\begin{verbatim}

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = sample_x, statistic = function(y, indices) exp(mean(y[indices])), 
    R = 1000)


Bootstrap Statistics :
    original    bias    std. error
t1* 19.13463 0.5589631    4.021793
\end{verbatim}

This results in three estimators, the raw estimator
\(\widehat{\theta}_1=\) 19.135, the second-order correction
\(\widehat{\theta}_2=\) 18.733, and the bootstrap estimator
\(\widehat{\theta}_3=\) 19.694.

How does this work with differing sample sizes? We now suppose that the
\(x_i\)'s are generated from a lognormal distribution \(LN(0,1)\), so
that \(\mu = \exp(0 + 1/2) = 1.648721\) and \(\theta = \exp(1.648721)\)
\(= 5.200326\). We use simulation to draw the sample sizes but then act
as if they were a realized set of observations. See the following
illustrative code.

The results of the comparison are summarized in Figure
\ref{fig:BootstrapCompare}. This figure shows that the bootstrap
estimator is closer to the true parameter value for almost all sample
sizes. The bias of all three estimators decreases as the sample size
increases.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/BootstrapCompare-1} 

}

\caption{Comparision of Estimates. True value of the parameter is given by the solid horizontal line at 5.20.}\label{fig:BootstrapCompare}
\end{figure}

\subsection{Confidence Intervals}\label{confidence-intervals}

The bootstrap procedure generates \emph{B} replicates
\(\hat{\theta}_1^*, \ldots,\hat{\theta}_B^*\) of the estimator
\(\hat{\theta}\). In \emph{Example 6.2.1,} we saw how to use standard
normal approximations to create a confidence interval for parameters of
interest. However, given that a major point is to use bootstrapping to
avoid relying on assumptions of approximate normality, it is not
surprising that there are alternative confidence intervals available.

For an estimator \(\hat{\theta}\), the \emph{basic} bootstrap confidence
interval is

\begin{equation} 
  \left(2 \hat{\theta} - q_U, 2 \hat{\theta} - q_L \right) ,
  \label{eq:basicBootCI}
\end{equation}

where \(q_L\) and \(q_U\) are lower and upper 2.5\% quantiles from the
bootstrap sample \(\hat{\theta}_1^*, \ldots,\hat{\theta}_B^*\).

To see where this comes from, start with the idea that \((q_L, q_U)\)
provides a 95\% interval for
\(\hat{\theta}_1^*, \ldots,\hat{\theta}_B^*\). So, for a random
\(\hat{\theta}_b^*\), there is a 95\% chance that
\(q_L \le \hat{\theta}_b^* \le q_U\). Reversing the inequalities and
adding \(\hat{\theta}\) to each side gives a 95\% interval

\[
\hat{\theta} -q_U \le \hat{\theta} - \hat{\theta}_b^* \le  \hat{\theta} -q_L .
\] So, \(\left( \hat{\theta}-q_U, \hat{\theta} -q_L\right)\) is an 95\%
interval for \(\hat{\theta} - \hat{\theta}_b^*\). The bootstrap
approximation idea says that this is also a 95\% interval for
\(\theta - \hat{\theta}\). Adding \(\hat{\theta}\) to each side gives
the 95\% interval in equation \eqref{eq:basicBootCI}.

Many alternative bootstrap intervals are available. The easiest to
explain is the percentile bootstrap interval which is defined as
\(\left(q_L, q_U\right)\). However, this has the drawback of potentially
poor behavior in the tails which can be of concern in some actuarial
problems of interest.

\textbf{Example 6.2.3. Bodily Injury Claims and Risk Measures.} To see
how the bootstrap confidence intervals work, we return to the bodily
injury auto claims considered in Example 6.2.1. Instead of the loss
elimination ratio, suppose we wish to estimate the 95th percentile
\(F^{-1}(0.95)\) and a measure defined as \[
TVaR_{0.95)}[X] = \mathrm{E}[X | X > F^{-1}(0.95)] .
\] This measure is called the tail value-at-risk; it is the expected
value of \(X\) conditional on \(X\) exceeding the 95th percentile.
Section 10.2 explains how quantiles and the tail value-at-risk are the
two most important examples of so-called \emph{risk measures}. For now,
we will simply think of these as measures that we wish to estimate. For
the percentile, we use the nonparametric estimator \(F^{-1}_n(0.95)\)
defined in Section 4.1.1.3. For the tail value-at-risk, we use the
plug-in principle to define the nonparametric estimator

\[
TVaR_{n,0.95}[X] = \frac{\sum_{i=1}^n X_i I(X_i > F^{-1}_n(0.95))}{\sum_{i=1}^n I(X_i > F^{-1}_n(0.95))} ~.
\] In this expression, the denominator counts the number of observations
that exceed the 95th percentile \(F^{-1}_n(0.95)\). The numerator adds
up losses for those observations that exceed \(F^{-1}_n(0.95)\).
\protect\hyperlink{tab:64}{Table 6.4} summarizes the estimator for
selected fractions.

\paragraph{Table 6.4. Bootstrap Estimates of Quantiles at Selected
Fractions}\label{table-6.4.-bootstrap-estimates-of-quantiles-at-selected-fractions}
\addcontentsline{toc}{paragraph}{Table 6.4. Bootstrap Estimates of
Quantiles at Selected Fractions}

\begin{tabular}{r|r|r|r|r|r|r|r|r|r}
\hline
Fraction & NP Estimate & Bootstrap Bias & Bootstrap SD & Lower Normal 95\% CI & Upper Normal  95\% CI & Lower Basic 95\% CI & Upper Basic 95\% CI & Lower Percentile 95\% CI & Upper  Percentile 95\% CI\\
\hline
0.50 & 6500.00 & -126.66 & 205.29 & 6224.30 & 7029.01 & 6297.00 & 7000.00 & 6000.00 & 6703.00\\
\hline
0.80 & 9078.40 & 92.14 & 205.90 & 8582.70 & 9389.82 & 8493.54 & 9168.80 & 8988.00 & 9663.26\\
\hline
0.90 & 11454.00 & 38.17 & 462.87 & 10508.62 & 12323.04 & 10535.02 & 12373.00 & 10535.00 & 12372.98\\
\hline
0.95 & 13313.40 & 47.27 & 721.87 & 11851.29 & 14680.96 & 11126.80 & 14352.14 & 12274.66 & 15500.00\\
\hline
0.98 & 16758.72 & 58.44 & 1272.69 & 14205.86 & 19194.70 & 14517.44 & 19207.79 & 14309.65 & 19000.00\\
\hline
\end{tabular}

For example, when the fraction is 0.50, we see that lower and upper
2.5th quantiles of the bootstrap simulations are \(q_L=\) 6000 and
\(q_u=\) 6703, respectively. These form the percentile bootstrap
confidence interval. With the nonparametric estimator 6500, these yield
the lower and upper bounds of the basic confidence interval 6297 and
7000, respectively. \protect\hyperlink{tab:64}{Table 6.4} also shows
bootstrap estimates of the bias, standard deviation, and a normal
confidence interval, concepts introduced in the prior section.

\protect\hyperlink{tab:65}{Table 6.5} shows similar calculations for the
tail value-at-risk. In each case, we see that the bootstrap standard
deviation increases as the fraction increases. This is a reflection of
the fewer observations available to estimate quantiles as the fraction
increases, hence greater imprecision. Width of confidence intervals also
increase. Interestingly, there does not seem to be the same pattern in
the estimates of the bias.

\paragraph{Table 6.5. Bootstrap Estimates of TVaR at Selected Risk
Levels}\label{table-6.5.-bootstrap-estimates-of-tvar-at-selected-risk-levels}
\addcontentsline{toc}{paragraph}{Table 6.5. Bootstrap Estimates of TVaR
at Selected Risk Levels}

\begin{tabular}{r|r|r|r|r|r|r|r|r|r}
\hline
Fraction & NP Estimate & Bootstrap Bias & Bootstrap SD & Lower Normal 95\% CI & Upper Normal  95\% CI & Lower Basic 95\% CI & Upper Basic 95\% CI & Lower Percentile 95\% CI & Upper  Percentile 95\% CI\\
\hline
0.50 & 9794.69 & -126.05 & 293.39 & 9345.71 & 10495.78 & 9346.48 & 10472.56 & 9116.82 & 10242.90\\
\hline
0.80 & 12454.18 & 58.79 & 475.60 & 11463.23 & 13327.55 & 11437.50 & 13308.25 & 11600.12 & 13470.86\\
\hline
0.90 & 14720.05 & -40.96 & 704.89 & 13379.45 & 16142.56 & 13352.52 & 16022.97 & 13417.13 & 16087.57\\
\hline
0.95 & 17072.43 & -7.29 & 1141.01 & 14843.37 & 19316.07 & 14860.29 & 19297.01 & 14847.85 & 19284.57\\
\hline
0.98 & 20140.56 & 81.22 & 1623.89 & 16876.56 & 23242.10 & 16883.05 & 23117.58 & 17163.53 & 23398.07\\
\hline
\end{tabular}

\subsection{Parametric Bootstrap}\label{S:ParametricBootStrap}

The idea of the nonparametric bootstrap is to resample by drawing
independent variables from the empirical cumulative distribution
function \(F_n\). In contrast, with parametric bootstrap, we draw
independent variables from \(F_{\widehat{\theta}}\) where the underlying
distribution is assume to be in a parametric family
\(\mathcal{F}=\{F_{\theta},\theta\in\Theta\}\). Typically, parameters
from this distribution are estimated based on a sample and denoted as
\(\hat{\theta}\).

\textbf{Example 6.2.4. Lognormal distribution.} Consider again the
dataset

The classical (nonparametric) bootstrap was based on samples

while for the parametric bootstrap, we have to assume that the
distribution of \(x_i\)'s is from a specific family, for instance a
lognormal distribution.

\begin{verbatim}
    meanlog       sdlog   
  1.03630697   0.30593440 
 (0.06840901) (0.04837027)
\end{verbatim}

Then we draw from that distribution.

Figure \ref{fig:CoefVarCompare} compares the bootstrap distributions for
the coefficient of variation, one based on the nonparametric approach
and the other based on a parametric approach, assuming a lognormal
distribution.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/CoefVarCompare-1} 

}

\caption{Comparision of Nonparametric and Parametric Bootstrap Distributions for the Coefficient of Variation}\label{fig:CoefVarCompare}
\end{figure}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 6.2.5. Bootstrapping Censored Observations.} The
parametric bootstrap draws simulated realizations from a parametric
estimate of the distribution function. In the same way, we can draw
simulated realizations from estimates of a distribution function. As one
example, we might draw from smoothed estimates of a distribution
function introduced in Section 4.1.1.4. Another special case, considered
here, is to draw an estimate from the Kaplan-Meier estimator introduced
in Section 4.3.2.2. In this way, we can handle observations that are
censored.

Specifically, return to the bodily injury data in Examples 6.2.1 and
6.2.3 but now we include the 17 claims that were censored by policy
limits. In Example 4.3.6, we used this full dataset to estimate the
Kaplan-Meier estimator of the survival function introduced in Section
4.3.2.2. \protect\hyperlink{tab:66}{Table 6.6} present bootstrap
estimates of the quantiles from the Kaplan-Meier survival function
estimator. These include the bootstrap precision estimates, bias and
standard deviation, as well as the basic 95\% confidence interval.

\paragraph{Table 6.6. Bootstrap Kaplan-Meier Estimates of Quantiles at
Selected
Fractions}\label{table-6.6.-bootstrap-kaplan-meier-estimates-of-quantiles-at-selected-fractions}
\addcontentsline{toc}{paragraph}{Table 6.6. Bootstrap Kaplan-Meier
Estimates of Quantiles at Selected Fractions}

\begin{tabular}{r|r|r|r|r|r}
\hline
Fraction & KM NP Estimate & Bootstrap Bias & Bootstrap SD & Lower Basic 95\% CI & Upper Basic 95\% CI\\
\hline
0.50 & 6500 & 13.58 & 181.23 & 6093 & 6923\\
\hline
0.80 & 9500 & 173.61 & 423.41 & 8445 & 9949\\
\hline
0.90 & 12756 & 20.17 & 675.86 & 10812 & 14012\\
\hline
0.95 & 18500 & Inf & NaN & 12500 & 22300\\
\hline
0.98 & 25000 & Inf & NaN & -Inf & 27308\\
\hline
\end{tabular}

Results in \protect\hyperlink{tab:66}{Table 6.6} are consistent with the
results for the uncensored subsample in \protect\hyperlink{tab:64}{Table
6.4}. In \protect\hyperlink{tab:66}{Table 6.6}, we note the difficulty
in estimating quantiles at large fractions due to the censoring.
However, for moderate size fractions (0.50, 0.80, and 0.90), the
Kaplan-Meier nonparametric (KM NP) estimates of the quantile are
consistent with those \protect\hyperlink{tab:64}{Table 6.4}. The
bootstrap standard deviation is smaller at the 0.50 (corresponding to
the median) but larger at the 0.80 and 0.90 levels. The censored data
analysis summarized in \protect\hyperlink{tab:66}{Table 6.6} uses more
data than the uncensored subsample analysis in
\protect\hyperlink{tab:64}{Table 6.4} but also has difficulty extracting
information for large quantiles.

\section{Cross-Validation}\label{S:CrossValidation}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Compare and contrast cross-validation to simulation techniques and
  bootstrap methods.
\item
  Use cross-validation techniques for model selection
\item
  Explain the jackknife method as a special case of cross-validation and
  calculate jackknife estimates of bias and standard errors
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Cross-validation, briefly introduced in Section 4.2.4, is a technique
based on simulated outcomes and so let's think about its purposes in
contrast to other simulation techniques already introduced in this
chapter.

\begin{itemize}
\tightlist
\item
  Simulation, or Monte-Carlo, introduced in Section
  \ref{S:SimulationFundamentals}, allow us to compute expected values
  and other summaries of statistical distributions, such as
  \(p\)-values, readily.
\item
  Bootstrap, and other resampling methods introduced in Section
  \ref{S:Bootstrap}, provide estimators of the precision, or
  variability, of statistics.
\item
  Cross-validation is important when assessing how accurately a
  predictive model will perform in practice.
\end{itemize}

Overlap exists but nonetheless it is helpful to think about the broad
goals as associated with each statistical method.

To discuss cross-validation, let us recall from Section 4.2 some of the
key ideas of model validation. When assessing, or validating, a model,
we look to performance measured on \emph{new} data, or at least not
those that were used to fit the model. A classical approach, described
in Section 4.2.3, is to split the sample in two: a subpart (the
\emph{training} dataset) is used to fit the model and another one (the
\emph{testing} dataset) is used to validate. However, a limitation of
this approach is that results depend on the split; even though the
overall sample is fixed, the split between training and test subsamples
varies randomly. A different training sample means that model estimated
parameters will differ. Different model parameters and a different test
sample means that validation statistics will differ. Two analysts may
use the same data and same models yet reach different conclusions about
the viability of a model (based on different random splits), a
frustrating situation.

\subsection{k-Fold Cross-Validation}\label{k-fold-cross-validation}

To mitigate this difficulty, it is common to use a cross-validation
approach as introduced in Section 4.2.4. The key idea is to emulate the
basic test/training approach to model validation by repeating it many
times through averaging over different splits of the data. A key
advantage is that the validation statistic is not tied to a specific
parametric (or nonparametric) model - one can use a nonparametric
statistic or a statistic that has economic interpretations - and so this
can be used to compare models that are not nested (unlike likelihood
ratio procedures).

\textbf{Example 6.3.1. Wisconsin Property Fund.} For the 2010 property
fund data introduced in Section 1.3, we fit gamma and Pareto
distributions to the 1,377 claims data. For details of the related
goodness of fit, see Appendix Section 15.4.4. We now consider the
Kolmogorov-Smirnov statistic introduced in Section 4.1.2.2. When the
entire dataset was fit, the Kolmogorov-Smirnov goodness of fit statistic
for the gamma distribution turns out to be 0.0478 and for the Pareto
distribution is 0.2639. The lower value for the Pareto distribution
indicates that this distribution is a better fit than the gamma.

To see how k-fold cross-validation works, we randomly split the data
into \(k=8\) groups, or folds, each having about \(1377/8 \approx 172\)
observations. Then, we fit gamma and Pareto models to a data set with
the first seven folds (about \(172*7 = 1204\) observations), determine
estimated parameters, and then used these fitted models with the
held-out data to determine the Kolmogorov-Smirnov statistic.

The results appear in Figure \ref{fig:KScvFig} where horizontal axis is
Fold=1. This process was repeated for the other seven folds. The results
summarized in Figure \ref{fig:KScvFig} show that the Pareto consistently
provides a more reliable predictive distribution than the gamma.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/KScvFig-1} 

}

\caption{Cross Validated Kolmogorov-Smirnov (KS) Statistics for the Property Fund Claims Data. The solid black line is for the Pareto distribution, the green dashed line is for the gamma distribution. The KS statistic measures the largest deviation between the fitted distribution and the empirical distribution for each of 8 groups, or folds, of randomly selected data.}\label{fig:KScvFig}
\end{figure}

\subsection{Leave-One-Out
Cross-Validation}\label{leave-one-out-cross-validation}

A special case where \(k=n\) is known as leave-one-out cross validation.
This case is historically prominent and is closely related to jackknife
statistics, a precursor of the bootstrap technique.

Even though we present it as a special case of cross-validation, it is
helpful to given an explicit definition. Consider a generic statistic
\(\widehat{\theta}=t(\boldsymbol{x})\) that is an estimator for a
parameter of interest \(\theta\). The idea of the jackknife is to
compute \(n\) values \(\widehat{\theta}_{-i}=t(\boldsymbol{x}_{-i})\),
where \(\boldsymbol{x}_{-i}\) is the subsample of \(\boldsymbol{x}\)
with the \(i\)-th value removed. The average of these values is denoted
as \[
\overline{\widehat{\theta}}_{(\cdot)}=\frac{1}{n}\sum_{i=1}^n \widehat{\theta}_{-i} .
\] These values can be used to create estimates of the bias of the
statistic \(\widehat{\theta}\)

\begin{equation}
Bias_{jack} = (n-1) \left(\overline{\widehat{\theta}}_{(\cdot)} - \widehat{\theta}\right)
\label{eq:Biasjack}
\end{equation}

as well as a standard deviation estimate

\begin{equation}
s_{jack} =\sqrt{\frac{n-1}{n}\sum_{i=1}^n \left(\widehat{\theta}_{-i} -\overline{\widehat{\theta}}_{(\cdot)}\right)^2} ~.
\label{eq:sdjack}
\end{equation}

\textbf{Example 6.3.2. Coefficient of Variation.} To illustrate,
consider a small fictitious sample \(\boldsymbol{x}=\{x_1,\ldots,x_n\}\)
with realizations

\begin{verbatim}
sample_x <- c(2.46,2.80,3.28,3.86,2.85,3.67,3.37,3.40,
              5.22,2.55,2.79,4.50,3.37,2.88,1.44,2.56,2.00,2.07,2.19,1.77)
\end{verbatim}

Suppose that we are interested in the coefficient of variation
\(\theta = CV = \sqrt{\mathrm{Var~}X}/\mathrm{E~}X\).

With this dataset, the estimator of the coefficient of variation turns
out to be 0.31196. But how reliable is it? To answer this question, we
can compute the jackknife estimates of bias and its standard deviation.
The following code shows that the jackknife estimator of the bias is
\(Bias_{jack} =\) -0.00627 and the jackknife standard deviation is
\(s_{jack} =\) 0.01293.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 6.3.3. Bodily Injury Claims and Loss Elimination
Ratios.} In Example 6.2.1, we showed how to compute bootstrap estimates
of the bias and standard deviation for the loss elimination ratio using
the Example 4.1.11 bodily injury claims data. We follow up now by
providing comparable quantities using jackknife statistics.

\protect\hyperlink{tab:67}{Table 6.7} summarizes the results of the
jackknife estimation. It shows that jackknife estimates of the bias and
standard deviation of the loss elimination ratio
\(\mathrm{E}~\min(X,d)/\mathrm{E}~X\) are largely consistent with the
bootstrap methodology. Moreover, one can use the standard deviations to
construct normal based confidence intervals, centered around a
bias-corrected estimator. For example, at \(d=14000\), we saw in Example
4.1.11 that the nonparametric estimate of \emph{LER} is 0.97678. This
has an estimated bias of 0.00010, resulting in the (jackknife)
\emph{bias-corrected} estimator 0.97688. The 95\% confidence intervals
are produced by creating an interval of twice the length of 1.96
jackknife standard deviations, centered about the bias-corrected
estimator (1.96 is the approximate 97.5th quantile of the standard
normal distribution).

\paragraph{Table 6.7. Jackknife Estimates of LER at Selected
Deductibles}\label{table-6.7.-jackknife-estimates-of-ler-at-selected-deductibles}
\addcontentsline{toc}{paragraph}{Table 6.7. Jackknife Estimates of LER
at Selected Deductibles}

\begin{tabular}{r|r|r|r|r|r|r|r}
\hline
d & NP Estimate & Bootstrap Bias & Bootstrap SD & Jackknife Bias & Jackknife SD & Lower Jackknife 95\% CI & Upper Jackknife 95\% CI\\
\hline
4000 & 0.54113 & -0.00012 & 0.01228 & 0.00031 & 0.00061 & 0.53993 & 0.54233\\
\hline
5000 & 0.64960 & -0.00038 & 0.01400 & 0.00033 & 0.00068 & 0.64825 & 0.65094\\
\hline
10500 & 0.93563 & 0.00018 & 0.01065 & 0.00019 & 0.00053 & 0.93460 & 0.93667\\
\hline
11500 & 0.95281 & -0.00016 & 0.00918 & 0.00016 & 0.00047 & 0.95189 & 0.95373\\
\hline
14000 & 0.97678 & 0.00018 & 0.00701 & 0.00010 & 0.00034 & 0.97612 & 0.97745\\
\hline
18500 & 0.99382 & 0.00006 & 0.00342 & 0.00003 & 0.00017 & 0.99350 & 0.99415\\
\hline
\end{tabular}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Discussion.} One of the many interesting things about the
leave-one-out special case is the ability to replicate estimates
exactly. That is, when the size of the fold is only one, then there is
no additional uncertainty induced by the cross-validation. This means
that analysts can exactly replicate work of one another, an important
consideration.

Jackknife statistics were developed to understand precision of
estimators, producing estimators of bias and standard deviation in
equations \eqref{eq:Biasjack} and \eqref{eq:sdjack}. This crosses into goals
that we have associated with bootstrap techniques, not cross-validation
methods. This demonstrates how statistical techniques can be used to
achieve different goals.

\subsection{Cross-Validation and
Bootstrap}\label{cross-validation-and-bootstrap}

The bootstrap is useful in providing estimators of the precision, or
variability, of statistics. It can also be useful for model validation.
The bootstrap approach to model validation is similar to the
leave-one-out and \emph{k}-fold validation procedures:

\begin{itemize}
\tightlist
\item
  Create a bootstrap sample by re-sampling (with replacement) \(n\)
  indices in \(\{1,\cdots,n\}\). That will be our \emph{training
  sample}. Estimate the model under consideration based on this sample.
\item
  The \emph{test}, or \emph{validation sample}, consists of those
  observations not selected for training. Evaluate the fitted model
  (based on the training data) using the test data.
\end{itemize}

Repeat this process many (say \(B\)) times. Take an average over the
results and choose the model based on the average evaluation statistic.

\textbf{Example 6.3.4. Wisconsin Property Fund.} Return to Example 6.3.1
where we investigate the fit of the gamma and Pareto distributions on
the property fund data. We again compare the predictive performance
using the Kolmogorov-Smirnov statistic but this time using the bootstrap
procedure to split the data between training and testing samples. The
following provides illustrative code.

We did the sampling using \(B=\) 100 replications. The average \emph{KS}
statistic for the Pareto distribution was 0.058 compared to the average
for the gamma distribution, 0.261. This is consistent with earlier
results and provides another piece of evidence that the Pareto is a
better model for these data than the gamma.

\section{Importance Sampling}\label{S:ImportanceSampling}

Section \ref{S:SimulationFundamentals} introduced Monte Carlo techniques
using the inversion technique : to generate a random variable \(X\) with
distribution \(F\), apply \(F^{-1}\) to calls of a random generator
(uniform on the unit interval). What if we what to draw according to
\(X\), conditional on \(X\in[a,b]\) ?

One can use an accept-reject mechanism : draw \(x\) from distribution
\(F\)

\begin{itemize}
\tightlist
\item
  if \(x\in[a,b]\) : keep it (``\emph{accept}'')
\item
  if \(x\notin[a,b]\) : draw another one (``\emph{reject}'')
\end{itemize}

Observe that from \(n\) values initially generated, we keep here only
\([F(b)-F(a)]\cdot n\) draws, on average.

\textbf{Example 6.4.1. Draws from a Normal Distribution.} Suppose that
we draw from a normal distribution with mean 2.5 and variance 1,
\(N(2.5,1)\), but are only interested in draws greater that \(a \ge 2\)
and less than \(b \le 4\). That is, we can only use
\(F(4)-F(2) = \Phi(4-2.5)-\Phi(2-2.5)\) = 0.9332 - 0.3085 = 0.6247
proportion of the draws. Figure \ref{fig:sampleani1} demonstrates that
some draws lie with the interval \((2,4)\) and some are outside.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Instead, one can draw according to the conditional distribution
\(F^{\star}\) defined as

\[
F^{\star}(x) = \Pr(X \le x | a < X \le b) =\frac{F(x)-F(a)}{F(b)-F(a)}, \ \  \ \text{for } a < x \le b .
\]

Using the inverse transform method in Section \ref{S:InverseTransform},
we have that the draw

\[
X^\star=F^{\star-1}\left( U \right) = F^{-1}\left(F(a)+U\cdot[F(b)-F(a)]\right)
\]

has distribution \(F^{\star}\). Expressed another way, define \[
\tilde{U} = (1-U)\cdot F(a)+U\cdot F(b)
\] and then use \(F^{-1}(\tilde{U})\). With this approach, each draw
counts.

This can be related to the importance sampling mechanism : we draw more
frequently in regions where we expect to have quantities that have some
interest. This transform can be considered as a ``a change of measure.''

In Example 6.4.1., the inverse of the normal distribution is readily
available (in \texttt{R}, the function is \texttt{qnorm}). However, for
other applications, this is not the case. Then, one simply uses
numerical methods to determine \(X^\star\) as the solution of the
equation \(F(X^\star) =\tilde{U}\) where
\(\tilde{U}=(1-U)\cdot F(a)+U\cdot F(b)\). See the following
illustrative code.

\section{Monte Carlo Markov Chain (MCMC)}\label{S:MCMC}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{This section is being written and is not yet complete nor
edited. It is here to give you a flavor of what will be in the final
version.}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The idea of Monte Carlo techniques rely on the law of large number (that
insures the convengence of the average towards the integral) and the
central limit theorem (that is used to quantify uncertainty in the
computations). Recall that if \((X_i)\) is an i.id. sequence of random
variables with distribution \(F\), then \[
\frac{1}{\sqrt{n}}\left(\sum_{i=1}^n h(X_i)-\int h(x)dF(x)\right)\overset{\mathcal{L}}{\rightarrow }\mathcal{N}(0,\sigma^2),\text{ as }n\rightarrow\infty.
\] for some variance \(\sigma^2>0\). But actually, the ergodic theorem
can be used to weaker the previous result, since it is not necessary to
have independence of the variables. More precisely, if \((X_i)\) is a
Markov Process with invariant measure \(\mu\), under some additional
technical assumptions, we can obtain that \[
\frac{1}{\sqrt{n}}\left(\sum_{i=1}^n h(X_i)-\int h(x)d\mu(x)\right)\overset{\mathcal{L}}{\rightarrow }\mathcal{N}(0,\sigma_\star^2),\text{ as }n\rightarrow\infty.
\] for some variance \(\sigma_\star^2>0\).

Hence, from this property, we can see that it is possible not
necessarily to generate independent values from \(F\), but to generate a
markov process with invariante measure \(F\), and to consider means over
the process (not necessarily independent).

Consider the case of a constraint Gaussian vector : we want to generate
random pairs from a random vector \(\boldsymbol{X}\), but we are
interested only on the case where the sum of the composants is large
enough, which can be written \(\boldsymbol{X}^T\boldsymbol{1}> m\) for
some real valued \(m\). Of course, it is possible to use the
\emph{accept-reject} algorithm, but we have seen that it might be quite
inefficient. One can use Hastings Metropolis and Gibbs sampler to
generate a Markov process with such an invariant measure.

\subsection{Hastings Metropolis}\label{hastings-metropolis}

The algorithm is rather simple to generate from \(f\) : we start with a
feasible value \(x_1\). Then, at step \(t\), we need to specify a
transition kernel : given \(x_t\), we need a conditional distribution
for \(X_{t+1}\) given \(x_t\). The algorithm will work well if that
conditional distribution can easily be simulated. Let \(\pi(\cdot|x_t)\)
denote that probability.

Draw a potential value \(x_{t+1}^\star\), and \(u\), from a uniform
distribution. Compute \[
R=  \frac{f(x_{t+1}^\star)}{f(x_t)}
\] and

\begin{itemize}
\tightlist
\item
  if \(u < r\), then set \(x_{t+1}=x_t^\star\)
\item
  if \(u\leq r\), then set \(x_{t+1}=x_t\)
\end{itemize}

Here \(r\) is called the \emph{acceptance}-ratio: we accept the new
value with probability \(r\) (or actually the smallest between \(1\) and
\(r\) since \(r\) can exceed \(1\)).

For instance, assume that \(f(\cdot|x_t)\) is uniform on
\([x_t-\varepsilon,x_t+\varepsilon]\) for some \(\varepsilon>0\), and
where \(f\) (our target distribution) is the \(\mathcal{N}(0,1)\). We
will never \emph{draw} from \(f\), but we will use it to compute our
acceptance ratio at each step.

In the code above, \texttt{vec} contains values of
\(\boldsymbol{x}=(x_1,x_2,\cdots)\), \texttt{innov} is the innovation.

Now, if we use more simulations, we get

\subsection{Gibbs sampler}\label{gibbs-sampler}

Consider some vector \(\boldsymbol{X}=(X_1,\cdots,X_d)\) with
ind'ependent components, \(X_i\sim\mathcal{E}(\lambda_i)\). We sample to
sample from \(\boldsymbol{X}\) given
\(\boldsymbol{X}^T\boldsymbol{1}>s\) for some threshold \(s>0\).

\begin{itemize}
\tightlist
\item
  start with some starting point \(\boldsymbol{x}_0\) such that
\item
  pick up (randomly) \(i\in\{1,\cdots,d\}\)
\item
  \(X_i\) given \(X_i > s-\boldsymbol{x}_{(-i)}^T\boldsymbol{1}\) has an
  Exponential distribution \(\mathcal{E}(\lambda_i)\)
\item
  draw \(Y\sim \mathcal{E}(\lambda_i)\) and set
  \(x_i=y +(s-\boldsymbol{x}_{(-i)}^T\boldsymbol{1})_+\) until
  \(\boldsymbol{x}_{(-i)}^T\boldsymbol{1}+x_i>s\)
\end{itemize}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-210-1.pdf}

The construction of the sequence (MCMC algorithms are iterative) can be
visualized below

\section{Further Resources and
Contributors}\label{Simulation:further-reading-and-resources}

\begin{itemize}
\tightlist
\item
  Include historical references for jackknife (Quenouille, Tukey, Efron)
\item
  Here are some links to learn more about
  \href{https://freakonometrics.hypotheses.org/6470}{reproducibility and
  randomness} and how to go
  \href{https://freakonometrics.hypotheses.org/6638}{from a random
  generator to a sample function}.
\end{itemize}

\subsubsection*{Contributors}\label{contributors-5}
\addcontentsline{toc}{subsubsection}{Contributors}

\begin{itemize}
\tightlist
\item
  \textbf{Arthur Charpentier}, Université du Quebec á Montreal, and
  \textbf{Edward W. (Jed) Frees}, University of Wisconsin-Madison, are
  the principal authors of the initial version of this chapter. Email:
  \href{mailto:jfrees@bus.wisc.edu}{\nolinkurl{jfrees@bus.wisc.edu}}
  and/or
  \href{mailto:arthur.charpentier@gmail.com}{\nolinkurl{arthur.charpentier@gmail.com}}
  for chapter comments and suggested improvements.
\item
  Chapter reviewers include: Write Jed or Arthur; to add you name here.
\end{itemize}

\subsection{TS 6.A. Bootstrap Applications in Predictive
Modeling}\label{ts-6.a.-bootstrap-applications-in-predictive-modeling}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{This section is being written.}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\chapter{Premium Foundations}\label{C:PremiumFoundations}

\emph{Chapter Preview.} Setting prices for insurance products, premiums,
is an important task for actuaries and other data analysts. This chapter
introduces the foundations for pricing non-life products.

\section{Introduction to Ratemaking}\label{S:IntroductionRatemaking}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Describe expectations as a baseline method for determining insurance
  premiums
\item
  Analyze an accounting equation for relating premiums to losses,
  expenses and profits
\item
  Summarize a strategy for extending pricing to include heterogeneous
  risks and trends over time
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

This chapter explains how you can think about determining the
appropriate price for an insurance product. As described in Section
\ref{S:PredModApps}, one of the core actuarial functions is ratemaking,
where the analyst seeks to determine the right price for a risk.

As this is a core function, let us first take a step back to define
terms. A price is a quantity, usually of money, that is exchanged for a
good or service. In insurance, we typically use the word premium for the
amount of money charged for insurance protection against contingent
events. The amount of protection varies by risk being insured. For
example, in homeowners insurance the amount of insurance protection
depends on the value of the house. In life insurance, the amount of
protection depends on a policyholder's financial status (e.g.~income and
wealth) as well as a perceived need for financial security. So, it is
common to express insurance prices as a unit of the protection being
purchased, for example, an amount per thousands (e.g., of dollars or
Euros) for coverage of a home or benefit in the event of death. Because
they are expressed in standardized units, these prices/premiums are
known as rates.

To determine premiums, it is common in economics to consider the supply
and demand of a product. The demand is sensitive to price as well as the
existence of competiting firms and substitute products. The supply is a
function of the resources required for production. For the individual
firm, the price is set to meet some objective such as profit
maximization which is met by choosing the output level that balances
costs and revenues at the margins.

However, a peculiarity of insurance is that the costs of insurance
protection are not known at the sale of the contract. If the insured
contingent event, such as the loss of a house or life, does not occur,
then the contract costs are only administrative (to set up the contract)
and are relatively minor. If the insured event occurs, then the cost
includes not only administrative costs but also payment of the amount
insured and expenses to settle claims. So, the cost is random; when the
contract is written, by design neither the insurer nor the insured knows
the contract costs. Moreover, costs may not be revealed for months or
years. For example, a typical time to settlement in medical malpractice
is five years.

Because costs are unknown at the time of sale, insurance pricing differs
from common economic approaches. This chapter squarely addresses the
uncertain nature of costs by introducing traditional actuarial
approaches that determine prices as a function of insurance costs. As we
will see, this pricing approach is sufficient for some insurance markets
such as personal automobile or homeowners where the insurer has a
portfolio of many independent risks. However, there are other insurance
markets where actuarial prices only provide an input to general market
prices. To reinforce this distinction, actuarial cost-based premiums are
sometimes known as technical prices. From the perspective of economists,
corporate decisions such as pricing are to be evaluated with reference
to their impact on the firm's market value. This objective is more
comprehensive than the static notion of profit maximization. That is,
you can think of the value of the firm represents the capitalized value
of all future expected profits. Decisions impacting this value in turn
affect all groups having claims on the firm, including stockholders,
bondholders, policyowners (in the case of mutual companies), and so
forth.

For cost-based prices, it is helpful to think of a premium as revenue
source that provides for claim payments, contract expenses, and an
operating margin. We formalize this in an accounting equation

\begin{equation}
\small{
\text{Premium = Loss + Expense + UW Profit} .
}
\label{eq:AccountingEquation}
\end{equation}

Expenses can be split into those that vary by premium (such as sales
commissions) and those that do not (such as building costs and employee
salaries). The term \texttt{UW\ Profit} is a residual that stands for
underwriting profit. It may also include include a cost of capital (for
example, an annual dividend to company investors). Because fixed
expenses and costs of capital are difficult to interpret for individual
contracts, we think of the equation \eqref{eq:AccountingEquation}
relationship as holding over the sum of many contracts (a portfolio) and
work with it in \emph{aggregate}. Then, in Section \ref{S:AggRateMaking}
we use this approach to help us think about setting premiums, for
example by setting profit objectives. Specifically, Sections
\ref{S:PurePremium} and \ref{S:LossRatio} introduce two prevailing
methods used in practice for determining premiums, the pure premium and
the loss ratio methods.

The \texttt{Loss} in equation \eqref{eq:AccountingEquation} is random and
so, as a baseline, we use the \emph{expected costs} to determine rates.
There are several ways to motivate this perspective that we expand upon
in Section \ref{S:PricingPrinciples}. For now, we will suppose that the
insurer enters into many contracts with risks that are similar except,
by pure chance, in some cases the insured event occurs and in others it
does not. The insurer is obligated to pay the total amount of claims
payments for all contracts. If risks are similar, then all policyholders
should be responsible for the same amount which is the average claim
payment. So, from this perspective, it makes sense to look at the
average claim payment over many insureds. From probability theory,
specifically the law of large numbers, we know that the average of iid
risks is close to the expected amount, so we use the expectation as a
baseline pricing principle.

Nonetheless, by using expected losses, we essentially assume that the
uncertainty is non-existent. If the insurers sells enough independent
policies, this may be a reasonable approximation. However, there will be
other cases, such as a single contract issued to a large corporation to
insure all of its buildings against fire damage, where the use of only
an expectation for pricing is not sufficient. So, Section
\ref{S:PricingPrinciples} also summarizes alternative premium principles
that incorporate uncertainty into our pricing. Note that an emphasis of
this text is estimation of the entire distribution of losses so the
analyst is not restricted to working only with expectations.

The aggregate methods derived from equation \eqref{eq:AccountingEquation}
focus on collections of homogeneous risks that are similar except for
the occurrence of random losses. In statistical language that we have
introduced, this is a discussion about risks that have identical
distributions. Naturally, when examining risks that insurers work with,
there are many variations in the risks being insured including the
features of the contracts and the people being insured. Section
\ref{S:HeterogeneousRisks} extends pricing considerations to
heterogeneous collections of risks.

Section \ref{S:TrendDevelopment} introduces ideas of development and
trending. When developing rates, we want to use the most recent loss
experience because the goal is to develop rates that are forward
looking. However, at contract initiation, recent loss experience is
often not known; it may be several years until it is fully realized. So,
this section introduces concepts needed for incorporating recent loss
experience into our premium development. Development and trending of
experience is related to but also differs from the idea of experience
rating that suggests that experience reveals hidden information about
the insured and so should be incorporated in our forward thinking
viewpoint. Chapter \ref{C:Credibility} discusses this idea in more
detail.

The final section of this chapter introduces methods for selecting a
premium. This is done by comparing a premium rating method to losses
from a held-out portfolio and selecting the method that produces the
best match with the held-out data. For a typical insurance portfolio,
most policies produce zero losses, that is, do not have a claim. Because
the distribution of held-out losses is a combination of (a large number
of) zeros and continuous amounts, special techniques are useful. Section
\ref{S:GiniStatistic} introduces concepts of \emph{concentration curves}
and corresponding \emph{Gini statistics} to help in this selection.

The chapter also includes a technical supplement on government
regulation of insurance rates to keep our work grounded in applications.

\section{Aggregate Ratemaking Methods}\label{S:AggRateMaking}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Define a pure premium as a loss cost as well as in terms of frequency
  and severity
\item
  Calculate an indicated rate using pure premiums, expenses, and profit
  loadings
\item
  Define a loss ratio
\item
  Calculate an indicated rate change using loss ratios
\item
  Compare the pure premium and loss ratio methods for determining
  premiums
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

It is common to consider an aggregate portfolio of insurance experience.
Consistent with earlier notation, consider a collection of \emph{n}
contracts with losses \(X_1, \ldots, X_n\). In this section, we assume
that contracts have the same loss distribution, that is they form a
homogeneous portfolio, and so are iid. For motivation, you can think
about personal insurance such as auto or homeowners where insurers write
many contracts on risks that appear very similar. Further, the
assumption of identical distributions is not as limiting as you might
think. In Section \ref{S:ExposureToRisk} we will introduce the idea of
an exposure variable that allows us to rescale experience to make it
comparable. For example, by rescaling losses we will be able to treat
homeowner losses from a 100,000 house and a 200,000 house as coming from
the same distribution. For now, we simply assume that
\(X_1, \ldots, X_n\) are \emph{iid}.

\subsection{Pure Premium Method}\label{S:PurePremium}

If the number in the group, \emph{n}, is large, then the average
provides a good approximation of the expected loss \[
\small{
\mathrm{E}(X) \approx \frac{\sum_{i=1}^n X_i}{n} = \frac{\text{Loss}}{\text{Exposure}} = \text{Pure Premium}.
}
\] With this as motivation, we define the pure premium to be the sum of
losses divided by the exposure; it is also known as a loss cost. In the
case of homogeneous risks, all policies are treated the same and we can
use the number of policies \emph{n} for the exposure. In Section
\ref{S:ExposureToRisk} we extend the concept of exposure when policies
are not the same.

We can multiply and divide by the number of claims,
\texttt{claim\ count}, to get \[
\small{
\text{Pure Premium} = \frac{\text{claim count}}{\text{Exposure}} \times \frac{\text{Loss}}{\text{claim count}} = \text{frequency} \times \text{severity} .
}
\]

So, when premiums are determined using the pure premium method, we
either take the average loss (loss cost) or use the frequency severity
approach.

To get a bit closer to applications in practice, we now return to
equation \eqref{eq:AccountingEquation} that includes expenses. Equation
\eqref{eq:AccountingEquation} also refers to \texttt{UW\ Profit} for
underwriting profit. When rescaled by premiums, this is known as the
profit loading. Because claims are uncertain, the insurer must hold
capital to ensure that all claims are paid. Holding this extra capital
is a cost of doing business, investors in the company need to be
compensated for this, thus the extra loading.

We now decompose \texttt{Expenses} into those that vary by premium,
\texttt{Variable}, and those that do not, \texttt{Fixed} so that
\texttt{Expenses\ =\ Variable\ +\ Fixed}. Thinking of variable expenses
and profit as a fraction of premiums, we define

\[
\small{
V =  \frac{\text{Variable}}{\text{Premium}} ~~~ \text{and}~~~
Q = \frac{\text{UW Profit}}{\text{Premium}} ~.
}
\]

With these definitions and equation \eqref{eq:AccountingEquation}, we may
write \[
\small{
\begin{matrix}
\begin{array}{ll}
\text{Premium} &= \text{Losses + Fixed} + \text{Premium} \times \frac{\text{Variable + UW Profit}}{\text{Premium}}  \\
& = \text{Losses + Fixed} + \text{Premium} \times (V+Q) .
\end{array}
\end{matrix}
}
\] Solving for premiums yields

\begin{equation}
\small{
\text{Premium} = \frac{\text{Losses + Fixed}}{1-V-Q} .
}
\label{eq:PremiumEquation}
\end{equation}

Dividing by exposure, the rate can be calculated as

\[
\begin{matrix}
\begin{array}{ll}
\text{Rate} &= \frac{\text{Premium}}{\text{Exposure}} = \frac{\text{Losses/Exposure + Fixed/Exposure}}{1-V-Q} \\
&=   \frac{\text{Pure Premium + Fixed/Exposure}}{1-V-Q} ~.
\end{array}
\end{matrix}
\]

In words, this is

\[
\small{
\text{Rate} =\frac{\text{pure premium + fixed expense per exposure}}{\text{1 - variable expense factor - profit and contingencies factor}}  .
}
\]

\textbf{Example. CAS Exam 5, 2004, Number 13.} Determine the indicated
rate per exposure unit, given the following information:

\begin{itemize}
\tightlist
\item
  Frequency per exposure unit = 0.25
\item
  Severity = \$100
\item
  Fixed expense per exposure unit = \$10
\item
  Variable expense factor = 20\%
\item
  Profit and contingencies factor = 5\%
\end{itemize}

\textbf{Solution.} Under the pure premium method, the indicated rate is

\[
\begin{matrix}
\begin{array}{ll}
\text{Rate} &=  \frac{\text{pure premium + fixed expense per exposure}}{\text{1 - variable expense factor - profit and contingencies factor}}\\
&= \frac{\text{frequency} \times \text{severity} ~+~10}{1-0.20-0.05} = \frac{0.25 \times 100 +10}{1-0.20-0.05} = 46.67 .
\end{array}
\end{matrix}
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

From the example, note that the rates produced by the pure premium
method are commonly known as \emph{indicated rates}.

From our development, note also that the profit is associated with
underwriting aspect of the contract and not investments. Premiums are
typically paid at the beginning of a contract and insurers receive
investment income from holding this money. However, due in part to the
short-term nature of the contracts, investment income is typically
ignored in pricing. This builds a bit of conservatism into the process
that insurers welcome. It is probably most relevant in the very long
``tail'' lines such as workers' compensation and medical malpractice. In
these lines, it can sometimes take 20 years or even longer to settle
claims. But, these are also the most volatile lines; claim payments far
in the future are less extreme when viewed in discounted sense.

\subsection{Loss Ratio Method}\label{S:LossRatio}

The loss ratio is the ratio of the sum of losses to the premium

\[
\small{
\text{Loss Ratio} = \frac{\text{Loss}}{\text{Premium}} .
}
\]

When determining premiums, it is a bit counter-intuitive to emphasize
this ratio because the premium component is built into the denominator.
As we will see, the idea is that the loss ratio method develops rate
\textbf{changes} rather than rates; we can use rate changes to update
past experience to get a current rate. To do this, rate changes consist
of the ratio of the experience loss ratio to the target loss ratio. This
adjustment factor is then applied to current rates to get new indicated
rates.

To see how this works in a simple context, let us return to equation
\eqref{eq:AccountingEquation} but now ignore expenses to get
\(\small{\text{Premium = Losses  + UW Profit}}\). Dividing by premiums
yields

\[
\small{
\frac{\text{UW Profit}}{\text{Premium}} = 1 - LR = 1 - \frac{\text{Loss}}{\text{Premium}} .
}
\] Suppose that we have in mind a new ``target'' profit loading, say
\(Q_{target}\). Assuming that losses, exposure, and other things about
the contract stay the same, then to achieve the new target profit
loading we adjust the premium. Use the \emph{ICF} for the indicated
change factor that is defined through the expression

\[
\small{
\frac{\text{new UW Profit}}{\text{Premium}} = Q_{target} =  1 - \frac{\text{Loss}}{ICF \times \text{Premium}}.
}
\] Solving for \emph{ICF}, we get

\[
\small{
ICF =  \frac{\text{Loss}}{\text{Premium} \times (1-Q_{target})} = \frac{LR}{1-Q_{target}}.
}
\]

So, for example, if we have a current loss ratio = 85\% and a target
profit loading \(\small{Q_{target}=0.20}\), then
\(\small{ICF = 0.85/0.80 = 1.0625}\), meaning that we increase premiums
by 6.25\%.

Now let's see how this works with expenses in equation
\eqref{eq:AccountingEquation}. We can use the same development as in
Section \ref{S:PurePremium} and so start with equation
\eqref{eq:PremiumEquation}, solve for the profit loading to get

\[
\small{
Q = 1 - \frac{\text{Loss+Fixed}}{\text{Premium}} - V .
}
\] We interpret the quantity \texttt{Fixed\ /Premium\ +\ V} as the
``operating expense ratio.'' Now, fix the profit percentage \emph{Q} at
a target and adjust premiums through the ``indicated change factor''
\(ICF\) \[
\small{
Q_{target} = 1
-\frac{\text{Loss + Fixed}}{\text{Premium}\times ICF} - V .
}
\] Solving for \(ICF\) yields

\begin{equation}
\small{
ICF = \frac{\text{Loss + Fixed}}{\text{Premium} \times (1 - V - Q_{target})} .
}
\label{eq:IndicatedChangeFactor}
\end{equation}

\textbf{Example. Loss Ratio Indicated Change Factor.} Assume the
following information:

\begin{itemize}
\tightlist
\item
  Projected ultimate loss and LAE ratio = 65\%
\item
  Projected fixed expense ratio = 6.5\%
\item
  Variable expense = 25\%
\item
  Target UW profit = 10\%
\end{itemize}

With these assumptions, with equation \eqref{eq:IndicatedChangeFactor},
the indicated change factor can be calculated as \[
\small{
ICF = \frac{\text{(Losses + Fixed)}/\text{Premium}}{ 1 - V - Q_{target}} = \frac{0.65 + 0.065}{1- 0.25 - 0.10} = 1.10 .
}
\]

This means that overall average rate level should be increased by 10\%.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

We later provide a comparison of the pure premium and loss ratio methods
in Section \ref{S:CompareMethods}. As inputs, this section will require
discussions of trended exposures and \emph{on-level} premiums defined in
Section \ref{S:TrendDevelopment}.

\section{Pricing Principles}\label{S:PricingPrinciples}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Describe common actuarial pricing principles
\item
  Describe properties of pricing principles
\item
  Choose a pricing principle based on a desired property
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Approaches to pricing vary by the type of contract. For example,
personal automobile is common (it is known as part of the retail general
insurance market in the United Kingdom). Here, one can expect to do
pricing based on a large pool of independent contracts, a situation in
which expectations of losses provide an excellent starting point. In
contrast, an actuary may wish to price an insurance contract issued to a
large employer that covers complex health benefits for thousands of
employees. In this example, knowledge of the entire distribution of
potential losses, not just the expected value, is critical for starting
the pricing negotiations. To cover a range of potential applications,
this section describes general premium principles and their properties
that one can use to decide whether or not a specific principle is
applicable in a given situation.

\subsection{Premium Principles}\label{premium-principles}

This chapter introduces traditional actuarial pricing principles that
provide a price based only on the insurance loss distribution; the price
does not depend on the demand for insurance or other aspects of the
costs such as expenses. Assume that the loss \(X\) has a distribution
function \(F(\cdot)\) and that there exists some functional \(H\) that
takes \(F(\cdot)\) into the positive real line, denoted as \(P = H(F)\).
For notation purposes, it is often convenient to substitute the random
variable \(X\) for its distribution function and write \(P = H(X)\).
Table \ref{tab:PremPrinciples} provides several examples.

\begin{longtable}[]{@{}lc@{}}
\caption{\label{tab:PremPrinciples} Common Premium
Principles}\tabularnewline
\toprule
Description & Definition (\(H(X)\))\tabularnewline
\midrule
\endfirsthead
\toprule
Description & Definition (\(H(X)\))\tabularnewline
\midrule
\endhead
Net (pure) premium & \(E[X]\)\tabularnewline
Expected value & \((1+\alpha)E[X]\)\tabularnewline
Standard deviation & \(E[X]+\alpha ~SD(X)\)\tabularnewline
Variance & \(E[X]+\alpha ~Var(X)\)\tabularnewline
Zero utility & solution of \(u(w) = E ~u(w + P - X)\)\tabularnewline
Exponential & \(\frac{1}{\alpha} \ln E ~e^{\alpha X}\)\tabularnewline
\bottomrule
\end{longtable}

A premium principle is similar to a risk measure that is introduced in
Section \ref{S:RiskMeasure}. Mathematically, both are functions that
maps the loss rv of interest to a numerical value. From a practical
viewpoint, a premium principle provides a guide as to how much an
insurer will charge for accepting a risk \(X\). In contrast, a risk
measure quantifies the level of uncertainty, or riskiness, that an
insurer can use to decide on a capital level to be assured of remaining
solvent.

As noted above, the net, or pure, premium essentially assumes no
uncertainty. The expected value, standard deviation, and variance
principles each add an explicit loading for riskiness through parameter
\(\alpha \ge 0\). For the principle of zero utility, we think of an
insurer with utility function \(u(\cdot)\) and wealth \emph{w} as being
indifferent to accepting and not accepting risk \(X\). In this case,
\(P\) is known as an indifference price or, in economics, a reservation
price. With exponential utilility, the principle of zero utility reduces
to the exponential premium principle, that is, assuming
\(u(x) = (1-e^{-\alpha x})/\alpha\).

For small values of risk parameters, the variance principle is
approximately equal to exponential premium principle, as illustrated in
the following special case.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Special Case: Gamma Distribution}. Consider a loss that is gamma
distributed with parameters \(\alpha\) and \(\theta\). From the Appendix
D in \ref{C:SummaryDistributions}, the mean is \(\alpha \theta\) and the
variance is \(\alpha \theta^2\). Using \(\alpha_{Var}\) for the risk
parameter, the variance premium is
\(H_{Var}(X) = \alpha \theta+\alpha_{Var} ~(\alpha \theta^2)\). From
this appendix, it is straightforward to derive the well-known moment
generating function, \(M(t) = E e^{tX} = (1-t\theta)^{-\alpha}\). With
this and a risk parameter \(\alpha_{Exp}\), we may express the
exponential premium as

\[
H_{Exp}(X) = \frac{-\alpha}{\alpha_{Exp}} \ln\left(1-\alpha_{Exp} \theta\right).
\] To see the relationship between \(H_{Var}(X)\) and \(H_{Var}(X)\), we
choose \(\alpha_{Exp} = 2 \alpha_{Var}\). With an approximation from
calculus (\(\ln(1-x) = -x - x^2/2 - x^3/3 - \cdots\)), we write

\[
H_{Exp}(X) = \frac{-\alpha}{\alpha_{Exp}} \ln\left(1-\alpha_{Exp} \theta\right) 
= \frac{-\alpha}{\alpha_{Exp}} \left\{ -\alpha_{Exp} \theta -(\alpha_{Exp} \theta)^2/2 - \cdots\right\} \\
\approx \alpha \theta + \frac{\alpha_{Exp}}{2}(\alpha \theta^2 ) 
= H_{Var}(X). 
\]

\subsection{Properties of Premium
Principles}\label{properties-of-premium-principles}

Properties of premium principles help guide the selection of a premium
principle in applications. Table \ref{tab:Properties} provides examples
of properties of premium principles.

\begin{longtable}[]{@{}lc@{}}
\caption{\label{tab:Properties} Common Properties of Premium
Principles}\tabularnewline
\toprule
Description & Definition\tabularnewline
\midrule
\endfirsthead
\toprule
Description & Definition\tabularnewline
\midrule
\endhead
Nonnegative loading & \(H(X) \ge E[X]\)\tabularnewline
Additivity & \(H(X_1+X_2) = H(X_1) + H(X_2)\), for independent
\(X_1, X_2\)\tabularnewline
Scale invariance & \(H(cX) = c H(X)\), for \(c \ge 0\)\tabularnewline
Consistency & \(H(c+X) = c + H(X)\)\tabularnewline
No rip-off & \(H(X) \le max ~range ~\{X\}\)\tabularnewline
\bottomrule
\end{longtable}

This is simply a subset of the many properties quoted in the actuarial
literature. For example, the review paper of \citet{young2014premium}
lists 15 properties. See also the properties described as \emph{coherent
axioms} that we introduce for risk measures in Section
\ref{S:RiskMeasure}.

Some of the properties listed in Table \ref{tab:Properties} are mild in
the sense that they will nearly always be satisfied. For example, the
\emph{no rip-off} property indicates that the premium charge will be
smaller than the maximal value of the loss \(X\). Other properties may
not be so mild. For example, for a portfolio of independent risks, the
actuary may want the \emph{additivity} property to hold. It is easy to
see that this property holds for the expected value, variance, and
exponential premium principles but not for the standard deviation
principle. Another example is the \emph{consistency} property that does
not hold for the expected value principle when the risk loading
parameter \(\alpha\) is positive.

The \emph{scale invariance} principle is known as \emph{homogeneity of
degree one} in economics. It allows, for example, us to work in
different currencies (e.g., from dollars to Euros) as well as a host of
other applications and will be discussed further in the following
Section \ref{S:HeterogeneousRisks}. Although a generally accepted
principle, we note that this principle does not hold for a large values
of \(X\) that may border on a surplus constraint of an insurer; if an
insurer has a large probability of becoming insolvent, then that insurer
may not wish to use linear pricing. It is easy to check that this
principle holds for the expected value and standard deviation
principles, although not for the variance and exponential principles.

\section{Heterogeneous Risks}\label{S:HeterogeneousRisks}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Describe insurance exposures in terms of scale distributions
\item
  Explain an exposure in terms of common types of insurance such as auto
  and homeowners insurance
\item
  Describe how rating factors can be used to account for the
  heterogeneity among risks in a collection
\item
  Measure the impact of a rating factor through relativities
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

As noted in Section \ref{S:IntroductionRatemaking}, there are many
variations in the risks being insured, the features of the contracts,
and the people being insured. As an example, you might have a twin
brother or sister who works in the same town and earns a roughly similar
amount of money. Still, when it comes to selecting choices in rental
insurance to insure contents of your apartment, you can imagine
differences in the amount of contents to be insured, choices of
deductibles for the amount of risk retained, and perhaps different
levels of uncertainty given the relative safety of your neighborhoods.
People, and risks that they insure, are different.

When thinking about a collection of different, or heterogeneous, risks,
one option is to price all risks the same. This is common, for example,
in government sponsored programs for flood or health insurance. However,
it is also common to have different prices where the differences are
commensurate with the risk being insured.

\subsection{Exposure to Risk}\label{S:ExposureToRisk}

One easy way to make heterogeneous risks comparable is through the
concept of an exposure. To explain exposures, let us use \emph{scale
distributions} that we learned about in Chapter \ref{C:Severity}. To
recall a scale distribution, suppose that \(X\) has a parametric
distribution and define a rescaled version \(R = X/E\), \(E > 0\). If
\(R\) is in the same parametric family as \(X\), then the distribution
is said to be a scale distribution. As we have seen, the gamma,
exponential, and Pareto distributions are examples of scale
distributions.

Intuitively, the idea behind exposures is to make risks more comparable
to one another. For example, it may be that risks \(X_1, \ldots, X_n\)
come from different distributions and yet, with the choice of the right
exposures, the rates \(R_1, \ldots, R_n\) come from the same
distribution. Here, we interpret the rate \(R_i = X_i/E_i\) to be the
loss divided by exposure.

\protect\hyperlink{tab:7.3}{Table 7.3} provides a few examples. We
remark that this table refers to ``earned'' car and house years,
concepts that will be explained in Section \ref{S:TrendDevelopment}.

\[
\small{
\begin{matrix}
\begin{array}{ll}
\text{Type of Insurance} & \text{Exposure Basis} \\\hline
\text{Personal Automobile} &  \text{Earned Car Year, Amount of Insurance Coverage} \\
\text{Homeowners} &  \text{Earned House Year, Amount of Insurance Coverage}\\
\text{Workers Compensation}  & \text{Payroll}\\
\text{Commercial General Liability} &  \text{Sales Revenue, Payroll, Square Footage, Number of Units}\\
\text{Commercial Business Property}  & \text{Amount of Insurance Coverage}\\
\text{Physician's Professional Liability}  & \text{Number of Physician Years}\\
\text{Professional Liability}  & \text{Number of Professionals (e.g., Lawyers or Accountants)}\\
\text{Personal Articles Floater} &  \text{Value of Item} \\
  \hline
\end{array}
\end{matrix}
}
\]

\protect\hyperlink{tab:7.3}{Table 7.3} : Commonly used Exposures in
Different Types of Insurance

An exposure is a type of rating factor, a concept that we define
explicitly in the next Section \ref{S:RatingFactors}. It is typically
the most important rating factor, so important that both premiums and
losses are quoted on a ``per exposure'' basis.

For frequency and severity modeling, it is customary to think about the
frequency aspect as proportional to exposure and the severity aspect in
terms of loss per claim (not dependent upon exposure). However, this
does not cover the entire story. For many lines of business, it is
convenient for exposures to be proportional to inflation. Inflation is
typically viewed as unrelated to frequency but proportional to severity.

\subsubsection*{Criteria for Choosing an
Exposure}\label{criteria-for-choosing-an-exposure}
\addcontentsline{toc}{subsubsection}{Criteria for Choosing an Exposure}

An exposure base should meet the following criteria. It should:

\begin{itemize}
\tightlist
\item
  be an accurate measure of the quantitative exposure to loss
\item
  be easy for the insurer to determine (at the time the policy is
  calculated) and not subject to manipulation by the insured,
\item
  be easy to understand by the insured and to calculate by the insurer,
\item
  consider any preexisting exposure base established within the
  industry, and
\item
  for some lines of business, it is proportional to inflation. In this
  way, rates are not sensitive to the changing value of money over time
  as these changes are captured in exposure base.
\end{itemize}

To illustrate, consider personal automobile coverage. Instead of the
exposure basis ``earned car year,'' a more accurate measure of the
quantitative exposure to loss might be number of miles driven. However,
this measure is difficult to determine at the time the policy is issued
and subject to potential manipulation by the insured.

As another example, the exposure measure in commercial business
property, e.g.~fire insurance, is typically the amount of insurance
coverage. As property values grow with inflation, so will the amount of
insurance coverage. Thus, rates quoted on a per amount of insurance
coverage are less sensitive to inflation than otherwise.

\subsection{Rating Factors}\label{S:RatingFactors}

A rating factor, or rating variable, is simply a characteristic of the
policyholder or risk being insured by which rates vary. For example,
when you purchase auto insurance, it is likely that the insurer has
rates that differ by age, gender, type of car and where it is garaged,
accident history, and so forth. These variables are known as rating
factors. Although some variables may be continuous, such as age, most
are categorical - factor is a label that is used for categorical
variables. In fact, even with continuous variables such as age, it is
common to categorize them by creating groups such as ``young,''
``intermediate,'' and ``old'' for rating purposes.

\protect\hyperlink{tab:7.4}{Table 7.4} provides just a few examples. In
many jurisdictions, the personal insurance market (e.g., auto and
homeowners) is very competitive - using 10 or 20 variables for rating
purposes is not uncommon.

\[
\small{
\begin{matrix}
\begin{array}{l|l}\hline
\text{Type of Insurance} & \text{Rating Factors}\\\hline\hline
\text{Personal Automobile}  & \text{Driver Age and Gender, Model Year, Accident History}\\
\text{Homeowners}  & \text{Amount of Insurance, Age of Home, Construction Type}\\
\text{Workers Compensation}  & \text{Occupation Class Code}\\
\text{Commercial General Liability}  & \text{Classification, Territory, Limit of Liability}\\
\text{Medical Malpractice}  & \text{Specialty, Territory, Limit of Liability}\\
\text{Commercial Automobile}  & \text{Driver Class, Territory, Limit of Liability}\\
  \hline
\end{array}
\end{matrix}
}
\] \protect\hyperlink{tab:7.4}{Table 7.4} : Commonly used Rating Factors
in Different Types of Insurance

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example. Losses and Premium by Amount of Insurance and
Territory.} To illustrate, \protect\hyperlink{tab:7.5}{Table 7.5}
presents a small fictitious data set from \citet{werner2016basic}. The
data consists of loss and loss adjustment expenses (\emph{LossLAE}),
decomposed by three levels of \emph{AOI}, amount of insurance, and three
territories (\emph{Terr}). For each combination of \emph{AOI} and
\emph{Terr}, we also have available the number of policies issued, given
as the \emph{Exposure}.

\[
\small{
\begin{matrix}
\begin{array}{cc|rrr}
\hline
       AOI &       Terr &   Exposure &    LossLAE &    Premium \\\hline
       \text{Low} &          1 &          7 &     210.93 &     335.99 \\
    \text{Medium} &          1 &        108 &   4,458.05 &   6,479.87 \\
      \text{High} &          1 &        179 &  10,565.98 &  14,498.71 \\\hline
       \text{Low} &          2 &        130 &   6,206.12 &  10,399.79 \\
    \text{Medium} &          2 &        126 &   8,239.95 &  12,599.75 \\
      \text{High} &          2 &        129 &  12,063.68 &  17,414.65 \\\hline
       \text{Low} &          3 &        143 &   8,441.25 &  14,871.70 \\
    \text{Medium} &          3 &        126 &  10,188.70 &  16,379.68 \\
      \text{High} &          3 &         40 &   4,625.34 &   7,019.86 \\
      \hline
       \text{Total}    &       & 988 &  65,000.00 &     99,664.01   \\\hline
\hline
\end{array}
\end{matrix}
}
\] \protect\hyperlink{tab:7.5}{Table 7.5} : Losses and Premium by Amount
of Insurance and Territory

In this case, the rating factors \emph{AOI} and \emph{Terr} produce nine
cells. Note that one might combine the cell ``territory one with a low
amount of insurance''" with another cell because there are only 7
policies in that cell. Doing so is perfectly acceptable - considerations
of this sort is one of the main jobs of the analyst. An outline on
selecting variables is in Chapter \ref{C:RiskClass}, including Technical
Supplement TS \textbf{8.B}. Alternatively, you can also think about
reinforcing information about the cell (\emph{Terr} 1, Low \emph{AOI})
by ``borrowing'' information from neighboring cells (e.g., other
territories with the same \emph{AOI}, or other amounts of \emph{AOI}
within \emph{Terr} 1). This is the subject of credibility that is
introduced in Chapter \ref{C:Credibility}.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

To understand the impact of rating factors, it is common to use
relativities. A relativity is the difference of the expected risk
between a specific level of a rating factor and an accepted baseline
value. In this book, we work with relativities defined through ratios;
it is also possible to define relativities through arithmetic
differences. Thus, our relativity is defined as

\[
\text{Relativity}_j = \frac{\text{(Loss/Exposure)}_j}{\text{(Loss/Exposure)}_{Base}} .
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example. Losses and Premium by Amount of Insurance and Territory
- Continued.} Traditional classification methods consider only one
classification variable at a time - they are univariate. Thus, if we
wanted relativities losses and expenses (\emph{LossLAE}) by amount of
insurance, we might sum over territories to get the information
displayed in \protect\hyperlink{tab:7.6}{Table 7.6}.

\[
\small{
\begin{matrix}
\begin{array}{c|rrrr}
\hline
       AOI &   Exposure &    LossLAE & Loss/Exp &Relativity \\\hline
       \text{Low} &        280 &    14858.3 &   53.065   &0.835 \\
    \text{Medium} &        360 &    22886.7 &    63.574  &1.000 \\
      \text{High} &        348 &      27255.0 &   78.319  & 1.232 \\\hline
       \text{Total}    &        988 &  65,000.0 &            \\\hline
\hline
\end{array}
\end{matrix}
}
\]

\protect\hyperlink{tab:7.6}{Table 7.6} : Losses and Relativities by
Amount of Insurance

Thus, for example, losses and expenses per unit of exposure are 23.2\%
higher for risks with a high amount of insurance compared to those with
a medium amount. These relativities do not control for territory.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The introduction of rating factors allows the analyst to create cells
that define small collections of risks -- the goal is to choose the
right combination of rating factors so that all risks within a cell may
be treated the same. In statistical terminology, we want all risks
within a cell to have the same distribution (subject to rescaling by an
exposure variable). This is the foundation of insurance pricing. All
risks within a cell have the same price yet risks from different cells
may have different prices.

Said another way, insurers are allowed to charge different rates for
different risks; discrimination of risks is legal and routinely done.
Nonetheless, the basis of discrimination, the choice of risk factors, is
the subject of extensive debate. The actuarial community, insurance
management, regulators, and consumer advocates are all active
participants in this debate. Technical Supplement TS \textbf{7.A}
describes these issues from a regulatory perspective.

In addition to statistical criteria for assessing the significance of a
rating factor, analysts much pay attention to business concerns of the
company (e.g., is it expensive to implement a rating factor?), social
criteria (is a variable under the control of a policyholder?), legal
criteria (are there regulations that prohibit the use of a rating factor
such as gender?), and other societal issues. These questions are largely
beyond the scope of this text. Nonetheless, because they are so
fundamental to pricing of insurance, a brief overview is given in
Chapter \ref{C:RiskClass}, including Technical Supplement TS
\textbf{8.B.}

\section{Development and Trending}\label{S:TrendDevelopment}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Define and calculate different types of exposure and premium summary
  measures that appear in financial reports
\item
  Describe the development of a claim over several payments and link
  that to various unpaid claim measures, including those incurred but
  not reported (IBNR) as well as case reserves
\item
  Compare and contrast relative strengths and weaknesses of the pure
  premium and loss ratio methods for ratemaking
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

As we have seen in Section \ref{S:AggRateMaking}, insurers consider
aggregate information for ratemaking such as exposures to risk,
premiums, expenses, claims, and payments. This aggregate information is
also useful for managing an insurers' activities; financial reports are
commonly created at least annually and oftentimes quarterly. At any
given financial reporting date, information about recent policies and
claims will be ongoing and necessarily incomplete; this section
introduces concepts for projecting risk information so that it is usful
for ratemaking purposes. Information about the risks, such as exposures,
premium, claim counts, losses, and rating factors, is typically
organized into three databases:

\begin{itemize}
\tightlist
\item
  \emph{policy database} - contains information about the risk being
  insured, the policyholder, and the contract provisions
\item
  \emph{claims database} - contains information about each claim; these
  are linked to the policy database.
\item
  \emph{payment database} - contains information on each claims
  transaction, typically payments but may also changes to \emph{case
  reserves}. These are linked to the claims database.
\end{itemize}

With these detailed databases, it is straightforward (in principle) to
sum up policy level detail to aggregate information needed for financial
reports. This section describes various summary measures commonly used.

\subsection{Exposures and Premiums}\label{exposures-and-premiums}

A financial reporting period is a length of time that is fixed in the
calendar; we use January 1 to December 31 for the examples in this book
although other reporting periods are also common. The reporting period
is fixed but policies may begin at any time during the year. Even if all
policies have a common contract length of (say) one year, because of the
differing starting time, they can end at any time during the financial
reporting. Figure \ref{fig:Exposures} presents four illustrative
policies. Because of these differing starting and end times, there needs
to be some standards as to what types of measures are most useful for
summarizing experience in a given reporting period.



\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{LossDataAnalytics_files/figure-latex/Exposures-1} 

}

\caption{Timeline of Exposures for Four 12-Month Policies.}\label{fig:Exposures}
\end{figure}

Some commonly used exposure measures are:

\begin{itemize}
\tightlist
\item
  written exposures, the amount of exposures on policies issued
  (underwritten or written) during the period in question,
\item
  earned exposures, the exposure units actually exposed to loss during
  the period, that is, where coverage has already been provided
\item
  unearned exposures, represent the portion of the written exposures for
  which coverage has not yet been provided as of that point in time, and
\item
  in force exposures, exposure units exposed to loss at a given point in
  time.
\end{itemize}

\protect\hyperlink{tab:7.12}{Table 7.12} gives detailed illustrative
calculations for the four illustrative policies.

\[
\small{
\begin{matrix}
\begin{array}{cl|cc|cc|cc|c}
  \hline
&  & & & & & &&\text{In-Force} \\
&\text{Effective} & \text{Written}& \text{Exposure} & \text{Earned} &\text{Exposure}& \text{Unearned} &\text{Exposure}&  \text{Exposure} \\
{Policy} &\text{Date}         & 2019 & 2020 & 2019 & 2020 & 2019 & 2020 & \text{1 Jan 2020} \\   \hline
\text{A}&\text{1 Jan 2019}   & 1.00 & 0.00 & 1.00 & 0.00& 0.00 & 0.00 & 0.00 \\
\text{B}&\text{1 April 2019} & 1.00 & 0.00 & 0.75 & 0.25 & 0.25 & 0.00& 1.00 \\
\text{C}&\text{1 July 2019}  & 1.00 & 0.00 & 0.50 & 0.50 & 0.50 & 0.00& 1.00 \\
\text{D}&\text{1 Oct 2019}   & 1.00 & 0.00 & 0.25 & 0.75 & 0.75 & 0.00& 1.00 \\ \hline
 &  Total       & 4.00 & 0.00 & 2.50 & 1.50 & 1.50 & 0.00 & 3.00 \\
  \hline
  \hline
\end{array}
\end{matrix}
}
\] \protect\hyperlink{tab:7.12}{Table 7.12}: Exposures for Four 12-Month
Policies

This summarization is sometimes known as the calendar year method of
aggregation to serve as a contrast to the policy year method. In the
latter method, all policies start at the beginning of the year. This
method is useful for ratemaking methods based on individual contracts
and we do not pursue this further here.

In the same way as exposures, one can summarizes premiums. Premiums,
like exposures, can be either \emph{written}, \emph{earned},
\emph{unearned}, or \emph{in force}. Consider the following example.

\textbf{Example. 7.5.1. CAS Exam 5, 2003, Number 10.} A 12-month policy
is written on March 1, 2002 for a premium of \$900. As of December 31,
2002, which of the following is true?

\[
\small{
\begin{matrix}
\begin{array}{l|ccc}
  \hline
& \text{Calendar Year} & \text{Calendar Year} \\
& \text{2002 Written} & \text{2002 Earned} & \text{Inforce} \\
& \text{Premium} & \text{Premium}  & \text{Premium}  \\\hline
A. & 900 & 900 & 900 \\
B. & 750 & 750 & 900 \\
C. & 900 & 750 & 750 \\
D. & 750 & 750 & 750 \\
E. & 900 & 750 & 900 \\\hline
\end{array}
\end{matrix}
}
\]

Only earned premium differs from written premium and inforce premium and
therefore needs to be computed. Thus, earned premium at Dec 31, 2002,
equals \(\$900 \times 10/12 = \$750\). Answer E.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Losses, Claims, and
Payments}\label{losses-claims-and-payments}

Broadly speaking, the terms loss and claim refer to the amount of
compensation paid or potentially payable to the claimant under the terms
of the insurance policy. Definitions can vary:

\begin{itemize}
\tightlist
\item
  Sometimes, the term \emph{claim} is used interchangeably with the term
  \emph{loss}.
\item
  In some insurance and actuarial sources, the term \emph{loss} is used
  for the amount of damage sustained in an insured event. The
  \emph{claim} is the amount paid by the insurer with differences
  typically due to deductibles, upper policy limits, and the like.
\item
  In economic usages, a \emph{claim} is a demand for payment by an
  insured or by an injured third-party under the terms and conditions of
  insurance contract and the \emph{loss} is the amount paid by the
  insurer.
\end{itemize}

This text will follow the second bullet. However, when reading other
sources, you will need to take care when thinking about definitions for
the terms loss and claim.

To establish additional terminology, it is helpful to follow the
timeline of a claim as it develops. In Figure
\ref{fig:ClaimDevelopment}, the claim occurs at time \(t_1\) and the
insuring company is notified at time \(t_3\). There can be a long gap
between occurrence and notification such that the end of a company
financial reporting period, known as a valuation date, occurs (\(t_2\)).
In this case, the claim is said to be incurred but not reported at this
valuation date.

After claim notification, there may one or more loss payments. Not all
of the payments may be made by the next valuation date (\(t_4\)). As the
claim develops, eventually the company deems its financial obligations
on the claim to be resolved and declares the claim closed. However, it
is possible that new facts arise and the claim must be re-opened, giving
rise to additional loss payments prior to being closed again.



\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{LossDataAnalytics_files/figure-latex/ClaimDevelopment-1} 

}

\caption{Timeline of Claim Development.}\label{fig:ClaimDevelopment}
\end{figure}

\begin{itemize}
\tightlist
\item
  Accident date - the date of the occurrence which gave rise to the
  claim. This is also known as the \emph{date of loss} or the
  \emph{occurrence date}.
\item
  Report date - the date the insurer receives notice of the claim.
  Claims not currently known by the insurer are referred to as
  unreported claims or \emph{incurred but not reported (IBNR) claims}.
\end{itemize}

Until the claim is settled, the reported claim is considered an open
claim. Once the claim is settled, it is categorized as a \emph{closed
claim}. In some instances, further activity may occur after the claim is
closed, and the claim may be re-opened.

Recall that a loss is the amount paid or payable to claimants under the
terms of insurance policies. Further, we have

\begin{itemize}
\tightlist
\item
  \emph{Paid losses} are those losses for a particular period that have
  actually been paid to claimants.
\item
  Where there is an expectation that payment will be made in the future,
  a claim will have an associated \emph{case reserve} representing the
  estimated amount of that payment.
\item
  \emph{Reported Losses}, also known as \emph{case incurred}, is Paid
  Losses + Case Reserves
\end{itemize}

The \emph{ultimate loss} is the amount of money required to close and
settle all claims for a defined group of policies.

\subsection{Comparing Pure Premium and Loss Ratio
Methods}\label{S:CompareMethods}

Now that we have learned how exposures, premiums, and claims develop
over time, we can consider how they can be used for ratemaking. We have
seen that insurers offer many different types of policies that cover
different policyholders and amounts of risks. This aggregation is
sometimes loosely referred to as the mix of business. Importantly, the
mix changes over time as policyholders come and go, amounts of risks
change, and so forth. The exposures, premiums, and types of risks from a
prior financial reporting may not be representative of the period that
rates are being developed for. The process of extrapolating exposures,
premiums, and risk types is known as \textbf{trending}. For example, an
on-level earned premium is that earned premium that would have resulted
for the experience period had the current rates been in effect for the
entire period. Most trending methods used in practice are mathematically
straight-forward although they can become complicated given contractual
and administrative complexities. We refer the reader to standard
references that describe approaches in detail such as
\citet{werner2016basic} and \citet{friedland2013fundamentals}.

\subsubsection*{Loss Ratio Method}\label{loss-ratio-method}
\addcontentsline{toc}{subsubsection}{Loss Ratio Method}

The expression for the loss ratio method indicated change factor in
equation \eqref{eq:IndicatedChangeFactor} assumes a certain amount of
consistency in the portfolio experience over time. For another approach,
we can define the experience loss ratio to be: \[
\small{
LR_{experience} = \frac{\text{experience losses}}{\text{experience period earned exposure}\times \text{current rate}}  .
}
\]

Here, we think of the experience period earned exposure \(\times\)
current rate as the experience premium.

Using equation \eqref{eq:PremiumEquation}, we can write a loss ratio as \[
\small{
LR = \frac{\text{Losses}}{\text{Premium}}=\frac{1-V-Q}{\text{(Losses + Fixed)}/\text{Losses}}=\frac{1-V-Q}{1+G} ~,
}
\] where \(G = \text{Fixed} / \text{Losses}\), the ratio of fixed
expenses to premiums. With this expression, we define the \emph{target
loss ratio} \[
\small{
LR_{target} =
\frac{1-V-Q}{1+G} = \frac{1-\text{premium related expense factor - profit and contingencies factor}}
{1+\text{ratio of non-premium related expenses to losses}}  .
}
\]

With these, the indicated change factor is

\begin{equation}
\small{
ICF =\frac{LR_{experience}}{LR_{target}}.
}
\label{eq:RevisedIndicatedChangeFactor}
\end{equation}

Comparing equation \eqref{eq:IndicatedChangeFactor} to
\eqref{eq:RevisedIndicatedChangeFactor}, we see that the latter offers
more flexibility to explicitly incorporate trended experience. As the
loss ratio method is based on rate changes, this flexibility is
certainly warranted.

\subsubsection*{Comparison of Methods}\label{comparison-of-methods}
\addcontentsline{toc}{subsubsection}{Comparison of Methods}

Assuming that exposures, premiums, and claims have been trended to be
representative of a period that rates are being developed for, we are
now in a position to compare the pure premium and loss ratio methods for
ratemaking. We start with the observation that for the same data inputs,
these two approaches produce the same results. That is, they are
algebraically equivalent. However, the rely on different inputs:

\[
\small{
\begin{array}{l|l}\hline
\text{Pure Premium Method} & \text{Loss Ratio Method} \\ \hline
\text{Based on exposures} & \text{Based on premiums} \\
\text{Does not require existing rates} & \text{Requires existing rates} \\
\text{Does not use on-level premiums} & \text{Uses on-level premiums} \\
\text{Produces indicated rates} & \text{Produces indicated rate changes} \\
  \hline
\end{array}
}
\]

Comparing the pure premium and loss ratio methods, we note that:

\begin{itemize}
\tightlist
\item
  The pure premium method requires well-defined, responsive exposures.
\item
  The loss ratio method cannot be used for new business because it
  produces indicated rate changes.
\item
  The pure premium method is preferable where on-level premium is
  difficult to calculate. In some instances, such as commercial lines
  where individual risk rating adjustments are made to individual
  policies, it is difficult to determine the on-level earned premium
  required for the loss ratio method.
\end{itemize}

In many developed countries like the US where lines of business have
been in existence, the loss ratio approach is more popular.

\textbf{Example. 7.5.2. CAS Exam 5, 2006, Number 36.} You are given the
following information:

\begin{itemize}
\tightlist
\item
  Experience period on-level earned premium = \$500,000
\item
  Experience period trended and developed losses = \$300,000
\item
  Experience period earned exposure = 10,000
\item
  Premium-related expenses factor = 23\%
\item
  Non-premium related expenses = \$21,000
\item
  Profit and contingency factor = 5\%
\end{itemize}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Calculate the indicated rate level change using the loss ratio method.
\item
  Calculate the indicated rate level change using the pure premium
  method.
\item
  Describe one situation in which it is preferable to use the loss ratio
  method, and one situation in which it is preferable to use the pure
  premium method.
\item
  We will calculate the experience and target loss ratios, then take the
  ratio to get the indicated rate change. The experience loss ratio is
  \[
  \small{
  LR_{experience} =  \frac{\text{experience losses}}{\text{experience period premium}} =\frac{300000}{500000} = 0.60.
  }
  \] The target loss ratio is:
\end{enumerate}

\[
\small{
\begin{matrix}
\begin{array}{ll}
LR_{target}
&= \frac{1-V-Q}{1+G} = \frac{1-\text{premium related expense factor - profit and contingencies factor}}
{1+\text{ratio of non-premium related expenses to losses}}\\
&= \frac{1-0.23 - 0.05}{1+0.07} = 0.673  .
\end{array}
\end{matrix}
}
\]

Here, the ratio of non-premium related expenses to losses is
\(G = \frac{21000}{300000} = 0.07\).

Thus, the (new) indicated rate level change is

\[
\small{
ICF =\frac{LR_{experience}}{LR_{target}} -1  = \frac{0.60}{0.673} -1 = -10.8\%.
}
\] (b) Using the pure premium method, the indicated change factor,
\(ICF\), is

\[
\small{
\begin{matrix}
\begin{array}{ll}
ICF
&= \frac{\text{Losses + Fixed}}{\text{Premium} \times (1 - Q - V)}\\
&= \frac{300000+ 21000}{500000 \times (1 - 0.23 - 0.05)} = 0.892 .
\end{array}
\end{matrix}
}
\]

Thus, the indicated rate level change is \(0.892 -1 = -10.8\%\).

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  The loss ratio method is preferable when the exposure unit is not
  available.
\end{enumerate}

The loss ratio method is preferable when the exposure unit is not
reasonably consistent between risks.

The pure premium method is preferable for a new line of business.

The pure premium method is preferable where on-level premiums are
difficult to calculate.

\section{Selecting a Premium}\label{S:GiniStatistic}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Describe skewed distributions via a Lorenz curve and Gini index
\item
  Define a concentration curve and the corresponding Gini statistic
\item
  Use the concentration curve and Gini statistic for premium selection
  base on out-of-sample validation
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

For a portfolio of insurance contracts, insurers collect premiums and
pay out losses. After making adjustments for expenses and profit
considerations, tools for comparing distributions of premiums and losses
can be helpful when selecting a premium calculation principle.

\subsection{Classic Lorenz Curve}\label{classic-lorenz-curve}

In welfare economics, it is common to compare distributions via the
Lorenz curve, developed by Max Otto Lorenz \citep{lorenz1905methods}. A
Lorenz curve is a graph of the proportion of a population on the
horizontal axis and a distribution function of interest on the vertical
axis. It is typically used to represent income distributions. When the
income distribution is perfectly aligned with the population
distribution, the Lorenz curve results in a 45 degree line that is known
as the line of equality. Because the graph compares two distribution
functions, one can also think of a Lorenz curve as a type of pp plot
that was introduced in Section \ref{S:MS:GraphComparison}. The area
between the Lorenz curve and the line of equality is a measure of the
discrepancy between the income and population distributions. Two times
this area is known as the Gini index, introduced by Corrado Gini in
1912.

\textbf{Example -- Classic Lorenz Curve.} For an insurance example,
Figure \ref{fig:ClassicLorenz} shows a distribution of insurance losses.
This figure is based on a random sample of 2000 losses. The left-hand
panel shows a right-skewed histogram of losses. The right-hand panel
provides the corresponding Lorenz curve, showing again a skewed
distribution. For example, the arrow marks the point where 60 percent of
the policyholders have 30 percent of losses. The 45 degree line is the
line of equality; if each policyholder has the same loss, then the loss
distribution would be at this line. The Gini index, twice the area
between the Lorenz curve and the 45 degree line, is 37.6 percent for
this data set.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{LossDataAnalytics_files/figure-latex/ClassicLorenz-1} 

}

\caption{Distribution of Insurance Losses. The left-hand panel is a density plot of losses. The right-hand panel presents the same data using a Lorenz curve.}\label{fig:ClassicLorenz}
\end{figure}

\subsection{Performance Curve and a Gini
Statistic}\label{performance-curve-and-a-gini-statistic}

We now introduce a modification of the classic Lorenz curve and Gini
statistic that is useful in insurance applications. Specifically, we
introduce a performance curve that, in this case, is a graph of the
distribution of losses versus premiums, where both losses and premiums
are ordered by premiums. To make the ideas concrete, we provide some
notation and consider \(i=1, \ldots, n\) policies. For the \(i\)th
policy, let

\begin{itemize}
\tightlist
\item
  \(y_i\) denote the insurance loss,
\item
  \(\mathbf{x}_i\) be a set of rating variables known to the analyst,
  and
\item
  \(P_i=P(\mathbf{x}_i)\) be the associated premium that is a function
  of \(\mathbf{x}_i\).
\end{itemize}

The set of information used to calculate the performance curve for the
\(i\)th policy is \((y_i, P_i)\).

\subsubsection*{Performance Curve}\label{performance-curve}
\addcontentsline{toc}{subsubsection}{Performance Curve}

It is convenient to first sort the set of policies based on premiums
(from smallest to largest) and then compute the premium and loss
distributions. The premium distribution is

\begin{equation}
\hat{F}_P(s) =  \frac{ \sum_{i=1}^n P_i ~\mathrm{I}(P_i \leq s) }{\sum_{i=1}^n P_i}   ,
\label{eq:EmpPremDF}
\end{equation}

and the loss distribution is

\begin{equation}
\hat{F}_{L}(s) =  \frac{ \sum_{i=1}^n y_i ~\mathrm{I}(P_i \leq s) }{\sum_{i=1}^n y_i} ,
\label{eq:EmpLossDF}
\end{equation}

where \(\mathrm{I}(\cdot)\) is the indicator function, returning a 1 if
the event is true and zero otherwise. For a given value \(s\),
\(\hat{F}_P(s)\) gives the proportion of premiums less than or equal to
\(s\), and \(\hat{F}_{L}(s)\) gives the proportion of losses for those
policyholders with premiums less than or equal to \(s\). The graph
\(\left(\hat{F}_P(s),\hat{F}_{L}(s) \right)\) is known as a
\textbf{performance curve}.

\textbf{Example -- Loss Distribution.} Suppose we have \(n=5\)
policyholders with experience as follows. The data have been ordered by
premiums.

\begin{longtable}[]{@{}lcllrrr@{}}
\toprule
Variable & \(i\) & 1 & 2 & 3 & 4 & 5\tabularnewline
\midrule
\endhead
Premium & \(P(\mathbf{x}_i)\) & 2 & 4 & 5 & 7 & 16\tabularnewline
Cumulative Premiums & \(\sum_{j=1}^i P(\mathbf{x}_j)\) & 2 & 6 & 11 & 18
& 34\tabularnewline
Loss & \(y_i\) & 2 & 5 & 6 & 6 & 17\tabularnewline
Cumulative Loss & \(\sum_{j=1}^i y_j\) & 2 & 7 & 13 & 19 &
36\tabularnewline
\bottomrule
\end{longtable}

Figure \ref{fig:LorenzVsOrdered} compares the Lorenz to the performance
curve. The left-hand panel shows the Lorenz curve. The horizontal axis
is the cumulative proportion of policyholders (0, 0.2, 0.4, and so
forth) and the vertical axis is the cumulative proportion of losses (0,
2/36, 7/36, and so forth). For the Lorenz curve, you first order by the
loss size (which turns out to be the same order as premiums for this
simple dataset). This figure shows a large separation between the
distributions of losses and policyholders.

The right-hand panel shows the performance curve. Because observations
are sorted by premiums, the first point after the origin (reading from
left to right) is (2/34, 2/36). The second point is (6/34, 7/36), with
the pattern continuing. From the figure, we see that there is little
separation between losses and premiums.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{LossDataAnalytics_files/figure-latex/LorenzVsOrdered-1} 

}

\caption{Lorenz versus Performance Curve}\label{fig:LorenzVsOrdered}
\end{figure}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The performance curve can be helpful to the analyst who thinks about
forming profitable portfolios for the insurer. For example, suppose that
\(s\) is chosen to represent the 95\emph{th} percentile of the premium
distribution. Then, the horizontal axis, \(\hat{F}_P(s)\), represents
the fraction of premiums for this portfolio and the vertical axis,
\(\hat{F}_L(s)\), the fraction of losses for this portfolio. When
developing premium principles, analysts wish to avoid unprofitable
situations and make profits, or at least break even.

The expectation of the numerator in equation \eqref{eq:EmpLossDF} is
\(\sum_{i=1}^n \mathrm{E}~ y_i=\sum_{i=1}^n \mu_i\). Thus, if the
premium principle is chosen such that \(P_i= \mu_i\), then we anticipate
a close relation between the premium and loss distributions, resulting
in a 45 degree line. The 45 degree line presents equality between losses
and premiums, a break-even situation which is the benchmark for
insurance pricing.

\subsubsection*{Gini Statistic}\label{gini-statistic}
\addcontentsline{toc}{subsubsection}{Gini Statistic}

The classic Lorenz curve shows the proportion of policyholders on the
horizontal axis and the loss distribution function on the vertical axis.
The performance curve extends the classical Lorenz curve in two ways,
(1) through the ordering of risks and prices by prices and (2) by
allowing prices to vary by observation. We summarize the performance
curve in the same way as the classic Lorenz curve using a Gini
statistic, defined as twice the area between the curve and a 45 degree
line. The analyst seeks ordered performance curves that approach passing
through the 45 degree line; these have least separation between the loss
and premium distributions and therefore small Gini statistics.

Specifically, the Gini statistic can be calculated as follows. Suppose
that the empirical performance curve is given by
\(\{ (a_0=0, b_0=0), (a_1, b_1), \ldots,\) \((a_n=1, b_n=1) \}\) for a
sample of \(n\) observations. Here, we use \(a_j = \hat{F}_P(P_j)\) and
\(b_j = \hat{F}_{L}(P_j)\). Then, the empirical Gini statistic is

\begin{eqnarray}
\widehat{Gini} 
&=&  2\sum_{j=0}^{n-1} (a_{j+1} - a_j) \left \{
\frac{a_{j+1}+a_j}{2} - \frac{b_{j+1}+b_j}{2} \right\} \nonumber \\
&=& 1 - \sum_{j=0}^{n-1} (a_{j+1} - a_j) (b_{j+1}+b_j) .
\label{eq:GiniDefn}
\end{eqnarray}

To understand the formula for the Gini statistic, here is a sketch of a
parallelogram connecting points \((a_1, b_1)\), \((a_2, b_2)\), and a 45
degree line. You can use basic geometry to check that the area of the
figure is
\(Area = (a_2 - a_1) \left \{\frac{a_2+a_1}{2} - \frac{b_2+b_1}{2} \right\}\).
The definition of the Gini statistic in equation \eqref{eq:GiniDefn} is
simply twice the sum of the parallelograms. The second equality in
equation \eqref{eq:GiniDefn} is the result of some straight-forward
algebra.

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-214-1.pdf}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example -- Loss Distribution: Continued.} The Gini statistic for
the Lorenz curve (left-hand panel of Figure \ref{fig:LorenzVsOrdered})
is 34.4 percent. In contrast, the Gini statistic for performance curve
(right-hand panel) is 1.7 percent.

\subsection{Out-of-Sample Validation}\label{out-of-sample-validation}

The benefits of out-of-sample validation for model selection were
introduced in Section 4.2. We now demonstrate the use of the a Gini
statistic and performance curve in this context. The procedure follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use an in-sample data set to estimate several competing models, each
  producing a premium function.
\item
  Designate an out-of-sample, or validation, data set of the form
  \(\{(y_i, \mathbf{x}_i), i=1,\ldots,n\}\).
\item
  Use the explanatory variables from the validation sample to form
  premiums of the form \(P(\mathbf{x}_i)\).
\item
  Compute the Gini statistic for each model. Choose the model with the
  lowest Gini statistic.
\end{enumerate}

\textbf{Example -- Community Rating versus Premiums that Vary by State.}
Suppose that we have experience from 25 states and that, for each state,
we have available 200 observations that can be used to predict future
losses. For simplicity, assume that the analyst knows that these losses
were generated by a gamma distribution with a common shape parameter
equal to 5. Unknown to the analyst, the scale parameters vary by state,
from a low of 20 to 66.

\begin{itemize}
\tightlist
\item
  To compute base premiums, the analyst assumes a scale parameter that
  is common to all states that is to be estimated from the data. You can
  think of this common premium as based on a community rating principle.
\item
  As an alternative, the analyst allows the scale parameters to vary by
  state and will again use the data to estimate these parameters.
\end{itemize}

An out of sample validation set of 100 losses from each state is
available. For each of the two rating procedures, determine the
performance curve and the corresponding Gini statistic. Choose the rate
procedure with the lower Gini statistic.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Recall for the gamma distribution that the mean equals the shape times
the scale or, 5 times the scale parameter, for our example. So, you can
check that the maximum likelihood estimates are simply the average
experience.

For our base premium, we assume a common distribution among all states.
For these simulated data, the average in-sample loss is \(P_1\)=221.36.

As an alternative, we use averages that are state-specific; these
averages form our premiums \(P_2\). Because this illustration uses means
that vary by states, we anticipate this alternative rating procedure to
be preferred to the community rating procedure.

Out of sample claims were generated from the same gamma distribution as
the in-sample model, with 100 observations for each state. The following
\texttt{R} code shows how to calculate the performance curves.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{LossDataAnalytics_files/figure-latex/unnamed-chunk-217-1} 

}

\end{figure}

For these data, the Gini statistics are 19.6 percent for the flat rate
premium and -0.702 percent for the state-specific alternative. This
indicates that the state-specific alternative procedure is strongly
preferred to the base community rating procedure.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection*{Discussion}\label{discussion}
\addcontentsline{toc}{subsubsection}{Discussion}

In insurance claims modeling, standard out-of-sample validation measures
are not the most informative due to the high proportions of zeros
(corresponding to no claim) and the skewed fat-tailed distribution of
the positive values. In contrast, the Gini statistic works well with
many zeros (see the demonstration in \citep{frees2014insurance}).

The value of the performance curves and Gini statistics have been
recently advanced in the paper of \citet{denuit2019concentrationGini}.
Properties of an extended version, dealing with relatives for new
premiums, were developed by \citet{frees2011summarizing} and
\citet{frees2014insurance}. In these articles you can find formulas for
the standard errors and additional background information.

\section{Further Resources and
Contributors}\label{further-resources-and-contributors}

This chapter serves as a bridge between the technical introduction of
this book and an introduction to pricing and ratemaking for practicing
actuaries. For readers interested in learn practical aspects of pricing,
we recommend introductions by the Society of Actuaries in
\citet{friedland2013fundamentals} and by the Casualty Actuarial Society
in \citet{werner2016basic}. For a classic risk management introduction
to pricing, see \citet{niehaus2003risk}. See also \citet{finger2006risk}
and \citet{frees2014frequency}.

\citet{buhlmann1985premium} was the first in the academic literature to
argue that pricing should be done first at the portfolio level (he
referred to this as a \emph{top down} approach) which would be
subsequently reconciled with pricing of individual contracts. See also
the discussion in \citet{kaas2008modern}, Chapter 5.

For more background on pricing principles, a classic treatment is by
\citet{gerber1979introduction} with a more modern approach in
\citet{kaas2008modern}. For more discussion of a pricing from a
financial economics viewpoint, see \citet{bauer2013financial}.

\begin{itemize}
\tightlist
\item
  \textbf{Edward W. (Jed) Frees}, University of Wisconsin-Madison, and
  \textbf{José Garrido}, Concordia University are the principal authors
  of the initial version of this chapter. Email:
  \href{mailto:jfrees@bus.wisc.edu}{\nolinkurl{jfrees@bus.wisc.edu}}
  and/or
  \href{mailto:jose.garrido@concordia.ca}{\nolinkurl{jose.garrido@concordia.ca}}
  for chapter comments and suggested improvements.
\item
  Chapter reviewers include: Write Jed or José to add you name here.
\end{itemize}

\subsection*{TS 7.A. Rate Regulation}\label{ts-7.a.-rate-regulation}
\addcontentsline{toc}{subsection}{TS 7.A. Rate Regulation}

Insurance regulation helps to ensure the financial stability of insurers
and to protect consumers. Insurers receive premiums in return for
promises to pay in the event of a contingent (insured) event. Like other
financial institutions such as banks, there is a strong public interest
in promoting the continuing viability of insurers.

\subsubsection*{Market Conduct}\label{market-conduct}
\addcontentsline{toc}{subsubsection}{Market Conduct}

To help protect consumers, regulators impose administrative rules on the
behavior of market participants. These rules, known as market conduct
regulation, provide systems of regulatory controls that require insurers
to demonstrate that they are providing fair and reliable services,
including rating, in accordance with the statutes and regulations of a
jurisdiction.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Product regulation} serves to protect consumers by ensuring that
  insurance policy provisions are reasonable and fair, and do not
  contain major gaps in coverage that might be misunderstood by
  consumers and leave them unprotected.
\item
  The insurance product is the insurance contract (policy) and the
  coverage it provides. Insurance contracts are regulated for these
  reasons:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Insurance policies are complex legal documents that are often
    difficult to interpret and understand.
  \item
    Insurers write insurance policies and sell them to the public on a
    ``take it or leave it'' basis.
  \end{enumerate}
\end{enumerate}

Market conduct includes rules for \emph{intermediaries} such as agents
(who sell insurance to individuals) and brokers (who sell insurance to
businesses). Market conduct also includes \emph{competition policy
regulation}, designed to ensure an efficient and competitive marketplace
that offers low prices to consumers.

\subsubsection*{Rate Regulation}\label{rate-regulation}
\addcontentsline{toc}{subsubsection}{Rate Regulation}

\emph{Rate regulation} helps guide the development of premiums and so is
the focus of this chapter. As with other aspects of market conduct
regulation, the intent of these regulations is to ensure that insurers
not take unfair advantage of consumers. Rate (and policy form)
regulation is common worldwide.

The amount of regulatory scrutiny varies by insurance product. Rate
regulation is uncommon in life insurance. Further, in non-life
insurance, most commercial lines and reinsurance are free from
regulation. Rate regulation is common in automobile insurance, health
insurance, workers compensation, medical malpractice, and homeowners
insurance. These are markets in which insurance is mandatory or in which
universal coverage is thought to be socially desirable.

There are three principles that guide rate regulation: rates should

\begin{itemize}
\tightlist
\item
  be adequate (to maintain insurance company solvency),
\item
  but not excessive (not so high as to lead to exorbitant profits),
\item
  nor unfairly discriminatory (price differences must reflect expected
  claim and expense differences).
\end{itemize}

Recently, in auto and home insurance, the twin issues of availability
and affordability, which are not explicitly included in the guiding
principles, have been assuming greater importance in regulatory
decisions.

\subsubsection*{Rates are Not Unfairly
Discriminatory}\label{rates-are-not-unfairly-discriminatory}
\addcontentsline{toc}{subsubsection}{Rates are Not Unfairly
Discriminatory}

Some government regulations of insurance restrict the amount, or level,
of premium rates. These are based on the first two of the three guiding
rate regulation principles, that rates be adequate but not excessive.
This type of regulation is discussed further in the following section on
types of rate regulation.

Other government regulations restrict the type of information that can
be used in risk classification. These are based on the third guiding
principle, that rates not be unfairly discriminatory. ``Discrimination''
in an insurance context has a different meaning than commonly used; for
our purposes, discrimination means the ability to distinguish among
things or, in our case, policyholders. The real issue is what is meant
by the adjective ``fair.''

In life insurance, it has long been held that it is reasonable and fair
to charge different premium rates by age. For example, a life insurance
premium differs dramatically between an 80 year old and someone aged 20.
In contrast, it is unheard of to use rates that differ by:

\begin{itemize}
\tightlist
\item
  ethnicity/race,
\item
  political affiliation, or
\item
  religion.
\end{itemize}

It is not a matter of whether data can be used to establish statistical
significance among the levels of any of these variables. Rather, it is a
societal decision as to what constitutes notions of ``fairness.''

Different jurisdictions have taken different stances on what constitutes
a fair rating variable. For example, in some juridictions for some
insurance products, gender is no longer a permissible variable. As an
illustration, the European Union now prohibits the use of gender for
automobile rating. As another example, in the U.S., many discussions
have revolved around the use of credit ratings to be used in automobile
insurance pricing. Credit ratings are designed to measure consumer
financial reponsibility. Yet, some argue that credit rates are good
proxies for ethnicity and hence should be prohibited.

In an age where more data is being used in imaginative ways, discussions
of what constitutes a fair rating variable will only become more
important going forward and much of that discussion is beyond the scope
of this text. However, it is relevant to the discussion to remark that
actuaries and other data analysts can contribute to societal discussions
on what constitutes a ``fair'' rating variable in unique ways by
establishing the magnitude of price differences when using variables
under discussion.

\subsubsection*{Types of Rate
Regulation}\label{types-of-rate-regulation}
\addcontentsline{toc}{subsubsection}{Types of Rate Regulation}

There are several methods, that vary by the level of scrutiny, by which
regulators may restrict the rates that insurers offer.

The most restrictive is a government prescribed regulatory system, where
the government regulator determines and promulgates the rates,
classifications, forms, and so forth, to which all insurers must adhere.
Also restrictive are prior approval systems. Here, the insurer must file
rates, rules, and so forth, with government regulators. Depending on the
statute, the filing becomes effective when a specified waiting period
elapses (if the government regulator does not take specific action on
the filing, it is deemed approved automatically) or when the government
regulator formally approves the filing.

The least restrictive is a no file or \emph{record maintenance} system
where the insurer need not file rates, rules, and so forth, with the
government regulator. The regulator may periodically examine the insurer
to ensure compliance with the law. Another relatively flexible system is
the file only system, also known as \emph{competitive} rating, where the
insurer simply keeps files to ensure compliance with the law.

In between these two extremes are the (1) file and use, (2) use and
file, (3) modified prior approval, and (4) flex rating systems.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  File and Use: The insurer must file rates, rules, and so forth, with
  the government regulator. The filing becomes effective immediately or
  on a future date specified by the filer.
\item
  Use and File: The filing becomes effective when used. The insurer must
  file rates, rules, and so forth, with the government regulator within
  a specified time period after first use.
\item
  Modified Prior Approval: This is a hybrid of ``prior approval'' and
  ``file and use'' laws. If the rate revision is based solely on a
  change in loss experience then ``file and use'' may apply. However, if
  the rate revision is based on a change in expense relationships or
  rate classifications, then ``prior approval'' may apply.
\item
  Flex (or Band) Rating: The insurer may increase or decrease a rate
  within a ``flex band,'' or range, without approval of the government
  regulator. Generally, either ``file and use'' or ``use and file''
  provisions apply.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

For a broad introduction to government insurance regulation from a
global perspective, see the website of the
\href{https://www.iaisweb.org/home}{International Association of
Insurance Supervisors (IAIS)}.

\chapter{Risk Classification}\label{C:RiskClass}

\emph{Chapter Preview.} This chapter motivates the use of risk
classification in insurance pricing and introduces readers to the
Poisson regression as a prominent example of risk classification. In
Section \ref{S:RC:Introduction} we explain why insurers need to
incorporate various risk characteristics, or rating factors, of
individual policyholders in pricing insurance contracts. We then
introduce Section \ref{S:RC:PoissonRegression} the Poisson regression as
a pricing tool to achieve such premium differentials. The concept of
exposure is also introduced in this section. As most rating factors are
categorical, we show in Section \ref{S:CatVarMultiTarriff} how the
multiplicative tariff model can be incorporated in the Poisson
regression model in practice, along with numerical examples for
illustration.

\section{Introduction}\label{S:RC:Introduction}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn:

\begin{itemize}
\tightlist
\item
  Why premiums should vary across policyholders with different risk
  characteristics.\\
\item
  The meaning of the adverse selection spiral.\\
\item
  The need for risk classification.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Through insurance contracts, the policyholders effectively transfer
their risks to the insurer in exchange for premiums. For the insurer to
stay in business, the premium income collected from a pool of
policyholders must at least equal the benefit outgo. In general
insurance products where a premium is charged for a single period, say
annual, the gross insurance premium based on the equivalence principle
is stated as

\[
\text{Gross Premium = Expected Losses + Expected Expenses + Profit}.
\]

Thus, ignoring the frictional expenses associated with the
administrative expenses and the profit, the net or pure premium charged
by the insurer should be equal to the expected losses occurring from the
risk that is transferred from the policyholder.

If all policyholders in the insurance pool have identical risk profiles,
the insurer simply charges the same premium for all policyholders
because they have the same expected loss. In reality, however, the
policyholders are hardly homogeneous. For example, mortality risk in
life insurance depends on the characteristics of the policyholder, such
as, age, sex and life style. In auto insurance, those characteristics
may include age, occupation, the type or use of the car, and the area
where the driver resides. The knowledge of these characteristics or
variables can enhance the ability of calculating fair premiums for
individual policyholders, as they can be used to estimate or predict the
expected losses more accurately.

\textbf{Adverse Selection.} Indeed, if the insurer does not
differentiate the risk characteristics of individual policyholders and
simply charges the same premium to all insureds based on the average
loss in the portfolio, the insurer would face adverse selection, a
situation where individuals with a higher chance of loss are attracted
in the portfolio and low-risk individuals are repelled. For example,
consider a health insurance industry where smoking status is an
important risk factor for mortality and morbidity. Most health insurers
in the market require different premiums depending on smoking status, so
smokers pay higher premiums than non-smokers, with other characteristics
being identical. Now suppose that there is an insurer, we will call
EquitabAll, that offers the same premium to all insureds regardless of
smoking status, unlike other competitors. The net premium of EquitabAll
is naturally an average mortality loss accounting for both smokers and
non-smokers. That is, the net premium is a weighted average of the
losses with the weights being the proportion of smokers and non-smokers,
respectively. Thus it is easy to see that that a smoker would have a
good incentive to purchase insurance from EquitabAll than from other
insurers as the offered premium by EquitabAll is relatively lower. At
the same time non-smokers would prefer buying insurance from somewhere
else where lower premiums, computed from the non-smoker group only, are
offered. As a result, there will be more smokers and less non-smokers in
the EquitabAll's portfolio, which leads to larger-than-expected losses
and hence a higher premium for insureds in the next period to cover the
higher costs. With the raised new premium in the next period,
non-smokers in EquitabAll will have even greater incentives to switch
the insurer. As this cycle continues over time, EquitabAll would
gradually retain more smokers and less non-smokers in its portfolio with
the premium continually raised, eventually leading to a collapsing of
business. In the literature, this phenomenon is known as the adverse
selection spiral or death spiral. Therefore, incorporating and
differentiating important risk characteristics of individuals in the
insurance pricing process are a pertinent component for both the
determination of fair premium for individual policyholders and the long
term sustainability of insurers.

\textbf{Rating Factors}. In order to incorporate relevant risk
characteristics of policyholders in the pricing process, insurers
maintain some classification system that assigns each policyholder to
one of the risk classes based on a relatively small number of risk
characteristics that are deemed most relevant. These characteristics
used in the classification system are called the rating factors, which
are a priori variables in the sense that they are known before the
contract begins (e.g., sex, health status, vehicle type, etc, are known
during the underwriting). All policyholders sharing identical risk
factors thus are assigned to the same risk class, and are considered
homogeneous from the pricing viewpoint; the insurer consequently charge
them the same premium or rate.

Regarding the risk factors and premiums, the \emph{Actuarial Standard of
Practice} (ASOP No. 12) of the \citet{ASBStandards} states that the
actuary should select risk characteristics that are related to expected
outcomes, and that rates within a risk classification system would be
considered equitable if differences in rates reflect material
differences in expected cost for risk characteristics. In the process of
choosing risk factors, ASOP also requires the actuary to consider the
following: relationship of risk characteristics and expected outcomes,
causality, objectivity, practicality, applicable law, industry
practices, and business practices. Technical Supplement \textbf{TS 8.B}
provides additional discussion of selection of rating factors.

On the quantitative side, an important task for the actuary in building
any risk classification is to construct a statistical model that can
determine the expected loss given various rating factors of a
policyholder. The standard approach is to adopt a regression model which
produces the expected loss as the output when the relevant risk factors
are given as the inputs. In this chapter we learn the Poisson
regression, which can be used when the loss is a count variable, as a
prominent example of an insurance pricing tool.

\section{Poisson Regression Model}\label{S:RC:PoissonRegression}

The Poisson regression model has been successfully used in a wide range
of applications and has an advantage of allowing closed-form expressions
for important quantities, which provides a informative intuition and
interpretation. In this section we introduce the Poisson regression as a
natural extension of the Poisson distribution.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section you will:

\begin{itemize}
\tightlist
\item
  Understand Poisson regressions as convenient tool to combine
  individual Poisson distributions in a unified fashion.
\item
  Learn the concept of exposure and its importance.\\
\item
  Formally learn how to formulate the Poisson regression model using
  indicator variables when the explanatory variables are categorical.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Need for Poisson Regression}\label{S:RC:Need.Poi.reg}

\textbf{Poisson Distribution}

To introduce the Poisson regression, let us consider a hypothetical
health insurance portfolio where all policyholders are of the same age
and only one risk factor, smoking status, is relevant. Smoking status
thus is a categorical variable containing two different types: smoker
and non-smoker. In the statistical literature different types in a given
categorical variable are commonly called levels. As there are two levels
for the smoking status, we may denote smoker and non-smoker by level 1
and 2, respectively. Here the numbering is arbitrary and nominal.
Suppose now that we are interested in pricing a health insurance where
the premium for each policyholder is determined by the number of
outpatient visits to doctor's office during a year. The amount of
medical cost for each visit is assumed to be the same regardless of the
smoking status for simplicity. Thus if we believe that smoking status is
a valid risk factor in this health insurance, it is natural to consider
the data separately for each smoking status. In
\protect\hyperlink{tab:8.1}{Table 8.1} we present the data for this
portfolio.

\[
{\small
\begin{matrix}
\begin{array}{cc|cc|cc}
\hline
\text{Smoker} & \text{(level 1)}  & \text{Non-smoker}&\text{(level 2)}  & & \text{Both}\\
  \text{Count} & \text{Observed} &  \text{Count} & \text{Observed}  &   \text{Count} & \text{Observed} \\ \hline
0 & 2213 &   0 & 6671 &  0 & 8884 \\
1 & 178  &   1 & 430  &  1 & 608 \\
2 & 11   &   2 & 25   &  2 & 36 \\
3 & 6    &   3 & 9    &  3 & 15 \\
4 & 0    &   4 & 4    &  4 & 4 \\
5 & 1    &   5 & 2    &  5 & 3 \\ \hline
\text{Total} & 2409  &   \text{Total} & 7141 & \text{Total} & 9550 \\
\text{Mean} & 0.0926 &   \text{Mean} & 0.0746 & \text{Mean} & 0.0792 \\
\hline
    \end{array}
\end{matrix}
}
\]

\protect\hyperlink{tab:8.1}{Table 8.1} : Number of visits to doctor's
office in last year

As this dataset contains random counts, we try to fit a Poisson
distribution for each level.

As introduced in Section \ref{S:poisson-distribution}, the probability
mass function of the Poisson with mean \(\mu\) is given by

\begin{equation}
\Pr(Y=y)=\frac{\mu^y e^{-\mu}}{y!},\qquad y=0,1,2, \ldots
\label{eq:Pois-pmf}
\end{equation}

and \(\mathrm{E~}{(Y)}=\mathrm{Var~}{(Y)}=\mu\). In regression contexts,
it is common to use \(\mu\) for mean parameters instead of the Poisson
parameter \(\lambda\) although certainly both symbols are suitable. As
we saw in Section \ref{S:estimating-frequency-distributions}, the mle of
the Poisson distribution is given by the sample mean. Thus if we denote
the Poisson mean parameter for each level by \(\mu_{(1)}\) (smoker) and
\(\mu_{(2)}\) (non-smoker), we see from
\protect\hyperlink{tab:8.1}{Table 8.1} that \(\hat{\mu}_{(1)}=0.0926\)
and \(\hat{\mu}_{(2)}=0.0746\). This simple example shows the basic idea
of risk classification. Depending on the smoking status a policyholder
will have a different risk characteristic and it can be incorporated
through varying Poisson parameter in computing the fair premium. In this
example the ratio of expected loss frequencies is
\(\hat{\mu}_{(1)}/\hat{\mu}_{(2)}=1.2402\), implying that smokers tend
to visit doctor's office 24.02\(\%\) times more frequently compared to
non-smokers.

It is also informative to note that if the insurer charges the same
premium to all policyholders regardless of the smoking status, based on
the average characteristic of the portfolio, as was the case for
EquitabAll described in Introduction, the expected frequency (or the
premium) \(\hat{\mu}\) is 0.0792, obtained from the last column of
\protect\hyperlink{tab:8.1}{Table 8.1}. It is easily verified that

\begin{equation}
\hat{\mu} = \left(\frac{n_1}{n_1+n_2}\right)\hat{\mu}_{(1)}+\left(\frac{n_2}{n_1+n_2}\right)\hat{\mu}_{(2)}=0.0792,
\label{eq:coll-prem-avg}
\end{equation}

where \(n_i\) is the number of observations in each level. Clearly, this
premium is a weighted average of the premiums for each level with the
weight equal to the proportion of the insureds in that level.

\textbf{A simple Poisson regression}\\
In the example above, we have fitted a Poisson distribution for each
level separately, but we can actually combine them together in a unified
fashion so that a single Poisson model can encompass both smoking and
non-smoking statuses. This can be done by relating the Poisson mean
parameter with the risk factor. In other words, we make the Poisson
mean, which is the expected loss frequency, respond to the change in the
smoking status. The conventional approach to deal with a categorical
variable is to adopt indicator or dummy variables that take either 1 or
0, so that we turn the switch on for one level and off for others.
Therefore we may propose to use

\begin{equation}
\mu=\beta_0+\beta_1 x_1
\label{eq:lin-mu}
\end{equation}

or, more commonly, a log linear form

\begin{equation}
\log \mu=\beta_0+\beta_1 x_1,
\label{eq:log-lin-mu}
\end{equation}

where \(x_1\) is an indicator variable with

\begin{equation}
x_1=
\begin{cases}
     1 & \text{if smoker}, \\
     0 & \text{otherwise}.
\end{cases}
\label{eq:dummy-x1}
\end{equation}

We generally prefer the log linear relation \eqref{eq:log-lin-mu} to the
linear one in \eqref{eq:lin-mu} to prevent undesirable events of producing
negative \(\mu\) values, which may happen when there are many different
risk factors and levels. The setup \eqref{eq:log-lin-mu} and
\eqref{eq:dummy-x1} then results in different Poisson frequency parameters
depending on the level in the risk factor:

\begin{equation}
\log \mu=
\begin{cases}
     \beta_0+\beta_1 \\
     \beta_0 
\end{cases}
\quad \text{or equivalently,}\qquad \mu= \begin{cases}
     e^{\beta_0+\beta_1} & \text{if smoker (level 1)}, \\
     e^{\beta_0} & \text{if non-smoker (level 2)},
\end{cases} 
\label{eq:ind-mu}
\end{equation}

achieving what we aim for. This is the simplest form of the Poisson
regression. Note that we require a single indicator variable to model
two levels in this case. Alternatively, it is also possible to use two
indicator variables through a different coding scheme. This scheme
requires dropping the intercept term so that \eqref{eq:log-lin-mu} is
modified to

\begin{equation}
\log \mu=\beta_1 x_1+\beta_2 x_2,
\label{eq:log-lin-mu-2}
\end{equation}

where \(x_2\) is the second indicator variable with

\begin{equation}
x_2=
\begin{cases}
     1 & \text{if non-smoker}, \\
     0 & \text{otherwise}.
\end{cases}
\label{eq:dummy-x-2}
\end{equation}

Then we have, from \eqref{eq:log-lin-mu-2},

\begin{equation}
\log \mu=
\begin{cases}
     \beta_1 \\
     \beta_2 
\end{cases}
\quad \text{or}\qquad \mu= \begin{cases}
     e^{\beta_1} & \text{if smoker (level 1)}, \\
     e^{\beta_2} & \text{if non-smoker (level 2)}.
\end{cases}
\label{eq:ind-mu-2}
\end{equation}

The numerical result of \eqref{eq:ind-mu} is the same as \eqref{eq:ind-mu-2}
as all the coefficients are given as numbers in actual estimation, with
the former setup more common in most texts; we also stick to the former.

With this Poisson regression model we can easily understand how the
coefficients \(\beta_0\) and \(\beta_1\) are linked to the expected loss
frequency in each level. According to \eqref{eq:ind-mu}, the Poisson mean
of the smokers, \(\mu_{(1)}\), is given by

\begin{equation}
\mu_{(1)}=e^{\beta_0+\beta_1}=\mu_{(2)} \,e^{\beta_1} \quad \text{or}\quad  \mu_{(1)}/\mu_{(2)} =e^{\beta_1}
\label{eq:no-label}
\end{equation}

where \(\mu_{(2)}\) is the Poisson mean for the non-smokers. This
relation between the smokers and non-smokers suggests a useful way to
compare the risks embedded in different levels of a given risk factor.
That is, the proportional increase in the expected loss frequency of the
smokers compared to that of the non-smokers is simply given by a
multiplicative factor \(e^{\beta_1}\). Putting another way, if we set
the expected loss frequency of the non-smokers as the base value, the
expected loss frequency of the smokers is obtained by applying
\(e^{\beta_1}\) to the base value.

\textbf{Dealing with multi-level case}\\
We can readily extend the two-level case to a multi-level one where
\(l\) different levels are involved for a single rating factor. For this
we generally need \(l-1\) indicator variables to formulate

\begin{equation}
\log \mu=\beta_0+\beta_1 x_1+\cdots+\beta_{l-1} x_{l-1},
\label{eq:log-lin-mu-1}
\end{equation}

where \(x_k\) is an indicator variable that takes 1 if the policy
belongs to level \(k\) and 0 otherwise, for \(k=1,2, \ldots, l-1\). By
omitting the indicator variable associated with the last level in
\eqref{eq:log-lin-mu-1} we effectively chose level \(l\) as the base case,
but this choice is arbitrary and does not matter numerically. The
resulting Poisson parameter for policies in level \(k\) then becomes,
from \eqref{eq:log-lin-mu-1},

\begin{equation}
\nonumber
\mu= \begin{cases}
     e^{\beta_0+\beta_k} & \text{if the policy belongs to level $k$ (k=1,2, ..., l-1)}, \\
     e^{\beta_0} & \text{if the policy belongs to level $l$}.
\end{cases}
\end{equation}

Thus if we denote the Poisson parameter for policies in level \(k\) by
\(\mu_{(k)}\), we can relate the Poisson parameter for different levels
through \(\mu_{(k)}=\mu_{(l)}\, e^{\beta_k}\), \(k=1,2, \ldots, l-1\).
This indicates that, just like the two-level case, the expected loss
frequency of the \(k\)th level is obtained from the base value
multiplied by the relative factor \(e^{\beta_k}\). This relative
interpretation becomes more powerful when there are many risk factors
with multi-levels, and leads us to a better understanding of the
underlying risk and more accurate prediction of future losses. Finally,
we note that the varying Poisson mean is completely driven by the
coefficient parameters \(\beta_k\)'s, which are to be estimated from the
dataset; the procedure of the parameter estimation will be discussed
later in this chapter.

\subsection{Poisson Regression}\label{poisson-regression}

We now describe the Poisson regression in a formal and more general
setting. Let us assume that there are \(n\) independent policyholders
with a set of rating factors characterized by a \(k\)-variate
vector\footnote{For example, if there are 3 risk factors each of which
  the number of levels are 2, 3 and 4, respectively, we have
  \(k=(2-1)\times(3-1)\times (4-1)=6\).}. The \(i\)th policyholder's
rating factor is thus denoted by vector
\(\mathbf{ x}_i=(1, x_{i1}, \ldots, x_{ik})^{\prime}\), and the
policyholder has recorded the loss count \(y_i \in \{0,1,2, \ldots \}\)
from the last period of loss observation, for \(i=1, \ldots, n\). In the
regression literature, the values \(x_{i1}, \ldots, x_{ik}\) are
generally known as the \emph{explanatory variables}, as these are
measurements providing information about the variable of interest
\(y_i\). In essence, regression analysis is a method to quantify the
relationship between a variable of interest and explanatory variables.

We also assume, for now, that all policyholders have the same one unit
period for loss observation, or equal exposure of 1, to keep things
simple; we will discuss more details on the exposure in the following
subsection.

As done before, we describe the Poisson regression through its mean
function. For this we first denote \(\mu_i\) to be the expected loss
count of the \(i\)th policyholder under the Poisson specification
\eqref{eq:Pois-pmf}:

\begin{equation}
\mu_i=\mathrm{E~}{(y_i|\mathbf{ x}_i)}, \qquad y_i \sim Pois(\mu_i), \, i=1, \ldots, n.
\label{eq:mui-glm}
\end{equation}

The condition inside the expectation operation in \eqref{eq:mui-glm}
indicates that the loss frequency \(\mu_i\) is the model output
responding to the given set of risk factors or explanatory variables. In
principle the conditional mean \(\mathrm{E~}{(y_i|\mathbf{ x}_i)}\) in
\eqref{eq:mui-glm} can take different forms depending on how we specify
the relationship between \(\mathbf{ x}\) and \(y\). The standard choice
for the Poisson regression is to adopt the exponential function, as we
mentioned previously, so that

\begin{equation}
\mu_i=\mathrm{E~}{(y_i|\mathbf{ x}_i)}=e^{\mathbf{ x}^{\prime}_i\beta}, \qquad y_i \sim Pois(\mu_i), \, i=1, \ldots, n.
\label{eq:mean-ft-Pois}
\end{equation}

Here \(\beta=(\beta_0, \ldots, \beta_k)^{\prime}\) is the vector of
coefficients so that
\(\mathbf{ x}^{\prime}_i\beta=\beta_0+\beta_1x_{i1} +\ldots+\beta_k x_{ik}\).
The exponential function in \eqref{eq:mean-ft-Pois} ensures that
\(\mu_i >0\) for any set of rating factors \(\mathbf{ x}_i\). Often
\eqref{eq:mean-ft-Pois} is rewritten as a log linear form

\begin{equation}
\log \mu_i=\log \mathrm{E~}{(y_i|\mathbf{ x}_i)}=\mathbf{ x}^{\prime}_i\beta, \qquad y_i \sim Pois(\mu_i), \, i=1, \ldots, n
\label{eq:mean-ft-Pois-2}
\end{equation}

to reveal the relationship when the right side is set as the linear
form, \(\mathbf{ x}^{\prime}_i\beta\). Again, we see that the mapping
works well as both sides of \eqref{eq:mean-ft-Pois-2}, \(\log \mu_i\) and
\(\mathbf{ x}_i\beta\), can now cover the entire real values. This is
the formulation of the Poisson regression, assuming that all
policyholders have the same unit period of exposure. When the exposures
differ among the policyholders, however, as is the case in most
practical cases, we need to revise this formulation by adding exposure
component as an additional term in \eqref{eq:mean-ft-Pois-2}.

\subsection{Incorporating Exposure}\label{incorporating-exposure}

\textbf{Concept of Exposure}

In order to determine the size of potential losses in any type of
insurance, one must always know the corresponding exposure. The concept
of exposure is an extremely important ingredient in insurance pricing,
though we usually take it for granted. For example, when we say the
expected claim frequency of a health insurance policy is 0.2, it does
not mean much without the specification of the exposure such as, in this
case, per month or per year. In fact, all premiums and losses need the
exposure precisely specified and must be quoted accordingly; otherwise
all subsequent statistical analyses and predictions will be distorted.

In the previous section we assumed the same unit of exposure across all
policyholders, but this is hardly realistic in practice. In health
insurance, for example, two different policyholders with different
lengths of insurance coverage (e.g., 3 months and 12 months,
respectively) could have recorded the same number of claim counts. As
the expected number of claim counts would be proportional to the length
of coverage, we should not treat these two policyholders' loss
experiences identically in the modeling process. This motivates the need
of the concept of \emph{exposure} in the Poisson regression.

The Poisson distribution in \eqref{eq:Pois-pmf} is parametrized via its
mean. To understand the exposure, we alternatively parametrize the
Poisson \emph{pmf} in terms of the \emph{rate} parameter \(\lambda\),
based on the definition of the Poisson process:

\begin{equation}
\Pr(Y=y)=\frac{(\lambda t)^y e^{-\lambda t}}{y!},\qquad y=0,1,2, \ldots
\label{eq:Pois-pmf-2}
\end{equation}

with \(\mathrm{E~}{(Y)}=\mathrm{Var~}{(Y)}=\lambda t\). Here \(\lambda\)
is known as the rate or intensity per unit period of the Poisson process
and \(t\) represents the length of time or \emph{exposure}, a known
constant value. For given \(\lambda\) the Poisson distribution
\eqref{eq:Pois-pmf-2} produces a larger expected loss count as the
exposure \(t\) gets larger. Clearly, \eqref{eq:Pois-pmf-2} reduces to
\eqref{eq:Pois-pmf} when \(t=1\), which means that the mean and the rate
become the same for the unit exposure, the case we considered in the
previous subsection.

In principle, the exposure does not need to be measured in units of time
and may represent different things depending the problem at hand. For
example:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In health insurance, the rate may be the occurrence of a specific
  disease per 1,000 people and the exposure is the number of people
  considered in the unit of 1,000.
\item
  In auto insurance, the rate may be the number of accidents per year of
  a driver and the exposure is the length of the observed period for the
  driver in the unit of year.\\
\item
  For workers compensation that covers lost wages resulting from an
  employee's work-related injury or illness, the rate may be the
  probability of injury in the course of employment per dollar and the
  exposure is the payroll amount in dollars.
\item
  In marketing, the rate may be the number of customers who enter a
  store per hour and the exposure is the number of hours observed.\\
\item
  In civil engineering, the rate may be the number of major cracks on
  the paved road per 10 kms and the exposure is the length of road
  considered in the unit of 10 kms.\\
\item
  In credit risk modelling, the rate may be the number of default events
  per 1000 firms and the exposure is the number of firms under
  consideration in the unit of 1,000.
\end{enumerate}

Actuaries may be able to use different exposure bases for a given
insurable loss. For example, in auto insurance, both the number of
kilometers driven and the number of months covered by insurance can be
used as exposure bases. Here the former is more accurate and useful in
modelling the losses from car accidents, but more difficult to measure
and manage for insurers. Thus, a good exposure base may not be the
theoretically best one due to various practical constraints. As a rule,
an exposure base must be easy to determine, accurately measurable,
legally and socially acceptable, and free from potential manipulation by
policyholders.

\textbf{Incorporating exposure in Poisson regression}\\
As exposures affect the Poisson mean, constructing Poisson regressions
requires us to carefully separate the rate and exposure in the modelling
process. Focusing on the insurance context, let us denote the rate of
the loss event of the \(i\)th policyholder by \(\lambda_i\), the known
exposure (the length of coverage) by \(m_i\) and the expected loss count
under the given exposure by \(\mu_i\). Then the Poisson regression
formulation in \eqref{eq:mean-ft-Pois} and \eqref{eq:mean-ft-Pois-2} should
be revised in light of \eqref{eq:Pois-pmf-2} as

\begin{equation}
\mu_i=\mathrm{E~}{(y_i|\mathbf{ x}_i)}=m_i \,\lambda_i=m_i \, e^{\mathbf{ x}^{\prime}_i\beta}, \qquad y_i \sim Pois(\mu_i), \, i=1, \ldots, n,
\label{eq:mean-ft-Pois-6}
\end{equation}

which gives

\begin{equation}
\log \mu_i=\log m_i+\mathbf{ x}^{\prime}_i\beta, \qquad y_i \sim Pois(\mu_i), \, i=1, \ldots,
\label{eq:mean-ft-Pois-7}
\end{equation}

Adding \(\log m_i\) in \eqref{eq:mean-ft-Pois-7} does not pose a problem
in fitting as we can always specify this as an extra explanatory
variable, as it is a known constant, and fix its coefficient to 1. In
the literature the log of exposure, \(\log m_i\), is commonly called the
offset.

\subsection{Exercises}\label{exercises-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Regarding \protect\hyperlink{tab:8.1}{Table 8.1} answer the following.

  \begin{enumerate}
  \def\labelenumii{(\alph{enumii})}
  \tightlist
  \item
    Verify the mean values in the table.\\
  \item
    Verify the number in equation \eqref{eq:coll-prem-avg}.\\
  \item
    Produce the fitted Poisson counts for each smoking status in the
    table.
  \end{enumerate}
\item
  In the Poisson regression formulation \eqref{eq:mui-glm}, consider using
  \(\mu_i=\mathrm{E~}{(y_i|\mathbf{ x}_i)}=({\mathbf{ x}^{\prime}_i\beta})^2\),
  for \(i=1, \ldots, n\), instead of the exponential function. What
  potential issue would you have?
\end{enumerate}

\section{Categorical Variables and Multiplicative
Tariff}\label{S:CatVarMultiTarriff}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section you will learn:

\begin{itemize}
\tightlist
\item
  The multiplicative tariff model when the rating factors are
  categorical.\\
\item
  How to construct the Poisson regression model based on the
  multiplicative tariff structure.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Rating Factors and Tariff}\label{rating-factors-and-tariff}

In practice most rating factors in insurance are \emph{categorical
variables}, meaning that they take one of the pre-determined number of
possible values. Examples of categorical variables include sex, type of
cars, the driver's region of residence and occupation. Continuous
variables, such as age or auto mileage, can also be grouped by bands and
treated as categorical variables. Thus we can imagine that, with a small
number of rating factors, there will be many policyholders falling into
the same risk class, charged with the same premium. For the remaining of
this chapter we assume that all rating factors are categorical
variables.

To illustrate how categorical variables are used in the pricing process,
we consider a hypothetical auto insurance with only two rating factors:

\begin{itemize}
\tightlist
\item
  Type of vehicle: Type A (personally owned) and B (owned by
  corporations). We use index \(j=1\) and \(2\) to respectively
  represent each level of this rating factor.\\
\item
  Age band of the driver: Young (age \(<\) 25), middle (25 \(\le\) age
  \(<\) 60) and old age (age \(\ge\) 60). We use index \(k=1, 2\) and
  \(3\), respectively, for this rating factor.
\end{itemize}

From this classification rule, we may create an organized table or list,
such as the one shown in \protect\hyperlink{tab:8.2}{Table 8.2},
collected from all policyholders. Clearly there are \(2 \times 3=6\)
different risk classes in total. Each row of the table shows a
combination of different risk characteristics of individual
policyholders. Our goal is to compute six different premiums for each of
these combinations. Once the premium for each row has been determined
using the given exposure and claim counts, the insurer can replace the
last two columns in \protect\hyperlink{tab:8.2}{Table 8.2} with a single
column containing the computed premiums. This new table then can serve
as a manual to determine the premium for a new policyholder given the
rating factors during the underwriting process. In non-life insurance, a
table (or a set of tables) or list that contains each set of rating
factors and the associated premium is referred to as a tariff. Each
unique combination of the rating factors in a tariff is called a
\emph{tariff cell}; thus, in \protect\hyperlink{tab:8.2}{Table 8.2} the
number of tariff cells is six, same as the number of risk classes.

\[
{\small
\begin{matrix}
\begin{array}{ccrrc}
 \hline
\text{Rating} &\text{factors}  &   \text{Exposure} & \text{Claim count} \\
\text{Type }(j) & \text{Age }(k) &  \text{in year} & \text{observed}\\
\hline \hline
j=1 & k=1 &  89.1 & 9\\
1 & 2   & 208.5& 8\\
1 & 3  & 155.2 & 6  \\
2  & 1  & 19.3 & 1 \\
2  & 2  & 360.4 & 13 \\
2   & 3  & 276.7 & 6 \\ \hline
\end{array}
\end{matrix}
}
\]

\protect\hyperlink{tab:8.2}{Table 8.2} : Loss record of the illustrative
auto insurer

Let us now look at the loss information in
\protect\hyperlink{tab:8.2}{Table 8.2} more closely. The exposure in
each row represents the sum of the length of insurance coverages, or
in-force times, in the unit of year, of all the policyholders in that
tariff cell. Similarly the claim counts in each row is the number of
claims at each cell. Naturally the exposures and claim counts vary due
to the different number of drivers across the cells, as well as
different in-force time periods among the drivers within each cell.

In light of the Poisson regression framework, we denote the exposure and
claim count of cell \((j,k)\) as \(m_{jk}\) and \(y_{jk}\),
respectively, and define the claim count per unit exposure as

\begin{equation}
\nonumber
z_{jk}= \frac{y_{jk}}{ m_{jk}}, \qquad j=1,2;\, k=1, 2,3.
\end{equation}

For example, \(z_{12}=8/208.5=0.03837\), meaning that a policyholder in
tariff cell (1,2) would have 0.03837 accidents if insured for a full
year on average. The set of \(z_{ij}\) values then corresponds to the
rate parameter in the Poisson distribution \eqref{eq:Pois-pmf-2} as they
are the event occurrence rates per unit exposure. That is, we have
\(z_{jk}=\hat{\lambda}_{jk}\) where \({\lambda}_{jk}\) is the Poisson
rate parameter. Producing \(z_{ij}\) values however does not do much
beyond comparing the average loss frequencies across risk classes. To
fully exploit the dataset, we will construct a pricing model from
\protect\hyperlink{tab:8.2}{Table 8.2} using the Poisson regression, for
the remaining part of the chapter.

We comment that actual loss records used by insurers typically include
much more risk factors, in which case the number of cells grows
exponentially. The tariff would then consist of a set of tables, instead
of one, separated by some of the basic rating factors, such as sex or
territory.

\subsection{Multiplicative Tariff
Model}\label{multiplicative-tariff-model}

In this subsection, we introduce the multiplicative tariff model, a
popular pricing structure that can be naturally used within the Poisson
regression framework. The developments here are based on
\protect\hyperlink{tab:8.2}{Table 8.2}. Recall that the loss count of a
policyholder is described by the Poisson regression model with rate
\(\lambda\) and the exposure \(m\), so that the expected loss count
becomes \(m\lambda\). As \(m\) is a known constant, we are essentially
concerned with modelling \(\lambda\), so that it responds to the change
in the rating factors. Among other possible functional forms, we
commonly choose the multiplicative\footnote{Preferring the
  multiplicative form to others (e.g., additive one) was already hinted
  in \eqref{eq:log-lin-mu}.} relation to model the Poisson rate
\(\lambda_{jk}\) for rating factor (\(j,k\)):

\begin{equation}
\lambda_{jk}= f_0 \times f_{1j} \times f_{2k}, \qquad j=1,2;\, k=1, 2,3.
\label{eq:multiplicative-tarrif}
\end{equation}

Here \(\{ f_{1j}, j=1,2\}\) are the parameters associated with the two
levels in the first rating factor, car type, and
\(\{ f_{2k}, k=1,2,3\}\) associated with the three levels in the age
band, the second rating factor. For instance, the Poisson rate for a
mid-aged policyholder with a Type B vehicle is given by
\(\lambda_{22}=f_0 \times f_{12} \times f_{22}\). The first term \(f_0\)
is some base value to be discussed shortly. Thus these six parameters
are understood as numerical representations of the levels within each
rating factor, and are to be estimated from the dataset.

The multiplicative form \eqref{eq:multiplicative-tarrif} is easy to
understand and use, because it clearly shows how the expected loss count
(per unit exposure) changes as each rating factor varies. For example,
if \(f_{11}=1\) and \(f_{12}=1.2\), then the expected loss count of a
policyholder with a vehicle of type B would be 20\(\%\) larger than type
A, when the other factors are the same. In non-life insurance, the
parameters \(f_{1j}\) and \(f_{2k}\) are known as relativities as they
determine how much expected loss should change relative to the base
value \(f_0\). The idea of relativity is quite convenient in practice,
as we can decide the premium for a policyholder by simply multiplying a
series of corresponding relativities to the base value.

Dropping an existing rating factor or adding a new one is also
transparent with this multiplicative structure. In addition, the insurer
may easily adjust the overall premium for all policyholders by
controlling the base value \(f_0\) without changing individual
relativities. However, by adopting the multiplicative form, we
implicitly assume that there is no serious interaction among the risk
factors.

When the multiplicative form is used we need to address an
identification issue. That is, for any \(c>0\), we can write

\begin{equation}
\lambda_{jk}= f_0 \times \frac{f_{1j}}{c} \times c\,f_{2k}. 
\end{equation}

By comparing with \eqref{eq:multiplicative-tarrif}, we see that the
identical rate parameter \(\lambda_{jk}\) can be obtained for very
different individual relativities. This over-parametrization, meaning
that many different sets of parameters arrive at the identical model,
obviously calls for some restriction on \(f_{1j}\) and \(f_{2k}\). The
standard practice is to make one relativity in each rating factor equal
to one. This can be made arbitrarily in theory, but the standard
practice is to make the relativity of most common class (base class)
equals to one. We will assume that \emph{type A vehicles} and
\emph{young drivers} to be the most common classes, that is,
\(f_{11} = 1\) and \(f_{21} = 1\). This way all other relativities are
uniquely determined. The tariff cell \((j,k)=(1,1)\) is then called the
base tariff cell, where the rate simply becomes \(\lambda_{11}=f_0\),
corresponding to the base value according to
\eqref{eq:multiplicative-tarrif}. Thus the base value \(f_0\) is generally
interpreted as the Poisson rate of the base tariff cell.

Again, \eqref{eq:multiplicative-tarrif} is log-transformed and rewritten
as

\begin{equation}
\log \lambda_{jk}= \log f_0 + \log f_{1j} + \log f_{2k},
\label{eq:log-linear-tariff}
\end{equation}

as it is easier to work with in estimating process, similar to
\eqref{eq:mean-ft-Pois-2}. This log linear form makes the log relativities
of the base level in each rating factor equal to zero, i.e.,
\(\log f_{11}=\log f_{21}=0\), and leads to the following alternative,
more explicit expression for \eqref{eq:log-linear-tariff}:

\begin{equation}
\log \lambda=\begin{cases}
      \log f_0 + \quad 0 \quad \,\,+ \quad 0 \quad \,\,& \text{for a policy in cell $(1,1)$}, \\
            \log f_0+ \quad 0 \quad \,\,+\log f_{22}& \text{for a policy in cell $(1,2)$}, \\
                  \log f_0+ \quad 0 \quad \,\,+\log f_{23}& \text{for a policy in cell $(1,3)$}, \\
                        \log f_0+\log f_{12}+ \quad 0 \quad \,\,& \text{for a policy in cell $(2,1)$}, \\
                              \log f_0+\log f_{12}+\log f_{22}& \text{for a policy in cell $(2,2)$}, \\
                                    \log f_0+\log f_{12}+\log f_{23}& \text{for a policy in cell $(2,3)$}. \\
\end{cases}
\label{eq:log-rate-Poi-tariff-3}
\end{equation}

This clearly shows that the Poisson rate parameter \(\lambda\) varies
across different tariff cells, with the same log linear form used in the
Poisson regression framework. In fact the reader may see that
\eqref{eq:log-rate-Poi-tariff-3} is an extended version of the early
expression \eqref{eq:ind-mu} with multiple risk factors and that the log
relativities now play the role of \(\beta_i\) parameters. Therefore all
the relativities can be readily estimated via fitting a Poisson
regression with a suitably chosen set of indicator variables.

\subsection{Poisson Regression for Multiplicative
Tariff}\label{poisson-regression-for-multiplicative-tariff}

\textbf{Indicator Variables for Tariff Cells}

We now explain how the relativities can be incorporated in the Poisson
regression. As seen early in this chapter we use indicator variables to
deal with categorical variables. For our illustrative auto insurer,
therefore, we define an indicator variable for the first rating factor
as

\begin{equation}
x_1=
\begin{cases}
      1 & \text{ for vehicle type B}, \\
      0 & \text{ otherwise}.
\end{cases}
\end{equation}

For the second rating factor, we employ two indicator variables for the
age band, that is,

\begin{equation}
x_2=
\begin{cases}
     1 & \text{for age band 2}, \\
     0 & \text{otherwise}.
\end{cases}
\end{equation}

and

\begin{equation}
x_3=
\begin{cases}
     1 & \text{for age band 3}, \\
     0 & \text{otherwise}.
\end{cases}
\end{equation}

The triple \((x_1, x_2, x_3)\) then can effectively and uniquely
determine each risk class. By observing that the indicator variables
associated with Type A and Age band 1 are omitted, we see that tariff
cell \((j,k)=(1,1)\) plays the role of the base cell. We emphasize that
our choice of the three indicator variables above has been carefully
made so that it is consistent with the choice of the base levels in the
multiplicative tariff model in the previous subsection (i.e.,
\(f_{11}=1\) and \(f_{21}=1\)).\\

With the proposed indicator variables we can rewrite the log rate
\eqref{eq:log-linear-tariff} as

\begin{equation}
\log \lambda_{}= \log f_0+ \log f_{12}  \times x_1 + \log f_{22} \times x_2 +\log f_{23} \times x_3,
\label{eq:log-linear-tariff-3}
\end{equation}

which is identical to \eqref{eq:log-rate-Poi-tariff-3} when each triple
value is actually applied. For example, we can verify that the base
tariff cell \((j,k)=(1,1)\) corresponds to \((x_1, x_2,x_3)=(0, 0, 0)\),
and in turn produces \(\log \lambda=\log f_0\) or \(\lambda= f_0\) in
\eqref{eq:log-linear-tariff-3} as required.

\textbf{Poisson regression for the tariff model}\\
Under this specification, let us consider \(n\) policyholders in the
portfolio with the \(i\)th policyholder's risk characteristic given by a
vector of explanatory variables
\(\mathbf{ x}_i=(x_{i1}, x_{i2},x_{i3})^{\prime}\), for
\(i=1, \ldots, n\). We then recognize \eqref{eq:log-linear-tariff-3} as

\begin{equation}
\log \lambda_{i}= \beta_0+ \beta_1 \, x_{i1} + \beta_{2} \, x_{i2} +\beta_3  \, x_{i3}=\mathbf{ x}^{\prime}_i\beta, \qquad i=1, \ldots, n,
\end{equation}

where \(\beta_0, \ldots, \beta_3\) can be mapped to the corresponding
log relativities in \eqref{eq:log-linear-tariff-3}. This is exactly the
same setup as in \eqref{eq:mean-ft-Pois-7} except for the exposure
component. Therefore, by incorporating the exposure in each risk class,
the Poisson regression model for this multiplicative tariff model
finally becomes

\begin{equation}
\log \mu_i=\log \lambda_{i}+\log m_i= \log m_i+ \beta_0+ \beta_1 \, x_{i1} + \beta_{2} \, x_{i2} +\beta_3  \, x_{i3}=\log m_i+\mathbf{ x}^{\prime}_i\beta, 
\end{equation}

for \(i=1, \ldots, n\). As a result, the relativities are given by

\begin{equation}
{f}_0=e^{\beta_0}, \quad {f}_{12}=e^{\beta_1}, \quad {f}_{22}=e^{\beta_2} \quad \text{and}\quad {f}_{23}=e^{\beta_3},
\label{eq:relativity-1}
\end{equation}

with \(f_{11}=1\) and \(f_{21}=1\) from the original construction. For
the actual dataset, \(\beta_i\), \(i=0,1, 2, 3\), is replaced with the
\emph{mle} \(b_i\) using the method in the technical supplement at the
end of this chapter (Section 8.A).

\subsection{Numerical Examples}\label{numerical-examples}

We present two numerical examples of the Poisson regression. In the
first example we construct a Poisson regression model from
\protect\hyperlink{tab:8.2}{Table 8.2}, which is a dataset of a
hypothetical auto insurer. The second example uses an actual industry
dataset with more risk factors. As our purpose is to show how the
Poisson regression model can be used under a given classification rule,
we are not concerned with the quality of the Poisson model fit in this
chapter.

\textbf{Example 8.1: Poisson regression for the illustrative auto
insurer}

In the last few subsections we considered a dataset of a hypothetical
auto insurer with two risk factors, as given in
\protect\hyperlink{tab:8.2}{Table 8.2}. We now apply the Poisson
regression model to this dataset. As done before, we have set
\((j,k)=(1,1)\) as the base tariff cell, so that \(f_{11}=f_{21}=1\).
The result of the regression gives the coefficient estimates
\((b_0, b_1,b_2,b_3)=(-2.3359, -0.3004, -0.7837, -1.0655 )\), which in
turn produces the corresponding relativities

\begin{equation}
\nonumber
{f}_0=0.0967, \quad {f}_{12}=  0.7405, \quad {f}_{22}=0.4567 \quad \text{and}\quad {f}_{23}=0.3445.
\end{equation}

from the relation given in \eqref{eq:relativity-1}. The \texttt{R} script
and the output are as follows.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 8.2. Poisson regression for Singapore insurance claims
data}

This actual dataset is a subset of the data used by
\citep{frees2008hierarchical}. The data are from the General Insurance
Association of Singapore, an organisation consisting of non-life
insurers in Singapore. The data contains the number of car accidents for
\(n=7,483\) auto insurance policies with several categorical explanatory
variables and the exposure for each policy. The explanatory variables
include four risk factors: the type of the vehicle insured (either
automobile (A) or other (O), denoted by \(\tt{Vtype}\)), the age of the
vehicle in years (\(\tt{Vage}\)), gender of the policyholder
(\(\tt{Sex}\)) and the age of the policyholder (in years, grouped into
seven categories, denoted \(\tt{Age}\)).

Based on the data description, there are several things to remember
before constructing a model. First, there are 3,842 policies with
vehicle type A (automobile) and 3,641 policies with other vehicle types.
However, age and sex information is available for the policies of
vehicle type A only; the drivers of all other types of vehicles are
recorded to be aged 21 or less with sex unspecified, except for one
policy, indicating that no driver information has been collected for
non-automobile vehicles. Second, type A vehicles are all classified as
private vehicles and all the other types are not.

When we include these risk factors, we assume all unspecified sex to be
male. As the age information is only applicable to type A vehicles, we
set the model accordingly. That is, we apply the age variable only to
vehicles of type A. Also we used five vehicle age bands, simplifying the
original seven bands, by combining vehicle ages 0,1 and 2; the combined
band is marked as level 2\footnote{corresponding to
  \(\texttt{VAgecat1}\)} in the data file. Thus our Poisson model has
the following explicit form:

\begin{align*}
\log \mu_i= \mathbf{ x}^{\prime}_i\beta+&\log m_i=\beta_0+\beta_1 I(Sex_i=M)+ \sum_{t=2}^6 \beta_t\, I(Vage_i=t) \\
&+  \sum_{t=7}^{13} \beta_t \,I(Vtype_i=A)\times I(Age_i=t-7)+\log m_i.
\end{align*}

The fitting result is given in \protect\hyperlink{tab:8.3}{Table 8.3},
for which we have several comments.

\begin{itemize}
\tightlist
\item
  The claim frequency is higher for male by 17.3\%, when other rating
  factors are held fixed. However, this may have been affected by the
  fact that all unspecified sex has been assigned to male.\\
\item
  Regarding the vehicle age, the claim frequency gradually decreases as
  the vehicle gets old, when other rating factors are held fixed. The
  level starts from 2 for this variable but, again, the numbering is
  nominal and does not affect the numerical result.\\
\item
  The policyholder age variable only applies to type A (automobile)
  vehicle, and there is no policy in the first age band. We may
  speculate that younger drivers less than age 21 drive their parents'
  cars rather than having their own because of high insurance premiums
  or related regulations. The missing relativity may be estimated by
  some interpolation or the professional judgement of the actuary. The
  claim frequency is the lowest for age band 3 and 4, but gets
  substantially higher for older age bands, a reasonable pattern seen in
  many auto insurance loss datasets.
\end{itemize}

We also note that there is no base level in the policyholder age
variable, in the sense that no relativity is equal to 1. This is because
the variable is only applicable to vehicle type A. This does not cause a
problem numerically, but one may set the base relativity as follows if
necessary for other purposes. Since there is no policy in age band 0, we
consider band 1 as the base case. Specifically, we treat its relativity
as a product of 0.918 and 1, where the former is the common relativity
(that is, the common premium reduction) applied to all policies with
vehicle type A and the latter is the base value for age band 1. Then the
relativity of age band 2 can be seen as \(0.917=0.918 \times 0.999\),
where 0.999 is understood as the relativity for age band 2. The
remaining age bands can be treated similarly.

\[
{\small 
\begin{matrix}
\begin{array}{clcc}
\hline
\text{Rating factor} & \text{Level} & \text{Relativity in the tariff} & \text{Note}\\ \hline\hline
\text{Base value}  &  & 0.167 & f_0\\ \hline
\text{Sex} & 1 (F) & 1.000 & \text{Base level}\\
 & 2 (M) & 1.173 &\\\hline
 \text{Vehicle age} & 2 (0-2\text{ yrs}) & 1.000 & \text{Base level}\\
  & 3 (3-5\text{ yrs}) & 0.843 \\
  & 4 (6-10\text{ yrs}) & 0.553 \\
  & 5 (11-15\text{ yrs}) & 0.269 \\
  & 6 (16+\text{ yrs}) & 0.189 &\\\hline
  \text{Policyholder age} & 0 (0-21) & \text{N/A} & \text{No policy} \\
  \text{(Only applicable to} & 1 (22-25) & 0.918 \\
 \text{vehicle type A)}  & 2 (26-35) & 0.917 \\
  & 3 (36-45) & 0.758 \\
  & 4 (46-55) & 0.632 \\
  & 5 (56-65) &  1.102\\
  & 6 (65+) & 1.179\\ \hline \hline
\end{array}
\end{matrix}
}
\]

\protect\hyperlink{tab:8.3}{Table 8.3} : Singapore insurance claims data

Let us try several examples based on \protect\hyperlink{tab:8.3}{Table
8.3}. Suppose a male policyholder aged 40 who owns a 7-year-old vehicle
of type A. The expected claim frequency for this policyholder is then
given by

\begin{equation}
\lambda=0.167 \times 1.173 \times 0.553 \times 0.758 = 0.082.
\end{equation}

As another example consider a female policyholder aged 60 who owns a
3-year-old vehicle of type O. The expected claim frequency for this
policyholder is

\begin{equation}
\lambda=0.167 \times 1 \times 0.843  = 0.141.
\end{equation}

Note that for this policy the age band variable is not used as the
vehicle type is not A. The \texttt{R} script is given as follows.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

As a concluding remark, we comment that the Poisson regression is not
the only possible count regression model. Actually, the Poisson
distribution can be restrictive in the sense that it has a single
parameter and its mean and the variance are always equal. There are
other count regression models that allow more flexible distributional
structure, such as negative binomial regressions and zero-inflated (ZI)
regressions; details of these alternative regressions can be found in
other texts listed in the next section.

\section{Further Resources and
Contributors}\label{RC:further-reading-and-resources}

\subsubsection*{Further Reading and
References}\label{further-reading-and-references}
\addcontentsline{toc}{subsubsection}{Further Reading and References}

The Poisson regression is a special member of a more general regression
model class known as the generalized linear model (glm). The glm
develops a unified regression framework for datasets when the response
variables are continuous, binary or discrete. The classical linear
regression model with normal error is also a member of the glm. There
are many standard statistical texts dealing with the glm, including
\citep{mccullagh1989generalized}. More accessible texts are
\citep{dobson2008introduction}, \citep{agresti1996introduction} and
\citep{faraway2016extending}. For actuarial and insurance applications
of the glm see \citep{frees2009regression}, \citep{de2008generalized}.
Also, \citep{ohlsson2010non} discusses the glm in non-life insurance
pricing context with tariff analyses.

\subsubsection*{Contributor}\label{contributor-1}
\addcontentsline{toc}{subsubsection}{Contributor}

\begin{itemize}
\tightlist
\item
  \textbf{Joseph H. T. Kim}, Yonsei University, is the principal author
  of the initial version of this chapter. Email:
  \href{mailto:jhtkim@yonsei.ac.kr}{\nolinkurl{jhtkim@yonsei.ac.kr}} for
  chapter comments and suggested improvements.
\item
  Chapter reviewers include: Chun Yong Chew, Lina Xu, Jeffrey Zheng.
\end{itemize}

\subsection*{TS 8.A. Estimating Poisson Regression
Models}\label{ts-8.a.-estimating-poisson-regression-models}
\addcontentsline{toc}{subsection}{TS 8.A. Estimating Poisson Regression
Models}

The principles of maximum likelihood estimation (\emph{mle}) are
introduced in Sections \ref{S:parameter-estimation} and
\ref{S:MaxLikeEstimation}, defined in Section \ref{S:AppA:MLE}, and
theoretically developed in Chapter \ref{C:AppC}. Here we present the
\emph{mle} procedure of the Poisson regression so that the reader can
see how the explanatory variables are treated in maximizing the
likelihood function in the regression setting.

\textbf{Maximum Likelihood Estimation for Individual Data}

In the Poisson regression the varying Poisson mean is determined by
parameters \(\beta_i\)'s, as shown in \eqref{eq:mean-ft-Pois-7}. In this
subsection we use the maximum likelihood method to estimate these
parameters. Again, we assume that there are \(n\) policyholders and the
\(i\)th policyholder is characterized by
\(\mathbf{ x}_i=(1, x_{i1}, \ldots, x_{ik})^{\prime}\) with the observed
loss count \(y_i\). Then, from \eqref{eq:mean-ft-Pois-6} and
\eqref{eq:mean-ft-Pois-7}, the log-likelihood function of vector
\(\beta=(\beta_0, \dots, \beta_k)\) is given by

\begin{align}
\nonumber \log L(\beta)    &= l(\beta)=\sum^n_{i=1} \left( -\mu_i +y_i \, \log \mu_i -\log y_i! \right)  \\
    &  = \sum^n_{i=1} \left( -m_i \exp(\mathbf{ x}^{\prime}_i\beta) +y_i \,(\log m_i+\mathbf{ x}^{\prime}_i\beta)  -\log y_i! \right)
\label{eq:ll-Poi-reg}
\end{align}

To obtain the \emph{mle} of
\(\beta=(\beta_0, \ldots, \beta_k)^{\prime}\), we
differentiate\footnote{We use matrix derivative here.} \(l(\beta)\) with
respect to vector \(\beta\) and set it to zero:

\begin{equation}
\frac{\partial}{\partial \beta}l(\beta)\Bigg{|}_{\beta=\mathbf{b}}=\sum^n_{i=1} \left(y_i -m_i \exp(\mathbf{ x}^{\prime}_i \mathbf{ b}) \right)\mathbf{ x}_i=\mathbf{ 0}.
\label{eq:score-ft-Poi}
\end{equation}

Numerically solving this equation system gives the \emph{mle} of
\(\beta\), denoted by \(\mathbf{ b}=(b_0, b_1, \ldots, b_k)^{\prime}\).
Note that, as \(\mathbf{ x}_i=(1, x_{i1}, \ldots, x_{ik})^{\prime}\) is
a column vector, equation \eqref{eq:score-ft-Poi} is a system of \(k+1\)
equations with both sides written as column vectors of size \(k+1\). If
we denote \(\hat{\mu}_i=m_i \exp(\mathbf{ x}^{\prime}_i \mathbf{ b})\),
we can rewrite \eqref{eq:score-ft-Poi} as

\begin{equation}
\sum^n_{i=1} \left(y_i -\hat{\mu}_i \right)\mathbf{ x}_i=\mathbf{ 0}.
\end{equation}

Since the solution \(\mathbf{ b}\) satisfies this equation, it follows
that the first among the array of \(k+1\) equations, corresponding to
the first constant element of \(\mathbf{ x}_i\), yields

\begin{equation}
\sum^n_{i=1}\left( y_i -\hat{\mu}_i \right)\times 1={ 0},
\end{equation}

which implies that we must have

\begin{equation}
n^{-1}\sum_{i=1}^n y_i =\bar{y}=n^{-1}\sum_{i=1}^n \hat{\mu}_i.
\end{equation}

This is an interesting property saying that the average of the
individual losses, \(\bar{y}\), is same as the average of the estimated
values. That is, the sample mean is preserved under the fitted Poisson
regression model.

\textbf{Maximum Likelihood Estimation for Grouped Data}

Sometimes the data are not available at the individual policy level. For
example, \protect\hyperlink{tab:8.2}{Table 8.2} provides collective loss
information for each risk class after grouping individual policies. When
this is the case, \(y_i\) and \(m_i\), the quantities needed for the
\emph{mle} calculation in \eqref{eq:score-ft-Poi}, are unavailable for
each \(i\). However this does not pose a problem as long as we have the
total loss counts and total exposure for each risk class.

To elaborate, let us assume that there are \(K\) different risk classes,
and further that, in the \(k\)th risk class, we have \(n_k\) policies
with the total exposure \(m_{(k)}\) and the average loss count
\(\bar{y}_{(k)}\), for \(k=1, \ldots, K\); the total loss count for the
\(k\)th risk class is then \(n_k\, \bar{y}_{(k)}\). We denote the set of
indices of the policies belonging to the \(k\)th class by \(C_k\). As
all policies in a given risk class share the same risk characteristics,
we may denote \(\mathbf{ x}_i=\mathbf{ x}_{(k)}\) for all \(i \in C_k\).
With this notation, we can rewrite \eqref{eq:score-ft-Poi} as

\begin{align}
\nonumber \sum^n_{i=1} \left(y_i -m_i \exp(\mathbf{ x}^{\prime}_i \mathbf{ b}) \right)\mathbf{ x}_i &= \sum^K_{k=1}\Big{\{}\sum_{i \in C_k} \left(y_i -m_i \exp(\mathbf{ x}^{\prime}_i \mathbf{ b}) \right)\mathbf{ x}_i  \Big{\}} \\
\nonumber     &  =\sum^K_{k=1}\Big{\{} \sum_{i \in C_k} \left(y_i -m_i \exp(\mathbf{ x}^{\prime}_{(k)} \mathbf{ b}) \right)\mathbf{ x}_{(k)}  \Big{\}} \\
\nonumber     &  =\sum^K_{k=1}\Big{\{}  \Big(\sum_{i \in C_k}y_i -\sum_{i \in C_k}m_i \exp(\mathbf{ x}^{\prime}_{(k)} \mathbf{ b}) \Big)\mathbf{ x}_{(k)}  \Big{\}} \\
      &  =\sum^K_{k=1} \Big(n_k\, \bar{y}_{(k)}-m_{(k)} \exp(\mathbf{ x}^{\prime}_{(k)} \mathbf{ b}) \Big)\mathbf{ x}_{(k)} =0.
\label{eq:score-ft-Poi-2}
\end{align}

Since \(n_k\, \bar{y}_{(k)}\) in \eqref{eq:score-ft-Poi-2} represents the
total loss count for the \(k\)th risk class and \(m_{(k)}\) is its total
exposure, we see that for the Poisson regression the \emph{mle}
\(\mathbf{ b}\) is the same whether if we use the individual data or the
grouped data.

\textbf{Information matrix}\\
Section \ref{S:AppC:LF} defines information matrices. Taking second
derivatives to \eqref{eq:ll-Poi-reg} gives the information matrix of the
\emph{mle} estimators,

\begin{equation}
\mathbf{ I}(\beta)=-\mathrm{E~}{\left( \frac{\partial^2}{\partial \beta\partial \beta^{\prime}}l(\beta) \right)}=\sum^n_{i=1}m_i \exp(\mathbf{ x}^{\prime}_i \mathbf{ \beta})\mathbf{ x}_i \mathbf{ x}_i^{\prime}=\sum^n_{i=1} {\mu}_i \mathbf{ x}_i \mathbf{ x}_i^{\prime}.
\label{eq:Inf-mtx-Poi}
\end{equation}

For actual datasets, \({\mu}_i\) in \eqref{eq:Inf-mtx-Poi} is replaced
with \(\hat{\mu}_i=m_i \exp(\mathbf{ x}^{\prime}_i \mathbf{ b})\) to
estimate the relevant variances and covariances of the \emph{mle}
\(\mathbf{ b}\) or its functions.

For grouped datasets, we have

\begin{equation}
\mathbf{ I}(\beta)=\sum^K_{k=1} \Big{\{}\sum_{i \in C_k}m_i \exp(\mathbf{ x}^{\prime}_i \mathbf{ \beta})\mathbf{ x}_i \mathbf{ x}_i^{\prime} \Big{\}}=\sum^K_{k=1} m_{(k)} \exp(\mathbf{ x}^{\prime}_{(k)} \mathbf{ \beta})\mathbf{ x}_{(k)} \mathbf{ x}_{(k)}^{\prime}.
\end{equation}

\subsection*{TS 8.B. Selecting Rating
Factors}\label{ts-8.b.-selecting-rating-factors}
\addcontentsline{toc}{subsection}{TS 8.B. Selecting Rating Factors}

A complete discussion of rating factor selection is beyond the scope of
this book. In addition to technical analyses, you have to think
carefully about the type of business (personal, commercial) as well as
the regulatory landscape. Nonetheless, a broad overview of some key
concerns may serve to ground the reader as one thinks about the pricing
of insurance contracts.

\subsubsection*{Statistical Criteria}\label{statistical-criteria}
\addcontentsline{toc}{subsubsection}{Statistical Criteria}

From an analyst's perspective, the discussion starts with the
statistical significance of a rating factor. If the factor is not
statistically significant, then the variable is not even worthy of
consideration for inclusion in a rating plan. The statistical
significance is judged not only on an in-sample basis but also on how
well it fares on an out-of-sample basis, as per our discussion in
\textbf{Section 4.2.}

It is common in insurance applications to have many rating factors.
Handling multivariate aspects can be difficult with traditional
univariate methods. Analysts employ techniques such as \emph{generalized
linear models} as described in \emph{Section 8.3}.

Rating factors are introduced are use to create cells that contain
similar risks. A rating group should be large enough to measure costs
with sufficient accuracy. There is an inherent trade-off between
theoretical accuracy and homogeneity

As an example, most insurers charge the same automobile insurance
premiums for drivers between the ages of 30 and 50, not varying the
premium by age. Presumably costs do not vary much by age, or cost
variances are due to other identifiable factors.

\subsubsection*{Operational Criteria}\label{operational-criteria}
\addcontentsline{toc}{subsubsection}{Operational Criteria}

From a business perspective, statistical criteria only provide a
starting point for discussions of potential inclusion of rating factors.
Inclusion of a rating factor must also induce economically meaningful
results. From an insured's perspective, if differentiation by a factor
produces little change in a rate then it is not worth including. From an
insurer's perspective, the inclusion of a factor should help segment the
marketplace in a way that helps attract the business that they seek. For
example, we introduce the Gini index in Section \ref{S:GiniStatistic} as
one metric that insurers use to describe the financial impact of a
rating variable.

Rating factors should also be objective, inexpensive to administer, and
verifiable. For example, automobile insurance underwriters often talk of
``maturity'' and ``responsibility'' as important criteria for youthful
drivers. Yet, these are difficult to define objectively and to apply
consistently. As another example, in automobile it has long been known
that amount of miles (or kilometers) driven is an excellent rating
factor. However, insurers have been reluctant to adopt this factor
because it is subject to abuse. Historically, driving mileage has not
been used because of the difficulty in verifying this variable (it is
far too easy to alter the car's odometer to change reported mileage).
Going forward, modern day drivers and cars are equipped with global
positioning devices and other equipment that allow insurers to use
distance driven as a rating factor because it can be verified.

\subsubsection*{Rating Factors from the Perspective of a
Consumer}\label{rating-factors-from-the-perspective-of-a-consumer}
\addcontentsline{toc}{subsubsection}{Rating Factors from the Perspective
of a Consumer}

Insurance companies sell insurance products to a variety of consumers;
consequently, companies are affected by public perception. On the one
hand, free market competition dictates rating factors that insurers use,
as is common in commercial insurance. On the other hand, insurance may
be required by law. This is common in personal insurance such as third
party automobile liability and homeowners. In these instances, the
mandatory and de facto mandatory purchase of insurance may mean that
free market competition is insufficient to protect policyholders. Here,
the following items affect the social acceptability of using a
particular risk characteristic as a rating variable:

\begin{itemize}
\tightlist
\item
  Affordability - introduction of some variables may be mitigated by
  resulting high costs of insurance.
\item
  Causality - other things being equal, a rating variable is easier to
  justify if there is a ``causal'' relationship with losses. A good
  example is the effects of smoking in life insurance. For many years,
  this factor was viewed with suspicion by the industry. However, over
  time, scientific evidence provided overwhelming evidence as this an
  important predictor of mortality.
\item
  Controllability - A controllable variable is one that is under the
  control of the insured, e.g., installing burglar alarms. The use of
  controllable rating variables encourages accident prevention.
\item
  Privacy concerns - people are reluctant to disclose personal
  information. In today's world with increasing emphasis on social media
  and the availability of personal information, consumer advocates are
  concerned that the benefits of big data skew heavily in insurers'
  favor. They reason that insureds do not have equivalent new tools to
  compare quality of coverage/policies and performance of insurance
  companies.
\end{itemize}

\textbf{Example: Youthful Drivers.} In some cases, a particular risk
characteristic may identify a small group of insureds whose risk level
is extremely high, and if used as a rating variable, the resulting
premium may be unaffordable for that high-risk class. To the extent that
this occurs, companies may wish to or be required by regulators to
combine classes and introduce subsidies. For example, 16-year-old
drivers are generally higher risk than 17-year-old drivers. Some
companies have chosen to use the same rates for 16- and 17-year-old
drivers to minimize the affordability issues that arise when a family
adds a 16-year-old to the auto policy.

\subsubsection*{Societal Effects of Rating
Factors}\label{societal-effects-of-rating-factors}
\addcontentsline{toc}{subsubsection}{Societal Effects of Rating Factors}

With public discussions of rating factors, it is also important to think
about the societal effects of classification.

For example, does a rating variable encourage ``good'' behavior? As an
example, we return to the use of distance driven as a rating factor.
Many people advocate for including this variable as a factor. The
motivation is that if insurance, like fuel, is priced based on distance
driven, this will induce consumers to reduce the amount driven, thereby
benefitting society.

One can consider other aspects of societal effects of classification,
see, for example, \citet{niehaus2003risk}:

\begin{itemize}
\tightlist
\item
  Re-distributive Effects - provide a cross-subsidy from e.g., high
  risks to low risks
\item
  Classification Costs - Money spent by society, insurers, to classify
  people appropriately.
\end{itemize}

\subsubsection*{Legal Criteria}\label{legal-criteria}
\addcontentsline{toc}{subsubsection}{Legal Criteria}

For example, some states have statutes prohibiting the use of gender in
rating insurance while others permit it as a rating variable. As a
result, an insurer writing in multiple states may include gender as a
rating variable in those states where it is permitted, but not include
it in a state that prohibits its use for rating.

If allowed by law, the company may continue to charge the average rate
but utilize the characteristic to identify, attract, and select the
lower-risk insureds that exist in the insured population; this is called
\emph{skimming the cream.}

\chapter{Experience Rating Using Credibility
Theory}\label{C:Credibility}

\emph{Chapter Preview.} This chapter introduces credibility theory which
is an important actuarial tool for estimating pure premiums,
frequencies, and severities for individual risks or classes of risks.
Credibility theory provides a convenient framework for combining the
experience for an individual risk or class with other data to produce
more stable and accurate estimates. Several models for calculating
credibility estimates will be discussed including limited fluctuation,
Bühlmann, Bühlmann-Straub, and nonparametric and semiparametric
credibility methods. The chapter will also show a connection between
credibility theory and Bayesian estimation which was introduced in
Chapter \ref{C:ModelSelection}.

\section{Introduction to Applications of Credibility
Theory}\label{introduction-to-applications-of-credibility-theory}

What premium should be charged to provide insurance? The answer depends
upon the exposure to the risk of loss. A common method to compute an
insurance premium is to rate an insured using a classification rating
plan. A classification plan is used to select an insurance rate based on
an insured's rating characteristics such as geographic territory, age,
etc. All classification rating plans use a limited set of criteria to
group insureds into a ``class'' and there will be variation in the risk
of loss among insureds within the class.

An experience rating plan attempts to capture some of the variation in
the risk of loss among insureds within a rating class by using the
insured's own loss experience to complement the rate from the
classification rating plan. One way to do this is to use a credibility
weight \(Z\) with \(0\leq Z \leq 1\) to compute

\begin{equation*} 
\hat{R}=Z\bar{X}+(1-Z)M,
\end{equation*}

\begin{eqnarray*}
\hat{R}&=&\textrm{credibility weighted rate for risk,}\\
           \bar{X}&=&\textrm{average loss for the risk over a specified time period,}\\
                  M&=&\textrm{the rate for the classification group, often called the manual rate.}\\
\end{eqnarray*}

For a risk whose loss experience is stable from year to year, \(Z\)
might be close to 1. For a risk whose losses vary widely from year to
year, \(Z\) may be close to 0.

Credibility theory is also used for computing rates for individual
classes within a classification rating plan. When classification plan
rates are being determined, some or many of the groups may not have
sufficient data to produce stable and reliable rates. The actual loss
experience for a group will be assigned a credibility weight \(Z\) and
the complement of credibility \(1-Z\) may be given to the average
experience for risks across all classes. Or, if a class rating plan is
being updated, the complement of credibility may be assigned to the
current class rate. Credibility theory can also be applied to the
calculation of expected frequencies and severities.

Computing numeric values for \(Z\) requires analysis and understanding
of the data. What are the variances in the number of losses and sizes of
losses for risks? What is the variance between expected values across
risks?

\section{Limited Fluctuation
Credibility}\label{limited-fluctuation-credibility}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Calculate full credibility standards for number of claims, average
  size of claims, and aggregate losses.
\item
  Learn how the relationship between means and variances of underlying
  distributions affects full credibility standards.
\item
  Determine credibility-weight \(Z\) using the square-root partial
  credibility formula.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Limited fluctuation credibility, also called ``classical credibility'',
was given this name because the method explicitly attempts to limit
fluctuations in estimates for claim frequencies, severities, or losses.
For example, suppose that you want to estimate the expected number of
claims \(N\) for a group of risks in an insurance rating class. How many
risks are needed in the class to ensure that a specified level of
accuracy is attained in the estimate? First the question will be
considered from the perspective of how many claims are needed.

\subsection{Full Credibility for Claim Frequency}\label{S:frequency}

Let \(N\) be a random variable representing the number of claims for a
group of risks, for example, risks within a particular rating
classification. The observed number of claims will be used to estimate
\(\mu_N=\mathrm{E}[N]\), the expected number of claims. How big does
\(\mu_N\) need to be to get a good estimate? One way to quantify the
accuracy of the estimate would be a statement like: ``The observed value
of \(N\) should be within 5\(\%\) of \(\mu_N\) at least 90\(\%\) of the
time." Writing this as a mathematical expression would give
\(\Pr[0.95\mu_N\leq N \leq1.05\mu_N] \geq 0.90\). Generalizing this
statement by letting \(k\) replace 5\(\%\) and probability \(p\) replace
0.90 gives the equation

\begin{equation}
\Pr[(1-k)\mu_N\leq N \leq(1+k)\mu_N] \geq p.
\label{eq:kpercent-interval}
\end{equation}

The expected number of claims required for the probability on the
left-hand side of \eqref{eq:kpercent-interval} to equal \(p\) is called
the full credibility standard.

If the expected number of claims is greater than or equal to the full
credibility standard then full credibility can be assigned to the data
so \(Z=1\). Usually the expected value \(\mu_N\) is not known so full
credibility will be assigned to the data if the actual observed number
of claims \(n\) is greater than or equal to the full credibility
standard. The \(k\) and \(p\) values must be selected and the actuary
may rely on experience, judgment, and other factors in making the
choices.

Subtracting \(\mu_N\) from each term in \eqref{eq:kpercent-interval} and
dividing by the standard deviation \(\sigma_N\) of \(N\) gives

\begin{equation}
\Pr\left[\frac{-k\mu_N}{\sigma_N}\leq \frac{N-\mu_N}{\sigma_N} \leq \frac{k\mu_N}{\sigma_N}\right] \geq p.
\label{eq:normalized-interval}
\end{equation}

In limited fluctuation credibility the standard normal distribution is
used to approximate the distribution for \((N-\mu_N)/\sigma_N\). If
\(N\) is the sum of many claims from a large group of similar risks and
the claims are independent, then the approximation may be reasonable.

Let \(y_p\) be the value such that
\(\Pr[-y_p\leq (N-\mu_N)/\sigma_N \leq y_p]=\Phi(y_p)-\Phi(-y_p)=p\)
where \(\Phi( )\) is the cumulative distribution function of the
standard normal. Because \(\Phi(-y_p)=1-\Phi(y_p)\), the equality can be
rewritten as \(2\Phi(y_p)-1=p\). Solving for \(y_p\) gives
\(y_p=\Phi^{-1}((p+1)/2)\) where \(\Phi^{-1}( )\) is the inverse of
\(\Phi( )\).

Equation \eqref{eq:normalized-interval} will be satisfied if
\(k\mu_N/\sigma_N \geq y_p\) assuming the normal approximation. First we
will consider this inequality for the case when \(N\) has a Poisson
distribution: \(\Pr[N=n] = \lambda^n\textrm{e}^{-\lambda}/n!\). Because
\(\lambda=\mu_N=\sigma_N^2\) for the Poisson, taking square roots yields
\(\mu_N^{1/2}=\sigma_N\). So, \(k\mu_N/\mu_N^{1/2} \geq y_p\) which is
equivalent to \(\mu_N \geq (y_p/k)^2\). Let's define \(\lambda_{kp}\) to
be the value of \(\mu_N\) for which equality holds. Then the full
credibility standard for the Poisson distribution is

\begin{equation}
\lambda_{kp} = \left(\frac{y_p}{k}\right)^2 \textrm{with } y_p=\Phi^{-1}((p+1)/2).
\label{eq:full-credibility-Poisson}
\end{equation}

If the expected number of claims \(\mu_N\) is greater than or equal to
\(\lambda_{kp}\) then equation \eqref{eq:kpercent-interval} is assumed to
hold and full credibility can be assigned to the data. As noted
previously, because \(\mu_N\) is usually unknown, full credibility is
given if the observed number of claims \(n\) satisfies
\(n \geq \lambda_{kp}.\)

\textbf{Example 9.2.1.} The full credibility standard is set so that the
observed number of claims is to be within 5\% of the expected value with
probability \(p=0.95\). If the number of claims has a Poisson
distribution find the number of claims needed for full credibility.

\textbf{Solution} Referring to a normal table,
\(y_p=\Phi^{-1}((p+1)/2)=\Phi^{-1}((0.95+1)/2)\)=\(\Phi^{-1}(0.975)=1.960\).
Using this value and \(k=.05\) then
\(\lambda_{kp} = (y_p/k)^{2}=(1.960/0.05)^{2}=1,536.64\). After rounding
up the full credibility standard is 1,537.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

If claims are not Poisson distributed then equation
\eqref{eq:normalized-interval} does not imply
\eqref{eq:full-credibility-Poisson}. Setting the upper bound of
\((N-\mu_N)/\sigma_N\) in \eqref{eq:normalized-interval} equal to \(y_p\)
gives \(k\mu_N/\sigma_N=y_p\). Squaring both sides and moving everything
to the right side except for one of the \(\mu_N\)'s gives
\(\mu_N=(y_p/k)^2(\sigma_N^2/\mu_N)\). This is the full credibility
standard for frequency and will be denoted by \(n_f\),

\begin{equation}
n_f=\left(\frac{y_p}{k}\right)^2\left(\frac{\sigma_N^2}{\mu_N}\right)=\lambda_{kp}\left(\frac{\sigma_N^2}{\mu_N}\right).
\label{eq:full-credibility-frequency}
\end{equation}

This is the same equation as the Poisson full credibility standard
except for the \((\sigma_N^2/\mu_N)\) multiplier. When the claims
distribution is Poisson this extra term is one because the variance
equals the mean.

\textbf{Example 9.2.2.} The full credibility standard is set so that the
total number of claims is to be within 5\(\%\) of the observed value
with probability \(p=0.95\). The number of claims has a negative
binomial distribution

\begin{equation*}
\Pr(N=x)={x+r-1\choose x} \left(\frac{1}{1+\beta}\right)^r \left(\frac{\beta}{1+\beta}\right)^x
\end{equation*}

with \(\beta=1\). Calculate the full credibility standard.

\textbf{Solution} From the prior example, \(\lambda_{kp} =1,536.64\).
The mean and variance for the negative binomial are
\(\mathrm{E}(N)=r\beta\) and \(\mathrm{Var}(N)=r\beta(1+\beta)\) so
\((\sigma_N^2/\mu_N)=(r\beta(1+\beta)/(r\beta))=1+\beta\) which equals 2
when \(\beta=1\). So,
\(n_f=\lambda_{kp}(\sigma_N^2/\mu_N)=1,536.64(2)=3,073.28\) and rounding
up gives a full credibility standard of 3,074.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

We see that the negative binomial distribution with
\((\sigma_N^2/\mu_N)>1\) requires more claims for full credibility than
a Poisson distribution for the same \(k\) and \(p\) values. The next
example shows that a binomial distribution which has
\((\sigma_N^2/\mu_N)<1\) will need fewer claims for full credibility.

\textbf{Example 9.2.3.} The full credibility standard is set so that the
total number of claims is to be within 5\(\%\) of the observed value
with probability \(p=0.95\). The number of claims has a binomial
distribution

\begin{equation*}
\Pr(N=x)={m\choose x}q^x(1-q)^{m-x}.
\end{equation*}

Calculate the full credibility standard for \(q=1/4\).

\textbf{Solution} From the first example in this section
\(\lambda_{kp} =1,536.64\). The mean and variance for a binomial are
\(\mathrm{E}(N)=mq\) and \(\mathrm{Var}(N)=mq(1-q)\) so
\((\sigma_N^2/\mu_N)=(mq(1-q)/(mq))=1-q\) which equals 3/4 when
\(q=1/4\). So,
\(n_f=\lambda_{kp}(\sigma_N^2/\mu_N)=1,536.64(3/4)=1,152.48\) and
rounding up gives a full credibility standard of 1,153.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Rather than using expected number of claims to define the full
credibility standard, the number of exposures can be used for the full
credibility standard. An exposure is a measure of risk. For example, one
car insured for a full year would be one car-year. Two cars each insured
for exactly one-half year would also result in one car-year. Car-years
attempt to quantify exposure to loss. Two car-years would be expected to
generate twice as many claims as one car-year if the vehicles have the
same risk of loss. To translate a full credibility standard denominated
in terms of number of claims to a full credibility standard denominated
in exposures one needs a reasonable estimate of the expected number of
claims per exposure.

\textbf{Example 9.2.4.} The full credibility standard should be selected
so that the observed number of claims will be within 5\(\%\) of the
expected value with probability \(p=0.95\). The number of claims has a
Poisson distribution. If one exposure is expected to have about 0.20
claims per year, find the number of exposures needed for full
credibility.

\textbf{Solution} With \(p=0.95\) and \(k=.05\),
\(\lambda_{kp} = (y_p/k)^{2}=(1.960/0.05)^{2}=1,536.64\) claims are
required for full credibility. The claims frequency rate is 0.20
claims/exposures. To convert the full credibility standard to a standard
denominated in exposures the calculation is: (1,536.64 claims)/(0.20
claims/exposures) = 7,683.20 exposures. This can be rounded up to 7,684.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Frequency can be defined as the number of claims per exposure. Letting
\(m\) represent number of exposures then the observed claim frequency is
\(N/m\) which is used to estimate \(\mathrm{E}(N/m)\):

\begin{equation*}
\Pr[(1-k)\mathrm{E}(N/m)\leq N/m \leq(1+k)\mathrm{E}(N/m)] \geq p.
\end{equation*}

Because the number of exposures is not a random variable,
\(\mathrm{E}(N/m)=\mathrm{E}(N)/m=\mu_N/m\) and the prior equation
becomes

\begin{equation*}
\Pr\left[(1-k)\frac{\mu_N}{m}\leq \frac{N}{m} \leq(1+k)\frac{\mu_N}{m}\right] \geq p.
\end{equation*}

Multiplying through by \(m\) results in equation
\eqref{eq:kpercent-interval} at the beginning of the section. The full
credibility standards that were developed for estimating expected number
of claims also apply to frequency.

\subsection{Full Credibility for Aggregate Losses and Pure
Premium}\label{full-credibility-for-aggregate-losses-and-pure-premium}

Aggregate losses are the total of all loss amounts for a risk or group
of risks. Letting \(S\) represent aggregate losses then

\begin{equation*}
S=X_1+X_2+\cdots+X_N.
\end{equation*}

The random variable \(N\) represents the number of losses and random
variables \(X_1, X_2,\ldots,X_N\) are the individual loss amounts. In
this section it is assumed that \(N\) is independent of the loss amounts
and that \(X_1, X_2,\ldots,X_N\) are iid.

The mean and variance of \(S\) are

\begin{equation*}
\mu_S=\mathrm{E}(S)=\mathrm{E}(N)\mathrm{E}(X)=\mu_N\mu_X\textrm{  and}
\end{equation*}

\begin{equation*}
\sigma^{2}_S=\mathrm{Var}(S)=\mathrm{E}(N)\mathrm{Var}(X)+[\mathrm{E}(X)]^{2}\mathrm{Var}(N)=\mu_N\sigma^{2}_X+\mu^{2}_X\sigma^{2}_N.
\end{equation*}

where \(X\) is the amount of a single loss.

Observed losses \(S\) will be used to estimate expected losses
\(\mu_S=\mathrm{E}(S)\). As with the frequency model in the previous
section, the observed losses must be close to the expected losses as
quantified in the equation

\begin{equation*}
\Pr[(1-k)\mu_S\leq S \leq(1+k)\mu_S] \geq p.
\end{equation*}

\noindent After subtracting the mean and dividing by the standard
deviation,

\begin{equation*}
\Pr\left[\frac{-k\mu_S}{\sigma_S}\leq (S-\mu_S)/\sigma_S \leq \frac{k\mu_S}{\sigma_S}\right] \geq p
\end{equation*}

.

As done in the previous section the distribution for
\((S-\mu_S)/\sigma_S\) is assumed to be normal and
\(k\mu_S/\sigma_S=y_p=\Phi^{-1}((p+1)/2)\). This equation can be
rewritten as \(\mu_S^2=(y_p/k)^2\sigma_S^2\). Using the prior formulas
for \(\mu_S\) and \(\sigma_{S}^2\) gives
\((\mu_N\mu_X)^2=(y_p/k)^2(\mu_N\sigma^{2}_X+\mu^{2}_X\sigma^{2}_N)\).
Dividing both sides by \(\mu_N\mu_X^2\) and reordering terms on the
right side results in a full credibility standard \(n_S\) for aggregate
losses

\begin{equation}
n_S=\left(\frac{y_p}{k}\right)^2\left[\left(\frac{\sigma_N^2}{\mu_N}\right)+\left(\frac{\sigma_X}{\mu_X}\right)^2\right]=\lambda_{kp}\left[\left(\frac{\sigma_N^2}{\mu_N}\right)+\left(\frac{\sigma_X}{\mu_X}\right)^2\right].
\label{eq:full-credibility-losses}
\end{equation}

\textbf{Example 9.2.5.} The number of claims has a Poisson distribution.
Individual loss amounts are independently and identically distributed
with a Type II Pareto distribution
\(F(x)=1-[\theta/(x+\theta)]^{\alpha}\). The number of claims and loss
amounts are independent. If observed aggregate losses should be within
5\(\%\) of the expected value with probability \(p=0.95\), how many
losses are required for full credibility?

\textbf{Solution} Because the number of claims is Poisson,
\((\sigma_N^2/\mu_N)=1\). The mean of the Pareto is
\(\mu_X=\theta/(\alpha-1)\) and the variance is
\(\sigma_X^2=\theta^{2}\alpha/[(\alpha-1)^{2}(\alpha-2)]\) so
\((\sigma_X/\mu_X)^2=\alpha/(\alpha-2)\). Combining the frequency and
severity terms gives
\([(\sigma_N^2/\mu_N)+(\sigma_X/\mu_X)^2]=2(\alpha-1)/(\alpha-2)\). From
a normal table \(y_p=\Phi^{-1}((0.95+1)/2)=1.960\). The full credibility
standard is
\(n_S=(1.96/0.05)^{2}[2(\alpha-1)/(\alpha-2)]=3,073.28(\alpha-1)/(\alpha-2)\).
Suppose \(\alpha=3\) then \(n_S=6,146.56\) for a full credibility
standard of 6,147. Note that considerably more claims are needed for
full credibility for aggregate losses than frequency alone.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

When the number of claims is Poisson distributed then equation
\eqref{eq:full-credibility-losses} can be simplified using
\((\sigma_N^2/\mu_N)=1\). It follows that
\([(\sigma_N^2/\mu_N)+(\sigma_X/\mu_X)^2]=[1+(\sigma_X/\mu_X)^2]=[(\mu_X^2+\sigma_X^2)/\mu_X^2]=\mathrm{E}(X^2)/\mathrm{E}(X)^2\)
using the relationship \(\mu_X^2+\sigma_X^2=\mathrm{E}(X^2)\). The full
credibility standard is
\(n_S=\lambda_{kp}\mathrm{E}(X^2)/\mathrm{E}(X)^2\).

The pure premium \(PP\) is equal to aggregate losses \(S\) divided by
exposures \(m\): \(PP=S/m\). The full credibility standard for pure
premium will require

\begin{equation*}
\Pr\left[(1-k)\mu_{PP}\leq PP \leq(1+k)\mu_{PP}\right] \geq p.
\end{equation*}

\noindent The number of exposures \(m\) is assumed fixed and not a
random variable so \(\mu_{PP}=\mathrm{E}(S/m)=\mathrm{E}(S)/m=\mu_S/m\).

\begin{equation*}
\Pr\left[(1-k)\left(\frac{\mu_S}{m}\right)\leq \left(\frac{S}{m}\right) \leq(1+k)\left(\frac{\mu_S}{m}\right)\right] \geq p.
\end{equation*}

\noindent Multiplying through by exposures \(m\) returns the bounds for
losses

\begin{equation*}
\Pr[(1-k)\mu_S\leq S \leq(1+k)\mu_S] \geq p.
\end{equation*}

\noindent This means that the full credibility standard \(n_{PP}\) for
the pure premium is the same as that for aggregate losses

\begin{equation*}
n_{PP}=n_S=\lambda_{kp}\left[\left(\frac{\sigma_N^2}{\mu_n}\right)+\left(\frac{\sigma_X}{\mu_X}\right)^2\right].
\end{equation*}

\subsection{Full Credibility for
Severity}\label{full-credibility-for-severity}

Let \(X\) be a random variable representing the size of one claim. Claim
severity is \(\mu_X=\mathrm{E}(X)\). Suppose that
\({X_1,X_2, \ldots, X_n}\) is a random sample of \(n\) claims that will
be used to estimate claim severity \(\mu_X\). The claims are assumed to
be \emph{iid}. The average value of the sample is

\begin{equation*}
\bar{X}=\frac{1}{n}\left(X_1+X_2+\cdots+X_n\right).
\end{equation*}

How big does \(n\) need to be to get a good estimate? Note that \(n\) is
not a random variable whereas it is in the aggregate loss model.

In Section \ref{S:frequency} the accuracy of an estimator for frequency
was defined by requiring that the number of claims lie within a
specified interval about the mean number of claims with a specified
probability. For severity this requirement is

\begin{equation*}
\Pr[(1-k)\mu_X\leq \bar{X} \leq(1+k)\mu_X ]\geq p
\end{equation*}

\noindent where \(k\) and \(p\) need to be specified. Following the
steps in Section \ref{S:frequency}, the mean claim severity \(\mu_X\) is
subtracted from each term and the standard deviation of the claim
severity estimator \(\sigma_{\bar{X}}\) is divided into each term
yielding

\begin{equation*}
\Pr\left[\frac{-k\mu_X}{\sigma_{\bar{X}}}\leq (\bar{X}-\mu_X)/\sigma_{\bar{X}} \leq \frac{k\mu_X}{\sigma_{\bar{X}}}\right] \geq p
\end{equation*}

.

As in prior sections, it is assumed that
\((\bar{X}-\mu_X)/\sigma_{\bar{X}}\) is approximately normally
distributed and the prior equation is satisfied if
\(k\mu_X/\sigma_{\bar{X}}\geq y_p\) with \(y_p=\Phi^{-1}((p+1)/2)\).
Because \(\bar{X}\) is the average of individual claims
\(X_1, X_2,\dots, X_n\), its standard deviation is equal to the standard
deviation of an individual claim divided by \(\sqrt{n}\):
\(\sigma_{\bar{X}}=\sigma_X/\sqrt{n}\). So,
\(k\mu_X/(\sigma_X/\sqrt{n})\geq y_p\) and with a little algebra this
can be rewritten as \(n \geq (y_p/k)^2(\sigma_X/\mu_X)^2\). The full
credibility standard for severity is

\begin{equation}
n_X=\left(\frac{y_p}{k}\right)^2\left(\frac{\sigma_X}{\mu_X}\right)^2=\lambda_{kp}\left(\frac{\sigma_X}{\mu_X}\right)^2.
\label{eq:full-credibility-severity}
\end{equation}

Note that the term \(\sigma_X/\mu_X\) is the coefficient of variation
for an individual claim. Even though \(\lambda_{kp}\) is the full
credibility standard for frequency given a Poisson distribution, there
is no assumption about the distribution for the number of claims.

\textbf{Example 9.2.6.} Individual loss amounts are independently and
identically distributed with a Type II Pareto distribution
\(F(x)=1-[\theta/(x+\theta)]^{\alpha}\). How many claims are required
for the average severity of observed claims to be within 5\(\%\) of the
expected severity with probability \(p=0.95\)?

\textbf{Solution} The mean of the Pareto is \(\mu_X=\theta/(\alpha-1)\)
and the variance is
\(\sigma_X^2=\theta^{2}\alpha/[(\alpha-1)^{2}(\alpha-2)]\) so
\((\sigma_X/\mu_X)^2=\alpha/(\alpha-2)\). From a normal table
\(y_p=\Phi^{-1}((0.95+1)/2)=1.960\). The full credibility standard is
\(n_X=(1.96/0.05)^{2}[\alpha/(\alpha-2)]=1,536.64\alpha/(\alpha-2)\).
Suppose \(\alpha=3\) then \(n_X=4,609.92\) for a full credibility
standard of 4,610.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Partial Credibility}\label{partial-credibility}

In prior sections full credibility standards were calculated for
estimating frequency (\(n_f\)), pure premium (\(n_{PP}\)), and severity
(\(n_X\)) - in this section these full credibility standards will be
denoted by \(n_{0}\). In each case the full credibility standard was the
expected number of claims required to achieve a defined level of
accuracy when using empirical data to estimate an expected value. If the
observed number of claims is greater than or equal to the full
credibility standard then a full credibility weight \(Z=1\) is given to
the data.

In limited fluctuation credibility, credibility weights \(Z\) assigned
to data are

\begin{equation*} 
Z=\quad \sqrt{\frac{n}{n_{0}}} \quad \textrm{if} \quad   n < n_{0} \quad \textrm{and}  \quad Z=\quad 1 \quad \textrm{for} \quad   n \geq n_{0}
\end{equation*}

where \(n_0\) is the full credibility standard. The quantity \(n\) is
the number of claims for the data that is used to estimate the expected
frequency, severity, or pure premium.

\textbf{Example 9.2.7.} The number of claims has a Poisson distribution.
Individual loss amounts are independently and identically distributed
with a Type II Pareto distribution
\(F(x)=1-[\theta/(x+\theta)]^{\alpha}\). Assume that \(\alpha=3\). The
number of claims and loss amounts are independent. The full credibility
standard is that the observed pure premium should be within 5\(\%\) of
the expected value with probability \(p=0.95\). What credibility \(Z\)
is assigned to a pure premium computed from 1,000 claims?

\textbf{Solution} Because the number of claims is Poisson,

\[
\frac{\mathrm{E}(X^2)}{[\mathrm{E}~(X)]^2}
=\frac{\sigma_N^2}{\mu_N}+\left(\frac{\sigma_X}{\mu_X}\right)^2.  
\]

The mean of the Pareto is \(\mu_X=\theta/(\alpha-1)\) and the second
moment is \(\mathrm{E}(X^2)=2\theta^{2}/[(\alpha-1)(\alpha-2)]\) so
\(\mathrm{E}(X^2)/[\mathrm{E}~(X)]^2=2(\alpha-1)/(\alpha-2)\). From a
normal table \(y_p=\Phi^{-1}((0.95+1)/2)=1.960\). The full credibility
standard is

\[n_{PP}=(1.96/0.05)^{2}[2(\alpha-1)/(\alpha-2)]=3,073.28(\alpha-1)/(\alpha-2)\]
and if \(\alpha=3\) then \(n_0=n_{PP}=6,146.56\) or 6,147 if rounded up.
The credibility assigned to 1,000 claims is
\(Z=(1,000/6,147)^{1/2}=0.40\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Limited fluctuation credibility uses the formula \(Z=\sqrt{n/n_0}\) to
limit the fluctuation in the credibility-weighted estimate to match the
fluctuation allowed for data with expected claims at the full
credibility standard. Variance or standard deviation is used as the
measure of fluctuation. Next we show an example to explain why the
square-root formula is used.

Suppose that average claim severity is being estimated from a sample of
size \(n\) that is less than the full credibility standard \(n_0=n_X\).
Applying credibility theory the estimate \(\hat{\mu}_X\) would be

\begin{equation*}
\hat{\mu}_X=Z\bar{X}+(1-Z)M_X
\end{equation*}

with \(\bar{X}=(X_1+X_2+\cdots+X_n)/n\) and \(iid\) random variables
\(X_i\) representing the sizes of individual claims. The complement of
credibility is applied to \(M_X\) which could be last year's estimated
average severity adjusted for inflation, the average severity for a much
larger pool of risks, or some other relevant quantity selected by the
actuary. It is assumed that the variance of \(M_X\) is zero or
negligible. With this assumption

\begin{equation*}
\mathrm{Var}(\hat{\mu}_X)=\mathrm{Var}(Z\bar{X})=Z^2\mathrm{Var}(\bar{X})=\frac{n}{n_0}\mathrm{Var}(\bar{X}).
\end{equation*}

Because \(\bar{X}=(X_1+X_2+\cdots+X_n)/n\) it follows that
\(\mathrm{Var}(\bar{X})=\mathrm{Var}(X_i)/n\) where random variable
\(X_i\) is one claim. So,

\begin{equation*}
\mathrm{Var}(\hat{\mu}_X)=\frac{n}{n_0}\mathrm{Var}(\bar{X})=\frac{n}{n_0}\frac{\mathrm{Var}(X_i)}{n}=\frac{\mathrm{Var}(X_i)}{n_0}.
\end{equation*}

The last term is exactly the variance of a sample mean \(\bar{X}\) when
the sample size is equal to the full credibility standard \(n_0=n_X\).

\section{Bühlmann Credibility}\label{buhlmann-credibility}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Compute a credibility-weighted estimate for the expected loss for a
  risk or group of risks.
\item
  Determine the credibility \(Z\) assigned to observations.
\item
  Calculate the values required in Bühlmann credibility including the
  Expected Value of the Process Variance (\emph{EPV}), Variance of the
  Hypothetical Means (\emph{VHM}) and collective mean \(\mu\).
\item
  Recognize situations when the Bühlmann model is appropriate.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

A classification rating plan groups policyholders together into classes
based on risk characteristics. Although policyholders within a class
have similarities, they are not identical and their expected losses will
not be exactly the same. An experience rating plan can supplement a
class rating plan by credibility weighting an individual policyholder's
loss experience with the class rate to produce a more accurate rate for
the policyholder.

In the presentation of Buhlmann credibility it is convenient to assign a
risk parameter \(\theta\) to each policyholder. Losses \(X\) for the
policyholder will have a common distribution function \(F_{\theta}(x)\)
with mean \(\mu(\theta)=\mathrm{E}(X|\theta)\) and variance
\(\sigma^2(\theta)=\mathrm{Var}(X|\theta)\). Losses \(X\) can represent
pure premiums, aggregate losses, number of claims, claim severities, or
some other measure of loss for a period of time, often one year. Risk
parameter \(\theta\) may be continuous or discrete and may be
multivariate depending on the model.

If a policyholder with risk parameter \(\theta\) had losses
\(X_1, \ldots, X_n\) during \(n\) time periods then the goal is to find
E(\(\mu(\theta)|X_1,\ldots, X_n)\), the conditional expectation of
\(\mu(\theta)\) given \(X_1,\ldots, X_n\). The Bühlmann
credibility-weighted estimate for E(\(\mu(\theta)|X_1,\ldots, X_n)\) for
the policyholder is

\begin{equation}
\hat{\mu}(\theta)=Z\bar{X}+(1-Z)\mu 
\label{eq:buhlcred}
\end{equation}

with

\begin{eqnarray*} 
\theta&=&\textrm{a risk parameter that identifies a policyholder's risk level}\\
\hat{\mu}(\theta)&=&\textrm{estimated expected loss for a policyholder with parameter }\theta\\
 & & \textrm{and loss experience } \bar{X}\\
\bar{X}&=&(X_1+\cdots+X_n)/n \textrm{ is the average of $n$ observations of the policyholder } \\
 Z&=&\textrm{credibility assigned to $n$ observations } \\
\mu&=&\textrm{the expected loss for a randomly chosen policyholder in the class.}\\
\end{eqnarray*}

For a selected policyholder, random variables \(X_j\) are assumed to be
\emph{iid} for \(j=1,\ldots,n\) because it is assumed that the
policyholder's exposure to loss is not changing through time. The
quantity \(\bar{X}\) is the average of \(n\) observations and
\(\mathrm{E}(\bar{X}|\theta)=\mathrm{E}(X_j|\theta)=\mu(\theta)\).

If a policyholder is randomly chosen from the class and there is no loss
information about the risk then the expected loss is
\(\mu=\mathrm{E}(\mu(\theta))\) where the expectation is taken over all
\(\theta\)'s in the class. In this situation \(Z=0\) and the expected
loss is \(\hat\mu(\theta)=\mu\) for the risk. The quantity \(\mu\) can
also be written as \(\mu=\mathrm{E}(X_j)\) or
\(\mu=\mathrm{E}(\bar{X})\) and is often called the overall mean or
collective mean. Note that E(\(X_j\)) is evaluated with the law of total
expectation: E(\(X_j\))=E(E(\(X_j|\theta)\)).

\textbf{Example 9.3.1.} The number of claims \(X\) for an insured in a
class has a Poisson distribution with mean \(\theta>0\). The risk
parameter \(\theta\) is exponentially distributed within the class with
pdf \(f(\theta)=e^{-\theta}\). What is the expected number of claims for
an insured chosen at random from the class?

\textbf{Solution} Random variable \(X\) is Poisson with parameter
\(\theta\) and E\((X|\theta)=\theta\). The expected number of claims for
a randomly chosen insured is
\(\mu=\mathrm{E}(\mu(\theta))=\mathrm{E}(\mathrm{E}(X|\theta))=\)E\((\theta)=\int_{0}^{\infty}\theta e^{-\theta} d\theta=1\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In the prior example the risk parameter \(\theta\) is a random variable
with an exponential distribution. In the next example there are three
types of risks and the risk parameter has a discrete distribution.

\textbf{Example 9.3.2.} For any risk (policyholder) in a population the
number of losses \(N\) in a year has a Poisson distribution with
parameter \(\lambda\). Individual loss amounts \(X_i\) for a risk are
independent of \(N\) and are \emph{iid} with Type II Pareto distribution
\(F(x)=1-[\theta/(x+\theta)]^{\alpha}\). There are three types of risks
in the population as follows:

\[\begin{matrix}
\begin{array}{|c|c|c|c|}
\hline
\text{Risk } & \text{Percentage} & \text{Poisson} & \text{Pareto} \\
\text{Type} & \text{of Population} & \text{Parameter} & \text{Parameters} \\
\hline
A & 50\% & \lambda=0.5 & \theta=1000, \alpha=2.0 \\
B & 30\% & \lambda=1.0 & \theta=1500, \alpha=2.0 \\  
C & 20\% & \lambda=2.0 & \theta=2000, \alpha=2.0 \\              
\hline
\end{array}
\end{matrix}\] If a risk is selected at random from the population, what
is the expected aggregate loss in a year?

\textbf{Solution} The expected number of claims for a risk is
E(\(N|\lambda\))=\(\lambda\). The expected value for a Pareto
distributed random variable is
E(\(X | \theta, \alpha\))=\(\theta/(\alpha-1)\). The expected value of
the aggregate loss random variable \(S=X_1+\cdots+X_N\) for a risk with
parameters \(\lambda\), \(\alpha\), and \(\theta\) is
E(\(S\))=E(\(N\))E(\(X\))=\(\lambda\theta/(\alpha-1)\). The expected
aggregate loss for a risk of type A is
E(\(S_{\textrm{A}}\))=(0.5)(1000)/(2-1)=500. The expected aggregate loss
for a risk selected at random from the population is
E(\(S\))=0.5{[}(0.5)(1000){]}+0.3{[}(1.0)(1500){]}+0.2{[}(2.0)(2000){]}=1500.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

What is the risk parameter for a risk (policyholder) in the prior
example? One could say that the risk parameter has three components
\((\lambda,\theta,\alpha)\) with possible values (0.5,1000,2.0),
(1.0,1500,2.0), and (2.0,2000,2.0) depending on the type of risk.

Note that in both of the examples the risk parameter is a random
quantity with its own probability distribution. We do not know the value
of the risk parameter for a randomly chosen risk.

Although formula \eqref{eq:buhlcred} was introduced using experience
rating as an example, the Bühlmann credibility model has wider
application. Suppose that a rating plan has multiple classes.
Credibility formula \eqref{eq:buhlcred} can be used to determine
individual class rates. The overall mean \(\mu\) would be the average
loss for all classes combined, \(\bar{X}\) would be the experience for
the individual class, and \(\hat{\mu}(\theta)\) would be the estimated
loss for the class.

\subsection{\texorpdfstring{Credibility Z, \emph{EPV}, and
\emph{VHM}}{Credibility Z, EPV, and VHM}}\label{S:EPV-VHM-Z}

When computing the credibility estimate
\(\hat{\mu}(\theta)=Z\bar{X}+(1-Z)\mu\), how much weight \(Z\) should go
to experience \(\bar{X}\) and how much weight \((1-Z)\) to the overall
mean \(\mu\)? In Bühlmann credibility there are three factors that need
to be considered:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How much variation is there in a single observation \(X_j\) for a
  selected risk? With \(\bar{X}=(X_1+\cdots+X_n)/n\) and assuming that
  the observations are \emph{iid} conditional on \(\theta\), it follows
  that Var(\(\bar{X}|\theta)\)=Var(\(X_j|\theta)/n\). For larger
  Var(\(\bar{X}|\theta)\) less credibility weight \(Z\) should be given
  to experience \(\bar{X}\). The Expected Value of the Process Variance,
  abbreviated \emph{EPV}, is the expected value of Var(\(X_j|\theta\))
  across all risks:

  \begin{equation*}
  EPV=\mathrm{E}(\mathrm{Var}(X_j|\theta)). 
  \end{equation*}

  Because Var(\(\bar{X}|\theta)\)=Var(\(X_j|\theta)/n\) it follows that
  E(Var(\(\bar{X}|\theta)\))=EPV/\(n\).
\item
  How homogeneous is the population of risks whose experience was
  combined to compute the overall mean \(\mu\)? If all the risks are
  similar in loss potential then more weight \((1-Z)\) would be given to
  the overall mean \(\mu\) because \(\mu\) is the average for a group of
  similar risks whose means \(\mu(\theta)\) are not far apart. The
  homogeneity or heterogeneity of the population is measured by the
  Variance of the Hypothetical Means with abbreviation \emph{VHM}:

  \begin{equation*}
  VHM=\mathrm{Var}(\mathrm{E}(X_j|\theta))=\mathrm{Var}(\mathrm{E}(\bar{X}|\theta)). 
  \end{equation*}

  Note that we used
  \(\mathrm{E}(\bar{X}|\theta)=\mathrm{E}(X_j|\theta)\) for the second
  equality.
\item
  How many observations \(n\) were used to compute \(\bar{X}\)? A larger
  sample would infer a larger \(Z\).
\end{enumerate}

\textbf{Example 9.3.3.} The number of claims \(N\) in a year for a risk
in a population has a Poisson distribution with mean \(\lambda>0\). The
risk parameter \(\lambda\) is uniformly distributed over the interval
\((0,2)\). Calculate the EPV and \emph{VHM} for the population.

\textbf{Solution} Random variable \(N\) is Poisson with parameter
\(\lambda\) so Var\((N|\lambda)=\lambda\). The Expected Value of the
Process variance is
EPV=E(Var(\(N|\lambda\)))=E\((\lambda)=\int_{0}^{2}\lambda \frac{1}{2} d\lambda=1\).
The Variance of the Hypothetical Means is
\emph{VHM}=Var(E(N\(|\lambda\)))=
Var(\(\lambda\))=E(\(\lambda^2)-(\mathrm{E}(\lambda))^2=\int_{0}^{2}\lambda^2 \frac{1}{2} d\lambda-(1)^2=\frac{1}{3}\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The Bühlmann credibility formula includes values for \(n\), \emph{EPV},
and \emph{VHM}:

\begin{equation}
Z=\frac{n}{n+K} \quad , \quad K =\frac{EPV}{VHM}. 
\label{eq:buhlZ} 
\end{equation}

If the \emph{VHM} increases then \(Z\) increases. If the \emph{EPV}
increases then \(Z\) gets smaller. Unlike limited fluctuation
credibility where \(Z=1\) when the expected number of claims is greater
than the full credibility standard, \(Z\) can approach but not equal 1
as the number of observations \(n\) goes to infinity.

If you multiply the numerator and denominator of the \(Z\) formula by
(\emph{VHM}/\(n\)) then \(Z\) can be rewritten as

\begin{equation*}
Z=\frac{VHM}{VHM+(EPV/n)} . 
\end{equation*}

The number of observations \(n\) is captured in the term
(\emph{EPV}/\(n\)). As shown in bullet (1) at the beginning of the
section, E(Var(\(\bar{X}|\theta)\))=\emph{EPV}/\(n\). As the number of
observations get larger, the expected variance of \(\bar{X}\) gets
smaller and credibility \(Z\) increases so that more weight gets
assigned to \(\bar{X}\) in the credibility-weighted estimate
\(\hat{\mu}(\theta)\).

\textbf{Example 9.3.4.} Use the law of total variance to show that
Var(\(\bar{X}\)) = \emph{VHM} + (\emph{EPV}/n) and derive a formula for
\(Z\) in terms of \(\bar{X}\).

\textbf{Solution} The quantity Var(\(\bar{X}\)) is called the
unconditional variance or the total variance of \(\bar{X}\). The law of
total variance says

\begin{equation*} 
\mathrm{Var}(\bar{X})=\textrm{E(Var}(\bar{X}|\theta))+\textrm{Var(E}(\bar{X}|\theta)). 
\end{equation*}

In bullet (1) at the beginning of this section we showed
E(Var(\(\bar{X}|\theta)\))=\emph{EPV}/\(n\). In the second bullet (2),
Var(E(\(\bar{X}|\theta\)))=\emph{VHM}. Reordering the right hand side
gives Var(\(\bar{X}\))= \emph{VHM} +(\emph{EPV}/\(n\)). Another way to
write the formula for credibility \(Z\) is
\(Z\)=Var(E(\(\bar{X}|\theta\)))/Var(\(\bar{X}\)). This implies
\((1-Z)\)=E(Var(\(\bar{X}|\theta\)))/Var(\(\bar{X}\)).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The following long example and solution demonstrate how to compute the
credibility-weighted estimate with frequency and severity data.

\textbf{Example 9.3.5.} For any risk in a population the number of
losses \(N\) in a year has a Poisson distribution with parameter
\(\lambda\). Individual loss amounts \(X\) for a selected risk are
independent of \(N\) and are \emph{iid} with exponential distribution
\(F(x)=1-e^{-x/\beta}\). There are three types of risks in the
population as shown below. A risk was selected at random from the
population and all losses were recorded over a five-year period. The
total amount of losses over the five-year period was 5,000. Use Bühlmann
credibility to estimate the annual expected aggregate loss for the
risk.\\
\[\begin{matrix}
\begin{array}{|c|c|c|c|}
\hline
\text{Risk } & \text{Percentage} & \text{Poisson} & \text{Exponential} \\
\text{Type} & \text{of Population} & \text{Parameter} & \text{Parameter} \\
\hline
A & 50\% & \lambda=0.5 & \beta=1000 \\
B & 30\% & \lambda=1.0 & \beta=1500 \\  
C & 20\% & \lambda=2.0 & \beta=2000 \\              
\hline
\end{array}
\end{matrix}\]

\textbf{Solution} Because individual loss amounts \(X\) are
exponentially distributed, E(\(X| \beta\))=\(\beta\) and
Var(\(X| \beta\))=\(\beta^2\). For aggregate loss \(S=X_1+\cdots+X_N\),
the mean is E(\(S\))=E(\(N\))E(\(X\)) and process variance is
Var(\(S\))=E(\(N\))Var(\(X\))+{[}E(\(X\)){]}\(^2\)Var(\(N\)). With
Poisson frequency and exponentially distributed loss amounts,
E(\(S| \lambda, \beta\))=\(\lambda\beta\) and Var(\(S| \lambda, \beta\))
= \(\lambda\beta^2+\beta^2\lambda=2\lambda\beta^2\).\\
\textbf{Population mean \(\mu\)}: Risk means are
\(\mu\)(A)=0.5(1000)=500; \(\mu\)(B)=1.0(1500)=1500;
\(\mu\)(C)=2.0(2000)=4000; and
\(\mu\)=0.50(500)+0.30(1500)+0.20(4000)=1,500.\\
\textbf{VHM}:
\emph{VHM}=\(0.50(500-1500)^2+0.30(1500-1500)^2+0.20(4000-1500)^2\)=1,750,000.\\
\textbf{EPV}: Process variances are
\(\sigma^2(A)=2(0.5)(1000)^2=1,000,000\);
\(\sigma^2(B)=2(1.0)(1500)^2=4,500,000\);
\(\sigma^2(C)=2(2.0)(2000)^2=16,000,000\); and
\emph{EPV}=0.50(1,000,000)+0.30(4,500,000)+0.20(16,000,000)=5,050,000.\\
\textbf{\(\mathbf{\bar{X}}\)}: \(\bar{X}_5=5,000/5\)=1,000.\\
\textbf{\(\mathbf{K}\)}: \(K=5,050,000/1,750,000\)=2.89.\\
\textbf{\(\mathbf{Z}\)}: There are five years of observations so
\(n=5\). \(Z=5/(5+2.89)\)=0.63.\\
\textbf{\(\boldsymbol{\hat{\mu}(\theta)}\)}:
\(\hat{\mu}(\theta)=0.63(1,000)+(1-0.63)1,500=\boxed{\mathbf{1,185.00}}\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In real world applications of Bühlmann credibility the value of
\(K=EPV/VHM\) must be estimated. Sometimes a value for \(K\) is selected
using judgment. A smaller \(K\) makes estimator \(\hat{\mu}(\theta)\)
more responsive to actual experience \(\bar{X}\) whereas a larger \(K\)
produces a more stable estimate by giving more weight to \(\mu\).
Judgment may be used to balance responsiveness and stability. A later
section in this chapter will discuss methods for determining \(K\) from
data.

For a policyholder with risk parameter \(\theta\), Bühlmann credibility
uses a linear approximation \(\hat{\mu}(\theta)=Z\bar{X}+(1-Z)\mu\) to
estimate E(\(\mu(\theta)|X_1,\ldots,X_n\)), the expected loss for the
policyholder given prior losses \(X_1,\ldots, X_n\). We can rewrite this
as \(\hat{\mu}(\theta)=a+b\bar{X}\) which makes it obvious that the
credibility estimate is a linear function of \(\bar{X}\).

If E(\(\mu(\theta)|X_1,\ldots,X_n\)) is approximated by the linear
function \(a+b\bar{X}\) and constants \(a\) and \(b\) are chosen so that
E{[}(E(\(\mu(\theta)|X_1,\ldots,X_n)-(a+b\bar{X}))^2\){]} is minimized,
what are \(a\) and \(b\)? The answer is \(b=n/(n+K)\) and \(a=(1-b)\mu\)
with \(K=EPV/VHM\) and \(\mu=E(\mu(\theta))\). More details can be found
in references \citep{buhlmann}, \citep{buhlmanngisler},
\citep{klugman2012}, and \citep{tse}.

Bühlmann credibility is also called least-squares credibility, greatest
accuracy credibility, or Bayesian credibility.

\section{Bühlmann-Straub Credibility}\label{buhlmann-straub-credibility}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Compute a credibility-weighted estimate for the expected loss for a
  risk or group of risks using the Bühlmann-Straub model.
\item
  Determine the credibility \(Z\) assigned to observations.
\item
  Calculate required values including the Expected Value of the Process
  Variance (\emph{EPV}), Variance of the Hypothetical Means (\emph{VHM})
  and collective mean \(\mu\).
\item
  Recognize situations when the Bühlmann-Straub model is appropriate.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

With standard Bühlmann or least-squares credibility as described in the
prior section, losses \(X_1,\ldots,X_n\) arising from a selected
policyholder are assumed to be \emph{iid}. If the subscripts indicate
year 1, year 2 and so on up to year \(n\), then the \emph{iid}
assumption means that the policyholder has the same exposure to loss
every year. For commercial insurance this assumption is frequently
violated.

Consider a commercial policyholder that uses a fleet of vehicles in its
business. In year 1 there are \(m_1\) vehicles in the fleet, \(m_2\)
vehicles in year 2, .., and \(m_n\) vehicles in year \(n\). The exposure
to loss from ownership and use of this fleet is not constant from year
to year. The annual losses for the fleet are not \emph{iid}.

Define \(Y_{jk}\) to be the loss for the \(k^{th}\) vehicle in the fleet
for year \(j\). Then, the total losses for the fleet in year \(j\) are
\(Y_{j1}+\cdots+Y_{jm_j}\) where we are adding up the losses for each of
the \(m_j\) vehicles. In the Bühlmann-Straub model it is assumed that
random variables \(Y_{jk}\) are \emph{iid} across all vehicles and years
for the policyholder. With this assumption the means
E(\(Y_{jk}|\theta)=\mu(\theta)\) and variances
Var(\(Y_{jk}|\theta)=\sigma^2(\theta)\) are the same for all vehicles
and years. The quantity \(\mu(\theta)\) is the expected loss and
\(\sigma^2(\theta)\) is the variance in the loss for one year for one
vehicle for a policyholder with risk parameter \(\theta\).

If \(X_j\) is the average loss per unit of exposure in year \(j\),
\(X_j=(Y_{j1}+\cdots+Y_{jm_j})/m_j\), then E(\(X_j|\theta)=\mu(\theta)\)
and Var(\(X_j|\theta)=\sigma^2(\theta)/m_j\) for policyholder with risk
parameter \(\theta\). Note that we used the fact that the \(Y_{jk}\) are
\emph{iid} for a given policyholder. The average loss per vehicle for
the entire \(n\)-year period is

\begin{equation*}
\bar{X}= \frac{1}{m} \sum_{j=1}^{n} m_j X_{j} \quad , \quad  m=\sum_{j=1}^{n}  m_j. 
\end{equation*}

It follows that E\((\bar{X}|\theta)=\mu(\theta)\) and
Var\((\bar{X}|\theta)=\sigma^2(\theta)/m\) where \(\mu(\theta)\) and
\(\sigma^2(\theta)\) are the mean and variance for a single vehicle for
one year for the policyholder.

\textbf{Example 9.4.1.} Prove that
Var\((\bar{X}|\theta)=\sigma^2(\theta)/m\) for a risk with risk
parameter \(\theta\).

\textbf{Solution}

\begin{eqnarray*}
\mathrm{Var}(\bar{X}|\theta)&=&\mathrm{Var}\left(\frac{1}{m} \sum_{j=1}^{n} m_j X_j|\theta \right)\\
                                  &=&\frac{1}{m^2}\sum_{j=1}^{n} \mathrm{Var}(m_j X_{j}|\theta)=\frac{1}{m^2}\sum_{j=1}^{n} m_j^2 \mathrm{Var}(X_j|\theta)\\
                                  &=&\frac{1}{m^2}\sum_{j=1}^{n} m_j^2 (\sigma^2(\theta)/m_j)=\frac{\sigma^2(\theta)}{m^2}\sum_{j=1}^{n} m_j=\sigma^2(\theta)/m.\\
\end{eqnarray*}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The Buhlmann-Straub credibility estimate is:

\begin{equation}\hat{\mu}(\theta)=Z\bar{X}+(1-Z)\mu 
\label{eq:bscred} 
\end{equation}

with

\begin{eqnarray*} 
\theta&=&\textrm{a risk parameter that identifies a policyholder's risk level}\\
\hat{\mu}(\theta)&=&\textrm{estimated expected loss for one exposure for the policyholder}\\
 & & \textrm{with loss experience } \bar{X}\\
\bar{X}&=& \frac{1}{m} \sum_{j=1}^{n} m_j X_j \textrm{ is the average loss per exposure for $m$ exposures.}\\
 & & \textrm{$X_j$ is the average loss per exposure and $m_j$ is the number of exposures in year $j$.} \\
Z&=&\textrm{credibility assigned to $m$ exposures } \\
 \mu&=&\textrm{expected loss for one exposure for randomly chosen}\\
 & & \textrm{ policyholder from population.}\\
\end{eqnarray*}

Note that \(\hat{\mu}(\theta)\) is the estimator for the expected loss
for one exposure. If the policyholder has \(m_j\) exposures then the
expected loss is \(m_j\hat{\mu}(\theta)\).

In an example in the prior section it was shown that
\(Z\)=Var(E(\(\bar{X}|\theta\)))/Var(\(\bar{X}\)) where \(\bar{X}\) is
the average loss for \(n\) observations. In equation \eqref{eq:bscred} the
\(\bar{X}\) is the average loss for \(m\) exposures and the same \(Z\)
formula can be used:

\[
Z=\frac{\mathrm{Var}(\mathrm{E}(\bar{X}|\theta))}{\mathrm{Var}(\bar{X})}=
\frac{\mathrm{Var}(\mathrm{E}(\bar{X}|\theta))}{\mathrm{E}(\mathrm{Var}(\bar{X}|\theta))+\mathrm{Var}(\mathrm{E}(\bar{X}|\theta))}. 
\]

The denominator was expanded using the law of total variance. As noted
above \(\mathrm{E}(\bar{X}|\theta)=\mu(\theta)\) so
\(\mathrm{Var}(\mathrm{E}(\bar{X}|\theta))=\mathrm{Var}(\mu(\theta))=VHM\).
Because Var\((\bar{X}|\theta)=\sigma^2(\theta)/m\) it follows that
E(Var(\(\bar{X}|\theta\)))=E(\(\sigma^2(\theta))/m\)=\emph{EPV}/m.
Making these substitutions and a little algebra gives

\begin{equation}
Z=\frac{m}{m+K} \quad , \quad K =\frac{EPV}{VHM}. 
\label{eq:bsZ} 
\end{equation}

This is the same \(Z\) as for Bühlmann credibility except number of
exposures \(m\) replaces number of years or observations \(n\).

\textbf{Example 9.4.2.} A commercial automobile policyholder had the
following exposures and claims over a three-year period:
\[\begin{matrix}
\begin{array}{|c|c|c|}
\hline
\text{Year} & \text{Number of Vehicles} & \text{Number of Claims} \\
\hline
1 &   9 &  5  \\
2 & 12 &  4  \\  
3 & 15 &  4  \\              
\hline
\end{array}
\end{matrix}\]

\begin{itemize}
\tightlist
\item
  The number of claims in a year for each vehicle in the policyholder's
  fleet is Poisson distributed with the same mean (parameter)
  \(\lambda\).
\item
  Parameter \(\lambda\) is distributed among the policyholders in the
  population with \emph{pdf} \(f(\lambda)=6\lambda(1-\lambda)\) with
  \(0<\lambda<1\).
\end{itemize}

The policyholder has 18 vehicles in its fleet in year 4. Use
Bühlmann-Straub credibility to estimate the expected number of
policyholder claims in year 4.

\textbf{Solution} The expected number of claims for one vehicle for a
randomly chosen policyholder is
\(\mu=\mathrm{E}(\lambda)=\int_{0}^{1} \lambda[6\lambda(1-\lambda)] d\lambda=1/2\).
The average number of claims per vehicle for the policyholder is
\(\bar{X}\)=13/36. The Expected Value of the Process Variance for a
single vehicle is \emph{EPV}=E(\(\lambda)=1/2\). The Variance of the
Hypothetical Means across policyholders is
\emph{VHM}=Var(\(\lambda\))=E(\(\lambda^2\))-\((\mathrm{E}(\lambda))^2=\int_{0}^{1} \lambda^2[6\lambda(1-\lambda)] d\lambda-(1/2)^2=(3/10)-(1/4)=(6/20)-(5/20)=1/20\).
So, \(K\)=\emph{EPV}/\emph{VHM}=(1/2)/(1/20)=10. The number of exposures
in the experience period is \(m=9+12+15=36\). The credibility is
\(Z=36/(36+10)=18/23\). The credibility-weighted estimate for the number
of claims for one vehicle is
\(\hat{\mu}(\theta)=Z\bar{X}+(1-Z)\mu\)=(18/23)(13/36)+(5/23)(1/2)=9/23.
With 18 vehicles in the fleet in year 4 the expected number of claims is
18(9/23)=162/23=7.04 .

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{Bayesian Inference and Bühlmann
Credibility}\label{bayesian-inference-and-buhlmann-credibility}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Use Bayes Theorem to determine a formula for the expected loss of a
  risk given a likelihood and prior distribution.
\item
  Determine the posterior distributions for the Gamma-Poisson and
  Beta-Binomial Bayesian models and compute expected values.
\item
  Understand the connection between the Bühlmann and Bayesian estimates
  for the Gamma-Poisson and Beta-Binomial models.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Section \ref{S:MS:BayesInference} reviews Bayesian inference and it is
assumed that the reader is familiar with that material. The reader is
also advised to read the Bühlmann credibility section in this chapter.
This section will compare Bayesian inference with Bühlmann credibility
and show connections between the two models.

A risk with risk parameter \(\theta\) has expected loss
\(\mu(\theta)=E(X|\theta)\) with random variable \(X\) representing pure
premium, aggregate loss, number of claims, claim severity, or some other
measure of loss during a period of time. If the risk has \(n\) losses
\(X_1,\ldots, X_n\) during n separate periods of time, then these losses
are assumed to be \(iid\) for the policyholder and
\(\mu(\theta)=E(X_i|\theta)\) for \(i=1,..,n\).

If the risk had \(n\) losses \(x_1,\ldots, x_n\) then
E(\(\mu(\theta)|x_1,\ldots, x_n)\) is the conditional expectation of
\(\mu(\theta)\). The Bühlmann credibility formula
\(\hat{\mu}(\theta)=Z\bar{X}+(1-Z)\mu\) is a linear function of
\(\bar{X}=(x_1+\cdots+x_n)/n\) used to estimate
\(E(\mu(\theta)|x_1,\ldots,x_n)\).

The expectation \(E(\mu(\theta)|x_1,\ldots,x_n)\) can be calculated from
the conditional density function \(f(x|\theta)\) and the posterior
distribution \(\pi(\theta|x_1,\ldots,x_n)\):

\begin{eqnarray*}  
\mathrm{E}(\mu(\theta)|x_1,\ldots,x_n)&=&\int \mu(\theta) \pi(\theta|x_1,\ldots,x_n) d\theta \\
                           \mu(\theta)&=&\mathrm{E}(X|\theta)=\int  xf(x|\theta) dx .\\
\end{eqnarray*}

The posterior distribution comes from Bayes theorem

\begin{equation*} 
\pi(\theta|x_1,\ldots,x_n)=\frac{\prod_{j=1}^{n} f(x_j|\theta)}{f(x_1,\ldots,x_n)}\pi({\theta}).
\end{equation*}

The conditional density function \(f(x|\theta)\) and the prior
distribution \(\pi(\theta)\) must be specified. The numerator
\(\prod_{j=1}^{n} f(x_j|\theta)\) on the right-hand side is called the
likelihood. The denominator \(f(x_1,\ldots,x_n)\) is the joint density
function for \(n\) losses \(x_1,\ldots,x_n\).

\subsection{Gamma-Poisson Model}\label{gamma-poisson-model}

In the Gamma-Poisson model the number of claims \(X\) has a Poisson
distribution Pr(\(X=x|\lambda)=\lambda^xe^{-\lambda}/x!\) for a risk
with risk parameter \(\lambda\). The prior distribution for \(\lambda\)
is gamma with
\(\pi(\lambda)=\beta^\alpha\lambda^{\alpha-1}e^{-\beta\lambda}/\Gamma(\alpha)\).
(Note that a rate parameter \(\beta\) is being used in the gamma
distribution rather than a scale parameter.) The mean of the gamma is
E(\(\lambda)=\alpha/\beta\) and the variance is
Var(\(\lambda)=\alpha/\beta^2\). In this section we will assume that
\(\lambda\) is the expected number of claims per year though we could
have chosen another time interval.

If a risk is selected at random from the population then the expected
number of claims in a year is
E(\(N\))=E(E(\(N|\lambda\)))=E(\(\lambda\))=\(\alpha/\beta\). If we had
no observations for the selected risk then the expected number of claims
for the risk is \(\alpha/\beta\).

During \(n\) years the following number of claims by year was observed
for the randomly selected risk: \(x_1,\ldots,x_n\). From Bayes theorem
the posterior distribution is

\begin{equation*} 
\pi(\lambda|x_1,\ldots,x_n)=\frac{\prod_{j=1}^{n} (\lambda^{x_j}e^{-\lambda}/x_j!)}{\Pr(X_1=x_1,\ldots,X_n=x_n)}\beta^\alpha\lambda^{\alpha-1}e^{-\beta\lambda}/\Gamma(\alpha). 
\end{equation*}

\noindent Combining terms that have a \(\lambda\) and putting all other
terms into constant \(C\) gives

\begin{equation*} 
\pi(\lambda|x_1,\ldots,x_n)=C\lambda^{(\alpha+\sum_{j=1}^{n}x_j)-1}e^{-(\beta+n)\lambda}. 
\end{equation*}

This is a gamma distribution with parameters
\(\alpha'=\alpha+\sum_{j=1}^{n}x_j\) and \(\beta'=\beta+n\). The
constant must be \(C={\beta'}^{\alpha'}/\Gamma(\alpha')\) so that
\(\int_{0}^{\infty}\pi(\lambda|x_1,\ldots,x_n) d\lambda=1\) though we do
not need to know \(C\). As explained in chapter four the gamma
distribution is a conjugate prior for the Poisson distribution so the
posterior distribution is also gamma.

Because the posterior distribution is gamma the expected number of
claims for the selected risk is

\begin{equation*}  
\mathrm{E}(\lambda|x_1,\ldots,x_n) = \frac{\alpha+\sum_{j=1}^{n}x_j}{\beta+n}=\frac{\alpha + \textrm{number of claims}}{\beta+\textrm{number of years}}. 
\end{equation*}

This formula is slightly different from chapter four because parameter
\(\beta\) is multiplied by \(\lambda\) in the exponential of the gamma
\emph{pdf} whereas in chapter four \(\lambda\) is divided by parameter
\(\theta\). We have chosen this form for the exponential to simplify the
equation for the expected number of claims.

Now we will compute the Bühlmann credibility estimate for the
Gamma-Poisson model. The variance for a Poisson distribution with
parameter \(\lambda\) is \(\lambda\) so
\emph{EPV}=E(Var(\(X|\lambda\)))=E(\(\lambda\))=\(\alpha/\beta\). The
mean number of claims per year for the risk is \(\lambda\) so
\emph{VHM}=Var(E(\(X|\lambda\)))=Var(\(\lambda\))=\(\alpha/\beta^2\).
The credibility parameter is
\(K\)=\emph{EPV}/\emph{VHM}=\((\alpha/\beta)/(\alpha/\beta^2)=\beta\).
The overall mean is E(E(\(X|\lambda\)))=E(\(\lambda\))=\(\alpha/\beta\).
The sample mean is \(\bar{X}=(\sum_{j=1}^{n}x_j)/n\). The
credibility-weighted estimate for the expected number of claims for the
risk is

\begin{equation*} 
\hat{\mu}=\frac{n}{n+\beta}\frac{\sum_{j=1}^{n}x_j}{n} +(1-\frac{n}{n+\beta})\frac{\alpha}{\beta}=\frac{\alpha+\sum_{j=1}^{n}x_j}{\beta+n}. 
\end{equation*}

For the Gamma-Poisson model the Bühlmann credibility estimate matches
the Bayesian analysis result.

\subsection{Beta-Binomial Model}\label{beta-binomial-model}

The Beta-Binomial model is useful for modeling the probability of an
event. Assume that random variable \(X\) is the number of successes in
\(n\) trials and that \(X\) has a binomial distribution
Pr(\(X=x|p)=\binom{n}{x}p^x(1-p)^{n-x}\). In the Beta-Binomial model the
prior distribution for probability \(p\) is a beta distribution with
\emph{pdf}

\begin{equation*}  
\pi(p)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{\alpha-1}(1-p)^{\beta-1} , \quad  0<p<1, \alpha>0, \beta>0.
\end{equation*}

The posterior distribution for \(p\) given an outcome of \(x\) successes
in \(n\) trials is

\begin{equation*} 
\pi(p|x)=\frac{\binom{n}{x}p^x(1-p)^{n-x}}{\Pr(x)}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{\alpha-1}(1-p)^{\beta-1}.
\end{equation*}

Combining terms that have a \(p\) and putting everything else into the
constant \(C\) yields

\begin{equation*} 
\pi(p| x)=Cp^{\alpha+x-1}(1-p)^{\beta+(n-x)-1}.
\end{equation*}

This is a beta distribution with new parameters
\(\alpha^\prime=\alpha+x\) and \(\beta^\prime=\beta+(n-x)\). The
constant must be

\begin{equation*} 
C=\frac{\Gamma(\alpha+\beta+n)}{\Gamma(\alpha+x)\Gamma(\beta+n-x)}.
\end{equation*}

The mean for the beta distribution with parameters \(\alpha\) and
\(\beta\) is E(\(p)=\alpha/(\alpha+\beta)\). Given \(x\) successes in
\(n\) trials in the Beta-Binomial model the mean of the posterior
distribution is

\begin{equation*} 
E(p|x)=\frac{\alpha+x}{\alpha+\beta+n}.  
\end{equation*}

As the number of trials \(n\) and successes \(x\) increase, the expected
value of \(p\) approaches \(x/n\).

The Bühlmann credibility estimate for E(\(p|x\)) is exactly as the same
as the Bayesian estimate as demonstrated in the following example.

\textbf{Example 9.5.1} The probability that a coin toss will yield heads
is \(p\). The prior distribution for probability \(p\) is beta with
parameters \(\alpha\) and \(\beta\). On \(n\) tosses of the coin there
were exactly \(x\) heads. Use Bühlmann credibility to estimate the
expected value of \(p\).

\textbf{Solution} Define random variables \(Y_j\) such that \(Y_j=1\) if
the \(j^{th}\) coin toss is heads and \(Y_j=0\) if tails for
\(j=1,\ldots, n\). Random variables \(Y_j\) are \emph{iid} conditional
on \(p\) with Pr\([Y=1|p]=p\) and Pr\([Y=0|p]=1-p\) The number of heads
in \(n\) tosses can be represented by the random variable
\(X=Y_1+\cdots+Y_n\). We want to estimate \(p=E[Y_j]\) using Bühlmann
credibility: \(\hat{p} = Z\bar{Y} +(1-Z)\mu\). The overall mean is
\(\mu=E(E(Y_j|p))=E(p)=\alpha/(\alpha+\beta)\). The sample mean is
\(\bar{y}=x/n\). The credibility is \(Z=n/(n+K)\) and
K=\emph{EPV}/\emph{VHM}. With Var\((Y_j|p)=p(1-p)\) it follows that
\emph{EPV}=E(Var\((Y_j|p)\))=E(\(p(1-p)\)). Because E\((Y_j|p)=p\) then
\emph{VHM}=Var\((E(Y_j|p))\)=Var(\(p\)). For the beta distribution

\begin{equation*}  
\mathrm{E}(p)=\frac{\alpha}{\alpha+\beta}, \mathrm{E}(p^2)=\frac{\alpha(\alpha+1)}{(\alpha+\beta)(\alpha+\beta+1)}, \textrm{ and } \mathrm{Var}(p)=\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}.
\end{equation*}

Parameter
\(K\)=\emph{EPV}/\emph{VHM}={[}E(\(p\))-E(\(p^2\)){]}/Var(\(p\)). With
some algebra this reduces to \(K=\alpha+\beta\). The Bühlmann
credibility-weighted estimate is

\begin{align*}
  \hat{p} &= \frac{n}{n+\alpha+\beta}\left(\frac{x}{n}\right)+\left(1-\frac{n}{n+\alpha+\beta}\right)\frac{\alpha}{\alpha+\beta} \\
  \hat{p} & =\frac{\alpha+x}{\alpha+\beta+n}\\
\end{align*}

which is the same as the Bayesian posterior mean.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Exact Credibility}\label{exact-credibility}

As demonstrated in the prior section, the Bühlmann credibility estimates
for the Gamma-Poisson and Beta-Binomial models exactly match the
Bayesian analysis results. The term exact credibility is applied in
these situations. Exact credibility may occur if the probability
distribution for \(X_j\) is in the linear exponential family and the
prior distribution is a conjugate prior. Besides these two models,
examples of exact credibility also include Gamma-Exponential and
Normal-Normal models.

It is also noteworthy that if the conditional mean
E\((\mu(\theta)|X_1,...,X_n)\) is linear in the past observations, then
the Bühlmann credibility estimate will coincide with the Bayesian
estimate. More information about exact credibility can be found in
\citep{buhlmanngisler}, \citep{klugman2012}, and \citep{tse}.

\section{Estimating Credibility
Parameters}\label{estimating-credibility-parameters}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Perform nonparametric estimation with the Bühlmann and Bühlmann-Straub
  credibility models.
\item
  Identify situations when semiparametric estimation is appropriate.
\item
  Use data to approximate the \emph{EPV} and \emph{VHM}.
\item
  Balance credibility-weighted estimates.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The examples in this chapter have provided assumptions for calculating
credibility parameters. In actual practice the actuary must use real
world data and judgment to determine credibility parameters.

\subsection{Full Credibility Standard for Limited Fluctuation
Credibility}\label{full-credibility-standard-for-limited-fluctuation-credibility}

Limited-fluctuation credibility requires a full credibility standard.
The general formula for aggregate losses or pure premium, as obtained in
formula \eqref{eq:full-credibility-losses}, is

\begin{equation*}
n_S=\left(\frac{y_p}{k}\right)^2\left[\left(\frac{\sigma_N^2}{\mu_N}\right)+\left(\frac{\sigma_X}{\mu_X}\right)^2\right]
\end{equation*}

\noindent with \(N\) representing number of claims and \(X\) the size of
claims. If one assumes \(\sigma_X=0\) then the full credibility standard
for frequency results. If \(\sigma_N=0\) then the full credibility
formula for severity follows. Probability \(p\) and \(k\) value are
often selected using judgment and experience.

In practice it is often assumed that the number of claims is Poisson
distributed so that \(\sigma_N^2/\mu_N=1\). In this case the formula can
be simplified to

\begin{equation*}
n_S=\left(\frac{y_p}{k}\right)^2\left[\frac{\mathrm{E}(X^2)}{(\mathrm{E}(X))^2}\right].
\end{equation*}

An empirical mean and second moment for the sizes of individual claim
losses can be computed from past data, if available.

\subsection{Nonparametric Estimation for Bühlmann and Bühlmann-Straub
Models}\label{nonparametric-estimation-for-buhlmann-and-buhlmann-straub-models}

Bayesian analysis as described previously requires assumptions about a
prior distribution and likelihood. It is possible to produce estimates
without these assumptions and these methods are often referred to as
empirical Bayes methods. Bühlmann and Bühlmann-Straub credibility with
parameters estimated from the data are included in the category of
empirical Bayes methods.

\textbf{Bühlmann Model} First we will address the simpler Bühlmann
model. Assume that there are \(r\) risks in a population. For risk \(i\)
with risk parameter \(\theta_i\) the losses for \(n\) periods are
\(X_{i1},\ldots, X_{in}\). The losses for a given risk are \emph{iid}
across periods as assumed in the Bühlmann model. For risk \(i\) the
sample mean is \(\bar{X}_i=\sum_{j=1}^{n}X_{ij}/n\) and the unbiased
sample process variance is
\(s_i^2=\sum_{j=1}^{n}(X_{ij}-\bar{X}_i)^2/(n-1)\). An unbiased
estimator for the \emph{EPV} can be calculated by taking the average of
\(s_i^2\) for the \(r\) risks in the population:

\begin{equation}  
\widehat{EPV}=\frac{1}{r}\sum_{i=1}^{r} s_i^2 = \frac{1}{r(n-1)} \sum_{i=1}^{r} \sum_{j=1}^{n}(X_{ij}-\bar{X}_i)^2 .
\label{eq:EPV-estimate}
\end{equation}

The individual risk means \(\bar{X}_i\) for \(i=1,\ldots, r\) can be
used to estimate the \emph{VHM}. An unbiased estimator of
Var(\(\bar{X}_i\)) is

\begin{equation*} 
\widehat{\mathrm{Var}}(\bar{X}_i)=\frac{1}{r-1} \sum_{i=1}^{r}(\bar{X}_i-\bar{X})^2 \textrm{  and  }  \bar{X}=\frac{1}{r}\sum_{i=1}^{r} \bar{X}_i,
\end{equation*}

but Var(\(\bar{X}_i\)) is not the \emph{VHM}. The total variance formula
or \emph{unconditional variance} formula is

\begin{equation*} 
\mathrm{Var}(\bar{X}_i)=\textrm{E(Var}(\bar{X}_i|\Theta=\theta_i))+\textrm{Var(E}(\bar{X}_i|\Theta=\theta_i)).
\end{equation*}

The \emph{VHM} is the second term on the right because
\(\mu(\theta_i)=\mathrm{E}(\bar{X}_i|\Theta=\theta_i)\) is the
hypothetical mean for risk \(i\). So,

\begin{equation*} 
VHM=\textrm{Var(E}(\mu(\theta_i)) = \mathrm{Var}(\bar{X}_i) - \textrm{E(Var}(\bar{X}_i|\Theta=\theta_i)).
\end{equation*}

As discussed previously in Section \ref{S:EPV-VHM-Z}, \emph{EPV}/n =
E(Var(\(\bar{X}_i|\Theta=\theta_i\))) and using the above estimators
gives an unbiased estimator for the \emph{VHM}:

\begin{equation} 
\widehat{VHM} = \frac{1}{r-1} \sum_{i=1}^{r}(\bar{X}_i-\bar{X})^2 - \frac{\widehat{EPV}}{n} .
\label{eq:VHM-estimate}
\end{equation}

Although the expected loss for a risk with parameter \(\theta_i\) is
\(\mu(\theta_i)\)=E(\(\bar{X}_i|\Theta=\theta_i\)), the variance of the
sample mean \(\bar{X}_i\) is greater than or equal to the variance of
the hypothetical means: Var(\(\bar{X}_i)\geq\)Var(\(\mu(\theta_i)\)).
The variance in the sample means Var(\(\bar{X}_i\)) includes both the
variance in the hypothetical means plus a process variance term.

In some cases formula \eqref{eq:VHM-estimate} can produce a negative value
for \(\widehat{VHM}\) because of the subtraction of \(\widehat{EPV}/n\),
but a variance cannot be negative. The process variance within risks is
so large that it overwhelms the measurement of the variance in means
between risks. In this case we cannot use this method to determine the
values needed for Bühlmann credibility.

\textbf{Example 9.6.1.} Two policyholders had claims over a three-year
period as shown in the table below. Estimate the expected number of
claims for each policyholder using Bühlmann credibility and calculating
necessary parameters from the data.

\[\begin{matrix}
\begin{array}{|c|c|c|}
\hline
\text{Year} & \text{Risk A} & \text{Risk B} \\
\hline
1 & 0 &  2 \\
2 & 1 &  1  \\  
3 & 0 &  2  \\              
\hline
\end{array}
\end{matrix}\]

\textbf{Solution} \(\bar{x}_A=\frac{1}{3}(0+1+0)=\frac{1}{3}\),
\(\bar{x}_B=\frac{1}{3}(2+1+2)=\frac{5}{3}\)

\(\bar{x}=\frac{1}{2}(\frac{1}{3}+\frac{5}{3})=1\)

\(s_A^2=\frac{1}{3-1}\left[(0-\frac{1}{3})^2+(1-\frac{1}{3})^2+(0-\frac{1}{3})^2\right]=\frac{1}{3}\)

\(s_B^2=\frac{1}{3-1}\left[(2-\frac{5}{3})^2+(1-\frac{5}{3})^2+(2-\frac{5}{3})^2\right]=\frac{1}{3}\)

\(\widehat{EPV}=\frac{1}{2}\left(\frac{1}{3}+\frac{1}{3}\right)=\frac{1}{3}\)

\(\widehat{VHM}=\frac{1}{2-1}\left[(\frac{1}{3}-1)^2+(\frac{5}{3}-1)^2\right]-\frac{1/3}{3}=\frac{7}{9}\)

\(K=\frac{1/3}{7/9}=\frac{3}{7}\)

\(Z=\frac{3}{3+(3/7))}=\frac{7}{8}\)

\(\hat{\mu}_A=\frac{7}{8}\left(\frac{1}{3}\right)+(1-\frac{7}{8})1=\frac{5}{12}\)

\(\hat{\mu}_B=\frac{7}{8}\left(\frac{5}{3}\right)+(1-\frac{7}{8})1=\frac{19}{12}\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 9.6.2.} Two policyholders had claims over a three-year
period as shown in the table below. Calculate the nonparametric estimate
for the \emph{VHM}.

\[\begin{matrix}
\begin{array}{|c|c|c|}
\hline
\text{Year} & \text{Risk A} & \text{Risk B} \\
\hline
1 & 3 &  3 \\
2 & 0 &  0  \\  
3 & 0 &  3  \\              
\hline
\end{array}
\end{matrix}\]

\textbf{Solution} \(\bar{x}_A=\frac{1}{3}(3+0+0)=1\),
\(\bar{x}_B=\frac{1}{3}(3+0+3)=2\)

\(\bar{x}=\frac{1}{2}(1+2)=\frac{3}{2}\)

\(s_A^2=\frac{1}{3-1}\left[(3-1)^2+(0-1)^2+(0-1)^2\right]=3\)

\(s_B^2=\frac{1}{3-1}\left[(3-2)^2+(0-2)^2+(3-2)^2\right]=3\)

\(\widehat{EPV}=\frac{1}{2}(3+3)=3\)

\(\widehat{VHM}=\frac{1}{2-1}\left[(1-\frac{3}{2})^2+(2-\frac{3}{2})^2\right]-\frac{3}{3}=-\frac{1}{2}.\)

The process variance is so large that it is not possible to estimate the
\emph{VHM}.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Bühlmann-Straub Model} Empirical formulas for \emph{EPV} and
\emph{VHM} in the Bühlmann-Straub model are more complicated because a
risk's number of exposures can change from one period to another. Also,
the number of experience periods does not have to be constant across the
population. First some definitions:

\begin{itemize}
\tightlist
\item
  \(X_{ij}\) is the losses per exposure for risk \(i\) in period \(j\).
  Losses can refer to number of claims or amount of loss. There are
  \(r\) risks so \(i=1,\ldots,r\).
\item
  \(n_i\) is the number of observation periods for risk \(i\)
\item
  \(m_{ij}\) is the number of exposures for risk \(i\) in period \(j\)
  for \(j=1,\ldots,n_i\)
\end{itemize}

Risk \(i\) with risk parameter \(\theta_i\) has \(m_{ij}\) exposures in
period \(j\) which means that the losses per exposure random variable
can be written as \(X_{ij}=(Y_{i1}+\cdots+Y_{im_{ij}})/m_{ij}\). Random
variable \(Y_{ik}\) is the loss for one exposure. For risk \(i\) losses
\(Y_{ik}\) are \emph{iid} with mean E(\(Y_{ik}\))=\(\mu(\theta_i)\) and
process variance Var(\(Y_{ik}\))=\(\sigma^2(\theta_i)\). It follows that
Var(\(X_{ij})\)=\(\sigma^2(\theta_i)/m_{ij}\).

Two more important definitions are:

\begin{itemize}
\tightlist
\item
  \(\bar{X}_i=\frac{1}{m_i}\sum_{j=1}^{n_i} m_{ij}X_{ij}\) with
  \(m_i = \sum_{j=1}^{n_i} m_{ij}\). \(\bar{X}_i\) is the average loss
  per exposure for risk \(i\) for all observation periods combined.
\item
  \(\bar{X}=\frac{1}{m}\sum_{i=1}^{r} m_i \bar{X}_i\) with
  \(m=\sum_{i=1}^r m_i\). \(\bar{X}\) is the average loss per exposure
  for all risks for all observation periods combined.
\end{itemize}

An unbiased estimator for the process variance \(\sigma^2(\theta_i)\) of
one exposure for risk \(i\) is

\begin{equation*}  
{s_i}^2=\frac{\sum_{j=1}^{n_i} m_{ij}(X_{ij}-\bar{X}_i)^2}{n_i-1}. \end{equation*}

The weights \(m_{ij}\) are applied to the squared differences because
the \(X_{ij}\) are the averages of \(m_{ij}\) exposures. The weighted
average of the sample variances \({s_i}^2\) for each risk \(i\) in the
population with weights proportional to the number of \((n_i-1)\)
observation periods will produce the expected value of the process
variance (\emph{EPV}) estimate

\begin{equation*}  
\widehat{EPV}=\frac{\sum_{i=1}^r  (n_i-1){s_i}^2}{\sum_{i=1}^r (n_i-1)}=\frac{\sum_{i=1}^r \sum_{j=1}^{n_i} m_{ij}(X_{ij}-\bar{X}_i)^2}{\sum_{i=1}^r (n_i-1)}.     
\end{equation*}

The quantity \(\widehat{EPV}\) is an unbiased estimator for the expected
value of the process variance of one exposure for a risk chosen at
random from the population.

To calculate an estimator for the variance in the hypothetical means
(\emph{VHM}) the squared differences of the individual risk sample means
\(\bar{X}_i\) and population mean \(\bar{X}\) are used. An unbiased
estimator for the \emph{VHM} is

\begin{equation*}  
\widehat{VHM}=\frac{\sum_{i=1}^r m_i(\bar{X}_i-\bar{X})^2 - (r-1)\widehat{EPV}}{m-\frac{1}{m}\sum_{i=1}^r m_i^2}.  
\end{equation*}

This complicated formula is necessary because of the varying number of
exposures. Proofs that the \emph{EPV} and \emph{VHM} estimators shown
above are unbiased can be found in several references mentioned at the
end of this chapter including \citep{buhlmanngisler},
\citep{klugman2012}, and \citep{tse}.

\textbf{Example 9.6.3.} Two policyholders had claims shown in the table
below. Estimate the expected number of claims per vehicle for each
policyholder using Bühlmann-Straub credibility and calculating
parameters from the data.

\[\begin{matrix}
\begin{array}{|c|c|c|c|c|c|}
\hline
\text{Policyholder} &  & \text{Year 1} & \text{Year 2} & \text{Year 3} & \text{Year 4} \\
\hline
\text{A} & \text{Number of claims} & 0 & 2 & 2 & 3 \\
\hline
\text{A} & \text{Insured vehicles} &  1 & 2 & 2 & 2\\  
\hline
 & & & & & \\
\hline
\text{B} & \text{Number of claims} & 0 & 0 & 1 & 2\\    
\hline 
\text{B} & \text{Insured vehicles} &  0 & 2 & 3 & 4\\      
\hline
\end{array}
\end{matrix}\]

\textbf{Solution} \(\bar{x}_A=\frac{0+2+2+3}{1+2+2+2}=1\)

\(\bar{x}_B=\frac{0+1+2}{2+3+4}=\frac{1}{3}\)

\(\bar{x}=\frac{7(1)+9(1/3)}{7+9}=\frac{5}{8}\)

\(s_A^2=\frac{1}{4-1}\left[1(0-1)^2+2(1-1)^2+2(1-1)^2+2(\frac{3}{2}-1)^2\right]=\frac{1}{2 }\)

\(s_B^2=\frac{1}{3-1}\left[2(0-\frac{1}{3})^2+3(\frac{1}{3}-\frac{1}{3})^2+4(\frac{1}{2}-\frac{1}{3})^2\right]=\frac{1}{6}\)

\(\widehat{EPV}=\left[3\left(\frac{1}{2}\right)+2\left(\frac{1}{6}\right)\right]/(3+2)=\frac{11}{30}=0.3667\)

\(\widehat{VHM}=\left[(7(1-\frac{5}{8})^2+9(\frac{1}{3}-\frac{5}{8})^2-(2-1)\frac{11}{30}\right]/\left[16-\left(\frac{1}{16}\right)(7^2+9^2)\right]=0.1757\)

\(K=\frac{0.3667}{0.1757}=2.0871\)

\(m_A=7\), \(m_B=9\)

\(Z_A=\frac{7}{7+2.0871}=0.7703\), \(Z_B=\frac{9}{9+2.0871}=0.8118\)

\(\hat{\mu}_A=0.7703(1)+(1-0.7703)(5/8)=0.9139\)

\(\hat{\mu}_B=0.8118(1/3)+(1-0.8118)(5/8)=0.3882\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Semiparametric Estimation for Bühlmann and Bühlmann-Straub
Models}\label{semiparametric-estimation-for-buhlmann-and-buhlmann-straub-models}

In the prior section on nonparametric estimation, there were no
assumptions about the distribution of the losses per exposure
\(X_{ij}\). Assuming that the \(X_{ij}\) have a particular distribution
and using properties of the distribution along with the data to
determine credibility parameters is referred to as semiparametric
estimation.

An example of semiparametric estimation would be the assumption of a
Poisson distribution when estimating claim frequencies. The Poisson
distribution has the property that the mean and variance are identical
and this property can simplify calculations. The following simple
example comes from the prior section but now includes a Poisson
assumption about claim frequencies.

\textbf{Example 9.6.4.} Two policyholders had claims over a three-year
period as shown in the table below. Assume that the number of claims for
each risk has a Poisson distribution. Estimate the expected number of
claims for each policyholder using Bühlmann credibility and calculating
necessary parameters from the data. \[\begin{matrix}
\begin{array}{|c|c|c|}
\hline
\text{Year} & \text{Risk A} & \text{Risk B} \\
\hline
1 & 0 &  2 \\
2 & 1 &  1  \\  
3 & 0 &  2  \\              
\hline
\end{array}
\end{matrix}\]

\textbf{Solution} \(\bar{x}_A=\frac{1}{3}(0+1+0)=\frac{1}{3}\),
\(\bar{x}_B=\frac{1}{3}(2+1+2)=\frac{5}{3}\)

\(\bar{x}=\frac{1}{2}(\frac{1}{3}+\frac{5}{3})=1\)

With Poisson assumption the estimated variance for risk A is
\(\hat\sigma_A^2=\bar{x}_A=\frac{1}{3}\)

Similarly, \(\hat\sigma_B^2=\bar{x}_B=\frac{5}{3}\)

\(\widehat{EPV}=\frac{1}{2}(\frac{1}{3})+\frac{1}{2}(\frac{5}{3})=1\).
This is also \(\bar{x}\) because of Poisson assumption.

\(\widehat{VHM}=\frac{1}{2-1}\left[(\frac{1}{3}-1)^2+(\frac{5}{3}-1)^2\right]-\frac{1}{3}=\frac{5}{9}\)

\(K=\frac{1}{5/9}=\frac{9}{5}\)

\(Z_A=Z_B=\frac{3}{3+(9/5)}=\frac{5}{8}\)

\(\hat{\mu}_A=\frac{5}{8}\left(\frac{1}{3}\right)+(1-\frac{5}{8})1=\frac{7}{12}\)

\(\hat{\mu}_B=\frac{5}{8}\left(\frac{5}{3}\right)+(1-\frac{5}{8})1=\frac{17}{12}.\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Although we assumed that the number of claims for each risk was Poisson
distributed in the prior example, we did not need this additional
assumption because there was enough information to use nonparametric
estimation. In fact, the Poisson assumption might not be appropriate
because for risk B the sample mean is not equal to the sample variance:
\(\bar{x}_B=\frac{5}{3}\neq s_B^2=\frac{1}{3}\).

The following example is commonly used to demonstrate a situation where
semiparametric estimation is needed. There is insufficient information
for nonparametric estimation but with the Poisson assumption, estimates
can be calculated.

\textbf{Example 9.6.5.} A portfolio of 2,000 policyholders generated the
following claims profile during a five-year period: \[\begin{matrix}
\begin{array}{|c|c|}
\hline
\text{Number of Claims} &   \\
\text{In 5 Years}           &  \text{Number of policies}\\
\hline
 0 &  923 \\
 1 &  682 \\  
 2 &  249 \\  
 3 &  70   \\
 4 &  51   \\  
 5 &  25   \\     
\hline
\end{array}
\end{matrix}\] In your model you assume that the number of claims for
each policyholder has a Poisson distribution and that a policyholder's
expected number of claims is constant through time. Use Bühlmann
credibility to estimate the annual expected number of claims for
policyholders with 3 claims during the five-year period.

\textbf{Solution} Let \(\theta_i\) be the risk parameter for the
\(i^{th}\) risk in the portfolio with mean \(\mu(\theta_i)\) and
variance \(\sigma^2(\theta_i)\). With the Poisson assumption
\(\mu(\theta_i)=\sigma^2(\theta_i)\). The expected value of the process
variance is EPV=E(\(\sigma^2(\theta_i)\)) where the expectation is taken
across all risks in the population. Because of the Poisson assumption
for all risks it follows that
EPV=E(\(\sigma^2(\theta_i)\))=E(\(\mu(\theta_i)\)). An estimate for the
annual expected number of claims is \(\hat{\mu}(\theta_i)\)= (observed
number of claims)/5. This can also serve as the estimate for the
expected value of the process variance for a risk. Weighting the process
variance estimates (or means) by the number of policies in each group
gives the estimators

\begin{equation*}  
\widehat{EPV}=\bar{x}=\frac{923(0)+682(1)+249(2)+70(3)+51(4)+25(5)}{(5)(2000)}=0.1719.
\end{equation*}

Using the formula (\eqref{eq:VHM-estimate}), the \emph{VHM} estimator is

\begin{eqnarray*}
\widehat{VHM}&=&\frac{1}{2000-1}[923(0-0.1719)^2+682(0.20-0.1719)^2+249(0.40-0.1719)^2\\
                            &   &+70(0.60-0.1719)^2+51(0.80-0.1719)^2+25(1-0.1719)^2]-\frac{0.1719}{5}\\
                            &=& 0.0111\\
               \hat{K}  &=& \widehat{EPV}/\widehat{VHM}=0.1719/0.0111=15.49\\
               \hat{Z}  &=& \frac{5}{5+15.49}=0.2440\\
               \hat{\mu}_{3 \textrm{ claims}}& = & 0.2440(3/5)+(1-0.2440)0.1719=0.2764 .\\
\end{eqnarray*}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Balancing Credibility
Estimators}\label{balancing-credibility-estimators}

The credibility weighted model
\(\hat{\mu}(\theta_i)=Z_i\bar{X}_i+(1-Z_i)\bar{X}\), where \(\bar{X}_i\)
is the loss per exposure for risk \(i\) and \(\bar{X}\) is loss per
exposure for the population, can be used to estimate the expected loss
for risk \(i\). The overall mean is
\(\bar{X}=\sum_{i=1}^r(m_i/m) \bar{X}_i\) where \(m_i\) and \(m\) are
number of exposures for risk \(i\) and population, respectively.

For the credibility weighted estimators to be in balance we want

\begin{equation*}   
\bar{X}=\sum_{i=1}^r(m_i/m) \bar{X}_i=\sum_{i=1}^r(m_i/m) \hat{\mu}(\theta_i).
\end{equation*}

If this equation is satisfied then the estimated losses for each risk
will add up to the population total, an important goal in ratemaking,
but this may not happen if \(\bar{X}\) is used for the complement of
credibility.

In order to find a complement of credibility that will bring the
credibility-weighted estimators into balance we will set \(\hat{\mu}\)
as the complement of credibility and analyze the following equation:

\begin{equation*}   
\sum_{i=1}^r(m_i/m) \bar{X}_i=\sum_{i=1}^r(m_i/m) (Z_i\bar{X}_i+(1-Z_i)\hat{\mu}) .
\end{equation*}

A little algebra gives

\begin{equation*}   
\sum_{i=1}^r m_i \bar{X}_i=\sum_{i=1}^r m_i Z_i\bar{X}_i + \hat{\mu}\sum_{i=1}^r m_i(1-Z_i),
\end{equation*}

and

\begin{equation*}  
\hat{\mu}=\frac{\sum_{i=1}^r m_i(1-Z_i)\bar{X}_i}{\sum_{i=1}^r m_i(1-Z_i)}. \end{equation*}

Using this value for \(\hat{\mu}\) will bring the credibility weighted
estimators into balance.

If credibilities \(Z_i\) were computed using the Bühlmann-Straub model,
then \(Z_i=m_i/(m_i+K)\). The prior formula can be simplified using the
following relationship

\begin{equation*}  
m_i(1-Z_i)=m_i\left(1-\frac{m_i}{m_i+K}\right)=m_i\left(\frac{(m_i+K)-m_i}{m_i+K}\right)=KZ_i .
\end{equation*}

Therefore, a complement of credibility that will bring the
credibility-weighed estimators into balance with the overall mean loss
per exposure is

\begin{equation*}  
\hat{\mu}=\frac{\sum_{i=1}^r  Z_i \bar{X}_i}{\sum_{i=1}^r  Z_i}. \end{equation*}

\textbf{Example 9.6.6.} An example from the nonparametric
Bühlmann-Straub section had the following data for two risks. Find the
complement of credibility \(\hat{\mu}\) that will produce
credibility-weighted estimates that are in balance.

\[\begin{matrix}
\begin{array}{|c|c|c|c|c|c|}
\hline
\text{Policyholder} &  & \text{Year 1} & \text{Year 2} & \text{Year 3} & \text{Year 4} \\
\hline
\text{A} & \text{Number of claims} & 0 & 2 & 2 & 3 \\
\hline
\text{A} & \text{Insured vehicles} &  1 & 2 & 2 & 2\\  
\hline
 & & & & & \\
\hline
\text{B} & \text{Number of claims} & 0 & 0 & 1 & 2\\    
\hline 
\text{B} & \text{Insured vehicles} &  0 & 2 & 3 & 4\\      
\hline
\end{array}
\end{matrix}\]

\textbf{Solution} The credibilities from the prior example are
\(Z_A=\frac{7}{7+2.0871}=0.7703\) and \(Z_B=\frac{9}{9+2.0871}=0.8118\).
The sample means are \(\bar{x}_A=1\) and \(\bar{x}_B=1/3\). The balanced
complement of credibility is

\begin{equation*}  
\hat{\mu}=\frac{0.7703(1)+0.8118(1/3)}{0.7703+0.8118}=0.6579.
\end{equation*}

The updated credibility estimates are
\(\hat{\mu}_A=0.7703(1)+(1-0.7703)(.6579)=0.9214\) versus the previous
0.9139 and \(\hat{\mu}_B=0.8118(1/3)+(1-0.8118)(.6579)=0.3944\) versus
the previous 0.3882. Checking the balance on the new estimators:
(7/16)(0.9214)+(9/16)(0.3944)=0.6250. This exactly matches
\(\bar{X}=10/16=0.6250\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{Further Resources and
Contributors}\label{Cred-further-reading-and-resources}

\subsubsection*{Exercises}\label{exercises-4}
\addcontentsline{toc}{subsubsection}{Exercises}

Here are a set of exercises that guide the viewer through some of the
theoretical foundations of \textbf{Loss Data Analytics}. Each tutorial
is based on one or more questions from the professional actuarial
examinations, typically the Society of Actuaries Exam C.

\href{https://www.ssc.wisc.edu/~jfrees/loss-data-analytics/loss-data-analyticscredibility-guided-tutorials/}{Credibility
Guided Tutorials}

\subsubsection*{Contributors}\label{contributors-6}
\addcontentsline{toc}{subsubsection}{Contributors}

\begin{itemize}
\tightlist
\item
  \textbf{Gary Dean}, Ball State University is the author of the initial
  version of this chapter. Email:
  \href{mailto:cgdean@bsu.edu}{\nolinkurl{cgdean@bsu.edu}} for chapter
  comments and suggested improvements.
\item
  Chapter reviewers include: Liang (Jason) Hong, Ambrose Lo, Ranee
  Thiagarajah, Hongjuan Zhou.
\end{itemize}

\chapter{Insurance Portfolio Management including
Reinsurance}\label{C:PortMgt}

\emph{Chapter Preview}. An insurance portfolio is simply a collection of
insurance contracts. To help manage the uncertainty of the portfolio,
this chapter

\begin{itemize}
\tightlist
\item
  quantifies unusually large obligations by examining the tail of the
  distribution,
\item
  quantifies the overall riskiness by introducing summaries known as
  risk measures, and
\item
  discusses options of spreading portfolio risk through reinsurance, the
  purchase of insurance protection by an insurer.
\end{itemize}

\section{Introduction to Insurance
Portfolios}\label{introduction-to-insurance-portfolios}

Most of our analyses in prior chapters has been at the contract level
which is an agreement between a policyholder and an insurer. Insurers
hold, and manage, portfolios that are simply collections of contracts.
As in other areas of finance, there are management decision-making
choices that occur only at the portfolio level. For example, strategic
decision-making does not occur at the contract level. It happens in the
conference room, where management reviews available data and possibly
steers a new course. From the portfolio perspective, insurers want to do
capacity planning, set management policies, and balance the mix of
products being booked to grow revenue while controlling volatility.

Conceptually, one can think about an insurance company as nothing more
than a collection, or portfolio, of insurance contracts. In Chapter
\ref{C:AggLossModels} we learned about modeling insurance portfolios as
the sum of individual contracts based on assumptions of independence
among contracts. Because of their importance, this chapter focuses
directly on portfolio distributions.

\begin{itemize}
\item
  Insurance portfolios represent obligations of insurers and so we are
  particularly interested in probabilities of large outcomes as these
  represent unusually large obligations. To formalize this concept, we
  introduce the notion of a heavy-tail distribution in Section
  \ref{S:Tails}.
\item
  Insurance portfolios represent company obligations and so insurers
  keep an equivalent amount of assets to meet these obligations.
  \emph{Risk measures}, introduced in Section \ref{S:RiskMeasure},
  summarize the distribution of the insurance portfolio and these
  summary measures are used to quantify the amount of assets that an
  insurer needs to retain to meet obligations.
\item
  In Section \ref{S:CoverageModifications}, we learned about mechanisms
  that policyholders use to spread risks such as deductibles and policy
  limits. In the same way, insurers use similar mechanisms in order to
  spread portfolio risks. They purchase risk protection from reinsurers,
  an insurance company for insurers. This sharing of insurance portfolio
  risk is described in Section \ref{S:Reinsurance}.
\end{itemize}

\section{Tails of Distributions}\label{S:Tails}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Describe a heavy tail distribution intuitively.
\item
  Classify the heaviness of a distribution's tails based on moments.
\item
  Compare the tails of two distributions.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In 1998 freezing rain fell on eastern Ontario, southwestern Quebec and
lasted for six days. The event was double the amount of precipitation in
the area experienced in any prior ice storm and resulted in a
catastrophe that produced in excess of 840,000 insurance claims. This
number is 20\(\%\) more than that of claims caused by the Hurricane
Andrew - one of the largest natural disasters in the history of North
America. The catastrophe caused approximately 1.44 billion Canadian
dollars in insurance settlements which is the highest loss burden in the
history of Canada. This is not an isolated example - similar
catastrophic events that caused extreme insurance losses are Hurricanes
Harvey and Sandy, the 2011 Japanese earthquake and tsunami, and so
forth.

In the context of insurance, a few large losses hitting a portfolio and
then converting into claims usually represent the greatest part of the
indemnities paid by insurance companies. The aforementioned losses, also
called `extremes', are quantitatively modelled by the tails of the
associated probability distributions. From the quantitative modelling
standpoint, relying on probabilistic models with improper tails is
rather daunting. For instance, periods of financial stress may appear
with a higher frequency than expected, and insurance losses may occur
with worse severity. Therefore, the study of probabilistic behavior in
the tail portion of actuarial models is of utmost importance in the
modern framework of quantitative risk management. For this reason, this
section is devoted to the introduction of a few mathematical notions
that characterize the tail weight of random variables (\emph{rv}'s). The
applications of these notions will benefit us in the construction and
selection of appropriate models with desired mathematical properties in
the tail portion, that are suitable for a given task.

Formally, define \(X\) to be the (random) obligations that arise from a
collection (portfolio) of insurance contracts. We are particularly
interested in studying the right tail of the distribution of \(X\),
which represents the occurrence of large losses. Informally, \emph{a rv
is said to be heavy-tailed if high probabilities are assigned to large
values.} Note that this by no mean implies the probability density/mass
functions are increasing as the value of \emph{rv} goes to infinity.
Ineed for a real-valued \emph{rv}, the pdf/pmf must diminish at infinity
in order to guarantee the total probability to be equal to one. Instead,
what we concern about is the rate of decaying of the probability
function. Unwelcome outcomes are more likely to occur for an insurance
portfolio that is described by a loss \emph{rv} possessing heavier
(right) tail. Tail weight can be an absolute or a relative concept.
Specifically, for the former, we may consider a \emph{rv} to be
heavy-tailed if certain mathematical properties of the probability
distribution are met. For the latter, we can say the tail of one
distribution is heavier than the other if some tail measures are
larger/smaller.

Several quantitative approaches have been proposed to classify and
compare tail weight. Among most of these approaches, the survival
function serves as the building block. In what follows, we introduce two
simple yet useful tail classification methods both of which are based on
the behavior of the survival function of \(X\).

\subsection{Classification Based on
Moments}\label{classification-based-on-moments}

One way of classifying the tail weight of distribution is by assessing
the existence of raw moments. Since our major interest lies in the right
tails of distributions, we henceforth assume the obligation or loss
\emph{rv} \(X\) to be positive. At the outset, the \(k-\)th raw moment
of a continuous \emph{rv}~\(X\), introduced in Section
\ref{S:BasicQuantities}, can be computed as

\begin{eqnarray*}
    \mu_k' &=& k \int_0^{\infty} x^{k-1} S(x) dx, \\
\end{eqnarray*}

where \(S(\cdot)\) denotes the survival function of \(X\). This
expression emphasizes that the existence of the raw moments depends on
the asymptotic behavior of the survival function at infinity. Namely,
the faster the survival function decays to zero, the higher the order of
finite moment (\emph{k}) the associated rv possesses.You may interpret
\(k^{\ast}\) to be the largest value of \emph{k} so that the moment is
finite. Formally, define \(k^{\ast}:=\sup\{k > 0:\mu_k'<\infty \}\),
where \(sup\) represents the supremum operator. This observation leads
us to the moment-based tail weight classification method, which is
defined formally next.

\textbf{Definition 10.1.} Consider a positive loss random variable
\(X\).

\begin{itemize}
\tightlist
\item
  If all the positive raw moments exist, namely the maximal order of
  finite moment \(k^{\ast}=\infty\), then \(X\) is said to be
  \textbf{light tailed} based on the moment method.
\item
  If \(k^{\ast} < \infty\), then \(X\) is said to be heavy tailed based
  on the moment method.
\item
  Moreover, for two positive loss random variables \(X_1\) and \(X_2\)
  with maximal orders of moment \(k^{\ast}_1\) and \(k^{\ast}_2\)
  respectively, we say \(X_1\) has a \textbf{heavier (right) tail} than
  \(X_2\) if \(k^{\ast}_1\leq k^{\ast}_2\).
\end{itemize}

The first part of Definition 10.1 is an absolute concept of tail weight,
while the second part is a relative concept of tail weight which
compares the (right) tails between two distributions. Next, we present a
few examples that illustrate the applications of the moment-based method
for comparing tail weight.

\textbf{Example 10.2.1. Light tail nature of the gamma distribution.}
Let \(X\sim gamma(\alpha,\theta)\), with \(\alpha>0\) and \(\theta>0\),
then for all \(k>0\), show that \(\mu_k' < \infty\).

\textbf{Solution.}

\begin{eqnarray*}
    \mu_k' &=& \int_0^{\infty} x^k \frac{x^{\alpha-1} e^{-x/\theta}}{\Gamma(\alpha) \theta^{\alpha}} dx \\
    &=& \int_0^{\infty} (y\theta)^k  \frac{(y\theta)^{\alpha-1} e^{-y}}{\Gamma(\alpha) \theta^{\alpha}} \theta dy \\
    &=& \frac{\theta^k}{\Gamma(\alpha)} \Gamma(\alpha+k) < \infty.
\end{eqnarray*}

Since all the positive moments exist, i.e., \(k^{\ast}=\infty\), in
accordance with the moment-based classification method in Definition
10.1, the gamma distribution is light-tailed.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example 10.2.2. Light tail nature of the Weibull distribution.}
Let \(X\sim Weibull(\theta,\tau)\), with \(\theta>0\) and \(\tau>0\),
then for all \(k>0\), show that \(\mu_k' < \infty\).

\textbf{Solution.}

\begin{eqnarray*}
    \mu_k' &=& \int_0^{\infty} x^k \frac{\tau x^{\tau-1} }{\theta^{\tau}} e^{-(x/\theta)^{\tau}}dx \\
    &=& \int_0^{\infty}  \frac{ y^{k/\tau} }{\theta^{\tau}} e^{-y/\theta^{\tau}}dy \\
    &=& \theta^{k} \Gamma(1+k/\tau) < \infty.
\end{eqnarray*}

Again, due to the existence of all the positive moments, the Weibull
distribution is light-tailed.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The gamma and Weibull distributions are used quite extensively in the
actuarial practice. Applications of these two distributions are vast
which include, but are not limited to, insurance claim severity
modelling, solvency assessment, loss reserving, aggregate risk
approximation, reliability engineering and failure analysis. We have
thus far seen two examples of using the moment-based method to analyze
light-tailed distributions. We document a heavy-tailed example in what
follows.

\textbf{Example 10.2.3. Heavy tail nature of the Pareto distribution.}
Let \(X\sim Pareto(\alpha,\theta)\), with \(\alpha>0\) and \(\theta>0\),
then for \(k>0\)

\begin{eqnarray*}
    \mu_k^{'} &=& \int_0^{\infty} x^k \frac{\alpha \theta^{\alpha}}{(x+\theta)^{\alpha+1}} dx \\
    &=& \alpha \theta^{\alpha} \int_{\theta}^{\infty} (y-\theta)^k {y^{-(\alpha+1)}} dy.
\end{eqnarray*}

Consider a similar integration:

\begin{eqnarray*}
  g_k:=\int_{\theta}^{\infty} {y^{k-\alpha-1}} dy=\left\{
  \begin{array}{ll}
    <\infty, & \hbox{for } k<\alpha;\\
    =\infty, & \hbox{for } k\geq \alpha.
  \end{array}
\right.
\end{eqnarray*}

Meanwhile,

\[\lim_{y\rightarrow \infty} \frac{(y-\theta)^k {y^{-(\alpha+1)}}}{y^{k-\alpha-1}}=\lim_{y\rightarrow \infty}
(1-\theta/y)^{k}=1.\]

Application of the limit comparison theorem for improper integrals
yields \(\mu_k'\) is finite if and only if \(g_k\) is finite. Hence we
can conclude that the raw moments of Pareto \emph{rv}'s exist only up to
\(k<\alpha\), i.e., \(k^{\ast}=\alpha\), and thus the distribution is
heavy-tailed. What is more, the maximal order of finite moment depends
only on the shape parameter \(\alpha\) and it is an increasing function
of \(\alpha\). In other words, based on the moment method, the tail
weight of Pareto \emph{rv}'s is solely manipulated by \(\alpha\) -- the
smaller the value of \(\alpha\), the heavier the tail weight becomes.
Since \(k^{\ast}<\infty\), the tail of Pareto distribution is heavier
than those of the gamma and Weibull distributions.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

We conclude this section with an open discussion on the limitations of
the moment-based method. Despite its simple implementation and intuitive
interpretation, there are certain circumstances in which the application
of the moment-based method is not suitable. First, for more complicated
probabilistic models, the \(k\)-th raw moment may not be simple to
derive, and thus the identification of the maximal order of finite
moment can be challenging. Second, the moment-based method does not well
comply with main body of the well established heavy tail theory in the
literature. Specifically, the existence of moment generating functions
is arguably the most popular method for classifying heavy tail versus
light tail within the community of academic actuaries. However, for some
\emph{rv}'s such as the lognormal \emph{rv}'s, their moment generating
functions do not exist even that all the positive moments are finite. In
these cases, applications of the moment-based methods can lead to
different tail weight assessment. Third, when we need to compare the
tail weight between two light-tailed distributions both having all
positive moments exist, the moment-based method is no longer informative
(see, e.g., Examples 10.2.1 and 10.2.2).

\subsection{Comparison Based on Limiting Tail
Behavior}\label{comparison-based-on-limiting-tail-behavior}

In order to resolve the aforementioned issues of the moment-based
classification method, an alternative approach for comparing tail weight
is to directly study the limiting behavior of the survival functions.

\textbf{Definition 10.2.} For two \emph{rv}'s \(X\) and \(Y\), let

\[
\gamma:=\lim_{t\rightarrow \infty}\frac{S_X(t)}{S_Y(t)}.
\] We say that

\begin{itemize}
\tightlist
\item
  \(X\) has a \textbf{heavier right tail} than \(Y\) if
  \(\gamma=\infty\);\\
\item
  \(X\) and \(Y\) are \textbf{proportionally equivalent in the right
  tail} if \(\gamma =c\in \mathbf{R}_+\);
\item
  \(X\) has a \textbf{lighter right tail} than \(Y\) if \(\gamma=0\).
\end{itemize}

\textbf{Example 10.2.4. Comparison of Pareto to Weibull distributions.}
Let \(X\sim Pareto(\alpha, \theta)\) and
\(Y\sim Weibull(\tau, \theta)\), for \(\alpha>0\), \(\tau>0\), and
\(\theta>0\). Show that the Pareto has a heavier right tail than the
Weibull.

\textbf{Solution.}

\begin{eqnarray*}
    \lim_{t\rightarrow \infty}\frac{S_X(t)}{S_Y(t)} &=& \lim_{t\rightarrow \infty}\frac{(1+t/\theta)^{-\alpha}}{\exp\{-(t/\theta)^{\tau}\}} \\
    &=& \lim_{t\rightarrow \infty}\frac{\exp\{t/\theta^{\tau} \}}{(1+t^{1/\tau}/\theta)^{\alpha}} \\
    &=& \lim_{t\rightarrow \infty}\frac{\sum_{i=0}^{\infty}\left(\frac{t}{\theta^{\tau}}\right)^{i}/i!}{(1+t^{1/\tau}/\theta)^{\alpha}}\\
    &=& \lim_{t\rightarrow \infty} \sum_{i=0}^{\infty} \left(t^{-i/\alpha}+\frac{t^{(1/\tau-i/\alpha)}}{\theta} \right)^{-\alpha}/\theta^{\tau i}i!\\
    &=& \infty.
\end{eqnarray*}

Therefore, the Pareto distribution has a heavier tail than the Weibull
distribution. One may also realize that exponentials go to infinity
faster than polynomials, thus the aforementioned limit must be infinite.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

For some distributions of which the survival functions do not admit
explicit expressions, we may find the following alternative formula
useful:

\begin{eqnarray*}
    \lim_{t\to \infty} \frac{S_X(t)}{S_Y(t)} &=& \lim_{t \to \infty} \frac{S_X^{'}(t)}{S_Y^{'}(t)} \\
    &=& \lim_{t \to \infty} \frac{-f_X(t)}{-f_Y(t)}\\
 &=& \lim_{t\to \infty} \frac{f_X(t)}{f_Y(t)}.
\end{eqnarray*}

given that the density functions exist.

\textbf{Example 10.2.5. Comparison of Pareto to gamma distributions.}
Let \(X\sim Pareto(\alpha, \theta)\) and
\(Y\sim gamma(\alpha, \theta)\), for \(\alpha>0\) and \(\theta>0\). Show
that the Pareto has a heavier right tail than the gamma.

\textbf{Solution.}

\begin{eqnarray*}
    \lim_{t\to \infty} \frac{f_{X}(t)}{f_{Y}(t)} &=& \lim_{t \to \infty} \frac{\alpha \theta^{\alpha} (t+ \theta)^{-\alpha-1}}{t^{\tau-1} e^{-t/\lambda} \lambda^{-\tau} \Gamma(\tau)^{-1}} \\
 &\propto&  \lim_{t\to \infty} \frac{e^{t/\lambda}}{(t+\theta)^{\alpha+1} t^{\tau-1}} \\
    &=& \infty,
\end{eqnarray*}

as exponentials go to infinity faster than polynomials.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{Risk Measures}\label{S:RiskMeasure}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Define the idea of \emph{coherence} and determine whether or not a
  risk measure is coherent.
\item
  Define the value-at-risk and calculate this quantity for a given
  distribution.
\item
  Define the tail value-at-risk and calculate this quantity for a given
  distribution.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In the previous section, we studied two methods for classifying the
weight of distribution tails. We may claim that the risk associated with
one distribution is more dangerous (asymptotically) than the others if
the tail is heavier. However, knowing one risk is more dangerous
(asymptotically) than the others may not provide sufficient information
for a sophisticated risk management purpose, and in addition, one is
also interested in quantifying how much more. In fact, the magnitude of
risk associated with a given loss distribution is an essential input for
many insurance applications, such as actuarial pricing, reserving,
hedging, insurance regulatory oversight, and so forth.

\subsection{Coherent Risk Measures}\label{coherent-risk-measures}

To compare the magnitude of risk in a practically convenient manner, we
seek a function that maps the loss \emph{rv} of interest to a numerical
value indicating the level of riskiness, which is termed the risk
measure. Put mathematically, the risk measure simply summarizes the
distribution function of a \emph{rv} as a single number. Two simple risk
measures are the mean \(\mathrm{E}[X]\) and the standard deviation
\(\mathrm{SD}(X)=\sqrt{\mathrm{Var}(X)}\). Other classical examples of
risk measures include the standard deviation \emph{principle}

\begin{equation}
H_{\mathrm{SD}}(X):=\mathrm{E}[X]+\alpha \mathrm{SD}(X),\text{ for } \alpha\geq 0,
\label{eq:SD-principle} 
\end{equation}

and the variance principle \[
H_{\mathrm{Var}}(X):=\mathrm{E}[X]+\alpha \mathrm{Var}(X),\text{ for } \alpha\geq 0.
\] It is a simple matter to check that all the aforementioned functions
are risk measures in which we input the loss \emph{rv} and the functions
output a numerical value. On a different note, the function
\(H^{\ast}(X):=\alpha X^{\beta}\) for any real-valued
\(\alpha,\beta\neq 0\), is not a risk measure because \(H^{\ast}\)
produces another \emph{rv} rather than a single numerical value.

Since risk measures are scalar measures which aim to use a single
numerical value to describe the stochastic nature of loss \emph{rv}'s,
it should not be surprising to us that there is no risk measure which
can capture all the risk information of the associated \emph{rv}'s.
Therefore, when seeking useful risk measures, it is important for us to
keep in mind that the measures should be at least

\begin{itemize}
\tightlist
\item
  interpretable practically;\\
\item
  computable conveniently; and\\
\item
  able to reflect the most critical information of risk underpinning the
  loss distribution.
\end{itemize}

Several risk measures have been developed in the literature.
Unfortunately, there is no best risk measure that can outperform the
others, and the selection of appropriate risk measure depends mainly on
the application questions at hand. In this respect, it is imperative to
emphasize that \emph{risk} is a subjective concept, and thus even given
the same problem, there are multifarious approaches to assess risk.
However, for many risk management applications, there is a wide
agreement that economically sounded risk measures should satisfy four
major axioms which we are going to describe in detail next. Risk
measures that satisfy these axioms are termed coherent risk measures.

Consider in what follows a risk measure \(H(\cdot)\), then \(H\) is a
coherent risk measure if the following axioms are satisfied.

\begin{itemize}
\tightlist
\item
  \textbf{Axiom 1.} \emph{Subadditivity:} \(H(X+Y)\leq H(X)+H(Y)\). The
  economic implication of this axiom is that diversification benefits
  exist if different risks are combined.\\
\item
  \textbf{Axiom 2.} \emph{Monotonicity:} if \(\Pr[X\leq Y]=1\), then
  \(H(X)\leq H(Y)\). Recall that \(X\) and \(Y\) are \emph{rv}'s
  representing losses, the underlying economic implication is that
  higher losses essentially leads to a higher level of risk.\\
\item
  \textbf{Axiom 3.} \emph{Positive homogeneity:} \(H(cX)=cH(X)\) for any
  positive constant \(c\). A potential economic implication about this
  axiom is that risk measure should be independent of the monetary units
  in which the risk is measured. For example, let \(c\) be the currency
  exchange rate between the US and Canadian dollars, then the risk of
  random losses measured in terms of US dollars (i.e., X) and Canadian
  dollars (i.e., cX) should be different only up to the exchange rate
  \(c\) (i.e., \(cH(x)=H(cX)\)).\\
\item
  \textbf{Axiom 4.} \emph{Translation invariance:} \(H(X+c)=H(X)+c\) for
  any positive constant \(c\). If the constant \(c\) is interpreted as
  risk-free cash, this axiom tells that no additional risk is created
  for adding cash to an insurance portfolio, and injecting risk-free
  capital of \(c\) can only reduce the risk by the same amount.
\end{itemize}

Verifying the coherent properties for some risk measures can be quite
straightforward, but it can be very challenging sometimes. For example,
it is a simple matter to check that the mean is a coherent risk measure.

\textbf{Example. The Mean is a Coherent Risk Measure.}

For any pair of \emph{rv}'s \(X\) and \(Y\) having finite means and
constant \(c>0\),

\begin{itemize}
\tightlist
\item
  validation of \emph{subadditivity}:
  \(\mathrm{E}[X+Y]=\mathrm{E}[X]+\mathrm{E}[Y]\);
\item
  validation of \emph{monotonicity}: if \(\Pr[X\leq Y]=1\), then
  \(\mathrm{E}[X]\leq \mathrm{E}[Y]\);
\item
  validation of \emph{positive homogeneity}:
  \(\mathrm{E}[cX]=c\mathrm{E}[X]\);
\item
  validation of \emph{translation invariance}:
  \(\mathrm{E}[X+c]=\mathrm{E}[X]+c\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

With a little more effort, we can determine the following.

\textbf{Example. The Standard Deviation is not a Coherent Risk Measure.}

On a different note, the standard deviation is not a coherent risk
measure. Specifically, one can check that the standard deviation
satisfies

\begin{itemize}
\tightlist
\item
  validation of \emph{subadditivity}:
\end{itemize}

\begin{eqnarray*} 
\mathrm{SD}[X+Y]&=&\sqrt{\mathrm{Var}(X)+\mathrm{Var}(Y)+2\mathrm{Cov}(X,Y)}\\
      &\leq& \sqrt{\mathrm{SD}(X)^2+\mathrm{SD}(Y)^2+2\mathrm{SD}(X)\mathrm{SD}(Y)}\\
      &=& \mathrm{SD}(X)+\mathrm{SD}(Y);
\end{eqnarray*}

\begin{itemize}
\tightlist
\item
  validation of \emph{positive homogeneity}:
  \(\mathrm{SD}[cX]=c~\mathrm{SD}[X]\).
\end{itemize}

However, the standard deviation does not comply with translation
invariance property as for any positive constant \(c\), \[
\mathrm{SD}(X+c)=\mathrm{SD}(X)<\mathrm{SD}(X)+c.
\] Moreover, the standard deviation also does not satisfy the
monotonicity property. To see this, consider the following two
\emph{rv}'s:

\begin{eqnarray}
X=\left\{
    \begin{array}{ll}
      0, & \hbox{with probability $0.25$;} \\
      4, & \hbox{with probability $0.75$,}
    \end{array}
  \right.
\label{eq:special-x}
\end{eqnarray}

and \(Y\) is a degenerate \emph{rv} such that

\begin{eqnarray}
\Pr[Y = 4] = 1.
\label{eq:special-y}
\end{eqnarray}

It is easy to check that \(\Pr[X\leq Y]=1\), but
\(\mathrm{SD}(X)=\sqrt{4^2\cdot 0.25\cdot 0.75}=\sqrt{3}>\mathrm{SD}(Y)=0\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

We have so far checked that \(\mathrm{E}[\cdot]\) is a coherent risk
measure, but not \(\mathrm{SD}(\cdot)\). Let us now proceed to study the
coherent property for the standard deviation principle
\eqref{eq:SD-principle} which is a linear combination of coherent and
incoherent risk measures.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example. The Standard Deviation Principle \eqref{eq:SD-principle}
is a Coherent Risk Measure.}

To this end, for a given \(\alpha>0\), we check the four axioms for
\(H_{\mathrm{SD}}(X+Y)\) one by one:

\begin{itemize}
\tightlist
\item
  validation of \emph{subadditivity:}
\end{itemize}

\begin{eqnarray*}
  H_{\mathrm{SD}}(X+Y) &=& \mathrm{E}[X+Y]+\alpha \mathrm{SD}(X+Y) \\
  &\leq& \mathrm{E}[X]+\mathrm{E}[Y]+\alpha [\mathrm{SD}(X) +\mathrm{SD}(Y)]\\
  &=& H_{\mathrm{SD}}(X)+ H_{\mathrm{SD}}(Y);
\end{eqnarray*}

\begin{itemize}
\tightlist
\item
  validation of \emph{positive homogeneity:} \[
  H_{\mathrm{SD}}(cX)=c\mathrm{E}[X]+c\alpha\mathrm{SD}(X)=cH_{\mathrm{SD}}(X);
  \]
\item
  validation of \emph{translation invariance:} \[
  H_{\mathrm{SD}}(X+c)=\mathrm{E}[X]+c+\alpha\mathrm{SD}(X)=H_{\mathrm{SD}}(X)+c.
  \]
\end{itemize}

It only remains to verify the monotonicity property, which may or may
not be satisfied depending on the value of \(\alpha\). To see this,
consider again the setup of \eqref{eq:special-x} and \eqref{eq:special-y} in
which \(\Pr[X\leq Y]=1\). Let \(\alpha=0.1\cdot \sqrt{3}\), then
\(H_{\mathrm{SD}}(X)=3+0.3=3.3< H_{\mathrm{SD}}(Y)=4\) and the
monotonicity condition is met. On the other hand, let
\(\alpha=\sqrt{3}\), then
\(H_{\mathrm{SD}}(X)=3+3=6> H_{\mathrm{SD}}(Y)=4\) and the monotonicity
condition is not satisfied. More precisely, by setting

\[
  H_{\mathrm{SD}}(X) = 3+\alpha\sqrt{3} \leq4= H_{\mathrm{SD}}(Y),
\]

we find that the monotonicity condition is only satisfied for
\(0\leq\alpha\leq 1/\sqrt{3}\), and thus the standard deviation
principle \(H_{\mathrm{SD}}\) is coherent. This result appears to be
very intuitive to us since the standard deviation principle
\(H_{\mathrm{SD}}\) is a linear combination of two risk measures of
which one is coherent and the other is incoherent. If
\(\alpha\leq 1/\sqrt{3}\), then the coherent measure dominates the
incoherent one, thus the resulting measure \(H_{\mathrm{SD}}\) is
coherent and vice versa. Note that the aforementioned conclusion may not
be generalized to any pair of \emph{rv}'s \(X\) and \(Y\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The literature on risk measures has been growing rapidly in popularity
and importance. In the succeeding two subsections, we introduce two
indices which have recently earned an unprecedented amount of interest
among theoreticians, practitioners, and regulators. They are namely the
\emph{Value-at-Risk} (\emph{VaR}) and the \emph{Tail Value-at-Risk}
(\emph{TVaR}) measures. The economic rationale behind these two popular
risk measures is similar to that for the tail classification methods
introduced in the previous section, with which we hope to capture the
risk of extremal losses represented by the distribution tails.

\subsection{Value-at-Risk}\label{value-at-risk}

In Section \ref{S:MS:QuantileEstimator}, we defined the quantile of a
distribution. We now look to a special case of this and offer the formal
definition of the value-at-risk, or \emph{VaR}.

\textbf{Definition 10.3.} Consider an insurance loss random variable
\(X\). The value-at-risk measure of \(X\) with confidence level
\(q\in (0,1)\) is formulated as

\begin{eqnarray}
VaR_q[X]:=\inf\{x:F_X(x)\geq q\}.
\label{eq:Value-at-Risk}
\end{eqnarray}

Here, \(inf\) is the infimum operator so that the \emph{VaR} measure
outputs the smallest value of \(X\) such that the associated cdf first
exceeds or equates to \(q\).

Here is how we should interpret \emph{VaR} in the context of actuarial
applications. The \emph{VaR} is a measure of the `maximal' probable loss
for an insurance product/portfolio or a risky investment occurring
\(q\times 100\%\) of times, over a specific time horizon (typically, one
year). For instance, let \(X\) be the annual loss \emph{rv} of an
insurance product, \(VaR_{0.95}[X]=100\) million means that there is a
\(5\%\) chance that the loss will exceed 100 million over a given year.
Owing to this meaningful interpretation, \emph{VaR} has become the
industrial standard to measuring financial and insurance risks since
1990's. Financial conglomerates, regulators, and academics often utilize
\emph{VaR} to measure risk capital, ensure the compliance with
regulatory rules, and disclose the financial positions.

Next, we present a few examples about the computation of \emph{VaR}.

\textbf{Example 10.3.1. \emph{VaR} for the exponential distribution.}
Consider an insurance loss \emph{rv} \(X\sim Exp(\theta)\) for
\(\theta>0\), then the \emph{cdf} of \(X\) is given by \[
F_X(x)=1-e^{-x/\theta}, \text{ for } x>0.
\] Give a closed-form expression for the \emph{VaR}.

\textbf{Solution.}

Because exponential distribution is a continuous distribution, the
smallest value such that the \emph{cdf} first exceeds or equates to
\(q \in (0,1)\) must be at the point \(x_q\) satisfying \[
q=F_X(x_q)=1-\exp\{-x_q/\theta \}.
\] Thus \[
VaR_q[X]=F_X^{-1}(q)=-\theta[\log(1-q)].
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The result reported in Example 10.3.1 can be generalized to any
continuous \emph{rv}'s having strictly increasing \emph{cdf}.
Specifically, the \emph{VaR} of any continuous \emph{rv}'s is simply the
inverse of the corresponding \emph{cdf}. Let us consider another example
of continuous \emph{rv} which has the support from negative infinity to
positive infinity.

\textbf{Example 10.3.2. \emph{VaR} for the normal distribution.}
Consider an insurance loss \emph{rv} \(X\sim Normal(\mu,\sigma^2)\) with
\(\sigma>0\). In this case, one may interpret the negative values of
\(X\) as profit or revenue. Give a closed-form expression for the
\emph{VaR}.

\textbf{Solution.}

Because normal distribution is a continuous distribution, the \emph{VaR}
of \(X\) must satisfy

\begin{eqnarray*}
 q &=& F_X(VaR_q[X])\\
&=&\Pr\left[(X-\mu)/\sigma\leq (VaR_q[X]-\mu)/\sigma\right]\\
&=&\Phi((VaR_q[X]-\mu)/\sigma).
\end{eqnarray*}

Therefore, we have \[
VaR_q[X]=\Phi^{-1}(q)\ \sigma+\mu.
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In many insurance applications, we have to deal with transformations of
\emph{rv}'s. For instance, in Example 10.3.2, the loss
\emph{rv}~\(X\sim Normal(\mu, \sigma^2)\) can be viewed as a linear
transformation of a standard normal \emph{rv}~\(Z\sim Normal(0,1)\),
namely \(X=Z\sigma+\mu\). By setting \(\mu=0\) and \(\sigma=1\), it is
straightforward for us to check \(VaR_q[Z]=\Phi^{-1}(q).\) A useful
finding revealed from Example 10.3.2 is that the \emph{VaR} of a linear
transformation of the normal \emph{rv}'s is equivalent to the linear
transformation of the \emph{VaR} of the original \emph{rv}'s. This
finding can be further generalized to any \emph{rv}'s as long as the
transformations are strictly increasing.

\textbf{Example 10.3.3. \emph{VaR} for transformed variables.} Consider
an insurance loss \emph{rv}~\(Y\sim lognormal(\mu,\sigma^2)\), for
\(\mu\in \mathbf{R}\) and \(\sigma>0\). Give an expression of the
\(VaR\) of \(Y\) in terms of the standard normal inverse \emph{cdf}.

\textbf{Solution.}

Note that \(\log Y\sim Normal(\mu,\sigma^2)\), or equivalently let
\(X\sim Normal(\mu,\sigma^2)\), then \(Y\overset{d}{=}e^{X}\) which is
strictly increasing transformation. Here, the notation
`\(\overset{d}{=}\)' means equality in distribution. The \emph{VaR} of
\(Y\) is thus given by the exponential transformation of the \emph{VaR}
of \(X\). Precisely, for \(q\in (0,1)\), \[
VaR_{q}[Y]= e^{VaR_q[X]}=\exp\{\Phi^{-1}(q)\ \sigma+\mu\}.
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

We have thus far seen a number of examples about the \emph{VaR} for
continuous \emph{rv}'s, let us consider an example concerning the
\emph{VaR} for a discrete \emph{rv}.

\textbf{Example 10.3.4. \emph{VaR} for a discrete random variable.}
Consider an insurance loss \emph{rv} with the following probability
distribution: \[
{\small
\Pr[X=x]=\left\{
                  \begin{array}{ll}
                    1, & \hbox{with probability $0.75$} \\
                    3, & \hbox{with probability $0.20$} \\
                    4, & \hbox{with probability $0.05$.}
                  \end{array}
                \right.
}
\] Determine the \emph{VaR} at \(q = 0.6, 0.9, 0.95, 0.95001\).

\textbf{Solution.}

The corresponding \emph{cdf} of \(X\) is \[
F_X(x)=\left\{
         \begin{array}{ll}
           0, & \hbox{ $x<1$;} \\
           0.75, & \hbox{ $1\leq x<3$;} \\
           0.95, & \hbox{ $3\leq x<4$;} \\
           1, & \hbox{ $4\leq x$.}
         \end{array}
       \right.
\] By the definition of \emph{VaR}, we thus have then

\begin{itemize}
\tightlist
\item
  \emph{\(VaR_{0.6}[X]=1\);}
\item
  \emph{\(VaR_{0.9}[X]=3\);}
\item
  \emph{\(VaR_{0.95}[X]=3\);}
\item
  \emph{\(VaR_{0.950001}[X]=4\).}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Let us now conclude the current subsection by an open discussion of the
\emph{VaR} measure. Some advantages of utilizing \emph{VaR} include

\begin{itemize}
\tightlist
\item
  possessing a practically meaningful interpretation;
\item
  relatively simple to compute for many distributions with closed-form
  distribution functions;
\item
  no additional assumption is required for the computation of
  \emph{VaR}.
\end{itemize}

On the other hand, the limitations of \emph{VaR} can be particularly
pronounced for some risk management practices. We report some of them
herein:

\begin{itemize}
\tightlist
\item
  the selection of the confidence level \(q\in (0,1)\) is highly
  subjective, while the \emph{VaR} can be very sensitive to the choice
  of \(q\) (e.g., in Example 10.3.4, \(VaR_{0.95}[X]=3\) and
  \(VaR_{0.950001}[X]=4\));
\item
  the scenarios/loss information that are above the
  \((1-p)\times 100\%\) worst event, are completely neglected;
\item
  \emph{VaR} is not a coherent risk measure (specifically, the
  \emph{VaR} measure does not satisfy the subadditivity axiom, meaning
  that diversification benefits may not be fully reflected).
\end{itemize}

\subsection{Tail Value-at-Risk}\label{tail-value-at-risk}

Recall that the \emph{VaR} represents the \((1-p)\times100\%\) chance
maximal loss. As we mentioned in the previous section, one major
drawback of the \emph{VaR} measure is that it does not reflect the
extremal losses occurring beyond the \((1-p)\times100\%\) chance worst
scenario. For illustrative purposes, let us consider the following
slightly unrealistic yet inspiring example.

\textbf{Example 10.3.5.} Consider two loss \emph{rv}'s
\(X\sim Uniform [0,100]\), and \(Y\sim Exp(31.71)\). We use \emph{VaR}
at \(95\%\) confidence level to measure the riskiness of \(X\) and
\(Y\). Simple calculation yields (see, also, Example 10.3.1), \[
VaR_{0.95}[X]=VaR_{0.95}[Y]=95,
\] and thus these two loss distributions have the same level of risk
according to \(VaR_{0.95}\). However, it is clear that \(Y\) is riskier
than \(X\) if extremal losses are of major concern since \(X\) is
bounded above while \(Y\) is unbounded. Simply quantifying risk by using
\emph{VaR} at a specific confidence level could be misleading and may
not reflect the true nature of risk.

As a remedy, the \emph{Tail Value-at-Risk} (\emph{TVaR}) was proposed to
measure the extremal losses that are above a given level of \emph{VaR}
as an average. We document the definition of \emph{TVaR} in what
follows. For the sake of simplicity, we are going to confine ourselves
to continuous positive \emph{rv}'s only, which are more frequently used
in the context of insurance risk management. We refer the interested
reader to \citet{hardy2006} for a more comprehensive discussion of
\emph{TVaR} for both discrete and continuous \emph{rv}'s.

\textbf{Definition 10.4.} Fix \(q\in (0,1)\), the tail value-at-risk of
a (continuous) \emph{rv} \(X\) is formulated as

\begin{eqnarray*}
  TVaR_q[X] &:=& \mathrm{E}[X|X>VaR_q[X]],
\end{eqnarray*}

given that the expectation exists.

In light of Definition 10.4, the computation of \emph{TVaR} typically
consists of two major components - the \emph{VaR} and the average of
losses that are above the \emph{VaR}. The \emph{TVaR} can be computed
via a number of formulas. Consider a continuous positive
\emph{rv}~\(X\), for notional convenience, henceforth let us write
\(\pi_q:=VaR_q[X]\). By definition, the \emph{TVaR} can be computed via

\begin{eqnarray}
TVaR_{q}[X]=\frac{1}{(1-q)}\int_{\pi_q}^{\infty}xf_X(x)dx.
\label{eq:cte-pdf}
\end{eqnarray}

\textbf{Example 10.3.6. \emph{TVaR} for a normal distribution.} Consider
an insurance loss \emph{rv}~\(X\sim Normal (\mu,\sigma^2)\) with
\(\mu\in \mathbf{R}\) and \(\sigma>0\). Give an expression for
\emph{TVaR}.

\textbf{Solution.}

Let \(Z\) be the standard normal \emph{rv}. For \(q\in(0,1)\), the
\emph{TVaR} of \(X\) can be computed via

\begin{eqnarray*}
  TVaR_q[X] &=& \mathrm{E}[X|X>VaR_q[X]]\\
&=&\mathrm{E}[\sigma Z+\mu|\sigma Z+\mu>VaR_q[X]]\\
&=& \sigma\mathrm{E}[Z|Z>(VaR_q[X]-\mu)/\sigma]+\mu\\
&\overset{(1)}{=}& \sigma\mathrm{E}[Z|Z>VaR_q[Z]]+\mu,
\end{eqnarray*}

where `\(\overset{(1)}{=}\)' holds because of the results reported in
Example 10.3.2. Next, we turn to study
\(TVaR_q[Z]=\mathrm{E}[Z|Z>VaR_q[Z]]\). Let
\(\omega(q)=(\Phi^{-1}(q))^2/2\), we have

\begin{eqnarray*}
  (1-q)\ TVaR_q[Z] &=& \int_{\Phi^{-1}(q)}^{\infty} z \frac{1}{\sqrt{2\pi}} e^{-z^2/2}dz\\
&=& \int_{\omega(q)}^{\infty}  \frac{1}{\sqrt{2\pi}} e^{-x}dx\\
&=& \frac{1}{\sqrt{2\pi}} e^{-\omega(q)}\\
&=& \phi(\Phi^{-1}(q)).
\end{eqnarray*}

Thus, \[
TVaR_q[X]=\sigma\frac{\phi(\Phi^{-1}(q))}{1-q}+\mu.
\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

We mentioned earlier in the previous subsection that the \emph{VaR} of a
strictly increasing function of \emph{rv} is equal to the function of
\emph{VaR} of the original \emph{rv}. Motivated by the results in
Example 10.3.6, one can show that the \emph{TVaR} of a strictly
increasing linear transformation of \emph{rv} is equal to the function
of \emph{VaR} of the original \emph{rv} This is due to the linearity
property of expectations. However, the aforementioned finding cannot be
extended to non-linear functions. The following example of lognormal
\emph{rv} serves as a counter example.

\textbf{Example 10.3.7. \emph{TVaR} of a lognormal distribution.}
Consider an insurance loss \emph{rv}~\(X\sim lognormal (\mu,\sigma^2)\),
with \(\sigma>0\). Show that

\begin{eqnarray*}
  TVaR_q[X] &=& \frac{e^{\mu+\sigma^2/2}}{(1-q)} \Phi(\Phi^{-1}(q)-\sigma).
\end{eqnarray*}

\textbf{Solution.}

Recall that the \emph{pdf} of lognormal distribution is formulated as \[
f_X(x)=\frac{1}{\sigma\sqrt{2\pi} x}\exp\{-(\ln x-\mu )^2/2\sigma^2 \}, \text{ for } x>0.
\] Fix \(q\in(0,1)\), then the \emph{TVaR} of \(X\) can be computed via

\begin{eqnarray}
  TVaR_q[X] &=& \frac{1}{(1-q)} \int_{\pi_q}^{\infty} x f_X(x)dx \nonumber\\
&=&\frac{1}{(1-q)} \int_{\pi_q}^{\infty} \frac{1}{\sigma \sqrt{2\pi}} \exp\left\{ -\frac{(\log x-\mu)^2}{2\sigma^2}
\right\}dx\nonumber\\
&\overset{(1)}{=}&\frac{1}{(1-q)} \int_{\omega(q)}^{\infty} \frac{1}{\sqrt{2\pi}} e^{ -\frac{1}{2}w^2+\sigma w+\mu}dw\nonumber\\
&=&\frac{e^{\mu+\sigma^2/2}}{(1-q)} \int_{\omega(q)}^{\infty} \frac{1}{\sqrt{2\pi}} e^{ -\frac{1}{2}(w-\sigma)^2}dw\nonumber\\
&=&\frac{e^{\mu+\sigma^2/2}}{(1-q)} \Phi(\omega(q)-\sigma),
\label{eq:cte-normal}
\end{eqnarray}

where \(\overset{(1)}{=}\) holds by applying change of variable
\(w=(\log x-\mu)/\sigma\), and \(\omega(q)=(\log \pi_q-\mu)/\sigma\).
Evoking the formula of \emph{VaR} for lognormal \emph{rv} reported in
Example 10.3.2, we can simplify the expression \eqref{eq:cte-normal} into

\begin{eqnarray*}
  TVaR_q[X] &=& \frac{e^{\mu+\sigma^2/2}}{(1-q)} \Phi(\Phi^{-1}(q)-\sigma).
\end{eqnarray*}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Clearly, the \emph{TVaR} of lognormal \emph{rv} is not the exponential
of the \emph{TVaR} of normal \emph{rv}.

For distributions of which the distribution functions are more tractable
to work with, we may apply the integration by parts technique to rewrite
equation \eqref{eq:cte-pdf} as

\begin{eqnarray*}
TVaR_{q}[X]&=&\left[-x S_X(x)\big |_{\pi_q}^{\infty}+\int_{\pi_q}^{\infty}S_X(x)dx\right]\frac{1}{(1-q)}\\
&=& \pi_q +\frac{1}{(1-q)}\int_{\pi_q}^{\infty}S_X(x)dx.
\end{eqnarray*}

\textbf{Example 10.3.8. \emph{TVaR} of an exponential distribution.}
Consider an insurance loss \emph{rv}~\(X\sim Exp(\theta)\) for
\(\theta>0\). Give an expression for the \emph{TVaR}.

\textbf{Solution.}

We have seen from the previous subsection that \[
\pi_q=-\theta[\log(1-q)].
\] Let us now consider the \emph{TVaR}:

\begin{eqnarray*}
  TVaR_q[X] &=& \pi_q+\int_{\pi_q}^{\infty} e^{-x/\theta}dx/(1-q)\\
&=& \pi_q+\theta e^{-\pi_q/\theta}/(1-q)\\
&=& \pi_q+\theta.
\end{eqnarray*}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

It can also be helpful to express the \emph{TVaR} in terms of limited
expected values. Specifically, we have

\begin{eqnarray}
  TVaR_q[X] &=& \int_{\pi_q}^{\infty} (x-\pi_q+\pi_q)f_X(x)dx/(1-q) \nonumber\\
&=& \pi_q+\frac{1}{(1-q)}\int_{\pi_q}^{\infty} (x-\pi_q)f_X(x)dx\nonumber\\
&=& \pi_q+e_X(\pi_q)\nonumber\\
&=& \pi_q +\frac{\left({\mathrm{E}[X]-\mathrm{E}[X\wedge\pi_q]}\right)}{(1-q)},
\label{eq:cte-expectation}
\end{eqnarray}

where \(e_X(d):=\mathrm{E}[X-d|X>d]\) for \(d>0\) denotes the mean
excess loss function. For many commonly used parametric distributions,
the formulas for calculating \(\mathrm{E}[X]\) and
\(\mathrm{E}[X\wedge\pi_q]\) can be found in a table of distributions.

\textbf{Example 10.3.9. \emph{TVaR} of the Pareto distribution.}
Consider a loss \emph{rv}~\(X\sim Pareto(\theta,\alpha)\) with
\(\theta>0\) and \(\alpha>0\). The \emph{cdf} of \(X\) is given by \[
F_X(x)=1-\left(\frac{\theta}{\theta+x} \right)^{\alpha}, \text{ for } x>0 .
\]

Fix \(q\in (0,1)\) and set \(F_X(\pi_q)=q\), we readily obtain

\begin{eqnarray}
\pi_q=\theta\left[(1-q)^{-1/\alpha}-1 \right].
\label{eq:var-pareto}
\end{eqnarray}

According to the distribution table provided in the Society of
Actuaries, we know \[
\mathrm{E}[X]=\frac{\theta}{\alpha-1},
\] and \[
\mathrm{E}[X\wedge \pi_q]=\frac{\theta}{\alpha-1}\left[
1-\left(\frac{\theta}{\theta+\pi_q}\right)^{\alpha-1}
\right].
\] Evoking equation \eqref{eq:cte-expectation} yields

\begin{eqnarray*}
  TVaR_q[X] &=& \pi_q+\frac{\theta}{\alpha-1} \frac{(\theta/(\theta+\pi_q))^{\alpha-1}}
{(\theta/(\theta+\pi_q))^{\alpha}}\\
&=&\pi_q +\frac{\theta}{\alpha-1}\left( \frac{\pi_q+\theta}{\theta} \right)\\
&=& \pi_q+\frac{\pi_q+\theta}{\alpha-1},
\end{eqnarray*}

where \(\pi_q\) is given by \eqref{eq:var-pareto}.

Via a change of variables, we can also rewrite equation \eqref{eq:cte-pdf}
as

\begin{eqnarray}
  TVaR_{q}[X] &=& \frac{1}{(1-q)}\int_{q}^{1} VaR_{\alpha}[X]\ d\alpha.
  \label{eq:cte-var}
\end{eqnarray}

What this alternative formula \eqref{eq:cte-var} tells is that \emph{TVaR}
in fact is the average of \(VaR_{\alpha}[X]\) with varying degree of
confidence level over \(\alpha\in [q,1]\). Therefore, the \emph{TVaR}
effectively resolves most of the limitations of \emph{VaR} outlined in
the previous subsection. First, due to the averaging effect, the
\emph{TVaR} may be less sensitive to the change of confidence level
compared with \emph{VaR}. Second, all the extremal losses that are above
the \((1-q)\times 100\%\) worst probable event are taken in account.

In this respect, it is a simple matter for us to see that for any given
\(q\in (0,1)\) \[
TVaR_q[X]\geq VaR_q[X].
\] Third and perhaps foremost, \emph{TVaR} is a coherent risk measure
and thus is able to more accurately capture the diversification effects
of insurance portfolio. Herein, we do not intend to provide the proof of
the coherent feature for \emph{TVaR}, which is considered to be
challenging technically.

\section{Reinsurance}\label{S:Reinsurance}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Define basic reinsurance treaties including proportional, quota share,
  non-proportional, stop-loss, excess of loss, and surplus share.
\item
  Interpret the optimality of quota share for reinsurers and compute
  optimal quota share agreements.
\item
  Interpret the optimality of stop-loss for insurers.
\item
  Interpret and calculate optimal excess of loss retention limits.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Recall that reinsurance is simply insurance purchased by an insurer.
Insurance purchased by non-insurers is sometimes known as primary
insurance to distinguish it from reinsurance. Reinsurance differs from
personal insurance purchased by individuals, such as auto and homeowners
insurance, in contract flexibility. Like insurance purchased by major
corporations, reinsurance programs are generally tailored more closely
to the buyer. For contrast, in personal insurance buyers typically
cannot negotiate on the contract terms although they may have a variety
of different options (contracts) from which to choose.

The two broad types are proportional and non-proportional reinsurance. A
proportional reinsurance contract is an agreement between a reinsurer
and a ceding company (also known as the reinsured) in which the
reinsurer assumes a given percent of losses and premium. A reinsurance
contract is also known as a treaty. Non-proportional agreements are
simply everything else. As examples of non-proportional agreements, this
chapter focuses on stop-loss and excess of loss contracts. For all types
of agreements, we split the total risk \(X\) into the portion taken on
by the reinsurer, \(Y_{reinsurer}\), and that retained by the insurer,
\(Y_{insurer}\), that is, \(X= Y_{insurer}+Y_{reinsurer}\).

The mathematical structure of a basic reinsurance treaty is the same as
the coverage modifications of personal insurance introduced in Chapter
3. For a proportional reinsurance, the transformation
\(Y_{insurer} = c X\) is identical to a coinsurance adjustment in
personal insurance. For stop-loss reinsurance, the transformation
\(Y_{reinsurer} = \max(0,X-M)\) is the same as an insurer's payment with
a deductible \(M\) and \(Y_{insurer} = \min(X,M) = X \wedge M\) is
equivalent to what a policyholder pays with deductible \(M\). For
practical applications of the mathematics, in personal insurance the
focus is generally upon the expectation as this is a key ingredient used
in pricing. In contrast, for reinsurance the focus is on the entire
distribution of the risk, as the extreme events are a primary concern of
the financial stability of the insurer and reinsurer.

This section describes the foundational and most basic of reinsurance
treaties: Section \ref{S:ProportionalRe} for proportional and Section
\ref{S:NonProportionalRe} for non-proportional reinsurance. Section
\ref{S:AdditionalRe} gives a flavor of more complex contracts.

\subsection{Proportional Reinsurance}\label{S:ProportionalRe}

The simplest example of a proportional treaty is called quota share.

\begin{itemize}
\item
  In a quota share treaty, the reinsurer receives a flat percent, say
  50\%, of the premium for the book of business reinsured.
\item
  In exchange, the reinsurer pays 50\% of losses, including allocated
  loss adjustment expenses
\item
  The reinsurer also pays the ceding company a ceding commission which
  is designed to reflect the differences in underwriting expenses
  incurred.
\end{itemize}

The amounts paid by the direct insurer and the reinsurer are summarized
as \[
Y_{insurer} = c X \ \ \text{and} \ \ \ Y_{reinsurer} = (1-c) X,
\] where \(c\in (0,1)\) denotes the proportion retained by the insurer.
Note that \(Y_{insurer}+Y_{reinsurer}=X\).

\textbf{Example 10.4.1. Distribution of losses under quota share.} To
develop an intuition for the effect of quota-share agreement on the
distribution of losses, the following is a short \texttt{R}
demonstration using simulation. Note the relative shapes of the
distributions of total losses, the retained portion (of the insurer),
and the reinsurer's portion.

\begin{center}\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-81-1} \end{center}

\subsubsection{Quota Share is Desirable for
Reinsurers}\label{quota-share-is-desirable-for-reinsurers}

The quota share contract is particularly desirable for the reinsurer. To
see this, suppose that an insurer and reinsurer wish to enter a contract
to share total losses \(X\) such that
\[Y_{insurer}=g(X) \ \ \ \text{and} \ \ \ \ Y_{reinsurer}=X-g(X),\] for
some generic function \(g(\cdot)\) (known as the \emph{retention}
function). Suppose further that the insurer only cares about the
variability of retained claims and is indifferent to the choice of \(g\)
as long as \(Var(Y_{insurer})\) stays the same and equals, say, \(Q\).
Then, the following result shows that the quota share reinsurance treaty
minimizes the reinsurer's uncertainty as measured by
\(Var(Y_{reinsurer})\).

\textbf{Proposition}. Suppose that \(Var(Y_{insurer})=Q.\) Then,
\(Var ((1-c)X) \le Var(g(X))\) for all \(g(.)\), where \(c=Q/Var(X)\).

\textbf{Proof of the Proposition}. With
\(Y_{reinsurer} = X - Y_{insurer}\) and the law of total variation

\[
\begin{array}{ll}
Var (Y_{reinsurer}) &= Var (X-Y_{insurer}) \\
&= Var (X) + Var (Y_{insurer})  - 2 Cov (X,Y_{insurer}) \\
&=Var (X) + Q - 2 Corr (X,Y_{insurer}) \times \sqrt{Q} \sqrt{Var (X)}
\end{array}
\] In this expression, we see that \(Q\) and \(Var(X)\) do not change
with the choice of \(g\). Thus, we can minimize \(Var (Y_{reinsurer})\)
by maximizing the correlation \(Corr (X,Y_{insurer})\). If we use a
quota share reinsurance agreement, then
\(Corr (X,Y_{insurer})=Corr (X,(1-c)X)=1\), the maximum possible
correlation. This establishes the proposition.

\(\Box\)`

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The proposition is intuitively appealing - with quota share insurance,
the reinsurer shares the responsibility for very large claims in the
tail of the distribution. This is in contrast to non-proportional
agreements where reinsurers take responsibility for the very large
claims.

\subsubsection{Optimizing Quota Share Agreements for
Insurers}\label{optimizing-quota-share-agreements-for-insurers}

Now assume \(n\) risks in the porfolio, \(X_1, \ldots, X_n,\) so that
the portfolio sum is \(X= X_1 + \cdots + X_n\). For simplicity, we focus
on the case of independent risks. Let us consider a variation of the
basic quota share agreement where the amount retained by the insurer may
vary with each risk, say \(c_i\). Thus, the insurer's portion of the
portfolio risk is \(Y_{insurer} = \sum_{i=1}^n c_i X_i\). What is the
best choice of the proportions \(c_i\)?

To formalize this question, we seek to find those values of \(c_i\) that
minimize \(Var (Y_{insurer})\) subject to the constraint that
\(E (Y_{insurer}) = K.\) The requirement that \(E (Y_{insurer}) = K\)
suggests that the insurers wishes to retain a revenue in at least the
amount of the constant \(K\). Subject to this revenue constraint, the
insurer wishes to minimize the uncertainty of the retained risks as
measured by the variance.

\textbf{The Optimal Retention Proportions}

Minimizing \(Var(Y_{insurer})\) subject to \(E(Y_{insurer}) = K\) is a
constrained optimization problem - we can use the method of Lagrange
multipliers, a calculus technique, to solve this. To this end, define
the Lagrangian

\[
\begin{array}{ll}
L &= Var (Y_{insurer}) - \lambda (E (Y_{insurer}) - K) \\
&= \sum_{i=1}^n c_i^2 ~Var(X_i) - \lambda (\sum_{i=1}^n c_i ~E(X_i) - K) 
\end{array}
\] Taking a partial derivative with respect to \(\lambda\) and setting
this equal to zero simply means that the constraint,
\(E(Y_{insurer}) = K\), is enforced and we have to choose the
proportions \(c_i\) to satisfy this constraint. Moreover, taking the
partial derivative with respect to each proportion \(c_i\) yields \[
\frac{\partial}{\partial c_i} L = 2 c_i ~Var(X_i) - \lambda ~E(X_i) = 0 
\]

so that

\[
c_i  =  \frac{\lambda}{2} \frac{E(X_i)}{Var(X_i)} .
\] With our constraint, we may determine \(\lambda\) as the solution of

\[
\begin{array}{ll}
K &= \sum_{i=1}^3 c_i \mathrm{E}(X_i) \\
&= \frac{\lambda}{2} \sum_{i=1}^3 \frac{\mathrm{E}(X_i)^2}{Var(X_i)} 
\end{array}
\] and use this value of \(\lambda\) to determine the proportions.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

From the math, it turns out that the constant for the \(i\)th risk,
\(c_i\) is proportional to \(\frac{E(X_i)}{Var (X_i)}\). This is
intuitively appealing. Other things being equal, a higher revenue as
measured by \(E (X_i)\) means a higher value of \(c_i\). In the same
way, a higher value of uncertainty as measured by \(Var(X_i)\) means a
lower value of \(c_i\). The proportional scaling factor is determined by
the revenue requirement \(E(Y_{insurer}) = K\). The following example
helps to develop a feel for this relationship.

\textbf{Example 10.4.2. Three Pareto risks.} Consider three risks that
have a Pareto distribution. Provide a graph, and supporting code, that
give values of \(c_1\), \(c_2\), and \(c_3\) for a required revenue
\(K\). Note that these values increase linearly with \(K\).

\begin{center}\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-83-1} \end{center}

\subsection{Non-Proportional Reinsurance}\label{S:NonProportionalRe}

\subsubsection{The Optimality of Stop-Loss
Insurance}\label{the-optimality-of-stop-loss-insurance}

Under a stop-loss arrangement, the insurer sets a retention level
\(M (>0)\) and pays in full total claims for which \(X \le M\). Further,
for claims for which \(X > M\), the direct insurer pays \(M\) and the
reinsurer pays the remaining amount \(X-M\). Thus, the insurer retains
an amount \(M\) of the risk. Summarizing, the amounts paid by the direct
insurer and the reinsurer are

\[
Y_{insurer} =
\begin{cases}
X & \text{for } X \le M\\
M & \text{for } X >M \\
\end{cases} \ \ \ \ = \min(X,M) = X \wedge M
\]

and

\[
Y_{reinsurer} =
\begin{cases}
0 & \text{for } X \le M\\
X- M &  \text{for } X >M \\
\end{cases} \ \ \ \  = \max(0,X-M) .
\]

As before, note that \(Y_{insurer}+Y_{reinsurer}=X\).

The stop-loss type of contract is particularly desirable for the
insurer. Similar to earlier, suppose that an insurer and reinsurer wish
to enter a contract so that \(Y_{insurer}=g(X)\) and
\(Y_{reinsurer}=X-g(X)\) for some generic retention function
\(g(\cdot)\). Suppose further that the insurer only cares about the
variability of retained claims and is indifferent to the choice of \(g\)
as long as \(Var(Y_{insurer})\) can be minimized. Again, we impose the
constraint that \(E(Y_{insurer}) = K\); the insurer needs to retain a
revenue \(K\). Subject to this revenue constraint, the insurer wishes to
minimize uncertainty of the retained risks (as measured by the
variance). Then, the following result shows that the stop-loss
reinsurance treaty minimizes the reinsurer's uncertainty as measured by
\(Var(Y_{reinsurer})\).

\textbf{Proposition}. Suppose that \(E(Y_{insurer})=K.\) Then,
\(Var (X \wedge M) \le Var(g(X))\) for all \(g(.)\), where \(M\) is such
that \(E(X \wedge M)=K\).

\textbf{Proof of the Proposition}. Add and subtract a constant \(M\) and
expand the square to get \[
\begin{array}{ll}
Var(g(X)) &= E (g(X) - K)^2 = E (g(X) -M +M- K)^2 \\
&= E (g(X) -M)^2 +  (M- K)^2 +2 E (g(X) -M)(M- K) \\
&= E (g(X) -M)^2 -  (M- K)^2 ,
\end{array}
\] because \(E(g(X))= K.\)

Now, for any retention function, we have \(g(X) \le X\), that is, the
insurer's retained claims are less than or equal to total claims. Using
the notation \(g_{SL}(X) = X \wedge M\) for stop-loss insurance, we have

\[
\begin{array}{ll}
M- g_{SL}(X) &= M-(X \wedge M) \\
&= (M-X) \wedge 0 \\
&\le (M-g(X)) \wedge 0 .
\end{array}
\] Squaring each side yields
\[(M- g_{SL}(X))^2 \le (M-g(X))^2 \wedge 0 \le (M-g(X))^2.\]

Returning to our expression for the variance, we have \[
\begin{array}{ll}
Var(g_{SL}(X)) &= E (g_{SL}(X) -M)^2 -  (M- K)^2 \\
&\le E (g(X) -M)^2 -  (M- K)^2 = Var(g(X)) ,
\end{array}
\] for any retention function \(g\). This establishes the proposition.

\(\Box\)`

The proposition is intuitively appealing - with stop-loss insurance, the
reinsurer takes the responsibility for very large claims in the tail of
the distribution, not the insurer.

\subsubsection{Excess of Loss}\label{excess-of-loss}

A closely related form of non-proportional reinsurance is the excess of
loss coverage. Under this contract, we assume that the total risk \(X\)
can be thought of as composed as \(n\) separate risks
\(X_1, \ldots, X_n\) and that each of these risks are subject to an
upper limit, say, \(M_i\). So the insurer retains

\[
Y_{i,insurer} = X_i \wedge M_i \ \ \ \ Y_{insurer} = \sum_{i=1}^n Y_{i,insurer}
\] and the reinsurer is responsible for the excess,
\(Y_{reinsurer}=X - Y_{insurer}\). The retention limits may vary by risk
or may be the same for all risks, \(M_i =M\), for all \(i\).

\subsubsection{Optimal Choice for Excess of Loss Retention
Limits}\label{optimal-choice-for-excess-of-loss-retention-limits}

What is the best choice of the excess of loss retention limits \(M_i\)?
To formalize this question, we seek to find those values of \(M_i\) that
minimize \(Var(Y_{insurer})\) subject to the constraint that
\(E(Y_{insurer}) = K.\) Subject to this revenue constraint, the insurer
wishes to minimize the uncertainty of the retained risks (as measured by
the variance).

\textbf{The Optimal Retention Limits}

Minimizing \(Var(Y_{insurer})\) subject to \(E(Y_{insurer}) = K\) is a
constrained optimization problem - we can use the method of Lagrange
multipliers, a calculus technique, to solve this. As before, define the
Lagrangian \[
\begin{array}{ll}
L &= Var (Y_{insurer}) - \lambda (E(Y_{insurer}) - K) \\
&= \sum_{i=1}^n ~Var (X_i \wedge M_i) - \lambda (\sum_{i=1}^n ~E(X_i \wedge M_i)- K).
\end{array}
\]

We first recall the relationships

\[
E(X \wedge M) = \int_0^M ~(1- F(x))dx
\] and

\[
E(X \wedge M)^2 = 2\int_0^M ~x(1- F(x))dx.
\]

Taking a partial derivative with respect to \(\lambda\) and setting this
equal to zero simply means that the constraint, \(E(Y_{insurer}) = K\),
is enforced and we have to choose the limits \(M_i\) to satisfy this
constraint. Moreover, taking the partial derivative with respect to each
limit \(M_i\) yields

\[
\begin{array}{ll}
\frac{\partial}{\partial M_i} L 
&= \frac{\partial}{\partial M_i}  ~Var(X_i \wedge M_i)  - \lambda \frac{\partial}{\partial M_i} ~E(X_i \wedge M_i) \\
&= \frac{\partial}{\partial M_i} \left(E(X_i \wedge M_i)^2 -(E(X_i \wedge M_i))^2\right) - \lambda (1-F_i(M_i)) \\
&= 2 M_i (1-F_i(M_i)) - 2 E(X_i \wedge M_i) (1-F_i(M_i))-
\lambda (1-F_i(M_i)).
\end{array}
\]

Setting \(\frac{\partial}{\partial M_i} L =0\) and solving for
\(\lambda\), we get

\[
\lambda = 2 (M_i - E(X_i \wedge M_i)) .
\]

From the math, it turns out that the retention limit less the expected
insurer's claims, \(M_i - E(X_i \wedge M_i)\), is the same for
\emph{all} risks. This is intuitively appealing.

\textbf{Example 10.4.3. Excess of loss for three Pareto risks.} Consider
three risks that have a Pareto distribution, each having a different set
of parameters (so they are independent but non-identical). Show
numerically that the optimal retention limits \(M_1\), \(M_2\), and
\(M_3\) resulting retention limit minus expected insurer's claims,
\(M_i - E(X_i \wedge M_i)\), is the same for all risks, as we derived
theoretically. Further, graphically compare the distribution of total
risks to that retained by the insurer and by the reinsurer.

We first optimize the Lagrangian using the \texttt{R} package
\texttt{alabama} for \emph{Augmented Lagrangian Adaptive Barrier
Minimization Algorithm}.

The optimal retention limits \(M_1\), \(M_2\), and \(M_3\) resulting
retention limit minus expected insurer's claims,
\(M_i - E(X_i \wedge M_i)\), is the same for all risks, as we derived
theoretically.

\begin{verbatim}
[1] 1344.135
\end{verbatim}

\begin{verbatim}
[1] 1344.133
\end{verbatim}

\begin{verbatim}
[1] 1344.133
\end{verbatim}

We graphically compare the distribution of total risks to that retained
by the insurer and by the reinsurer.

\begin{center}\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-86-1} \end{center}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Additional Reinsurance Treaties}\label{S:AdditionalRe}

\subsubsection{Surplus Share Proportional
Treaty}\label{surplus-share-proportional-treaty}

Another proportional treaty is known as surplus share; this type of
contract is common in commercial property insurance.

\begin{itemize}
\tightlist
\item
  A surplus share treaty allows the reinsured to limit its exposure on
  any one risk to a given amount (the retained line).
\item
  The reinsurer assumes a part of the risk in proportion to the amount
  that the insured value exceeds the retained line, up to a given limit
  (expressed as a multiple of the retained line, or number of lines).
\end{itemize}

For example, let the retained line be \$100,000 and the given limit be 4
lines (\$400,000). Then, if \(X\) is the loss, the reinsurer's portion
is \(\min(400000, (X-100000)_+)\).

\subsubsection{Layers of Coverage}\label{layers-of-coverage}

One can also extend non-proportional stop-loss treaties by introducing
additional parties to the contract. For example, instead of simply an
insurer and reinsurer or an insurer and a policyholder, think about the
situation with all three parties, a policyholder, insurer, and
reinsurer, who agree on how to share a risk. More generally, we consider
\(k\) parties. If \(k=3\), it could be an insurer and two different
reinsurers.

\textbf{Example 10.4.4. Layers of coverage for three parties.}

\begin{itemize}
\item
  Suppose that there are \(k=3\) parties. The first party is responsible
  for the first 100 of claims, the second responsible for claims from
  100 to 3000, and the third responsible for claims above 3000.
\item
  If there are four claims in the amounts 50, 600, 1800 and 4000, then
  they would be allocated to the parties as follows:
\end{itemize}

\begin{longtable}[]{@{}lccccl@{}}
\toprule
Layer & Claim 1 & Claim 2 & Claim 3 & Claim 4 & Total\tabularnewline
\midrule
\endhead
(0, 100{]} & 50 & 100 & 100 & 100 & 350\tabularnewline
(100, 3000{]} & 0 & 500 & 1700 & 2900 & 5100\tabularnewline
(3000, \(\infty\)) & 0 & 0 & 0 & 1000 & 1000\tabularnewline
Total & 50 & 600 & 1800 & 4000 & 6450\tabularnewline
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

To handle the general situation with \(k\) groups, partition the
positive real line into \(k\) intervals using the cut-points
\[0 = M_0 < M_1 < \cdots < M_{k-1} < M_k = \infty.\]

Note that the \(j\)th interval is \((M_{j-1}, M_j]\). Now let \(Y_j\) be
the amount of risk shared by the \(j\)th party. To illustrate, if a loss
\(x\) is such that \(M_{j-1} <x \le M_j\), then \[\left(\begin{array}{c}
    Y_1\\ Y_2 \\ \vdots \\ Y_j \\Y_{j+1} \\ \vdots \\Y_k
    \end{array}\right)
    =\left(\begin{array}{c}
    M_1-M_0 \\ M_2-M_1  \\ \vdots \\ x-M_{j-1}  \\ 0 \\ \vdots \\0
    \end{array}\right)\]

More succinctly, we can write \[Y_j = \min(X,M_j) - \min(X,M_{j-1}) .\]

With the expression \(Y_j = \min(X,M_j) - \min(X,M_{j-1})\), we see that
the \(j\)th party is responsible for claims in the interval
\((M_{j-1}, M_j].\) With this, it is easy to check that
\(X = Y_1 + Y_2 + \cdots + Y_k.\) As emphasized in the following
example, we also remark that the parties need not be different.

\textbf{Example 10.4.5.} - Suppose that a policyholder is responsible
for the first 500 of claims and all claims in excess of 100,000. The
insurer takes claims between 100 and 100,000. - Then, we would use
\(M_1 = 100\), \(M_2 =100000\). - The policyholder is responsible for
\(Y_1 =\min(X,100)\) and
\(Y_3 = X - \min(X,100000) = \max(0, X-100000)\).

For additional reading, see the
\href{https://sites.google.com/a/wisc.edu/local-government-property-insurance-fund/home/reinsurance}{Wisconsin
Property Fund site} for an example on layers of reinsurance.

\subsubsection{Portfolio Management
Example}\label{portfolio-management-example}

Many other variations of the foundational contracts are possible. For
one more illustration, consider the following.

\textbf{Example. 10.4.6. Portfolio management.} You are the Chief Risk
Officer of a telecommunications firm. Your firm has several property and
liability risks. We will consider:

\begin{itemize}
\tightlist
\item
  \(X_1\) - buildings, modeled using a gamma distribution with mean 200
  and scale parameter 100.
\item
  \(X_2\) - motor vehicles, modeled using a gamma distribution with mean
  400 and scale parameter 200.
\item
  \(X_3\) - directors and executive officers risk, modeled using a
  Pareto distribution with mean 1000 and scale parameter 1000.
\item
  \(X_4\) - cyber risks, modeled using a Pareto distribution with mean
  1000 and scale parameter 2000.
\end{itemize}

Denote the total risk as \[X = X_1 + X_2 + X_3 + X_4 .\]

For simplicity, you assume that these risks are independent.

To manage the risk, you seek some insurance protection. You wish to
manage internally small building and motor vehicles amounts, up to
\(M_1\) and \(M_2\), respectively. You seek insurance to cover all other
risks. Specifically, the insurer's portion is
\[ Y_{insurer} = (X_1 - M_1)_+ + (X_2 - M_2)_+ + X_3 + X_4 ,\] so that
your retained risk is \(Y_{retained}= X- Y_{insurer} =\)
\(\min(X_1,M_1) + \min(X_2,M_2)\). Using deductibles \(M_1=\) 100 and
\(M_2=\) 200:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Determine the expected claim amount of (i) that retained, (ii) that
  accepted by the insurer, and (iii) the total overall amount.
\item
  Determine the 80th, 90th, 95th, and 99th percentiles for (i) that
  retained, (ii) that accepted by the insurer, and (iii) the total
  overall amount.
\item
  Compare the distributions by plotting the densities for (i) that
  retained, (ii) that accepted by the insurer, and (iii) the total
  overall amount.
\end{enumerate}

In preparation, here is the code needed to set the parameters.

With these parameters, we can now simulate realizations of the portfolio
risks.

\textbf{(a)} Here is the code for the expected claim amounts.

\begin{verbatim}
     Retained Insurer   Total
[1,]   269.05 5274.41 5543.46
\end{verbatim}

\textbf{(b)} Here is the code for the quantiles.

\begin{verbatim}
             80%     90%     95%      99%
Retained  300.00  300.00  300.00   300.00
Insurer  6075.67 7399.80 9172.69 14859.02
Total    6351.35 7675.04 9464.20 15159.02
\end{verbatim}

\textbf{(c)} Here is the code for the density plots of the retained,
insurer, and total portfolio risk.

\begin{center}\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-92-1} \end{center}

\section{Further Resources and
Contributors}\label{further-resources-and-contributors-1}

\begin{itemize}
\tightlist
\item
  \textbf{Edward W. (Jed) Frees}, University of Wisconsin-Madison, and
  \textbf{Jianxi Su}, Purdue University are the principal authors of the
  initial version of this chapter. Email:
  \href{mailto:jfrees@bus.wisc.edu}{\nolinkurl{jfrees@bus.wisc.edu}}
  and/or \href{mailto:jianxi@purdue.edu}{\nolinkurl{jianxi@purdue.edu}}
  for chapter comments and suggested improvements.
\item
  Chapter reviewers include: Fei Huang, Hirokazu (Iwahiro) Iwasawa, Peng
  Shi, Ping Wang, Chengguo Weng.
\end{itemize}

Some of the examples from this chapter were borrowed from
\citet{clark1996basics}, \citet{klugman2012}, and
\citet{bahnemann2015distributions}. These resources provide excellent
sources for additional discussions and examples.

\chapter{Loss Reserving}\label{C:LossReserves}

\emph{Chapter Preview.} This chapter introduces loss reserving (also
known as claims reserving) for property and casualty (P\&C, or general,
non-life) insurance products. In particular, the chapter sketches some
basic, though essential, analytic tools to assess the reserves on a
portfolio of P\&C insurance products. First, Section \ref{S:motivation}
motivates the need for loss reserving, then Section \ref{S:Data} studies
the available data sources and introduces some formal notation to tackle
loss reserving as a prediction challenge. Next, Section
\ref{S:Chain-ladder} covers the chain-ladder method and Mack's
distribution-free chain-ladder model. Section \ref{S:GLMs} then develops
a fully stochastic approach to determine the outstanding reserve with
generalized linear models (GLMs), including the technique of
bootstrapping to obtain a predictive distribution of the outstanding
reserve via simulation.

\section{Motivation}\label{S:motivation}

Our starting point is the lifetime of a P\&C insurance claim. Figure
\ref{fig:tikz-run-off} pictures the development of such a claim over
time and identifies the events of interest:

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/tikz-run-off-1} 

}

\caption{Lifetime or run-off of a claim}\label{fig:tikz-run-off}
\end{figure}

The insured event or accident occurs at time \(t_{occ}\). This incident
is reported to the insurance company at time \(t_{rep}\), after some
delay. If the filed claim is accepted by the insurance company, payments
will follow to reimburse the financial loss of the policyholder. In this
example the insurance company compensates the incurred loss with loss
payments at times \(t_1\), \(t_2\) and \(t_3\). Eventually, the claim
settles or closes at time \(t_{set}\).

Often claims will not settle immediately due to the presence of delay in
the reporting of a claim, delay in the settlement process or both. The
reporting delay is the time that elapses between the occurrence of the
insured event and the reporting of this event to the insurance company.
The time between reporting and settlement of a claim is known as the
settlement delay. For example, it is very intuitive that a material
damage claim settles quicker than a bodily injury claim involving a
complex type of injury. Closed claims may also reopen due to new
developments, e.g.~an injury that requires extra treatment. Put
together, the development of a claim typically takes some time. The
presence of this delay in the run-off of a claim requires the insurer to
hold capital in order to settle these claims in the future.

\subsection{Closed, IBNR, and RBNS Claims}\label{S:claim-types}

Based on the status of the claim's run-off we distinguish three types of
claims in the books of an insurance company. A first type of claim is a
closed claim. For these claims the complete development has been
observed. With the red line in Figure \ref{fig:tikz-closed} indicating
the present moment, all events from the claim's development take place
before the present moment. Hence, these events are observed at the
present moment. For convenience, we will assume that a closed claim can
not reopen.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/tikz-closed-1} 

}

\caption{Lifetime of a closed claim}\label{fig:tikz-closed}
\end{figure}

An RBNS claim is one that has been \textbf{R}eported, \textbf{B}ut is
\textbf{N}ot fully \textbf{S}ettled at the present moment or the moment
of evaluation, that is, the moment when the reserves should be
calculated and booked by the insurer. Occurrence, reporting and possibly
some loss payments take place before the present moment, but the closing
of the claim happens in the future, beyond the present moment.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/tikz-RBNS-1} 

}

\caption{Lifetime of an RBNS claim}\label{fig:tikz-RBNS}
\end{figure}

An IBNR claim is one that has \textbf{I}ncurred in the past \textbf{B}ut
is \textbf{N}ot yet \textbf{R}eported. For such a claim the insured
event took place, but the insurance company is not yet aware of the
associated claim. This claim will be reported in the future and its
complete development (from reporting to settlement) takes place in the
future.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/tikz-IBNR-1} 

}

\caption{Lifetime of an IBNR claim}\label{fig:tikz-IBNR}
\end{figure}

Insurance companies will reserve capital to fulfill their future
liabilities with respect to both RBNS as well as IBNR claims. The future
development of such claims is uncertain and predictive modeling
techniques will be used to calculate appropriate reserves, from the
historical development data observed on similar claims.

\subsection{Why Reserving?}\label{why-reserving}

The inverted production cycle of the insurance market and the claim
dynamics pictured in Section \ref{S:claim-types} motivate the need for
reserving and the design of predictive modeling tools to estimate
reserves. In insurance, the premium income precedes the costs. An
insurer will charge a client a premium, before actually knowing how
costly the insurance policy or contract will become. In typical
manufacturing industry this is not the case and the manufacturer knows -
before selling a product - what the cost of producing this product was.
At a specified evaluation moment \(\tau\) the insurer will predict his
outstanding liabilities with respect to contracts sold in the past. This
is the claims reserve or loss reserve; it is the capital necessary to
settle open claims from past exposures. It is a very important element
on the balance sheet of the insurer, more specifically on the
liabilities side of this balance sheet.

\section{Loss Reserve Data}\label{S:Data}

\subsection{From Micro to Macro}\label{from-micro-to-macro}

We now shed light on the data available to estimate the outstanding
reserve for a portfolio of P\&C contracts. Insurance companies typically
register data on the development of an individual claim as sketched in
the timeline on the left hand side of Figure \ref{fig:tikz-micro-macro}.
We refer to data registered at this level as \textbf{granular or
micro-level} data. Typically, an actuary aggregates the information
registered on the individual development of claims across all claims in
a portfolio. This aggregation leads to data structured in a triangular
format as shown on the right hand side of Figure
\ref{fig:tikz-micro-macro}. Such data are called \textbf{aggregate or
macro-level} data because each cell in the triangle displays information
obtained by aggregating the development of multiple claims.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/tikz-micro-macro-1} 

}

\caption{From granular data to run-off triangle}\label{fig:tikz-micro-macro}
\end{figure}

The triangular display used in loss reserving is called a
\textbf{run-off or development} triangle. On the vertical axis the
triangle lists the accident or occurrence years during which a portfolio
is followed. The loss payments booked for a specific claim are connected
to the year during the which the insured event occurred. The horizontal
axis indicates the payment delay since occurrence of the insured event.

\subsection{Run-off Triangles}\label{run-off-triangles}

A first example of a run-off triangle with incremental payments is
displayed in Figure \ref{fig:tikz-triangle} (taken from
\citet{WuthrichMerz2008}, Table 2.2, also used in
\citet{WuthrichMerz2015}, Table 1.4). Accident years (or years of
occurrence) are shown on the vertical axis and run from 2004 up to 2013.
These refer to the year during which the insured event occurred. The
horizontal axis indicates the payment delay in years since occurrence of
the insured event. \emph{0 delay} is used for payments made in the year
of occurrence of the accident or insured event. \emph{One year} of delay
is used for payments made in the year after occurrence of the accident.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/tikz-triangle-1} 

}

\caption{A run-off triangle with incremental payment data. Source: @WuthrichMerz2008, Table 2.2.}\label{fig:tikz-triangle}
\end{figure}

For example, cell \((2004, 0)\) in the above triangle displays the
number \(5,947\), the total amount paid in the year 2004 for all claims
occurring in year 2004. Thus, it is the total amount paid with 0 years
of delay on all claims that occurred in the year 2004. Similarly, the
number in cell \((2012,1)\) displays the total \(2,357.9\) paid in the
year 2013 for all claims that occurred in year 2012.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/tikz-cum-triangle-1} 

}

\caption{A run-off triangle with cumulative payment data. Source: @WuthrichMerz2008, Table 2.2.}\label{fig:tikz-cum-triangle}
\end{figure}

Whereas the triangle in Figure \ref{fig:tikz-triangle} displays
incremental payment data, the Figure \ref{fig:tikz-cum-triangle} shows
the same information in cumulative format. Now, cell \((2004,1)\)
displays the total claim amount paid \emph{up to} payment delay 1 for
all claims that occurred in year 2004. Therefore, it is the sum of the
amount paid in 2004 and the amount paid in 2005 on accidents that
occurred in 2004.

Different pieces of information can be stored in run-off triangles as
those shown in Figure \ref{fig:tikz-triangle} and Figure
\ref{fig:tikz-cum-triangle}. Depending on the kind of data stored, the
triangle will be used to estimate different quantities.

For example, in incremental format a cell may display:

\begin{itemize}
\tightlist
\item
  the claim payments, as motivated before
\item
  the number of claims that occurred in a specific year and were
  reported with a certain delay, when the goal is to estimate the number
  of IBNR claims
\item
  the change in incurred amounts, where incurred claim amounts are the
  sum of cumulative paid claims and the case estimates. The case
  estimate is the claims handler's expert estimate of the outstanding
  amount on a claim.
\end{itemize}

In cumulative format a cell may display:

\begin{itemize}
\tightlist
\item
  the cumulative paid amount, as motivated before
\item
  the total number of claims from an occurrence year, reported up to a
  certain delay
\item
  the incurred claim amounts.
\end{itemize}

Other sources of information are potentially available, e.g.~covariates
(like the type of claim), external information (like inflation, change
in regulation). Most claims reserving methods designed for run-off
triangles are rather based on a single source of information, although
recent contributions focus on the use of more detailed data for loss
reserving.

\subsection{Loss Reserve Notation}\label{loss-reserve-notation}

\subsubsection*{Run-off Triangles}\label{run-off-triangles-1}
\addcontentsline{toc}{subsubsection}{Run-off Triangles}

To formalize the displays shown in Figures \ref{fig:tikz-triangle} and
\ref{fig:tikz-cum-triangle}, we let \(i\) refer to the occurrence or
accident year, the year in which the insured event happened. In our
notation the first accident year considered in the portfolio is denoted
with 1 and the latest, most recent accident year is denoted with \(I\).
Then, \(j\) refers to the payment delay or development year, where a
delay equal to 0 corresponds to the accident year itself. Figure
\ref{fig:tikz-math-triangle} shows a triangle where the same number of
years is considered in both the vertical as well as the horizontal
direction, hence \(j\) runs from 0 up to \(J = I-1\).

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/tikz-math-triangle-1} 

}

\caption{Mathematical notation for a run-off triangle. Source: @WuthrichMerz2008}\label{fig:tikz-math-triangle}
\end{figure}

The random variable \(X_{ij}\) denotes the incremental claims paid in
development period \(j\) on claims from accident year \(i\). Thus,
\(X_{ij}\) is the total amount paid in development year \(j\) for all
claims that happened in occurrence year \(i\). These payments are
actually paid out in accounting or calendar year \(i+j\). Taking a
cumulative point of view, \(C_{ij}\) is the cumulative amount paid up
until (and including) development year \(j\) for accidents that occurred
in year \(i\). Ultimately, a total amount \(C_{iJ}\) is paid in the
final development year \(J\) for claims that occurred in accident year
\(i\). In this chapter time is expressed in years, though other time
units can be used as well, e.g.~semesters or quarters.

\subsubsection*{The Loss Reserve}\label{the-loss-reserve}
\addcontentsline{toc}{subsubsection}{The Loss Reserve}

At the evaluation moment \(\tau\), the data in the upper triangle have
been observed, whereas the lower triangle has to be predicted. Here, the
evaluation moment is the end of accident year \(I\) which implies that a
cell \((i,j)\) with \(i+j \leq I\) is observed, and a cell \((i,j)\)
with \(i+j > I\) belongs to the future and has to be predicted. Thus,
for a cumulative run-off triangle, the goal of a loss reserving method
is to predict \(C_{i,I-1}\), the ultimate claim amount for occurrence
year \(i\), corresponding to the final development period \(I-1\) in
Figure \ref{fig:tikz-cum-triangle}. We assume that - beyond this period
- no further payments will follow, although this assumption can be
relaxed.

Since \(C_{i,I-1}\) is cumulative, it includes both an observed part as
well as a part that has to be predicted. Therefore, the outstanding
liability or loss reserve for accident year \(i\) is

\begin{eqnarray*}
\mathcal{R}^{(0)}_{i} = \sum_{\ell=I-i+1}^{I-1} X_{i\ell} = C_{i,I}-C_{i,I-i}.
\end{eqnarray*}

We express the reserve either as a sum of incremental data, the
\(X_{i\ell}\), or as a difference between cumulative numbers. In the
latter case the outstanding amount is the ultimate cumulative amount
\(C_{i,I}\) minus the most recently observed cumulative amount
\(C_{i,I-i}\). Following \citet{WuthrichMerz2015}, the notation
\(\mathcal{R}^{(0)}_{i}\) refers to the reserve for occurrence year
\(i\) where \(i=1,\ldots,I\). The superscript \((0)\) refers to the
evaluation of the reserve at the present moment, say \(\tau = 0\). We
understand \(\tau = 0\) at the end of occurrence year \(I\), the most
recent calendar year for which data are observed and registered.

\subsection{R Code to Summarize Loss Reserve Data}\label{S:Rcode}

We use the \texttt{ChainLadder} package \citep{R-chainladder} to import
run-off triangles in \texttt{R} and to explore the trends present in
these triangles. The package's vignette nicely documents its functions
for working with triangular data. First, we explore two ways to import a
triangle.

\subsubsection*{Long Format Data}\label{long-format-data}
\addcontentsline{toc}{subsubsection}{Long Format Data}

The dataset \texttt{triangle\_W\_M\_long.txt} stores the cumulative
run-off triangle from \citet{WuthrichMerz2008} (Table 2.2) in long
format. That is: each cell in the triangle is one row in this data set,
and three features are stored: the payment size (cumulative, in this
example), the year of occurrence (\(i\)) and the payment delay (\(j\)).
We import the .txt file and store the resulting data frame as
\texttt{my\_triangle\_long}:

\begin{verbatim}
   payment origin dev
1  5946975   2004   0
2  9668212   2004   1
3 10563929   2004   2
4 10771690   2004   3
5 10978394   2004   4
6 11040518   2004   5
\end{verbatim}

We use the \texttt{as.triangle} function from the \texttt{ChainLadder}
package to transform the data frame into a triangular display. The
resulting object \texttt{my\_triangle} is now of type \texttt{triangle}.

\begin{verbatim}
 'triangle' int [1:10, 1:10] 5946975 6346756 6269090 5863015 5778885 6184793 5600184 5288066 5290793 5675568 ...
 - attr(*, "dimnames")=List of 2
  ..$ origin: chr [1:10] "2004" "2005" "2006" "2007" ...
  ..$ dev   : chr [1:10] "0" "1" "2" "3" ...
\end{verbatim}

We display the triangle and recognize the numbers (in thousands) from
Figure \ref{fig:tikz-cum-triangle}. Cells in the lower triangle are
indicated as \emph{not available}, \texttt{NA}.

\begin{verbatim}
      dev
origin    0    1     2     3     4     5     6     7     8     9
  2004 5947 9668 10564 10772 10978 11041 11106 11121 11132 11148
  2005 6347 9593 10316 10468 10536 10573 10625 10637 10648    NA
  2006 6269 9245 10092 10355 10508 10573 10627 10636    NA    NA
  2007 5863 8546  9269  9459  9592  9681  9724    NA    NA    NA
  2008 5779 8524  9178  9451  9682  9787    NA    NA    NA    NA
  2009 6185 9013  9586  9831  9936    NA    NA    NA    NA    NA
  2010 5600 8493  9057  9282    NA    NA    NA    NA    NA    NA
  2011 5288 7728  8256    NA    NA    NA    NA    NA    NA    NA
  2012 5291 7649    NA    NA    NA    NA    NA    NA    NA    NA
  2013 5676   NA    NA    NA    NA    NA    NA    NA    NA    NA
\end{verbatim}

\subsubsection*{Triangular Format Data}\label{triangular-format-data}
\addcontentsline{toc}{subsubsection}{Triangular Format Data}

Alternatively, the triangle may be stored in a .csv file with the
occurrence years in the rows and the development years in the column
cells. We import this .csv file and transform the resulting
\texttt{my\_triangle\_csv} to a matrix.

We inspect the triangle:

\begin{verbatim}
      dev
origin    0    1     2     3     4     5     6     7     8     9
  2004 5947 9668 10564 10772 10978 11041 11106 11121 11132 11148
  2005 6347 9593 10316 10468 10536 10573 10625 10637 10648    NA
  2006 6269 9245 10092 10355 10508 10573 10627 10636    NA    NA
  2007 5863 8546  9269  9459  9592  9681  9724    NA    NA    NA
  2008 5779 8524  9178  9451  9682  9787    NA    NA    NA    NA
  2009 6185 9013  9586  9831  9936    NA    NA    NA    NA    NA
  2010 5600 8493  9057  9282    NA    NA    NA    NA    NA    NA
  2011 5288 7728  8256    NA    NA    NA    NA    NA    NA    NA
  2012 5291 7649    NA    NA    NA    NA    NA    NA    NA    NA
  2013 5676   NA    NA    NA    NA    NA    NA    NA    NA    NA
\end{verbatim}

\subsubsection*{From Cumulative to Incremental, and vice
versa}\label{from-cumulative-to-incremental-and-vice-versa}
\addcontentsline{toc}{subsubsection}{From Cumulative to Incremental, and
vice versa}

The \texttt{R} functions \texttt{cum2incr()} and \texttt{incr2cum()}
enable us to switch from cumulative to incremental displays, and vice
versa, in an easy way.

\begin{verbatim}
      dev
origin    0    1   2   3   4   5  6  7  8  9
  2004 5947 3721 896 208 207  62 66 15 11 16
  2005 6347 3246 723 152  68  37 53 11 12 NA
  2006 6269 2976 847 263 153  65 54  9 NA NA
  2007 5863 2683 723 191 133  88 43 NA NA NA
  2008 5779 2745 654 273 230 105 NA NA NA NA
  2009 6185 2828 573 245 105  NA NA NA NA NA
  2010 5600 2893 563 226  NA  NA NA NA NA NA
  2011 5288 2440 528  NA  NA  NA NA NA NA NA
  2012 5291 2358  NA  NA  NA  NA NA NA NA NA
  2013 5676   NA  NA  NA  NA  NA NA NA NA NA
\end{verbatim}

We recognize the incremental triangle from Figure
\ref{fig:tikz-triangle}.

\subsubsection*{Visualizing Triangles}\label{visualizing-triangles}
\addcontentsline{toc}{subsubsection}{Visualizing Triangles}

To explore the evolution of the cumulative payments per occurrence year,
Figure \ref{fig:plottriangle} shows \texttt{my\_triangle} using the
\texttt{plot} function available for objects of type \texttt{triangle}
in the \texttt{ChainLadder} package. Each line in this plot pictures an
occurrence year (from 2004 to 2013, labelled as 1 to 10). Development
periods are labelled from 1 to 10 (instead of 0 to 9, as used above).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(my_triangle)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{LossDataAnalytics_files/figure-latex/plottriangle-1} 

}

\caption{Claim Development by Occurrence Year}\label{fig:plottriangle}
\end{figure}

Alternatively, the \texttt{lattice} argument creates one plot per
occurrence year.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(my_triangle, }\DataTypeTok{lattice =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{LossDataAnalytics_files/figure-latex/plot_triangle_lattice-1} \end{center}

Instead of plotting the cumulative triangle stored in
\texttt{my\_triangle}, we can plot the incremental run-off triangle.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(my_triangle_incr)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{LossDataAnalytics_files/figure-latex/plot_triangle_incr-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(my_triangle_incr, }\DataTypeTok{lattice =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{LossDataAnalytics_files/figure-latex/plot_triangle_incr-2} \end{center}

\section{The Chain-Ladder}\label{S:Chain-ladder}

The most widely used method to estimate outstanding loss reserves is the
so-called chain-ladder method. The origins of this method are obscure
but was firmly entrenched in practical applications by the early 1970's,
\citet{taylor1986claims}. As will be seen, the name refers to the
chaining of a sequence of (year-to-year development) factors into a
ladder of factors; immature losses climb toward maturity when multiplied
by this concatenation of ratios, hence the apt description
\emph{chain-ladder method.} We will start with exploring the
chain-ladder method in its deterministic or algorithmic version, hence
without making any stochastic assumptions. Then we will describe Mack's
distribution-free chain-ladder model.

\subsection{The Deterministic Chain-Ladder}\label{S:DeterministicCL}

The deterministic chain-ladder method focuses on the run-off triangle in
cumulative form. Recall that a cell \((i,j)\) in this triangle displays
the cumulative amount paid up until development period \(j\) for claims
that occurred in year \(i\). The chain-ladder method assumes that
\textbf{development factors} \(f_j\) (also called age-to-age factors,
link ratios or chain-ladder factors) exist such that

\[
C_{i,j+1} = f_j \times C_{i,j}.
\]

Thus, the development factor tells you how the cumulative amount in
development year \(j\) grows to the cumulative amount in year \(j+1\).
We highlight the cumulative amount in period 0 in \textcolor{blue} and
the cumulative amount in period 1 in \textcolor{red} on the Figure
\ref{fig:tikz-cum-triangle-cl1} taken from \citet{WuthrichMerz2008}
(Table 2.2, also used in \citet{WuthrichMerz2015}, Table 1.4).

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/tikz-cum-triangle-cl1-1} 

}

\caption{A run-off triangle with cumulative payment data highlighting the cumulative amount in period 0 in blue and the cumulative amount in period 1 in red. Source: @WuthrichMerz2008, Table 2.2.}\label{fig:tikz-cum-triangle-cl1}
\end{figure}

The chain-ladder method then presents an intuitive recipe to estimate or
calculate these development factors. Since the first development factor
\(f_0\) describes the development of the cumulative claim amount from
development period 0 to development period 1, it can be estimated as the
ratio of the cumulative amounts in red and the cumulative amounts in
blue, highlighted in the Figure \ref{fig:tikz-cum-triangle-cl1}. In math
notation we then obtain the following estimate \(\hat{f}_0^{CL}\) for
the first development factor \(f_0\), given observations
\(\mathcal{D}_I\):

\[
\hat{f}^{CL}_{\color{magenta}{0}} = \frac{\sum_{i=1}^{10-\color{magenta}{0}-1} \color{red}{C_{i,\color{magenta}{0}+1}}}{\sum_{i=1}^{10-\color{magenta}{0}-1} \color{blue}{C_{i\color{magenta}{0}}}}= 1.4925.
\]

Note that the index \(i\), used in the sums in the numerator and
denominator, runs from the first occurrence period (1) to the last
occurrence period (9) for which both development periods 0 and 1 are
observed. As such, this development factor measures how the data in blue
grow to the data in red, averaged across all occurrence periods for
which both periods are observed. The chain-ladder method then uses this
development factor estimator to predict the cumulative amount
\(C_{10,1}\) (i.e.~the cumulative amount paid up until and including
development year 1 for accidents that occurred in year 10). This
prediction is obtained by multiplying the most recent observed
cumulative claim amount for occurrence period 10 (i.e. \(C_{10,0}\) with
development period 0) with the estimated development factor
\(\hat{f}^{CL}_0\):

\[
\hat{C}_{10, 1} = C_{10,0} \cdot \hat{f}^{CL}_0 = 5,676\cdot 1.4925=8,471.
\] Going forward with this reasoning, the next development factor
\(f_1\) can be estimated. Since \(f_1\) captures the development from
period 1 to period 2, it can be estimated as the ratio of the numbers in
red and the numbers in blue as highlighted in Figure
\ref{fig:tikz-cum-triangle-cl2}.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/tikz-cum-triangle-cl2-1} 

}

\caption{A run-off triangle with cumulative payment data highlighting the cumulative amount in period 1 in blue and the cumulative amount in period 2 in red. Source: @WuthrichMerz2008, Table 2.2.}\label{fig:tikz-cum-triangle-cl2}
\end{figure}

The mathematical notation of the estimate \(\hat{f}_1^{CL}\) for the
next development factor \(f_1\), given observations \(\mathcal{D}_I\),
equals:

\[
\hat{f}^{CL}_{\color{magenta}{1}} = \frac{\sum_{i=1}^{10-\color{magenta}{1}-1} \color{red}{C_{i,\color{magenta}{1}+1}}}{\sum_{i=1}^{10-\color{magenta}{1}-1} \color{blue}{C_{i\color{magenta}{1}}}}=1.0778.
\] Consequently, this factor measures how the cumulative paid amount in
development period 1 grows to period 2, averaged across all occurrence
periods for which both periods are observed. The index \(i\) now runs
from period 1 to 8, since these are the occurrence periods for which
both development periods 1 and 2 are observed. This estimate for the
second development factor is then used to predict the missing,
unobserved cells in development period 2:

\[
\begin{array}{rl}
\hat{C}_{10,2} &= C_{10,0} \cdot \hat{f}^{CL}_0 \cdot \hat{f}_1^{CL} = \hat{C}_{10,1} \cdot \hat{f}_1^{CL} = 8,471 \cdot 1.0778 = 9,130 \\
\hat{C}_{9,2}  &= C_{9,1} \cdot \hat{f}^{CL}_1 = 7,649 \cdot 1.0778 = 8,244.
\end{array}
\] Note that for \(\hat{C}_{10,2}\) you actually use the estimate
\(\hat{C}_{10,1}\) and multiply it with the estimated development factor
\(\hat{f}_1^{CL}\).

We continue analogously and obtain following predictions, printed in
italics in the Figure \ref{fig:tikz-cum-triangle-cl3}:

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/tikz-cum-triangle-cl3-1} 

}

\caption{A run-off triangle with cumulative payment data including predictions in italic. Source: [@WuthrichMerz2008], Table 2.2.}\label{fig:tikz-cum-triangle-cl3}
\end{figure}

Eventually we need to estimate the values in the final column. The last
development factor \(f_8\) measures the growth from development period 8
to development period 9 in the triangle. Since only the first row in the
triangle has both cells observed, this last factor is estimated as the
ratio of the value in red and the value in blue in Figure
\ref{fig:tikz-cum-triangle-cl3b}.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/tikz-cum-triangle-cl3b-1} 

}

\caption{A run-off triangle with cumulative payment data highlighting the cumulative amount in period 8 in blue and the cumulative amount in period 9 in red. Source: [@WuthrichMerz2008], Table 2.2.}\label{fig:tikz-cum-triangle-cl3b}
\end{figure}

Given observations \(\mathcal{D}_I\), this factor estimate
\(\hat{f}^{CL}_{8}\) is equal to:

\[
\hat{f}^{CL}_{\color{magenta}{8}} = \frac{\sum_{i=1}^{10-\color{magenta}{8}-1} \color{red}{C_{i,\color{magenta}{8}+1}}}{\sum_{i=1}^{10-\color{magenta}{8}-1} \color{blue}{C_{i\color{magenta}{8}}}}=1.001.
\] Typically this last development factor is close to 1 and hence the
cash flows paid in the final development period are minor. Using this
development factor estimate, we can now estimate the remaining
cumulative claim amounts in the column by multiplying the values for
development year 8 with this factor.

The general math notation for the chain ladder predictions for the lower
triangle (\(i+j>I\)) is as follows:

\[
\begin{array}{rl}
\hat{C}_{ij}^{CL} &= C_{i,I-i} \cdot \prod_{l=I-i}^{j-1} \hat{f}_l^{CL} \\
\hat{f}_j^{CL} &= \frac{\sum_{i=1}^{I-j-1} C_{i,j+1}}{\sum_{i=1}^{I-j-1} C_{ij}},
\end{array}
\] where \(C_{i,I-i}\) is on the last observed diagonal. It is clear
that an important assumption of the chain-ladder method is that the
proportional developments of claims from one development period to the
next are similar for all occurrence years.

This yields the following Figure \ref{fig:tikz-cum-triangle-cl4}:

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/tikz-cum-triangle-cl4-1} 

}

\caption{A run-off triangle with cumulative payment data including predictions in italic. Source: @WuthrichMerz2008, Table 2.2.}\label{fig:tikz-cum-triangle-cl4}
\end{figure}

The numbers in the last column show the estimates for the ultimate claim
amounts. The estimate for the outstanding claim amount
\(\hat{\mathcal{R}}_i^{CL}\) for a particular occurrence period
\(i=I-J+1,\ldots, I\) is then given by the difference between the
ultimate claim amount and the cumulative amount as observed on the most
recent diagonal:

\[
\hat{\mathcal{R}}_i^{CL} =\hat{C}_{iJ}^{CL}-C_{i,I-i}.
\] This is the chain-ladder estimate for the reserve necessary to
fulfill future liabilities with respect to claims that occurred in this
particular occurrence period. These reserves per occurrence period and
for the total summed over all occurrence periods are summarized in
Figure \ref{fig:tikz-cum-triangle-cl5}.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/tikz-cum-triangle-cl5-1} 

}

\caption{Reserves per occurence period and for total}\label{fig:tikz-cum-triangle-cl5}
\end{figure}

\subsection{Mack's Distribution-Free Chain-Ladder
Model}\label{macks-distribution-free-chain-ladder-model}

At this stage, the traditional chain-ladder method provides a point
estimator \(\hat{C}^{CL}_{iJ}\) for the forecast of \(C_{iJ}\), using
the information \(\mathcal{D}_I\). Since the chain-ladder method is a
purely deterministic and intuitively natural algorithm to complete a
run-off triangle, we are not able to determine how reliable that point
estimator is or to model the variation of the future payments. To answer
such questions an underlying stochastic model that reproduces the
chain-ladder reserve estimates is needed.

In this section we will focus on the distribution-free chain-ladder
model as an underlying stochastic model, introduced in \citet{Mack1993}.
This method allows us to estimate the standard errors of the
chain-ladder predictions. In the next Section \ref{S:GLMs}, generalized
linear models are used to develop a fully stochastic approach for
predicting the outstanding reserve.

In Mack's approach the following conditions (without assuming a
distribution) hold:

\begin{itemize}
\item
  Cumulative claims \((C_{ij})_{j=0,\ldots,J}\) are independent over
  different occurrence periods \(i\).
\item
  There exist fixed constants \(f_0, \ldots, f_{J-1}\) and
  \(\sigma^2_0,\ldots, \sigma^2_{J-1}\) such that for all
  \(i=1,\ldots, I\) and \(j=0,\ldots,J-1\):
\end{itemize}

\[
\begin{array}{rl}
E[C_{i,j+1}|C_{i0},\ldots,C_{ij}] &= f_j \cdot C_{ij} \\
\text{Var}(C_{i,j+1}|C_{ij}) &= \sigma^2_j \cdot C_{ij}.
\end{array}
\]

This means that the cumulative claims \((C_{ij})_{j=0,\ldots,J}\) are
Markov processes (in the development periods \(j\)) and hence the future
only depends on the present.

Under these assumptions, the expected value of the ultimate claim amount
\(C_{i,J}\), given the available data in the upper triangle, is the
cumulative amount on the most recent diagonal (\(C_{i, I-1}\))
multiplied with appropriate development factors \(f_j\). In mathematical
notation we obtain for known development factors \(f_j\) and
observations \(\mathcal{D}_I\):

\[
E[C_{iJ}|\mathcal{D}_I] = C_{i,I-i} \prod_{j=I-i}^{J-1} f_j.
\] This is exactly what the deterministic chain-ladder method does, as
explained in Section \ref{S:DeterministicCL}. In practice, the
development factors are not known and need to be estimated from the data
that is available in the upper triangle. In Mack's approach we obtain
exactly the same expression for estimating the development factors
\(f_j\) at time \(I\) as in the deterministic chain-ladder algorithm:

\[
\hat{f}_j^{CL} =\frac{\sum_{j=1}^{I-j-1} C_{i,j+1}}{\sum_{i=1}^{I-j-1} C_{ij}}.
\] The predictions for the cells in the lower triangle (i.e.~for cells
\$C\_\{i,j\} \$where \(i+j>I\)) are then obtained by replacing the
unknown factors \(f_j\) by their corresponding estimates
\(\hat{f}_j^{CL}\):

\[
\hat{C}^{CL}_{ij} = C_{i,I-i}\prod_{l=I-i}^{j-1} \hat{f}_l^{CL}.
\]

To quantify the prediction error that comes with the chain-ladder
predictions, Mack also introduced variance parameters \(\sigma^2_j\). To
gain insight in the estimation of these variance parameters, so-called
individual development factors \(f_{i,j}\) are introduced (which are
specific to occurrence period \(i\)):

\[
f_{i,j} = \frac{C_{i,j+1}}{C_{ij}}.
\] These individual development also describe how the cumulative amount
grows from period \(j\) to period \(j+1\), but they consider the ratio
of only two cells (instead of taking the ratio of two sums over all
available occurrence periods). Note that the development factors can be
written as a weighted average of individual development factors:

\[
\hat{f}_j^{CL} = \sum_{i=1}^{I-j-1} \frac{C_{ij}}{\sum_{i=1}^{I-j-1} C_{ij}} f_{i,j},
\] where the weights are equal to the cumulative claims \(C_{ij}\).

Let us now estimate the variance parameters \(\sigma^2\) by writing
Mack's variance assumption in equivalent ways. First, the variance of
the ratio of \(C_{i,j+1}\) and \(c_{i,j}\) conditional on
\(C_{i,0},\ldots, C_{i,j}\) is proportional to the inverse of
\(C_{i,j}\):

\[
\text{Var}[C_{i,j+1}/C_{ij}|C_{i0},\ldots,C_{ij}] ~ \propto ~ \frac{1}{C_{ij}}.
\]

This reminds us of a typical \emph{weighted least squares} setting where
the weights are the inverse of the variability of a response. Therefore,
a more volatile or imprecise response variable will get less weight. The
\(C_{i,j}\) play the role of the weights. Using the unknown variance
parameter \(\sigma^2_j\) this variance assumption can be written as:

\[
\text{Var}[C_{i,j+1}|C_{i0},\ldots,C_{ij}] = \sigma^2_j \cdot C_{ij},
\] The connection with weighted least squares then directly leads to an
unbiased estimate for the unknown variance parameter \(\sigma^2_j\) in
the form of a weighted residual sum of squares:

\[
\hat{\sigma}^2_j = \frac{1}{I-j-2}\sum_{i=1}^{I-j-1} C_{ij}\left(\frac{C_{i,j+1}}{C_{ij}}-\hat{f}_j^{CL}\right)^2.
\] The weights are again equal to \(C_{i,j}\) and the residuals are the
differences between the ratios \(C_{i,j+1}/C_{i,j}\) and the individual
development factors.

We now have all ingredients required to calibrate the distribution-free
chain-ladder model to the data. The next step is then to analyze the
prediction uncertainty and the prediction error. Hereto we use the
chain-ladder predictor where we replace the unknown development factors
with their estimators:

\[
\hat{C}_{iJ}^{CL} = C_{i,I-i} \prod_{l=I-i}^{J-1} \hat{f}_l^{CL}
\]

We use this expression either as an estimator for the conditional
expectation of the ultimate claim amount (given the observed upper
triangle) or as a predictor for the ultimate claim amount as a random
variable (given the observed upper triangle).

In statistics the simplest measure to analyze the uncertainty that comes
with a point estimate or prediction is the \emph{Mean Squared Error of
Prediction} (\emph{MSEP}). Here we consider a conditional \emph{MSEP},
conditional on the data observed in the upper triangle:

\[
MSEP_{C_{iJ}|\mathcal{D}_I}\left(\hat{C}_{iJ}^{CL}\right) = E\left[\left(C_{iJ}-\hat{C}_{iJ}^{CL}\right)^2|\mathcal{D}_I\right]. 
\] This conditional \emph{MSEP} measures:

\begin{itemize}
\tightlist
\item
  the distance between the (true) ultimate claim \(C_{iJ}\) and its
  chain-ladder predictor \(\hat{C}_{iJ}^{CL}\) at time \(I\), and
\item
  the total prediction uncertainty over the entire run-off of the
  nominal ultimate claim \(C_{iJ}\). It does not consider time value of
  money, a risk margin nor any dynamics in its developments.
\end{itemize}

The \emph{MSEP} that comes with the estimate for the ultimate cumulative
claim amount is equal to the \emph{MSEP} that measures the squared
distance between the true and the estimated reserve:

\[
\begin{array}{rl}
MSEP_{\hat{\mathcal{R}}^{I}_{i}|\mathcal{D}_I}(\hat{\mathcal{R}}^{I}_i) &= E[(\hat{\mathcal{R}}^I_i-\mathcal{R}^I_i)^2|\mathcal{D}_I] \\
&= E[(\hat{C}^{CL}_{iJ}-C_{iJ})^2|\mathcal{D}_I] = MSEP(\hat{C}_{iJ}). 
\end{array}
\] The reason for this equivalence is the fact that the reserve is the
ultimate claim amount minus the most recently observed claim amount. The
latter is observed and used in both \(\mathcal{R}^I_i\) and
\(\hat{\mathcal{R}}^I_i\).

It is interesting to decompose this \emph{MSEP} into a component that
captures \emph{process variance} and a component that captures
\emph{parameter estimation variance}:

\[
\begin{array}{rl}
MSEP_{C_{iJ|\mathcal{D}_I}}\left(\hat{C}_{iJ}^{CL}\right) &=
E\left[ \left( C_{iJ} - \hat{C}_{iJ} \right)^2 | \mathcal{D}_I\right] \\
&= \text{Var}(C_{iJ}|\mathcal{D}_I) + \left( E[C_{iJ}|\mathcal{D}_I]-\hat{C}_{iJ}^{CL} \right)^2 \\
&= \color{magenta}{\text{process variance}} + \color{magenta}{\text{parameter estimation variance}},
\end{array}
\]

for a \(\mathcal{D}_I\) measurable estimator/predictor \(\hat{C}_{iJ}\).
The process variance component captures the volatility or uncertainty in
the random variable \(C_{i,J}\) and the parameter estimation variance
measures the error that arises from replacing the unknown development
factors \(f_j\) with their estimated values. This result follows
immediately from following equality about the variance of a shifted
random variable \(X\) where the shift \(a\) is deterministic:

\[
E(X-a)^2 = \text{Var}(X)+\left[EX-a\right]^2.
\]

Applied to the expression of the \emph{MSEP} you treat \(\hat{C}_{i,J}\)
as fixed because you work conditionally on the data in the upper
triangle and \(\hat{C}_{i,J}\) only uses information from this upper
triangle.

\citet{Mack1993} then derived the important formula for the conditional
\emph{MSEP} in the distribution-free chain-ladder model for a single
occurrence period \(i\):

\[
\widehat{MSEP_{C_{iJ}|\mathcal{D}_I}}  =
\left(\hat{C}_{iJ}^{CL}\right)^2 \sum_{j=I-i}^{J-1} \left[ \frac{\hat{\sigma}_j^2}{(\hat{f}_j^{CL})^2} \left(\frac{1}{\hat{C}^{CL}_{ij}}+\frac{1}{\sum_{n=1}^{I-j-1}C_{nj}}\right)\right].
\]

For the derivation of this popular formula, we refer to his paper. Note
that it is an estimate of the \emph{MSEP} since the unknown parameters
\(f_j\) and \(\sigma_j\) need to be estimated and since the estimation
error cannot be calculated explicitly.

Mack also derived a formula for the \emph{MSEP} for the total reserve,
across all occurrence periods:

\[
\begin{array}{ll}
\widehat{MSEP_{\sum_{i=1}^I \hat{C}^{CL}_{iJ}}}\left( \sum_{i=1}^I \hat{C}^{CL}_{iJ}  \right) \\
 \ \ \ \ \ \sum_{i=1}^I \widehat{MSEP_{C_{iJ}|\mathcal{D}_I}}\left( \hat{C}^{CL}_{iJ} \right)
\color{blue}{+2 \sum_{1\leq i< k \leq I} \hat{C}_{iJ}^{CL} \hat{C}_{kJ}^{CL}
\sum_{j=I-i}^{J-1} \frac{ \hat{\sigma}_j^2/\left( \hat{f}_j^{\text{CL}} \right)^2 }{ \sum_{n=1}^{I-j-1} C_{nj} }}.
\end{array}
\] The result is the sum of the \emph{MSEP}s per occurrence period plus
a covariance term. This covariance term is added because the MSEPs for
different occurrence periods \(i\) use the same parameter estimates
\(\hat{f}_j^{CL}\) of \(f_j\) for different accident years \(i\).

\subsection{R code for Chain-Ladder
Predictions}\label{r-code-for-chain-ladder-predictions}

We use the object \texttt{my\_triangle} of type \texttt{triangle} that
was created in Section \ref{S:Rcode}. The distribution-free chain-ladder
model of \citet{Mack1993} is implemented in the \texttt{ChainLadder}
package \citep{R-chainladder} (as a special form of weighted least
squares) and can be applied on the data \texttt{my\_triangle} to predict
outstanding claim amounts and to estimate the standard error around
those forecasts.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CL <-}\StringTok{ }\KeywordTok{MackChainLadder}\NormalTok{(my_triangle)}
\NormalTok{CL}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
MackChainLadder(Triangle = my_triangle)

         Latest Dev.To.Date   Ultimate      IBNR Mack.S.E CV(IBNR)
2004 11,148,124       1.000 11,148,124         0        0      NaN
2005 10,648,192       0.999 10,663,318    15,126      716   0.0474
2006 10,635,751       0.998 10,662,008    26,257    1,131   0.0431
2007  9,724,068       0.996  9,758,606    34,538    3,121   0.0904
2008  9,786,916       0.991  9,872,218    85,302    7,654   0.0897
2009  9,935,753       0.984 10,092,247   156,494   33,347   0.2131
2010  9,282,022       0.970  9,568,143   286,121   73,469   0.2568
2011  8,256,211       0.948  8,705,378   449,167   85,400   0.1901
2012  7,648,729       0.880  8,691,971 1,043,242  134,338   0.1288
2013  5,675,568       0.590  9,626,383 3,950,815  410,818   0.1040

                 Totals
Latest:   92,741,334.00
Dev:               0.94
Ultimate: 98,788,397.77
IBNR:      6,047,063.77
Mack.S.E     462,977.83
CV(IBNR):          0.08
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(}\KeywordTok{summary}\NormalTok{(CL)}\OperatorTok{$}\NormalTok{Totals)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             Totals
Latest:    92741334
Dev:              1
Ultimate:  98788398
IBNR:       6047064
Mack S.E.:   462978
CV(IBNR):         0
\end{verbatim}

The development factors are obtained as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(CL}\OperatorTok{$}\NormalTok{f,}\DataTypeTok{digits =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 1.4925 1.0778 1.0229 1.0148 1.0070 1.0051 1.0011 1.0010 1.0014 1.0000
\end{verbatim}

We can also print the complete run-off triangle (including predictions).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CL}\OperatorTok{$}\NormalTok{FullTriangle}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      dev
origin       0       1        2        3        4        5        6
  2004 5946975 9668212 10563929 10771690 10978394 11040518 11106331
  2005 6346756 9593162 10316383 10468180 10536004 10572608 10625360
  2006 6269090 9245313 10092366 10355134 10507837 10573282 10626827
  2007 5863015 8546239  9268771  9459424  9592399  9680740  9724068
  2008 5778885 8524114  9178009  9451404  9681692  9786916  9837277
  2009 6184793 9013132  9585897  9830796  9935753 10005044 10056528
  2010 5600184 8493391  9056505  9282022  9419776  9485469  9534279
  2011 5288066 7728169  8256211  8445057  8570389  8630159  8674567
  2012 5290793 7648729  8243496  8432051  8557190  8616868  8661208
  2013 5675568 8470989  9129696  9338521  9477113  9543206  9592313
      dev
origin        7        8        9
  2004 11121181 11132310 11148124
  2005 10636546 10648192 10663318
  2006 10635751 10646884 10662008
  2007  9734574  9744764  9758606
  2008  9847905  9858214  9872218
  2009 10067393 10077931 10092247
  2010  9544579  9554570  9568143
  2011  8683939  8693029  8705378
  2012  8670566  8679642  8691971
  2013  9602676  9612728  9626383
\end{verbatim}

The MSEP for the total reserve across all occurrence periods is given
by:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CL}\OperatorTok{$}\NormalTok{Total.Mack.S.E}\OperatorTok{^}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
           9 
214348469061 
\end{verbatim}

It is strongly advised to validate Mack's assumptions by checking that
there are no trends in the residual plots. The last four plots that we
obtain with the following command show respectively the standardized
residuals versus the fitted values, the origin period, the calendar
period and the development period.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(CL)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{LossDataAnalytics_files/figure-latex/ResidualPlot-1} \end{center}

The top left-hand plot is a bar-chart of the latest claims position plus
IBNR and Mack's standard error by occurrence period. The top right-hand
plot shows the forecasted development patterns for all occurrence
periods (starting with 1 for the oldest occurrence period).

When setting the argument \texttt{lattice=TRUE} we obtain a plot of the
development, including the prediction and estimated standard errors by
occurrence period:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(CL, }\DataTypeTok{lattice=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{LossDataAnalytics_files/figure-latex/PlotLattice-1} \end{center}

\section{GLMs and Bootstrap for Loss Reserves}\label{S:GLMs}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{This section is being written and is not yet complete nor
edited. It is here to give you a flavor of what will be in the final
version.}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

This section covers regression models to analyze run-off triangles. When
analyzing the data in a run-off triangle with a regression model, the
standard toolbox for model building, estimation and prediction becomes
available. Using these tools we are able to go beyond the point estimate
and standard error as derived in Section \ref{S:Chain-ladder}. More
specifically, we build a generalized linear model (GLM) for the
incremental payments \(X_{ij}\) in Figure \ref{fig:tikz-triangle}.
Whereas the chain-ladder method works with cumulative data, typical GLMs
assume the response variables to be independent and therefore work with
incremental run-off triangles.

\subsection{Model Specification}\label{model-specification}

Let \(X_{ij}\) denote the incremental payment in cell \((i,j)\) of the
run-off triangle. We assume the \(X_{ij}\)s to be independent with a
density \(f(x_{ij};\theta_{ij},\phi)\) from the exponential family of
distributions. We identify

\begin{itemize}
\tightlist
\item
  \(\mu_{ij}=E[X_{ij}]\) the expected value of cell \(X_{ij}\)
\item
  \(\phi\) the dispersion parameter and
  \(\text{Var}[X_{ij}]=\phi \cdot V(\mu_{ij})\), where \(V(.)\) is the
  variance function
\item
  \(\eta_{ij}\) the linear predictor such that \(\eta_{ij}=g(\mu_{ij})\)
  with \(g\) the link function.
\end{itemize}

Distributions from the exponential family and their default link
functions are listed on
\url{http://stat.ethz.ch/R-manual/R-patched/library/stats/html/family.html}.
We now discuss three specific GLMs widely used for loss reserving.

First, the Poisson regression model was introduced in Section
\ref{S:RC:PoissonRegression}. In this model, we assume that \(X_{ij}\)
has a Poisson distribution with parameter

\[
\mu_{ij} = \pi_i \cdot \gamma_j,
\]

a cross-classified structure that captures a multiplicative effect of
the occurrence year \(i\) and the development period \(j\). The proposed
model structure is not identifiable without an additional constraint on
the parameters, e.g. \(\sum_{j=0}^J \gamma_j=1\). This constraint gives
an explicit interpretation to \(\pi_i\) (with \(i=1,\ldots,I\)) as the
exposure or volume measure for occurrence year \(i\) and \(\gamma_j\) as
the fraction of the total volume paid out with delay \(j\). However,
when calibrating GLMs in \texttt{R} alternative constraints such as
\(\pi_1=1\) or \(\gamma_1=1\), or a reparametrization where
\(\mu_{ij} = \exp{(\mu+\alpha_i+\beta_j)}\) are easier to implement. We
continue with the latter specification, including
\(\alpha_1 = \beta_0 = 0\), the so-called corner constraints. This GLM
treats the occurrence year and the payment delay as factor variables and
fits a parameter per level, next to an intercept \(\mu\). The corner
constraints put the effect of the first level of a factor variable equal
to zero. The Poisson assumption is particularly useful for a run-off
triangle with numbers of reported claims, often used in the estimation
of the number of IBNR claims (see Section \ref{S:Data}).

Second, an interesting modification of the basic Poisson regression
model is the \textbf{over-dispersed Poisson} regression model where
\(Z_{ij}\) has a Poisson distribution with parameter \(\mu_{ij}/\phi\)
and

\[
\begin{array}{rl}
X_{ij} &\sim  \phi \cdot Z_{ij}  \\
\mu_{ij} &= \exp{(\mu + \alpha_i + \beta_j)}.
\end{array}
\]

Consequently, \(X_{ij}\) has the same specification for the mean as in
the basic Poisson regression model, but now

\[
\text{Var}[X_{ij}] = \phi^2 \cdot \text{Var}[Z_{ij}] = \phi \cdot \exp{(\mu + \alpha_i + \beta_j)}.
\]

This construction allows for under (when \(\phi <1\)) and
over-dispersion (with \(\phi>1\)). Because \(X_{ij}\) no longer follows
a well-known distribution, this approach is referred to as
quasi-likelihood. It is particularly useful to model a run-off triangle
with incremental payments, as these typically reveal over-dispersion.

Third, the \textbf{gamma} regression model is relevant to model a
run-off triangle with claim payments. Recall from Section
\ref{S:Loss:Gamma} (see also the Appendix Chapter
\ref{C:SummaryDistributions}) with parameters \(\alpha\) and \(\theta\).
From these, we reparameterize and define a new parameter
\(\mu = \alpha \cdot \theta\) while retaining the scale parameter
\(\theta\). Further, assume that \(X_{ij}\) has a gamma distribution and
allow \(\mu\) to vary by \(ij\) such that

\[
\mu_{ij} =  \exp{(\mu + \alpha_i + \beta_j)}.
\]

\subsection{Model Estimation and
Prediction}\label{model-estimation-and-prediction}

We now estimate the regression parameters \(\mu\), \(\alpha_i\) and
\(\beta_j\) in the proposed GLMs. In \texttt{R} the \texttt{glm}
function is readily available to estimate these parameters via maximum
likelihood estimation (mle) or quasi-likelihood estimation (in the case
of the over-dispersed Poisson). Having the parameter estimates
\(\hat{\mu}\), \(\hat{\alpha}_i\) and \(\hat{\beta}_j\) available, a
point estimate for each cell in the upper triangle follows

\[
\hat{X}_{ij} =\hat{E[X_{ij}]} = \exp{(\hat{\mu}+\hat{\alpha}_i+\hat{\beta}_j)},\ \text{with}\ i+j\leq I.
\] Similarly, a cell in the lower triangle will be predicted as

\[
\hat{X}_{ij} = \hat{E[X_{ij}]} = \exp{(\hat{\mu}+\hat{\alpha}_i+\hat{\beta}_j)},\ \text{with}\ i+j> I.
\]

Point estimates for outstanding reserves (per occurrence year \(i\) or
the total reserve) then follow by summing the cell-specific estimates.
By combining the observations in the upper triangle with their point
estimates, we can construct properly defined residuals and use these for
residual inspection.

\subsection{Bootstrap}\label{bootstrap}

\section{Further Resources and
Contributors}\label{LossRe:further-reading-and-resources}

\subsubsection*{Contributors}\label{contributors-7}
\addcontentsline{toc}{subsubsection}{Contributors}

\begin{itemize}
\tightlist
\item
  \textbf{Katrien Antonio}, KU Leuven and University of Amsterdam,
  \textbf{Jan Beirlant}, KU Leuven, and \textbf{Tim Veerdonck},
  University of Antwerp, are the principal authors of the initial
  version of this chapter. Email:
  \href{mailto:katrien.antonio@kuleuven.be}{\nolinkurl{katrien.antonio@kuleuven.be}}
  for chapter comments and suggested improvements.
\end{itemize}

\subsubsection*{Further Readings and
References}\label{further-readings-and-references-1}
\addcontentsline{toc}{subsubsection}{Further Readings and References}

As displayed in Figure \ref{fig:tikz-run-off}, similar timelines and
visualizations are discussed (among others) in \citet{WuthrichMerz2008},
\citet{AntonioPlat2014} and \citet{WuthrichMerz2015}.

Over time actuaries started to think about possible underlying models
and we mention some important contributions:

\begin{itemize}
\tightlist
\item
  \citet{Kremer1982}: two-way ANOVA
\item
  \citet{Kremer1984}, \citet{Mack1991}: Poisson model
\item
  \citet{Mack1993}: distribution-free chain-ladder model
\item
  \citet{Renshaw1989}; \citet{RenshawVerrall1998}: over-dispersed
  Poisson model
\item
  \citet{Gisler2006}; \citet{GislerWuthrich2008}; \citet{Buhlmann2009}:
  Bayesian chain-ladder model.
\end{itemize}

The various stochastic models proposed in actuarial literature rely on
different assumptions and have different model properties, but have in
common that they provide exactly the chain-ladder reserve estimates. For
more information we also refer to \citet{MackVenter2000} and to the
lively discussion that was published in \emph{ASTIN Bulletin: Journal of
the International Actuarial Association} in 2006 \citep{Venter2006}.

To read more about exponential families and generalized linear models,
see, for example, \citet{McCullagh1989} and \citet{WuthrichMerz2008}. We
refer to \citep{Kremer1982}, \citep{RenshawVerrall1998} and
\citep{EnglandVerrall2002}, and the overviews in \citep{Taylor2000},
\citep{WuthrichMerz2008} and \citep{WuthrichMerz2015} for more details
on the discussed GLMs. XXX presents alternative distributional
assumptions and specifications of the linear predictor.

\chapter{Experience Rating using Bonus-Malus}\label{C:BonusMalus}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{This chapter is being written and is not yet complete nor
edited. It is here to give you a flavor of what will be in the final
version.}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{Introduction}\label{S:ERBM:Intro}

Bonus-malus system, which is used interchangeably as ``no-fault
discount'', ``merit rating'', ``experience rating'' or ``no-claim
discount'' in different countries, is based on penalizing insureds who
are responsible for one or more claims by a premium surcharge, and
awarding insureds with a premium discount if they do not have any
claims. Insurers use bonus-malus systems for two main purposes; to
encourage drivers to drive more carefully in a year without any claims,
and to ensure insureds to pay premiums proportional to their risks based
on their claims experience.

No Claim Discount (NCD) system is an experience rating system commonly
used in motor insurance. It represents an attempt to categorize insureds
into homogeneous groups who pay premiums based on their claims
experience. Depending on the rules in the scheme, new policyholders may
be required to pay full premium initially, and obtain discounts in the
future years as a results of claim-free years. An NCD system rewards
policyholders for not making any claims during a year, or in other
words, it grants a bonus to a careful driver. This bonus principle may
affect policy holders' decisions whether to claim or not to claim,
especially when involving accidents with slight damages, which is known
as `hunger for bonus' phenomenon. The option of `hunger for bonus' under
an NCD system may reduce insurers' claim costs, and may be able to
offset the expected decrease in premium income.

\section{NCD System in Several Countries}\label{S:ERBM:NCD}

\subsection{NCD System in Malaysia}\label{ncd-system-in-malaysia}

Before the liberalization of Motor Tariff on 1st July 2017, the rating
of motor insurance in Malaysia was governed by Motor Tariff. Under the
tariff, the rate charged should not be lower than the rates specified
under the classes of risks, to ensure that the price competition among
insurers will not go below the country's economic level. The basic
rating factors considered were scope of insurance, cubic capacity of
vehicle and estimated value of vehicle (or sum insured, whichever is
lower). Under the Motor Tariff, the final premium to be paid is adjusted
by the policyholder's claim experience, or equivalently, his NCD
entitlement.

Effective on 1st July 2017, the premium rates for motor insurance are
liberalized, or de-tariffed. The pricing of premium is now determined by
individual insurers and takaful operators, and the consumers are able to
enjoy a wider choice of motor insurance products at competitive prices.
Since tariff liberalization encourages innovation and competition among
insurers and takaful operators, the premiums are based on broader risk
factors other than the two rating factors specified in the Motor Tariff,
i.e.~sum insured and cubic capacity of vehicle. Other rating factors may
be defined in the risk profile of an insured, such as age of vehicle,
age of driver, safety and security features of vehicle, geographical
location of vehicle and traffic offences of driver. As different
insurers and takaful operators have different ways of defining the risk
profile of an insured, the price of a policy may differ from one insurer
to another. However, the NCD structure from the Motor Tariff remains
`unchanged' and continue to exist, and is `transferable' from one
insurer, or from one takaful operator, to another.

The discounts in the Malaysian NCD system are divided into six classes,
starting from the initial class of 0\% discount, followed by classes of
25\%, 30\%, 38.3\%, 45\% and 55\% discounts. Table 1 provides the
classes of NCD system in Malaysia. A claim-free year indicates that a
policyholder is entitled to move one-step forward to the next discount
class, such as from a 0\% discount to a 25\% discount in the renewal
year. If a policyholder is already at the highest class, which is at a
55\% discount, a claim-free year indicates that the policyholder remains
in the same class. On the other hand, if one or more claims are made
within the year, the NCD will be forfeited and the policyholder has to
start at 0\% discount in the renewal year. In other words, the
policyholder has to pay a full premium for the next year's renewal
premium, regardless of his current class of NCD.

For an illustration purpose, Figure \ref{fig:TransitionMsia} shows the
transition diagram for the NCD classes under the Malaysian Motor Tariff.
The transition starts at class 0, and increase one by one if an insured
has a no-claim year. If an insured has one or more claims within the
year, the current class automatically returns to class 0.

\[
\begin{matrix}
\text{Table 1: Classes of NCD (Malaysia)}\\
\begin{array}{*{20}c}
\hline
\text{Classes (claim-free years)} & \text{Discounts}\\
\hline\\
{0} & {0}\\
{1} & {25}\\
{2} & {30}\\
{3} & {38.33}\\
{4} & {45}\\
{5\text{ and above}} & {55}\\\\
\hline
\end{array}
\end{matrix}
\]

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{Figures/FigureTransitionMsia} 

}

\caption{Transition diagram for NCD classes (Malaysia)}\label{fig:TransitionMsia}
\end{figure}

\subsection{NCD System in Other
Countries}\label{ncd-system-in-other-countries}

The NCD system in Brazil are subdivided into seven classes, with the
following premium levels (Lemaire 1998): 100, 90, 85, 80, 75, 70, and
65. These premium levels are also equivalent to the following discount
classes: 0\%, 10\%, 15\%, 20\%, 25\%, 30\% and 45\%. New policyholders
have to start at 0\% discount, or at level 100, and a claim-free year
indicates that a policyholder can move forward at a one-class discount.
If one or more claims incurred within the year, the policyholder has to
move backward, also at a one-class discount. Table 2 and Figure
\ref{fig:TransitionBrazil} respectively show the classes and the
transition diagram for the NCD system in Brazil.

\[
\begin{matrix}
\text{Table 2: Classes of NCD (Brazil)}\\
\begin{array}{*{20}c}
\hline
\text{Classes (claim-free years)} & \text{Discounts}\\
\hline\\
{0} & {0}\\
{1} & {10}\\
{2} & {15}\\
{3} & {20}\\
{4} & {25}\\
{5} & {30}\\
{6\text{ and above}} & {45}\\\\
\hline
\end{array}
\end{matrix}
\]

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{Figures/FigureTransitionBrazil} 

}

\caption{Transition diagram for NCD classes (Brazil)}\label{fig:TransitionBrazil}
\end{figure}

The NCD system in Switzerland, which is implemented in 1990, are
subdivided into twenty-two classes, with the following premium levels:
270, 250, 230, 215, 200, 185, 170, 155, 140, 130, 120, 110, 100, 90, 80,
75, 70, 65, 60, 55, 50 and 45 (Lemaire and Zi 1994). These levels are
also equivalent to the following loadings: 170\%, 150\%, 130\%, 115\%,
100\%, 85\%, 70\%, 55\%, 40\%, 30\%, 20\%, and 10\%, and the following
discounts: 0\%, 10\%, 20\%, 25\%, 30\%, 35\%, 40\%, 45\%, 50\% and 55\%.
New policyholders have to start at 170\% loading, or at 270 premium
level, and a claim-free year indicates that a policyholder can move
one-class forward. If one or more claims incurred within the year, the
policyholder has to move four-classes backwards. Table 3 and Figure
\ref{fig:TransitionSwitz} respectively show the classes and the
transition diagram for the NCD system in Switzerland.

\[
\begin{matrix}
\text{Table 3: Classes of NCD (Switzerland)}\\
\begin{array}{*{20}c}
\hline
\text{Classes} & \text{Loadings} & \text{Classes} & \text{Discounts}\\
\text{(claim-free years)} & & \text{(claim-free years)} & \\
\hline\\
{0} & {170} & {12} & {0}\\
{1} & {150} & {13} & {10}\\
{2} & {130} & {14} & {20}\\
{3} & {115} & {15} & {25}\\
{4} & {100} & {16} & {30}\\
{5} & {85} & {17} & {35}\\
{6} & {70} & {18} & {40}\\
{7} & {55} & {19} & {45}\\
{8} & {40} & {20} & {50}\\
{9} & {30} & {21 \text{ and above}} & {55}\\
{10} & {20} && \\
{11} & {10} && \\\\
\hline
\end{array}
\end{matrix}
\]

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{Figures/FigureTransitionSwitz} 

}

\caption{Transition diagram for NCD classes (Switzerland)}\label{fig:TransitionSwitz}
\end{figure}

\section{BMS and Markov Chain Model}\label{S:ERBM:BMS}

A BMS, or an NCD system, can be represented by a discrete time parameter
Markov chain. Under this model, the state space consists of classes of
bonus-malus, and the state (or class) is assumed to shift randomly from
year to year. Therefore, it is important to know a brief description on
two elements; a stochastic process that has a Markov property, and a
discrete time Markov chain.

\textbf{Definition:} A \textbf{\emph{stochastic process}} has a Markov
property if the
\href{https://en.wikipedia.org/wiki/Conditional_probability_distribution}{probability
distribution} of future states, conditional on both past and present
states, depends only upon the present state and not on the sequence of
events that preceded it.

\textbf{Definition:} A \textbf{\emph{discrete time Markov chain}} is a
stochastic process that can be parameterized by empirically estimating
the transition probabilities between discrete states.

\subsection{Transition Probability}\label{transition-probability}

The randomness of the transition of the NCD classes is governed by the
transition probability in a given year. The definition of a transition
probability is provided here to understand the use of the probability
for representing the transition of the NCD classes.

\textbf{Definition:} The \textbf{\emph{transition probability}} from
state \(i\) (at time \(n\)) to state \(j\) (at time \(n + 1\)) is called
a one-step transition probability, and is denoted by
\(p_{ij} = Pr (X_{n + 1} = j|X_n = i)\), \(i = 1,2,...,k\),
\(j = 1,2,...,k\).

The transition probabilities can be represented by a \(k \times k\)
matrix. Assuming a homogeneous Markov process, a \emph{k}-state Markov
chain can be represented by a \textbf{\emph{matrix of transition
probabilities}} \({\bf P}\):

\[
{\bf P} = 
\left[ {\begin{array}{*{20}c}
p_{11} & p_{12} & \ldots & & & p_{1k} \\
p_{21} & p_{22} & \ldots & & & p_{2k} \\ 
\vdots & \ddots & & & & \vdots \\
 & & & & & \\
 & & & & & \\
p_{k1} & p_{k2} & \cdots & & & p_{kk}   
\end{array} } \right].
\]

Each row of the transition matrix represents the transition of flowing
out of state, whereas each column represents the transition of flowing
into the state. The cumulative transitions of flowing out of state must
equal 1, or each row of the matrix must sum to 1, i.e.
\(\sum\limits_j p_{ij} = 1\). All probabilities must also be
non-negative (since they are probabilities), i.e. \(p_{ij} \ge 0\).

Consider the Malaysian NCD system. Under this system, let the random
variable \(X_t\) denotes the class of NCD at time \(t\) and takes the
values in a state space \(\bf{S}\), where \({\bf S} = \{0,1,...,5\}\).
Therefore, the probability of a no-claim year is equal to the
probability of transition from state \(i\) to state \(j\), which is
\(p_{ij}\), \(i = 0,1,2,...,5\), \(j = 0,1,2,...,5\). If an insured has
one or more claims within the year, the probability of transitioning
back to state 0 is represented by \(p_{i0}\), which is also equivalent
to \(1 - {p_{ij}}.\) Therefore, there are only two transition
probabilities in each row; the probability of advancing to the next
state, \(p_{ij},\) and the probability of returning back to state zero,
\(p_{i0} = 1 - p_{ij}\).

In terms of transition probabilities, the Malaysian NCD system can be
represented by the following \((6 \times 6)\) transition matrix:

\[
{\bf P} = 
\left[ {\begin{array}{*{20}c}
p_{00}&p_{01}&0&0&0&0\\
p_{10}&0&p_{12}&0&0&0\\
p_{20}&0&0&p_{23}&0&0\\
p_{30}&0&0&0&p_{34}&0\\
p_{40}&0&0&0&0&p_{45}\\
p_{50}&0&0&0&0&p_{55}
\end{array} }\right] = 
\left[ {\begin{array}{*{20}c}
{1 - p_{01}}&p_{01}&0&0&0&0\\
{1 - p_{12}}&0&p_{12}&0&0&0\\
{1 - p_{23}}&0&0&p_{23}&0&0\\
{1 - p_{34}}&0&0&0&p_{34}&0\\
{1 - p_{45}}&0&0&0&0&p_{45}\\
{1 - p_{55}}&0&0&0&0&p_{55}
\end{array} }\right]
\]

\textbf{\emph{Example 1}}

\emph{Provide the transition matrix for the NCD system in Brazil.}

\textbf{\emph{Solution}}

\emph{Based on the NCD classes and the transition diagram shown
respectively in Table 2 and Figure \ref{fig:TransitionBrazil}, the
probability of a no-claim year is equal to the probability of moving
one-class forward, whereas the probability of having one or more claims
within the year is equal to the probability of moving one-class
backward. Therefore, each row can contain two or more transition
probabilities; one probability for advancing to the next state, and one
or more probabilities for moving one-class backwards. The transition
matrix is:}

\[
{\bf P} = 
\left[ {\begin{array}{*{20}{c}}
{1 - p_{01}}&p_{01}&0&0&0&0&0\\
{1 - p_{12}}&0&p_{12}&0&0&0&0\\
{1 - \sum\limits_j p_{2j}}&p_{21}&0&p_{23}&0&0&0\\
{1 - \sum\limits_j p_{3j}}&p_{31}&p_{32}&0&p_{34}&0&0\\\
{1 - \sum\limits_j p_{4j}}&p_{41}&p_{42}&p_{43}&0&p_{45}&0\\
{1 - \sum\limits_j p_{5j}}&p_{51}&p_{52}&p_{53}&p_{54}&0&p_{56}\\
{1 - \sum\limits_j p_{6j}}&p_{61}&p_{62}&p_{63}&p_{64}&p_{65}&p_{66}
\end{array} } \right]
\]

\textbf{\emph{Example 2}}

\emph{Provide the transition matrix for the NCD system in Switzerland.}

\textbf{\emph{Solution}}

\emph{From Table 3 and Figure} \ref{fig:TransitionSwitz}\emph{, the
probability of a no-claim year is equal to the probability of moving
one-class forward, whereas the probability of having one or more claims
within the year is equal to the probability of moving four-classes
backward. The transition matrix is:}

\[
\begin{matrix}
\left|
{\begin{array}{*{12}{c}}
1 - {p_{01}} & p_{01} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots\\
1 - {p_{12}} & 0 & p_{12} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots\\
1 - {p_{23}} & 0 & 0 & p_{23} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots\\
1 - {p_{34}} & 0 & 0 & 0 & p_{34} & 0 & 0 & 0 & 0 & 0 & 0 & \cdots\\
1 - {p_{45}} & 0 & 0 & 0 & 0 & p_{45} & 0 & 0 & 0 & 0 & 0 & \cdots\\
1 - \sum\limits_j {p_{5j}} & p_{51} & 0 & 0 & 0 & 0 & p_{56} & 0 & 0 & 0 & 0 & \cdots\\
1 - \sum\limits_j {p_{6j}} & 0 & p_{62} & 0 & 0 & 0 & 0 & p_{67} & 0 & 0 & 0 & \cdots\\
1 - \sum\limits_j {p_{7j}} & 0 & 0 & p_{73} & 0 & 0 & 0 & 0 & p_{78} & 0 & 0 & \cdots\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots\\
1 - \sum\limits_j {p_{19,j}} & 0 & 0 & p_{19,3} & 0 & 0 & 0 & p_{19,7} & 0 & 0 & 0 & \cdots\\
1 - \sum\limits_j {p_{20,j}} & 0 & 0 & 0 & p_{20,4} & 0 & 0 & 0 & p_{20,8} & 0 & 0 & \cdots\\
1 - \sum\limits_j {p_{21,j}} & p_{21,1} & 0 & 0 & 0 & p_{21,5} & 0 & 0 & 0 & p_{21,9} & 0 & \cdots
\end{array}} \right|
\end{matrix}
\]

\section{BMS and Stationary Distribution}\label{S:ERBM:StatDist}

\subsection{Stationary Distribution}\label{stationary-distribution}

It is important to note that the transition matrix represents the
transition probabilities of the NCD classes in one year. If we are
interested in the distribution of the transition in the long run (which
may take several years), the stationary probability can be used.

Several definitions and properties are provided here to understand the
use of a Markov chain model for representing the stationary distribution
of the NCD transitions. However, students are encouraged to refer to
textbooks on Markov Chain and stochastic processes available in the
literature for a more comprehensive knowledge on the related subjects.

\textbf{Definition:} A Markov chain is said to be
\textbf{\emph{irreducible}} if it is possible to move to any state from
any other state.

\textbf{Definition:} A state \(i\) has \textbf{period} \(k\) if any
return to state \(i\) must occur in multiples of \(k\) time steps. If
\(k = 1\), then the state is said to be \textbf{\emph{aperiodic}}.

\textbf{Definition:} A state \(i\) is said to be \textbf{transient} if,
given that we start in state \(i\), there is a non-zero probability that
we will never return to \(i\). State \(i\) is recurrent (or persistent)
if it is not transient. A state \(i\) is \textbf{\emph{positive
recurrent}} if the \textbf{mean recurrent time} (or expected return
time) is \textbf{\emph{finite}}.

\textbf{Definition:} A state \(i\) is said to be
\emph{\href{https://en.wikipedia.org/wiki/Ergodic_theory}{\textbf{ergodic}}}
if it is aperiodic and positive recurrent. In other words, a state \(i\)
is ergodic if it is recurrent, has a period of one (aperiodic), and has
finite mean recurrence time. If all states in an irreducible Markov
chain are ergodic, then the chain is said to be ergodic.

\textbf{Definition:} Let \(p_{ij}^{(n)} = \Pr ({X_n} = j\|{X_0} = i)\)
be the probability of transition from state \(i\) to state \(j\) in
\(n\) time steps. The \emph{n}-step transition probabilities for a
\textbf{\emph{stationary Markov chain}} satisfy the
\emph{\href{https://en.wikipedia.org/wiki/Chapman\%E2\%80\%93Kolmogorov_equation}{\textbf{Chapman--Kolmogorov
equation}}:}

\[p_{ij}^{(n)} = \sum\limits_r {p_{ir}^{(k)}p_{rj}^{(n - k)}}\]

for any \(k\), where \(0 < k < n\).

\textbf{Definition:} If a Markov chain is
\emph{\href{https://en.wikipedia.org/w/index.php?title=Time-homogeneous_Markov_chain\&action=edit\&redlink=1}{stationary}},
then the vector \({\bf{\pi }}\) is called a stationary distribution (or
\emph{\href{https://en.wikipedia.org/wiki/Invariant_measure}{invariant
measure}}) if it satisfies:

\[0 \le {\pi _j} \le 1\]

\[\sum\limits_j {{\pi _j}} = 1,\]

\[{\pi_j} = \sum\limits_i {\pi_i}p_{ij}\].

In terms of vectors and matrices, a stationary distribution
\({\bf{\pi }}\) is a row vector, whose entries are non-negative and sum
to one, that is unchanged by the operation:

\({\bf{\pi P}} = {\bf{\pi }}\).

If a Markov chain is stationary, then the transition matrix \({\bf{P}}\)
is the same after each step, so that the \(k\)-step transition
probability can be computed as the \(k\)-th power of the transition
matrix, \({{\bf{P}}^k}.\)

From the definitions and properties above, a BMS forms a regular Markov
chain if all of its states (NCD classes) are ergodic, and the chain is
not cyclic (or irreversible). The row vector, \({\bf{\pi }}\), which is
also the \textbf{\emph{left}} eigenvector of the transition matrix, is a
\textbf{\emph{stationary distribution}} defined by
\(0 \le {\pi _j} \le 1\) and \(\sum\limits_j {{\pi_j}} = 1\).

\textbf{\emph{Example 3}}

\emph{Find the stationary distribution for the NCD system in Malaysia
assuming that the probability of a no-claim year for all NCD classes are
\({p_0}\).}

\textbf{\emph{Solution}}

\emph{The transition matrix can be re-written as:}

\emph{The stationary distribution can be calculated using}
\({\pi _j} = \sum\limits_i {\pi _i}p_{ij}\)\emph{. The solutions are:}

\[
\begin{array}{l}
{\pi _0} = \sum\limits_i {\pi_i}p_{i0} = (1 - {p_0})\sum\limits_i {{\pi _i}} = 1 - {p_0}\\\\
{\pi _1} = \sum\limits_i {\pi _i}p_{i1} = {\pi_0}{p_{01}} = (1 - {p_0}){p_0}\\\\
{\pi _2} = \sum\limits_i {\pi _i}p_{i2} = {\pi _1}{p_{12}} = (1 - {p_0}){p_0}^2\\\\
{\pi _3} = \sum\limits_i {\pi _i}p_{i3} = {\pi _2}{p_{23}} = (1 - {p_0}){p_0}^3\\\\
{\pi _4} = \sum\limits_i {\pi _i}p_{i4} = {\pi _3}{p_{34}} = (1 - {p_0}){p_0}^4\\\\
{\pi _5} = \sum\limits_i {\pi _i}p_{i5} = {\pi _4}{p_{45}} + {\pi _5}{p_{55}} = (1 - {p_0}){p_0}^5 + {\pi _5}{p_0}\\\\
\therefore {\pi _5} = \frac{(1 - {p_0}){p_0}^5}{{(1 - {p_0})}} = {p_0}^5
\end{array}
\]

The stationary distribution (or steady state condition) shown in Example
3 represents the asymptotic distribution of the NCD system, or the
distribution in the long run. As an example, assuming that the
probability of a no-claim year is \(p_0 = 0.90,\) the
\textbf{\emph{stationary probabilities}} are:

\[
\begin{array}{l}
{\pi _0} = 1 - {p_0} = 0.1000\\\\
{\pi _1} = (1 - {p_0}){p_0} = 0.0900\\\\
{\pi _2} = (1 - {p_0}){p_0}^2 = 0.0810\\\\
{\pi _3} = (1 - {p_0}){p_0}^3 = 0.0729\\\\
{\pi _4} = (1 - {p_0}){p_0}^4 = 0.0656\\\\
{\pi _5} = {p_0}^5 = 0.5905
\end{array}
\]

In other words, \({\pi_0} = 0.10\) indicates that 10\% of insureds will
eventually belong to class 0 (or zero claim-free years),
\({\pi _1} = 0.09\) indicates that 9\% of insureds will eventually
belong to class 1 (or one claim-free year), and so forth, until
\({\pi _5} = 0.59\), which indicates that 59\% of insureds will
eventually belong to class 5 (or five successive claim-free years).

\subsection{R Program for Stationary
Distribution}\label{r-program-for-stationary-distribution}

We can use the left eigenvector of a transition matrix to calculate the
stationary distribution. The following R program can be used to
calculate the left eigenvector:

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]  0.1  0.9  0.0  0.0  0.0  0.0
## [2,]  0.1  0.0  0.9  0.0  0.0  0.0
## [3,]  0.1  0.0  0.0  0.9  0.0  0.0
## [4,]  0.1  0.0  0.0  0.0  0.9  0.0
## [5,]  0.1  0.0  0.0  0.0  0.0  0.9
## [6,]  0.1  0.0  0.0  0.0  0.0  0.9
\end{verbatim}

\begin{verbatim}
## eigen() decomposition
## $values
## [1]  1.0000000000+0.0000000000i -0.0004880382+0.0003546725i
## [3] -0.0004880382-0.0003546725i  0.0001865015+0.0005736028i
## [5]  0.0001865015-0.0005736028i  0.0006030733+0.0000000000i
## 
## $vectors
##              [,1]                                        [,2]
## [1,] 0.1615936+0i  0.00000000000011546+0.0000000000000838874i
## [2,] 0.1454342+0i -0.00000000006591101-0.0000000002025976972i
## [3,] 0.1308908+0i -0.00000009813867774+0.0000003022936262135i
## [4,] 0.1178017+0i  0.00038354290934278-0.0002787325538732070i
## [5,] 0.1060216+0i -0.70729837066103896+0.0000000000000000000i
## [6,] 0.9541940+0i  0.70691492595616912+0.0002784304627608034i
##                                             [,3]
## [1,]  0.00000000000011546-0.0000000000000838874i
## [2,] -0.00000000006591101+0.0000000002025976972i
## [3,] -0.00000009813867774-0.0000003022936262135i
## [4,]  0.00038354290934278+0.0002787325538732070i
## [5,] -0.70729837066103896+0.0000000000000000000i
## [6,]  0.70691492595616912-0.0002784304627608034i
##                                               [,4]
## [1,] -0.0000000000000440003+0.0000000000001357099i
## [2,]  0.0000000001723292683+0.0000000001248792837i
## [3,]  0.0000002567150759536-0.0000001869214542640i
## [4,] -0.0001468014877601952-0.0004505247500856750i
## [5,] -0.7070333538878578183+0.0004507115465249567i
## [6,]  0.7071798984882565753+0.0000000000000000000i
##                                               [,5]
## [1,] -0.0000000000000440003-0.0000000000001357099i
## [2,]  0.0000000001723292683-0.0000000001248792837i
## [3,]  0.0000002567150759536+0.0000001869214542640i
## [4,] -0.0001468014877601952+0.0004505247500856750i
## [5,] -0.7070333538878578183-0.0004507115465249567i
## [6,]  0.7071798984882565753+0.0000000000000000000i
##                           [,6]
## [1,]  0.0000000000001426323+0i
## [2,]  0.0000000002126778891+0i
## [3,]  0.0000003173909504803+0i
## [4,]  0.0004736602551468477+0i
## [5,]  0.7068696732221422252+0i
## [6,] -0.7073436510810596767+0i
\end{verbatim}

\begin{verbatim}
## [1] 0.1000+0i 0.0900+0i 0.0810+0i 0.0729+0i 0.0656+0i 0.5905+0i
\end{verbatim}

\textbf{\emph{Example 4}}

\emph{Find the stationary distribution for the NCD system in Brazil
assuming that the number of successive years of claims is Poisson
distributed with parameter} \(\lambda = 0.10\).

\textbf{\emph{Solution}}

\emph{Under the Poisson distribution, the probability of} \(k\)
\emph{successive years of claims is}
\(p_k = \frac{e^{ - 0.1}{(0.1)}^k}{{k!}},{\rm{ }}k = 0,1,2,.....\)

\emph{The transition matrix is:}

\[
{\small {\bf{P}} = \left[ {\begin{array}{*{20}{c}}
{1 - {p_0}}&{{p_0}}&0&0&0&0&0\\\\
{1 - {p_0}}&0&{{p_0}}&0&0&0&0\\\\
{1 -\sum\limits_i {{p_i}} }&{{p_1}}&0&{{p_0}}&0&0&0\\\\
{1 -\sum\limits_i {{p_i}} }&{{p_2}}&{{p_1}}&0&{{p_0}}&0&0\\\\
{1 - \sum\limits_i {{p_i}}}&{{p_3}}&{{p_2}}&{{p_1}}&0&{{p_0}}&0\\\\
{1 - \sum\limits_i{{p_i}} }&{{p_4}}&{{p_3}}&{{p_2}}&{{p_1}}&0&{{p_0}}\\\\
{1 - \sum\limits_i {{p_i}}}&{{p_5}}&{{p_4}}&{{p_3}}&{{p_2}}&{{p_1}}&{{p_0}}
\end{array}} \right] = 
\left[ {\begin{array}{*{20}{c}}
{0.0952}&{0.9048}&0&0&0&0&0\\\\
{0.0952}&0&{0.9048}&0&0&0&0\\\\
{0.0047}&{0.0905}&0&{0.9048}&0&0&0\\\\
{0.0002}&{0.0045}&{0.0905}&0&{0.9048}&0&0\\\\
{0.0000}&{0.0002}&{0.0045}&{0.0905}&0&{0.9048}&0\\\\
{0.0000}&{0.0000}&{0.0002}&{0.0045}&{0.0905}&0&{0.9048}\\\\
{0.0000}&{0.0000}&{0.0000}&{0.0002}&{0.0045}&{0.0905}&{0.9048}
\end{array}}\right]}
\]

\emph{Using R program, the stationary probabilities are:}

\[
\left[ {\begin{array}{*{20}{c}}
{{\pi _0}}\\\\
{{\pi_1}}\\\\
{{\pi _2}}\\\\
{{\pi _3}}\\\\
{{\pi _4}}\\\\
{{\pi_5}}\\\\
{{\pi _6}}
\end{array}} \right] = 
\left[ {\begin{array}{*{20}{c}}
{0.0000}\\\\
{0.0000}\\\\
{0.0003}\\\\
{0.0022}\\\\
{0.0145}\\\\
{0.0936}\\\\
{0.8894}
\end{array}} \right]
\]

\emph{The probabilities indicate that 89\% of insureds will eventually
belong to class 6 (six successive claim-free years), 9\% of insureds
will eventually belong to class 5 (five successive claim-free years),
and 1.5\% of insureds will eventually belong to class 4 (four successive
claim-free years). Other classes, after combined, have less than 1\% of
insureds.}

\textbf{\emph{Example 5}}

\emph{Using the results from Example 4, find the final premium under the
steady state condition assuming that the premium prior to implementing
the NCD is} \(m.\)

\textbf{\emph{Solution}}

\emph{Using the stationary probabilities from Example 4, the stationary
final premium is:}

\[
\begin{array}{l} = 
\sum\limits_j \text{(premium)} \times 
\text{(proportion in class } j \text{ in the long run)} \times \text{(1 - NCD in class } j)\\\\ 
= m[{\pi _0}(1) + {\pi _1}(1 - 0.9) + {\pi _2}(1 - 0.15) + \ldots + {\pi _6}(1 - 0.35)]\\\\ 
= m[0 + 0 + (0.0003)(0.85) + \ldots + (0.8894)(0.65)]\\\\
= 0.6565m
\end{array}
\]

\emph{The results indicate that the final premium reduce from m to
0.6565m in the long run (under steady state condition or stationary
condition) if the NCD is considered.}

\subsection{Premium Evolution}\label{premium-evolution}

Sometimes we are interested to observe the evolution of the mean premium
after \(n\) years (or \(n\) steps). Under the NCD system, the n-step
transition probability, \(p_{ij}^{(n)}\), can be used to observe the
evolution of the mean premium. A definition on \emph{n}-step transition
probabaility \(p_{ij}^{(n)}\) is provided here to understand the use of
the probability for observing the evolution of the mean premium.

\textbf{Definition:} Let \(p_{ij} = \Pr ({X_{n + 1}} = j|{X_n} = i)\) be
a \textbf{\emph{one-step transition probability}} from state \(i\) to
state \(j\), and \(p_{ij}^{(n)} = \Pr ({X_n} = j|{X_0} = i)\),
\(n = 1,2,...\), be an n-step transition probability from state \(i\) to
state \(j\). The probability \(p_{ij}^{(n)}\) can be obtained using the
\(n\)th power of transition matrix \({\bf{P}}\), which is
\({\bf{P}^n}.\)

\textbf{\emph{Example 6}}

\emph{Observe the premiums in 20 years under the NCD system in Malaysia,
assuming that the probability of claims is Poisson distributed with
parameter} \(\lambda = 0.10\) \emph{and the premium prior to
implementing the NCD is} \(m = 100\)\emph{.}

\textbf{\emph{Solution}}

\emph{Under the Malaysian NCD, we use the Poisson probability,}
\(p_k = \frac{e^{ - 0.1}{(0.1)}^k}{k!}\)\emph{, only for} \(k = 0,1.\)
\emph{Therefore, the transition matrix in the first year is:}

\[
{\bf{P}^{(1)}} = 
\left[{\begin{array}{*{20}{c}}
{0.0952}&{0.9048}&0&0&0&0\\\\
{0.0952}&0&{0.9048}&0&0&0\\\\
{0.0952}&0&0&{0.9048}&0&0\\\\
{0.0952}&0&0&0&{0.9048}&0\\\\
{0.0952}&0&0&0&0&{0.9048}\\\\
{0.0952}&0&0&0&0&{0.9048}
\end{array}} \right]
\]

\emph{The premium in the first year, after implementing the NCD, is:}

\[
\begin{aligned} 
&= \sum\limits_j \text{(premium)} \times 
\text{(average proportion in class } j) \times \text{(1 - NCD in class }j) \\
&= m\left[ \frac{\sum\limits_i {p_{i0}}}{6}{(1)} +
\frac{\sum\limits_i {p_{i1}}}{6}{(1 - 0.25)} + \ldots +
\frac{\sum\limits_i {p_{i5}}}{6}{(1 - 0.55)} \right]\\
&=
100[0.0952(1) + 0.1508(0.75) + \cdots + 0.3016(0.45)]\\
&= 62.55.
\end{aligned}
\]

\emph{Using similar steps, the premium in the} \(n\)\emph{-th year for}
\(n = 1,2,...,20\) \emph{can be observed. From R program, the premiums
in 20 years are:}

\emph{62.55, 59.87, 58.06, 57.06, 56.58, 56.58, 56.58, 56.58, 56.58,
56.58,}

\emph{56.58, 56.58, 56.58, 56.58, 56.58, 56.58, 56.58, 56.58, 56.58,
56.58.}

\subsection{R Program for Premium
Evolution}\label{r-program-for-premium-evolution}

The following R program can be used to find the premium in the
\emph{n-}th year and the premiums in 20 years under the NCD system in
Malaysia (to find the solution in Example 6).

\begin{verbatim}
##        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]
## [1,] 0.0952 0.9048 0.0000 0.0000 0.0000 0.0000
## [2,] 0.0952 0.0000 0.9048 0.0000 0.0000 0.0000
## [3,] 0.0952 0.0000 0.0000 0.9048 0.0000 0.0000
## [4,] 0.0952 0.0000 0.0000 0.0000 0.9048 0.0000
## [5,] 0.0952 0.0000 0.0000 0.0000 0.0000 0.9048
## [6,] 0.0952 0.0000 0.0000 0.0000 0.0000 0.9048
\end{verbatim}

\begin{verbatim}
##        [,1]       [,2]       [,3]      [,4]      [,5]      [,6]
## [1,] 0.0952 0.08613696 0.07793672 0.7407263 0.0000000 0.0000000
## [2,] 0.0952 0.08613696 0.07793672 0.0000000 0.7407263 0.0000000
## [3,] 0.0952 0.08613696 0.07793672 0.0000000 0.0000000 0.7407263
## [4,] 0.0952 0.08613696 0.07793672 0.0000000 0.0000000 0.7407263
## [5,] 0.0952 0.08613696 0.07793672 0.0000000 0.0000000 0.7407263
## [6,] 0.0952 0.08613696 0.07793672 0.0000000 0.0000000 0.7407263
\end{verbatim}

\begin{verbatim}
## [1] 58.06106
\end{verbatim}

\begin{verbatim}
##  [1] 62.55 59.87 58.06 57.06 56.58 56.58 56.58 56.58 56.58 56.58 56.58
## [12] 56.58 56.58 56.58 56.58 56.58 56.58 56.58 56.58 56.58
\end{verbatim}

\textbf{\emph{Example 7}}

\emph{Observe the premiums in 20 years under the NCD system in Brazil,
assuming that the probability of} \(k\) \emph{successive years of claims
is} \(p_k = \frac{e^{ - 0.1}{(0.1)}^k}{{k!}},{\rm{ }}k = 0,1,2,.....\),
\emph{and} \emph{the premium prior to implementing the NCD is}
\(m = 100\)\emph{.}

\textbf{\emph{Solution}}

\emph{The transition matrix for the NCD system in Brazil is:}

\[
{\bf P} = \left[
{\begin{array}{*{20}{c}}
{0.0952}&{0.9048}&0&0&0&0&0\\\\
{0.0952}&0&{0.9048}&0&0&0&0\\\\
{0.0047}&{0.0905}&0&{0.9048}&0&0&0\\\\
{0.0002}&{0.0045}&{0.0905}&0&{0.9048}&0&0\\\\
{0.0000}&{0.0002}&{0.0045}&{0.0905}&0&{0.9048}&0\\\\
{0.0000}&{0.0000}&{0.0002}&{0.0045}&{0.0905}&0&{0.9048}\\\\
{0.0000}&{0.0000}&{0.0000}&{0.0002}&{0.0045}&{0.0905}&{0.9048}
\end{array}} \right]
\]

\emph{Using R program the premiums in 20 years are:}

\emph{76.69, 73.76, 71.31, 69.38, 67.92, 66.93, 66.40, 66.05, 65.88,
65.78,}

\emph{65.72, 65.69, 65.67, 65.66, 65.66, 65.66, 65.66, 65.65, 65.65,
65.65.}

The results in Examples 6-7 allow us to observe the evolution of premium
for the NCD systems in Malaysia and Brazil assuming that the number of
successive years of claims is Poisson distributed with parameter
\(\lambda = 0.10\), and the premium prior to implementing the NCD is
\(m = 100\). The evolutions of premiums for both countries are provided
in Table 4, and are shown graphically in Figure \ref{fig:PremEvolution}.

\[
\begin{matrix}
\text{Table 4: Evolution of premium (Malaysia and Brazil)}\\
\begin{array}{*{20}c}
\hline
\text{year} & \text{premium} & \text{premium} & \text{year} & \text{premium} & \text{premium} & \\
& \text{Malaysia} & \text{Brazil} & & \text{Malaysia} & \text{Brazil} \\
\hline\\
    0     & 100   & 100   & 11    & 56.58 & 65.72 \\
    1     & 62.55 & 76.69 & 12    & 56.58 & 65.69 \\
    2     & 59.87 & 73.76 & 13    & 56.58 & 65.67 \\
    3     & 58.06 & 71.31 & 14    & 56.58 & 65.66 \\
    4     & 57.06 & 69.38 & 15    & 56.58 & 65.66 \\
    5     & 56.58 & 67.92 & 16    & 56.58 & 65.66 \\
    6     & 56.58 & 66.93 & 17    & 56.58 & 65.66 \\
    7     & 56.58 & 66.40 & 18    & 56.58 & 65.65 \\
    8     & 56.58 & 66.05 & 19    & 56.58 & 65.65 \\
    9     & 56.58 & 65.88 & 20    & 56.58 & 65.65 \\\\
\hline
\end{array}
\end{matrix}
\]

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{Figures/FigureEvolution} 

}

\caption{Evolution of premium (Malaysia and Brazil)}\label{fig:PremEvolution}
\end{figure}

\subsection{Convergence Rate}\label{convergence-rate}

Sometimes we are interested to know the variation between the
probability in the \emph{n}-th year, \(p_{ij}^{(n)}\), and the
stationary probability, \(\pi _j\). The variation between the
probabilities can be measured using:

\[\left| {average(p_{ij}^{(n)}) - {\pi _j}} \right|\].

Therefore, the total variation can be measured by the sum of variation
in all classes:

\[\sum\limits_j {\left| {average(p_{ij}^{(n)}) - {\pi _j}} \right|}\].

The total variation is also called the convergence rate because it
measures the convergence rate after \(n\) years (or \(n\) transitions).

\textbf{\emph{Example 8}}

\emph{Provide the total variations (convergence rate) in 20 years under
the NCD system in Malaysia, assuming that the probability of claims is
Poisson distributed with parameter} \(\lambda = 0.10\)\emph{.}

\textbf{\emph{Solution}}

\emph{Using R program, the stationary probabilities are:}

\[
\left[ {\begin{array}{*{20}{c}}
{{\pi _0}}\\\\{{\pi_1}}\\\\{{\pi _2}}\\\\
{{\pi _3}}\\\\{{\pi _4}}\\\\{{\pi_5}}
\end{array}} \right]
= \left[ {\begin{array}{*{20}{c}}
{0.0952}\\\\{0.0861}\\\\{0.0779}\\\\
{0.0705}\\\\{0.0638}\\\\{0.6064}
\end{array}} \right]
\]

\emph{The transition matrix in the first year is:}

\[
{\bf{P}}^{(1)} = \left[
{\begin{array}{*{20}{c}}
{0.0952}&{0.9048}&0&0&0&0\\\\
{0.0952}&0&{0.9048}&0&0&0\\\\
{0.0952}&0&0&{0.9048}&0&0\\\\
{0.0952}&0&0&0&{0.9048}&0\\\\
{0.0952}&0&0&0&0&{0.9048}\\\\
{0.0952}&0&0&0&0&{0.9048}
\end{array}} \right]
\]

\emph{The variation can be computed as:}

\[
\begin{array}{l}
\left| {\sum\limits_i {\frac{p_{i0}^{}}{6}} - {\pi _0}} \right| = 0\\\\
\left|{\sum\limits_i {\frac{p_{i1}^{}}{6}} - {\pi _1}} \right| =
0.0647\\\\
\vdots \\\\
\left| {\sum\limits_i
\frac{p_{i5}}{6} - {\pi _5}} \right| =
.3048
\end{array}
\]

\emph{Therefore, the total variation in the first year is}
\(\sum\limits_j {\left| {\sum\limits_i {\frac{p_{ij}}{6} - {\pi _j}} } \right|} = 0.6096\)\emph{.}

\emph{Using R program, the total variations (or convergence rate) in 20
years are:}

\emph{0.6096, 0.3941, 0.2252, 0.0958, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0.}

\subsection{R Program for Convergence
Rate}\label{r-program-for-convergence-rate}

The following R program can be used to calculate the total variation in
the \(n\)th year, and the total variations (convergence rates) in 20
years under the NCD system in Malaysia (the solution in Example 8).

\begin{verbatim}
##        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]
## [1,] 0.0952 0.9048 0.0000 0.0000 0.0000 0.0000
## [2,] 0.0952 0.0000 0.9048 0.0000 0.0000 0.0000
## [3,] 0.0952 0.0000 0.0000 0.9048 0.0000 0.0000
## [4,] 0.0952 0.0000 0.0000 0.0000 0.9048 0.0000
## [5,] 0.0952 0.0000 0.0000 0.0000 0.0000 0.9048
## [6,] 0.0952 0.0000 0.0000 0.0000 0.0000 0.9048
\end{verbatim}

\begin{verbatim}
## [1] 0.09520000+0i 0.08613696+0i 0.07793672+0i 0.07051715+0i 0.06380391+0i
## [6] 0.60640526+0i
\end{verbatim}

\begin{verbatim}
## [1] 0.6096105
\end{verbatim}

\begin{verbatim}
##  [1] 0.6096 0.3941 0.2252 0.0958 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
## [11] 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
\end{verbatim}

\textbf{\emph{Example 9}}

\emph{Provide the total variations (or convergence rate) in 20 years
under the NCD system in Brazil, assuming that the number of successive
years of claims is distributed as Poisson with parameter}
\(\lambda = 0.10\)

\textbf{\emph{Solution}}

\emph{Using R program, the total variations (or convergence rates) in 20
years for the NCD system in Brazil are:}

\emph{1.2617, 1.0536, 0.8465, 0.6412, 0.4362, 0.2316, 0.1531, 0.0747,
0.0480, 0.0232,}

\emph{0.0145, 0.0071, 0.0043, 0.0021, 0.0013, 0.0006, 0.0004, 0.0002,
0.0001, 0.0001.}

Examples 8-9 provide the degree of convergence for two different BMS
(two different countries). The Malaysian BMS reaches full stationary
only after four years, while the BMS in Brazil takes a longer period. As
mentioned in Lemaire (1998), a more sophisticated BMS would converge
more slowly, and is considered as a drawback as it takes a longer period
to stabilize. The main objective of a BMS is to separate the good
drivers from the bad drivers, and thus, it is desirable to have a
classification process that can be finalized (or stabilized) as soon as
possible.

\section{BMS and Premium Rating}\label{S:PremRtg}

\subsection{Premium Rating}\label{premium-rating}

Premium rating is the process of establishing premium rates in an
insurance system, or other risk transfer mechanisms. The process
involves a number of considerations such as statistical methods,
marketing goals, competition and legal restrictions. Premium rating
should fulfill four basic objectives generally agreed among the
actuaries; producing `fair' premium rates whereby high risks insureds
pay higher premium and vice versa, providing sufficient funds for paying
incurred losses and expenses, providing adequate margin for adverse
deviation, and producing reasonable returns to insurers.

Before we discuss the application of BMS in premium rating, a brief
discussion of the Poisson and negative binomial regression models are
provided here. The Poisson regression has been widely used for fitting
count (or frequency) data in insurance area. The negative binomial
regression model can be used for fitting count (or frequency) data with
overdispersion, which is a situation when the variance is larger than
the mean. However, students are encouraged to refer to textbooks on GLMs
or regression models available in the literature for a more
comprehensive knowledge on the related subjects.

\subsection{Frequency Model -- Poisson and Negative Binomial
Regressions}\label{frequency-model-poisson-and-negative-binomial-regressions}

Let \({({Y_1},{Y_2},...,{Y_n})^T}\) be the vector of count random
variables and \(n\) be the sample size. The probability mass function
(p.m.f.) for Poisson regression model is,

\[\Pr ({Y_i} = {y_i})
\begin{array}{*{20}{c}}
= \frac{{{\exp(- {\mu _i})}{\mu _i}^{y_i}}}{y_i{!}}&{y_i = 0,1,...}
\end{array}
\]

with mean and variance \(E({Y_i}) = Var({Y_i}) = {\mu _i}\). To take
into account non-negative values, the mean, or the fitted value, is
assumed to follow a log link,
\(E({Y_i}) = {\mu _i} = \exp ({\bf x}_{\bf i}{\bf '\beta })\), where
\({\bf{x}}_{\bf i}\) denotes the vector of explanatory variables and
\({\bf \beta}\) is the vector of regression parameters. The maximum
likelihood estimates can be obtained by maximizing the log likelihood.

The p.m.f. for negative binomial regression model is,

\[
\Pr ({Y_i} = {y_i}) = \frac{{\Gamma ({y_i} +
v)}}{y_i{!}\Gamma (v)}
\left( \frac{v}{v + {\mu _i}}
\right)^v
\left( \frac{\mu _i}{v + {\mu _i}}
\right)^{y_i},{y_i} = 0,1,2,...,
\]

where the mean is \(E({Y_i}) = {\mu _i}\), the variance is
\(Var({Y_i}) = {\mu _i}(1 + {v^{ - 1}}{\mu _i}) = {\mu _i}(1 + a{\mu _i})\),
and \({v^{ - 1}} = a\) denotes the dispersion parameter. The NB
regression model reduce to the Poisson regression model in the limit as
\(a \to 0\), and display overdispersion when \(a > 0\). The mean can
also be assumed to follow the log link,
\(E({Y_i}) = {\mu _i} = \exp ({{\bf x}_{\bf i}{\bf{'\beta }}})\), and
the maximum likelihood estimates can be obtained by maximizing the log
likelihood.

\subsection{Premium Rating with Bonus-Malus
Data}\label{premium-rating-with-bonus-malus-data}

Under this section, we will consider the statistical approach of
obtaining premium rates using count data (frequency data) that provides
bonus-malus information on the risk profiles of the insureds.

\textbf{Example 10}

Consider the Canadian private automobile liability insurance data from
Bailey and Simon (1960). The data provides information on the number of
claims incurred and exposures (expressed in terms of number of earned
car years). The data is classified into two rating factors; merit rating
and class rating. Altogether, there are twenty \((4 \times 5)\)
cross-classified rating classes of claim frequencies (20 count data).
Table 5 provides the description of the two rating factors. It can be
seen that merit rating provides information on the number of
accident-free years, and thus, the information can be used as
bonus-malus information. Using the Canadian automobile insurance data:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\item
  Fit the count data using Poisson regression model.
\item
  Calculate the premium assuming that the claim severity is 1000 for all
  rating classes.
\item
  Build a simple premium table, and suggest the percentage of discounts
  (or loadings) for the premiums if the BMS is to be considered.
\end{enumerate}

\[
\begin{matrix}
\text{Table 5: Rating factors (Canadian data)}\\
\begin{array}{*{20}c}
\hline
\text{Merit} & \text{Class}\\
\hline\\
\text{A = licensed, accident-free ≥ 3 years } & \text{1 = pleasure, no male operator < 25 } \\
\text{X = licensed, accident-free 2 years } & \text{2 = pleasure, non-principal male operator < 25 } \\
\text{Y = licensed, accident-free 1 year } & \text{3 = business use } \\
\text{B = others} & \text{4 = unmarried owner / principal operator < 25} \\
& \text{5 = married owner / principal operator < 25 } \\\\
\hline
\end{array}
\end{matrix}
\]

\textbf{Solution}

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\item
  Table 6 provides the parameter estimates (and standard errors) for the
  Poisson regression model which is fitted to the count data.
\item
  The premiums are calculated by multiplying the fitted frequencies with
  the claim severities. The fitted frequency is obtained by dividing the
  fitted count with the exposure. The fitted count,
  \(E({Y_i}) = {\mu _i} = {\hat y_i}\), is the fitted value obtained
  from the Poisson regression model. The premiums for all rating
  classes, are shown in Table 7.
\item
  Let the premium classified under merit B be the base premium. The
  premium relativities and the percentage of discounts for each rating
  class can be calculated, and the values are shown in Table 8. A simple
  premium table, which provides the percentage of discounts based on the
  number of accident-free years, can be constructed and is shown in
  Table 9. The discounts can be suggested for implementing the BMS.
\end{enumerate}

\[
\begin{matrix}
\text{Table 6: Poisson regression model (Canadian data)}\\
\begin{array}{*{20}c}
\hline
\text{Regression parameter} & \text{est.} & \text{std. error} & p\text{ -value} \\
\hline\\
\text{Intercept} & {-2.53} & {0.00} & {0.00} \\
\text{Class 2} & {0.30} & {0.01} & {0.00} \\
\text{Class 3} & {0.47} & {0.01} & {0.00} \\
\text{Class 4} & {0.53} & {0.01} & {0.00} \\
\text{Class 5} & {0.22} & {0.01} & {0.00} \\
\text{Merit X} & {0.27} & {0.01} & {0.00} \\
\text{Merit Y} & {0.36} & {0.01} & {0.00} \\
\text{Merit B} & {0.49} & {0.00} & {0.00} \\
          &       &       &  \\
\text{Log }L & {-394.96} &       &  \\\\
\hline
\end{array}
\end{matrix}
\]

\[
\begin{matrix}
\text{Table 7: Fitted count, fitted frequency and premium (Canadian data)}\\
\begin{array}{*{20}c}
\hline
\text{Class} & \text{Merit} & & & \text{Fitted frequency}  & \text{Premium}  \\
\text{rating} & \text{rating} & \text{Fitted count} & \text{Exposure} & \text{(count/exposure)} & \text{(frequency x 1000)}\\
\hline\\
    1     & \text{A} & 219950 & 2757520 & 0.08  & 80 \\
    1     & \text{X} & 13688 & 130706 & 0.1   & 105 \\
    1     & \text{Y} & 18608 & 163544 & 0.11  & 114 \\
    1     & \text{B} & 35773 & 273944 & 0.13  & 131 \\
    2     & \text{A} & 14052 & 130535 & 0.11  & 108 \\
    2     & \text{X} & 1022  & 7233  & 0.14  & 141 \\
    2     & \text{Y} & 1494  & 9726  & 0.15  & 154 \\
    2     & \text{B} & 3790  & 21504 & 0.18  & 176 \\
    3     & \text{A} & 31547 & 247424 & 0.13  & 128 \\
    3     & \text{X} & 2656  & 15868 & 0.17  & 167 \\
    3     & \text{Y} & 3705  & 20369 & 0.18  & 182 \\
    3     & \text{B} & 7862  & 37666 & 0.21  & 209 \\
    4     & \text{A} & 21170 & 156871 & 0.13  & 135 \\
    4     & \text{X} & 3137  & 17707 & 0.18  & 177 \\
    4     & \text{Y} & 4060  & 21089 & 0.19  & 193 \\
    4     & \text{B} & 12534 & 56730 & 0.22  & 221 \\
    5     & \text{A} & 6346  & 64130 & 0.1   & 99 \\
    5     & \text{X} & 525   & 4039  & 0.13  & 130 \\
    5     & \text{Y} & 687   & 4869  & 0.14  & 141 \\
    5     & \text{B} & 1393  & 8601  & 0.16  & 162 \\\\
\hline
\end{array}
\end{matrix}
\]

\[
\begin{matrix}
\text{Table 8: Premium relativities (Canadian data)}\\
\begin{array}{*{20}c}
\hline
\text{Class} & \text{Merit} & \text{Premium} & \text{Premium relativity} & \text{Discount} \\
\hline\\
    1     & \text{A} & 80    & 0.61  & 39 \\
          & \text{X} & 105   & 0.80 & 20 \\
          & \text{Y} & 114   & 0.87  & 13 \\
          & \text{B} & 131   & 1.00   & 0 \\
    2     & \text{A} & 108   & 0.61  & 39 \\
          & \text{X} & 141   & 0.80  & 20 \\
          & \text{Y} & 154   & 0.88  & 13 \\
          & \text{B} & 176   & 1.00     & 0 \\
    3     & \text{A} & 128   & 0.61  & 39 \\
          & \text{X} & 167   & 0.80  & 20 \\
          & \text{Y} & 182   & 0.87  & 13 \\
          & \text{B} & 209   & 1.00     & 0 \\
    4     & \text{A} & 135   & 0.61  & 39 \\
          & \text{X} & 177   & 0.80   & 20 \\
          & \text{Y} & 193   & 0.87  & 13 \\
          & \text{B} & 221   & 1.00     & 0 \\
    5     & \text{A} & 99    & 0.61  & 39 \\
          & \text{X} & 130   & 0.80   & 20 \\
          & \text{Y} & 141   & 0.87  & 13 \\
          & \text{B} & 162   & 1.00  & 0 \\\\
\hline
\end{array}
\end{matrix}\]

\[
\begin{matrix}
\text{Table 9: Premium rates and BMS (Canadian data)}\\
\begin{array}{*{20}c}
\hline
\text{Class} & \text{Premium} & \text{Years of} & \text{Discount for} \\
& & \text{accident-free} & \text{bonus-malus }\\
\hline\\
    1     & 131   & {3+} & 39 \\
          &       & 2     & 20 \\
          &       & 1     & 13 \\
          &       & 0     & 0 \\
    2     & 176   & {3+} & 39 \\
          &       & 2     & 20 \\
          &       & 1     & 13 \\
          &       & 0     & 0 \\
    3     & 209   & {3+} & 39 \\
          &       & 2     & 20 \\
          &       & 1     & 13 \\
          &       & 0     & 0 \\
    4     & 221   & {3+} & 39 \\
          &       & 2     & 20 \\
          &       & 1     & 13 \\
          &       & 0     & 0 \\
    5     & 162   & {3+} & 39 \\
          &       & 2     & 20 \\
          &       & 1     & 13 \\
          &       & 0     & 0 \\\\
\hline
\end{array}
\end{matrix}\]

\subsection{Premium Rating without Bonus-Malus
Data}\label{premium-rating-without-bonus-malus-data}

Under this section, we will consider the statistical approach of
obtaining premium rates using count data that does not provide
bonus-malus information on the risk profiles of the insureds.

\textbf{Example 11}

The dataset for private car Own Damage (OD) claim count in Malaysia
(Zamani and Ismail 2012, Ismail and Zamani 2013) is considered here. The
data is based on 1.01 million private car policies for a three-year
period of 2001-2003. The exposures are expressed in car-year units.
Table 10 shows the rating factors for the claim count data. Altogether,
there are \(5 \times 5 \times 5 \times 5 = 625\) cross-classified rating
classes of claim frequencies (625 count data). However, the number of
rating classes (sample size) reduces to 547 after excluding count data
with zero exposure. It can be seen from the description in Table 10 that
the data does not provide information on the number of claim-free years
(bonus-malus information). Using the Malaysian private car insurance
data:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\item
  Fit the count data using Poisson and negative binomial regression
  models. Compare both models and suggest a preferred model for the
  data.
\item
  Calculate the premium assuming that the claim severity is MYR 1000
  (Ringgit Malaysia) for all rating classes.
\item
  Build a premium table, assuming that the NCD is to be implemented.
\end{enumerate}

\[
\begin{matrix}
\text{Table 10: Rating factors (Malaysian data)}\\
\begin{array}{*{20}c}
\hline
\text{Rating factors} & \text{Description} \\
\hline\\
    \text{Vehicle age} & \text{0-1 year } \\
          & \text{2-3 years} \\
          & \text{4-5 years} \\
          & \text{6-7 years} \\
          & \text{8+ years} \\
    \text{Vehicle c.c.} & \text{0-1000} \\
          & \text{1001-1300} \\
          & \text{1301-1500} \\
          & \text{1501-1800} \\
          & \text{1801+} \\
    \text{Vehicle make} & \text{Local type 1} \\
          & \text{Local type 2} \\
          & \text{Foreign type 1} \\
          & \text{Foreign type 2} \\
          & \text{Foreign type 3} \\
    \text{Location} & \text{North} \\
          & \text{East} \\
          & \text{Central} \\
          & \text{South} \\
          & \text{East Malaysia} \\\\
\hline
\end{array}
\end{matrix}
\]

\textbf{Solution}

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\item
  Table 11 shows the parameter estimates (and standard errors) for the
  fitted Poisson and negative binomial regression models. The results
  indicate that the regression parameters of all models have similar
  estimates. The likelihood ratio for testing overdispersion between the
  Poisson and the NB models is 2844.14, indicating that the data is
  overdispersed and the NB is a better model. The AIC and BIC also
  indicate that the NB is a better model. Table 12 shows the parameter
  estimates (and standard errors) for the fitted negative binomial
  regression model with significant covariates (covariates with
  \(p\)-values larger than 0.10 are excluded). The NB model in Table 12
  is suggested as the preferred model for the data.
\item
  The premiums are calculated by multiplying the fitted frequencies with
  the claim severities. The premiums for the first 25 rating classes
  (vehicle age: 0-1 year and vehicle make: local-type 1) are shown in
  Table 13.
\item
  The Malaysian NCD system have six discount classes (0\%, 25\%, 30\%,
  38.3\%, 45\% and 55\%) for 0, 1, 2, 3, 4 and 5+ claim-free years. The
  final premium rates should be `inflated' accordingly to take into
  account the NCD (discount entitlement) of each insured. We can use the
  stationary distribution to approximate the proportion of insureds in
  each NCD class (in the long run). Assume that the probability of a
  no-claim year for all NCD classes is 0.90. The stationary
  probabilities are (from Example 3):
\end{enumerate}

\[
{\pi_0} = 0.1000,\rm{ }{\pi _1} = 0.0900,\rm{ }{\pi
_2} = 0.0810,\rm{ }{\pi _3} = 0.0729,\rm{ }{\pi _4} =
0.0656,\rm{ }{\pi _5} = 0.5905
\]

which indicates that 10\% of insureds eventually belong to class 0 (zero
claim-free years), 9\% of insureds eventually belong to class 1 (one
claim-free year), and so forth, until 59\% of insureds eventually belong
to class 5 (or five successive claim-free years). After implementing the
NCD, the premium (in the long run) will be reduced (on average) by the
following factor:

\[
\begin{array}{l} 
= \sum\limits_j \text{(proportion of insureds in class }j)\text{(1 - NCD
in class }j) \\\\ 
= (0.1000)(1) + (0.0900)(1 - 0.25) + ... + (0.5905)(1 - 0.55)\\\\ 
= {0}{.570962}
\end{array}
\]

Therefore, the final premiums (after adjusting for the NCD) should be
inflated by \(1 \div 0.57 = 1.75\). A premium table can be constructed
based on the factor (1.75). The final premiums for the first 5 rating
classes are shown in Table 14, and can be suggested for implementing the
NCD.

\[
\begin{matrix}
\text{Table 11: Poisson and negative binomial regression models (Malaysian data)}\\
\begin{array}{*{20}c}
\hline
\text{Parameters} & \text{Poisson} & \text{Poisson} & \text{NB} & \text{NB} \\
& \text{Est.} & {p}\text{ -value} & \text{Est.} & {p}\text{ -value} \\
\hline\\
    \text{Intercept} & {-3.04} & {0.00} & -3.17 & 0.00 \\
    \text{2-3 year} & {0.51} & {0.00} & 0.57  & 0.00 \\
    \text{4-5 year} & {0.52} & {0.00} & 0.52  & 0.00 \\
    \text{6-7 year} & {0.43} & {0.00} & 0.40   & 0.00 \\
    \text{8+ year} & {0.24} & {0.00} & 0.28  & 0.00 \\
    \text{1001-1300 cc} & {-0.31} & {0.00} & -0.14 & 0.00 \\
    \text{1301-1500 cc} & {-0.16} & {0.00} & 0.08  & 0.16 \\
    \text{1501-1800 cc} & {0.14} & {0.00} & 0.25  & 0.00 \\
    \text{1801+ cc} & {0.12} & {0.00} & 0.34  & 0.00 \\
    \text{Local type 2} & {-0.46} & {0.00} & -0.30  & 0.00 \\
    \text{Foreign type 1} & {-0.21} & {0.00} & -0.31 & 0.00 \\
    \text{Foreign type 2} & {0.18} & {0.00} & 0.31  & 0.00 \\
    \text{Foreign type 3} & {-0.02} & {0.43} & -0.16 & 0.04 \\
    \text{East} & {0.35} & {0.00} & 0.30  & 0.00 \\
    \text{Central} & {0.32} & {0.00} & 0.31  & 0.00 \\
    \text{South} & {0.26} & {0.00} & 0.36  & 0.00 \\
    \text{East Malaysia} & {0.13} & {0.00} & 0.11  & 0.08 \\
    {a} & {-} & {-} & 0.13  & 0.00 \\
          &       &       &       &  \\
    \text{Log likelihood} & & {-3,613.39} &       & -2191.32 \\
    \text{AIC} & & {7,260.79} &       & 4418.65 \\
    \text{BIC} & & {7,333.96} &       & 4496.13 \\\\
\hline
\end{array}
\end{matrix}
\]

\[
\begin{matrix}
\text{Table 12: Negative binomial regression model with significant covariates (Malaysian data)}\\
\begin{array}{*{20}c}
\hline
\text{Parameters} & \text{Est.} & p\text{ -value}\\
\hline \\
\text{Intercept} & {-3.14} & {0.00}\\
\text{2-3 year} & {0.58} & {0.00}\\
\text{4-5 year} & {0.53} & {0.00}\\
\text{6-7 year} & {0.41} & {0.00}\\
\text{8+ year} & {0.29} & {0.00}\\
\text{1001-1300 cc} & {-0.17} & {0.00}\\
\text{1501-1800 cc} & {0.23} & {0.00}\\
\text{1801+ cc} & {0.32} & {0.00}\\
\text{Local type 2} & {-0.32} & {0.00}\\
\text{Foreign type 1} & {-0.31} & {0.00}\\
\text{Foreign type 2} & {0.31} & {0.00}\\
\text{Foreign type 3} & {-0.18} & {0.02}\\
\text{East} & {0.30} & {0.00}\\
\text{Central} & {0.31} & {0.00}\\
\text{South} & {0.36} & {0.00}\\
\text{East Malaysia} & {0.11} & {0.08}\\
\alpha\,\text{(dispersion)} & {0.13} & {0.00}\\\\
\text{Log likelihood} & & {-2192.34}\\
\text{AIC} & & {4418.68}\\
\text{BIC} & & {4491.85}\\\\
\hline
\end{array}
\end{matrix}
\]

\[
\begin{matrix}
\text{Table 13: Premium rates (Malaysian data)}\\
\begin{array}{*{20}c}
\hline
\text{Vehicle} & \text{Vehicle} & \text{Vehicle} & \text{location} & \text{exposure} & \text{Fitted}
& \text{Premium}\\
\text{year} & \text{cc} & \text{make} & & & \text{count}
& \left(\frac{fitted\,count}{exposure} \times 1000 \right)\\
\hline\\
0-1\,\text{year} & {0-1000} & \text{local-1} & \text{north} & {34} & {1} & {29}\\
& & & \text{east} & {11} & {1} & {91}\\
& & & \text{central} & {184} & {11} & {60}\\
& & & \text{south} & {14} & {1} & {71}\\
& & & \text{east Msia} & {22} & {1} & {45}\\
& & \text{local-2} & \text{north} & {17881} & {562} & {31}\\
& & & \text{east} & {7581} & {322} & {42}\\
& & & \text{central} & {19699} & {844} & {43}\\
& & & \text{south} & {8268} & {371} & {45}\\
& & & \text{east Msia} & {12229} & {429} & {35}\\
& & \text{foreign-1} & \text{north} & {241} & {8} & {33}\\
& & & \text{east} & {44} & {2} & {45}\\
& & & \text{central} & {254} & {11} & {43}\\
& & & \text{south} & {35} & {2} & {57}\\
& & & \text{east Msia} & {113} & {4} & {35}\\
& & \text{foreign-2} & \text{north} & {64} & {4} & {63}\\
& & & \text{east} & {7} & {1} & {143}\\
& & & \text{central} & {164} & {13} & {79}\\
& & & \text{south} & {23} & {2} & {87}\\
& & & \text{east Msia} & {45} & {1} & {67}\\
& & \text{foreign-3} & \text{north} & {7512} & {274} & {36}\\
& & & \text{east} & {2747} & {135} & {49}\\
& & & \text{central} & {15551} & {774} & {50}\\
& & & \text{south} & {7408} & {387} & {52}\\
& & & \text{east Msia} & {6740} & {274} & {41}\\\\
\hline
\end{array}
\end{matrix}
\]

\[
\begin{matrix}
\text{Table 14: Premium, final premium and NCD (Malaysian data)}\\
\begin{array}{*{20}c}
\hline
\text{Vehicle} & \text{Vehicle} & \text{Vehicle} & \text{location} & \text{Premium} & \text{Final} & \text{Years of} & \text{NCD}\\
\text{year} & \text{cc} & \text{make} & & & \text{Premium}
& \text{claim-free} & \left( \% \right)\\
& & & & & \left( \text{premium} \times \text{1.75}\right) & & \\
\hline\\
0-1\,\text{year} & {0-1000} & \text{local-1} & \text{north} & {29} & {51} & {5+} & {55}\\
& & & & & & {4} & {45}\\
& & & & & & {3} & {38.33}\\
& & & & & & {2} & {30}\\
& & & & & & {1} & {25}\\
& & & & & & {0} & {0}\\
& & & \text{east} & {91} & {159} & {0} & {55}\\
& & & & & & {1} & {45}\\
& & & & & & {2} & {38.33}\\
& & & & & & {3} & {30}\\
& & & & & & {4} & {25}\\
& & & & & & {5+} & {0}\\
& & & \text{central} & {60} & {105} & {0} & {55}\\
& & & & & & {1} & {45}\\
& & & & & & {2} & {38.33}\\
& & & & & & {3} & {30}\\
& & & & & & {4} & {25}\\
& & & & & & {5+} & {0}\\
& & & \text{south} & {71} & {125} & {0} & {55}\\
& & & & & & {1} & {45}\\
& & & & & & {2} & {38.33}\\
& & & & & & {3} & {30}\\
& & & & & & {4} & {25}\\
& & & & & & {5+} & {0}\\
& & & \text{east Msia} & {45} & {80} & {0} & {55}\\
& & & & & & {1} & {45}\\
& & & & & & {2} & {38.33}\\
& & & & & & {3} & {30}\\
& & & & & & {4} & {25}\\
& & & & & & {5+} & {0}\\
\vdots & \vdots & \vdots & \vdots & & \vdots & \vdots & \vdots\\\\
\hline
\end{array}
\end{matrix}
\]

\textbf{FURTHER READING AND REFERENCES:}

Aitkin, M., Anderson, D., Francis, B., Hinde, J. 1990. \emph{Statistical
Modelling in GLIM}. New York: Oxford University Press.

Ajne, B. 1975. A note on the multiplicative ratemaking model.
\emph{ASTIN Bulletin} 8(2): 144-153.

Anderson, D., Feldblum, S., Modlin, C., Schirmacher, D., Schirmacher,
E., Thandi, N. 2004. A practitioner's guide to Generalized Linear
Models. \emph{Casualty Actuarial Society Discussion Paper Program}
1-115.

Bailey, R.A. 1963. Insurance rates with minimum bias. \emph{Proceedings
of the Casualty Actuarial Society} 50(93): 4-14.

Bailey, R.A., Simon, L.J. 1960. Two studies in automobile insurance
ratemaking. \emph{ASTIN Bulletin} 49(1): 192-217.

Bichsel, F. 1964. Erfahrungs-Tarifierung in der
Motorfahrzeug-halfplichtversicherung. \emph{Milleilungen der Vereinigung
Schweizerischer Versicherungsmathematiker} 119-129.

Boucher, J.P., Denuit, M., Guillen, M. 2007. Risk classification for
claim count: a comparative analysis of various zero-inflated mixed
Poisson and hurdle models. \emph{North American Actuarial Journal}
11(4): 110-131.

Brockmann, M.J., Wright, T.S. 1992. Statistical motor rating: making
effective use of your data\emph{. Journal of the Institute of Actuaries}
119(3): 457-543.

Brown, R.L. 1988. Minimum bias with generalized linear models.
\emph{Proceedings of the Casualty Actuarial Society} 75(143): 187-217.

Bühlmann, H. 1964. Optimale Prämienstufen systeme. \emph{Milleilungen
der Vereinigung Schweizerischer Versicherungsmathematiker} 193-213.

Cameron, A.C., Trivedi, P.K. 1986. Econometric models based on count
data: comparisons and applications of some estimators and tests.
\emph{Journal of Applied Econometrics} 1: 29-53.

Cameron, A.C., Trivedi, P.K. 1998. \emph{Regression Analysis of Count
Data}. New York: Cambridge University Press.

Chamberlain, C. 1980. Relativity pricing through analysis of variance.
\emph{Casualty Actuarial Society Discussion Paper Program} 4-24.

Corlier, F., Lemaire, J., \& Muhokolo, D. (1979). Simulation of an
automobile portfolio. \emph{The Geneva Papers on Risk and Insurance
Theory 4}:40-46.

Coutts, S.M. 1984. Motor insurance rating, an actuarial approach.
\emph{Journal of the Institute of Actuaries} 111: 87-148.

Delaporte, P. 1965. Tarification du risque individuel d'accidents par la
prime modelil ée sur le risqué. \emph{ASTIN Bulletin} 3: 251-271.

Denuit, M., Marechal, X., Pitrebois, S., Walhin, J.F. 2007.
\emph{Actuarial Modeling of Claim Counts: Risk Classification,
Credibility and Bonus-Malus Systems}. John Wiley and Sons: England.

Dionne, G., \& Vanasse, C. (1989). A generalization of automobile
insurance rating models: the negative binomial distribution with a
regression component. \emph{ASTIN Bullettin 19}(2): 199-212.

Frangos, N. E., Vrontos, S. D. 2001. Design of optimal bonus-malus
systems with a frequency and severity component on an individual basis
in automobile insurance. \emph{ASTIN Bulletin 31}(1): 1-22.

Harrington, S.E. 1986. Estimation and testing for functional form in
pure premium regression models. \emph{ASTIN Bulletin} 16: 31-43.

Hastings, N.A.J. 1976. Optimal claiming on vehicle insurance.
\emph{Operational Research Quarterly} 27: 805-813.

Hilbe, J. 2007. \emph{Negative Binomial Regression}. Cambridge, UK:
Cambridge University Press.

Ismail, N., Jemain, A.A. 2005. Bridging minimum bias and maximum
likelihood methods through weighted equation. \emph{Casualty Actuarial
Society Forum} Spring: 367-394.

Ismail, N., Jemain, A.A. 2007. Handling overdispersion with negative
binomial and generalized Poisson regression models. \emph{Casualty
Actuarial Society Forum} Winter: 103-158.

Ismail, N., Zamani, H. 2013. \textbf{Estimation of claim count data
using negative binomial, generalized Poisson, zero-inflated negative
binomial and zero-inflated generalized Poisson regression models.}
\emph{Casualty Actuarial Society E-Forum} Spring: 1-29.

Jung, J. 1968. On automobile insurance ratemaking. \emph{ASTIN Bulletin}
5(1): 41-48.

Kolderman, J., Volgenant, A. 1985. Optimal claiming in an automobile
insurance system with bonus-malus structure. \emph{Journal of the
Operational Research Society} 36: 239-247.

Lawless, J.F. 1987. Negative binomial and mixed Poisson regression.
\emph{Canadian Journal of Statistics} 15(3): 209-225.

Lemaire, J. 1976. Driver versus company, optimal behaviour of the policy
holder. \emph{Scandinavian Actuarial Journal} 59: 209-219\emph{.}

Lemaire, J. 1977. La soif du bonus. \emph{ASTIN Bulletin} 9: 181-190.

Lemaire, J. 1979. How to define a Bonus-Malus system with an exponential
utility function. \emph{ASTIN Bulletin} 10: 274-282.

Lemaire, J., Zi, H.M. 1994. A comparative analysis of 30 Bonus-Malus
systems. \emph{ASTIN Bulletin} 24: 287-309.

Lemaire, J. 1998. Bonus-Malus systems: The European and Asian approach
to merit-rating. \emph{North American Actuarial Journal 2}(1): 26-38.

Loimaranta, K. 1972. Some asymptotic properties of bonus systems.
\emph{ASTIN Bulletin} 6: 233-245.

McCullagh, P., Nelder, J.A. 1989. \emph{Generalized Linear Models (2nd
Edition).} Chapman and Hall: London.

Morillo, I., Bermúdez, L. 2003. Bonus--malus system using an exponential
loss function with an inverse Gaussian distribution. \emph{Insurance:
Mathematics and Economics 33}: 49-57.

Pitrebois, S., Denuit, M., Walhin, J.F. 2003. Fitting the Belgian
Bonus-Malus system. \emph{Belgian Actuarial Bulletin} 3(1): 58-62.

Renshaw, A.E., 1994. Modelling the claims process in the presence of
covariates. \emph{ASTIN Bulletin} 24(2): 265-285.

Tremblay, L. 1992. Using the Poisson inverse Gaussian in Bonus-Malus
systems. \emph{ASTIN Bulletin 22}(1): 97-106.

Vepsäläinen, S. 1972. Applications to a theory of bonus systems.
\emph{ASTIN Bulletin} 6: 212-221.

Walhin, J. F., Paris, J. 1999. Using mixed Poisson processes in
connection with Bonus-Malus systems. \emph{ASTIN Bulletin 29}(1): 81-99.

Winkelmann, R. 2008. \emph{Econometric Analysis of Count Data}.
Heidelberg: Springer Verlag.

Zamani, H., Ismail, N. 2012. Functional form for the generalized Poisson
regression model. \emph{Communications in Statistics (Theory and
Methods)} 41(20): 3666--3675.

\chapter{Data and Systems}\label{C:DataSystems}

\emph{Chapter Preview}. This chapter covers the learning areas on data
and systems outlined in the IAA (International Actuarial Association)
Education Syllabus published in September 2015. This chapter is
organized into three major parts: data, data analysis, and data analysis
techniques. The first part introduces data basics such as data types,
data structures, data storages, and data sources. The second part
discusses the process and various aspects of data analysis. The third
part presents some commonly used techniques for data analysis.

\section{Data}\label{data}

\subsection{Data Types and Sources}\label{data-types-and-sources}

In terms of how data are collected, data can be divided into two types
\citep{hox2005data}: primary data and secondary data. Primary data are
original data that are collected for a specific research problem.
Secondary data are data originally collected for a different purpose and
reused for another research problem. A major advantage of using primary
data is that the theoretical constructs, the research design, and the
data collection strategy can be tailored to the underlying research
question to ensure that the data collected indeed help to solve the
problem. A disadvantage of using primary data is that data collection
can be costly and time-consuming. Using secondary data has the advantage
of lower cost and faster access to relevant information. However, using
secondary data may not be optimal for the research question under
consideration.

In terms of the degree of organization of the data, data can be also
divided into two types
\citep{inmon2014, leary2013bigdata, hashem2015bigdata, abdullah2013data, pries2015}:
structured data and unstructured data. Structured data have a
predictable and regularly occurring format. In contrast, unstructured
data are unpredictable and have no structure that is recognizable to a
computer. Structured data consist of records, attributes, keys, and
indices and are typically managed by a database management system (DBMS)
such as IBM DB2, Oracle, MySQL, and Microsoft SQL Server. As a result,
most units of structured data can be located quickly and easily.
Unstructured data have many different forms and variations. One common
form of unstructured data is text. Accessing unstructured data is
clumsy. To find a given unit of data in a long text, for example,
sequentially search is usually performed.

In terms of how the data are measured, data can be classified as
qualitative or quantitative. Qualitative data are data about qualities,
which cannot be actually measured. As a result, qualitative data are
extremely varied in nature and include interviews, documents, and
artifacts \citep{miles2014}. Quantitative data are data about
quantities, which can be measured numerically with numbers. In terms of
the level of measurement, quantitative data can be further classified as
nominal, ordinal, interval, or ratio \citep{gan2011}. Nominal data, also
called categorical data, are discrete data without a natural ordering.
Ordinal data are discrete data with a natural order. Interval data are
continuous data with a specific order and equal intervals. Ratio data
are interval data with a natural zero.

There exist a number of data sources. First, data can be obtained from
university-based researchers who collect primary data. Second, data can
be obtained from organizations that are set up for the purpose of
releasing secondary data for general research community. Third, data can
be obtained from national and regional statistical institutes that
collect data. Finally, companies have corporate data that can be
obtained for research purpose.

While it might be difficult to obtain data to address a specific
research problem or answer a business question, it is relatively easy to
obtain data to test a model or an algorithm for data analysis. In the
modern era, readers can obtain datasets from the Internet easily. The
following is a list of some websites to obtain real-world data:

\begin{itemize}
\item
  \textbf{UCI Machine Learning Repository} This website (url:
  \url{http://archive.ics.uci.edu/ml/index.php}) maintains more than 400
  datasets that can be used to test machine learning algorithms.
\item
  \textbf{Kaggle} The Kaggle website (url:
  \url{https://www.kaggle.com/}) include real-world datasets used for
  data science competition. Readers can download data from Kaggle by
  registering an account.
\item
  \textbf{DrivenData} DrivenData aims at bringing cutting-edge practices
  in data science to solve some of the world's biggest social
  challenges. In its website (url: \url{https://www.drivendata.org/}),
  readers can participate data science competitions and download
  datasets.
\item
  \textbf{Analytics Vidhya} This website (url:
  \url{https://datahack.analyticsvidhya.com/contest/all/}) allows you to
  participate and download datasets from practice problems and hackathon
  problems.
\item
  \textbf{KDD Cup} KDD Cup is the annual Data Mining and Knowledge
  Discovery competition organized by ACM Special Interest Group on
  Knowledge Discovery and Data Mining. This website (url:
  \url{http://www.kdd.org/kdd-cup}) contains the datasets used in past
  KDD Cup competitions since 1997.
\item
  \textbf{U.S. Government's open data} This website (url:
  \url{https://www.data.gov/}) contains about 200,000 datasets covering
  a wide range of areas including climate, education, energy, and
  finance.
\item
  \textbf{AWS Public Datasets} In this website (url:
  \url{https://aws.amazon.com/datasets/}), Amazon provides a centralized
  repository of public datasets, including some huge datasets.
\end{itemize}

\subsection{Data Structures and
Storage}\label{data-structures-and-storage}

As mentioned in the previous subsection, there are structured data as
well as unstructured data. Structured data are highly organized data and
usually have the following tabular format:

\[\begin{matrix}
\begin{array}{lllll} \hline
 & V_1 & V_2 & \cdots & V_d \  
\\\hline
\textbf{x}_1 & x_{11} & x_{12} & \cdots & x_{1d} \\
\textbf{x}_2 & x_{21} & x_{22} & \cdots & x_{2d} \\
\vdots & \vdots & \vdots & \cdots & \vdots \\
\textbf{x}_n & x_{n1} & x_{n2} & \cdots & x_{nd} \\
\hline
\end{array}
\end{matrix}
\]

In other words, structured data can be organized into a table consists
of rows and columns. Typically, each row represents a record and each
column represents an attribute. A table can be decomposed into several
tables that can be stored in a relational database such as the Microsoft
SQL Server. The SQL (Structured Query Language) can be used to access
and modify the data easily and efficiently.

Unstructured data do not follow a regular format
\citep{abdullah2013data}. Examples of unstructured data include
documents, videos, and audio files. Most of the data we encounter are
unstructured data. In fact, the term ``big data'' was coined to reflect
this fact. Traditional relational databases cannot meet the challenges
on the varieties and scales brought by massive unstructured data
nowadays. NoSQL databases have been used to store massive unstructured
data.

There are three main NoSQL databases \citep{chen2014b}: key-value
databases, column-oriented databases, and document-oriented databases.
Key-value databases use a simple data model and store data according to
key-values. Modern key-value databases have higher expandability and
smaller query response time than relational databases. Examples of
key-value databases include Dynamo used by Amazon and Voldemort used by
LinkedIn. Column-oriented databases store and process data according to
columns rather than rows. The columns and rows are segmented in multiple
nodes to achieve expandability. Examples of column-oriented databases
include BigTable developed by Google and Cassandra developed by
FaceBook. Document databases are designed to support more complex data
forms than those stored in key-value databases. Examples of document
databases include MongoDB, SimpleDB, and CouchDB. MongoDB is an
open-source document-oriented database that stores documents as binary
objects. SimpleDB is a distributed NoSQL database used by Amazon.
CouchDB is another open-source document-oriented database.

\subsection{Data Quality}\label{data-quality}

Accurate data are essential to useful data analysis. The lack of
accurate data may lead to significant costs to organizations in areas
such as correction activities, lost customers, missed opportunities, and
incorrect decisions \citep{olson2003}.

Data has quality if it satisfies its intended use, that is, the data is
accurate, timely, relevant, complete, understood, and trusted
\citep{olson2003}. As a result, we first need to know the specification
of the intended uses and then judge the suitability for those uses in
order to assess the quality of the data. Unintended uses of data can
arise from a variety of reasons and lead to serious problems.

Accuracy is the single most important component of high-quality data.
Accurate data have the following properties \citep{olson2003}:

\begin{itemize}
\tightlist
\item
  The data elements are not missing and have valid values.
\item
  The values of the data elements are in the right ranges and have the
  right representations.
\end{itemize}

Inaccurate data arise from different sources. In particular, the
following areas are common areas where inaccurate data occur:

\begin{itemize}
\tightlist
\item
  Initial data entry. Mistakes (including deliberate errors) and system
  errors can occur during the initial data entry. Flawed data entry
  processes can result in inaccurate data.
\item
  Data decay. Data decay, also known as data degradation, refers to the
  gradual corruption of computer data due to an accumulation of
  non-critical failures in a storage device.
\item
  Data moving and restructuring. Inaccurate data can also arise from
  data extracting, cleaning, transforming, loading, or integrating.
\item
  Data using. Faulty reporting and lack of understanding can lead to
  inaccurate data.
\end{itemize}

Reverification and analysis are two approaches to find inaccurate data
elements. The first approach is done by people, who manually check every
data element by going back to the original source of the data. The
second approach is done by software with the skills of an analyst to
search through data to find possible inaccurate data elements. To ensure
that the data elements are 100\% accurate, we must use reverification.
However, reverification can be time-consuming and may not be possible
for some data. Analytical techniques can also be used to identify
inaccurate data elements. There are five types of analysis that can be
used to identify inaccurate data \citep{olson2003}: data element
analysis, structural analysis, value correlation, aggregation
correlation, and value inspection

Companies can create a data quality assurance program to create
high-quality databases. For more information about management of data
quality issues and data profiling techniques, readers are referred to
\citet{olson2003}.

\subsection{Data Cleaning}\label{data-cleaning}

Raw data usually need to be cleaned before useful analysis can be
conducted. In particular, the following areas need attention when
preparing data for analysis \citep{janert2010}:

\begin{itemize}
\item
  \textbf{Missing values} It is common to have missing values in raw
  data. Depending on the situations, we can discard the record, discard
  the variable, or impute the missing values.
\item
  \textbf{Outliers} Raw data may contain unusual data points such as
  outliers. We need to handle outliers carefully. We cannot just remove
  outliers without knowing the reason for their existence. Sometimes the
  outliers are caused by clerical errors. Sometimes outliers are the
  effect we are looking for.
\item
  \textbf{Junk} Raw data may contain junks such as nonprintable
  characters. Junks are typically rare and not easy to get noticed.
  However, junks can cause serious problems in downstream applications.
\item
  \textbf{Format} Raw data may be formatted in a way that is
  inconvenient for subsequent analysis. For example, components of a
  record may be split into multiple lines in a text file. In such cases,
  lines corresponding to a single record should be merged before loading
  to a data analysis software such as R.
\item
  \textbf{Duplicate records} Raw data may contain duplicate records.
  Duplicate records should be recognized and removed. This task may not
  be trivial depending on what you consider ``duplicate.''
\item
  \textbf{Merging datasets} Raw data may come from different sources. In
  such cases, we need to merge the data from different sources to ensure
  compatibility.
\end{itemize}

For more information about how to handle data in R, readers are referred
to \citet{forte2015} and \citet{buttrey2017}.

\section{Data Analysis Preliminaries}\label{data-analysis-preliminaries}

Data analysis involves inspecting, cleansing, transforming, and modeling
data to discover useful information to suggest conclusions and make
decisions. Data analysis has a long history. In 1962, statistician John
Tukey defined data analysis as:

\begin{quote}
procedures for analyzing data, techniques for interpreting the results
of such procedures, ways of planning the gathering of data to make its
analysis easier, more precise or more accurate, and all the machinery
and results of (mathematical) statistics which apply to analyzing data.

--- \citep{tukey1962data}
\end{quote}

Recently, Judd and coauthors defined data analysis as the following
equation\citep{judd2017}:

\[\hbox{Data} = \hbox{Model} + \hbox{Error},\] where Data represents a
set of basic scores or observations to be analyzed, Model is a compact
representation of the data, and Error is simply the amount the model
fails to represent accurately. Using the above equation for data
analysis, an analyst must resolve the following two conflicting goals:

\begin{itemize}
\tightlist
\item
  to add more parameters to the model so that the model represents the
  data better.
\item
  to remove parameters from the model so that the model is simple and
  parsimonious.
\end{itemize}

In this section, we give a high-level introduction to data analysis,
including different types of methods.

\subsection{Data Analysis Process}\label{S:process}

Data analysis is part of an overall study. For example, Figure
\ref{fig:study} shows the process of a typical study in behavioral and
social sciences as described in \citet{albers2017}. The data analysis
part consists of the following steps:

\begin{itemize}
\item
  \textbf{Exploratory analysis} The purpose of this step is to get a
  feel of the relationships with the data and figure out what type of
  analysis for the data makes sense.
\item
  \textbf{Statistical analysis} This step performs statistical analysis
  such as determining statistical significance and effect size.
\item
  \textbf{Make sense of the results} This step interprets the
  statistical results in the context of the overall study.
\item
  \textbf{Determine implications} This step interprets the data by
  connecting it to the study goals and the larger field of this study.
\end{itemize}

The goal of the data analysis as described above focuses on explaining
some phenomenon (See Section \ref{S:expred}).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{Figures/Figure1} 

}

\caption{The process of a typical study in behavioral and social sciences.}\label{fig:study}
\end{figure}

\citet{shmueli2010model} described a general process for statistical
modeling, which is shown in Figure \ref{fig:modeling}. Depending on the
goal of the analysis, the steps differ in terms of the choice of
methods, criteria, data, and information.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{Figures/Figure2} 

}

\caption{The process of statistical modeling.}\label{fig:modeling}
\end{figure}

\subsection{Exploratory versus
Confirmatory}\label{exploratory-versus-confirmatory}

There are two phases of data analysis \citep{good1983data}: exploratory
data analysis (EDA) and confirmatory data analysis (CDA).
\protect\hyperlink{tab:13.1}{Table 13.1} summarizes some differences
between EDA and CDA. EDA is usually applied to observational data with
the goal of looking for patterns and formulating hypotheses. In
contrast, CDA is often applied to experimental data (i.e., data obtained
by means of a formal design of experiments) with the goal of quantifying
the extent to which discrepancies between the model and the data could
be expected to occur by chance \citep{gelman2004eda}.

\[\begin{matrix}
\begin{array}{lll} \hline
 & \textbf{EDA} & \textbf{CDA} \\\hline
\text{Data} & \text{Observational data} & \text{Experimental data}\\[3mm]
\text{Goal} & \text{Pattern recognition,}  & \text{Hypothesis testing,}  \\
& \text{formulate hypotheses} & \text{estimation, prediction} \\[3mm]
\text{Techniques} & \text{Descriptive statistics,} & \text{Traditional statistical tools of} \\
& \text{visualization, clustering} & \text{inference, significance, and}\\
& & \text{confidence} \\
\hline
\end{array}
\end{matrix}
\] \protect\hyperlink{tab:13.1}{Table 13.1}: Comparison of exploratory
data analysis and confirmatory data analysis.

Techniques for EDA include descriptive statistics (e.g., mean, median,
standard deviation, quantiles), distributions, histograms, correlation
analysis, dimension reduction, and cluster analysis. Techniques for CDA
include the traditional statistical tools of inference, significance,
and confidence.

\subsection{Supervised versus
Unsupervised}\label{supervised-versus-unsupervised}

Methods for data analysis can be divided into two types
\citep{abbott2014, igual2017}: supervised learning methods and
unsupervised learning methods. Supervised learning methods work with
labeled data, which include a target variable. Mathematically,
supervised learning methods try to approximate the following function:
\[
Y = f(X_1, X_2, \ldots, X_p),
\] where \(Y\) is a target variable and \(X_1\), \(X_2\), \(\ldots\),
\(X_p\) are explanatory variables. Other terms are also used to mean a
target variable. \protect\hyperlink{tab:13.2}{Table 13.2} gives a list
of common names for different types of variables \citep{frees2009}. When
the target variable is a categorical variable, supervised learning
methods are called classification methods. When the target variable is
continuous, supervised learning methods are called regression methods.

\[\begin{matrix}
\begin{array}{ll}
\hline
\textbf{Target Variable}  &  \textbf{Explanatory Variable}\\\hline
\text{Dependent variable} & \text{Independent variable}\\
\text{Response} & \text{Treatment} \\
\text{Output} & \text{Input} \\
\text{Endogenous variable} & \text{Exogenous variable} \\
\text{Predicted variable} & \text{Predictor variable} \\
\text{Regressand} & \text{Regressor} \\
\hline
\end{array}
\end{matrix}
\] \protect\hyperlink{tab:13.2}{Table 13.2}: Common names of different
variables.

Unsupervised learning methods work with unlabeled data, which include
explanatory variables only. In other words, unsupervised learning
methods do not use target variables. As a result, unsupervised learning
methods are also called descriptive modeling methods.

\subsection{Parametric versus
Nonparametric}\label{parametric-versus-nonparametric}

Methods for data analysis can be parametric or nonparametric
\citep{abbott2014}. Parametric methods assume that the data follow a
certain distribution. Nonparametric methods do not assume distributions
for the data and therefore are called distribution-free methods.

Parametric methods have the advantage that if the distribution of the
data is known, properties of the data and properties of the method
(e.g., errors, convergence, coefficients) can be derived. A disadvantage
of parametric methods is that analysts need to spend considerable time
on figuring out the distribution. For example, analysts may try
different transformation methods to transform the data so that it
follows a certain distribution.

Since nonparametric methods make fewer assumptions, nonparametric
methods have the advantage that they are more flexible, more robust, and
applicable to non-quantitative data. However, a drawback of
nonparametric methods is that the conclusions drawn from nonparametric
methods are not as powerful as those drawn from parametric methods.

\subsection{Explanation versus Prediction}\label{S:expred}

There are two goals in data analysis
\citep{breiman2001modeling, shmueli2010model}: explanation and
prediction. In some scientific areas such as economics, psychology, and
environmental science, the focus of data analysis is to explain the
causal relationships between the input variables and the response
variable. In other scientific areas such as natural language processing
and bioinformatics, the focus of data analysis is to predict what the
responses are going to be given the input variables.

\citet{shmueli2010model} discussed in detail the distinction between
explanatory modeling and predictive modeling, which reflect the process
of using data and methods for explaining or predicting, respectively.
Explanatory modeling is commonly used for theory building and testing.
However, predictive modeling is rarely used in many scientific fields as
a tool for developing theory.

Explanatory modeling is typically done as follows:

\begin{itemize}
\item
  State the prevailing theory.
\item
  State causal hypotheses, which are given in terms of theoretical
  constructs rather than measurable variables. A causal diagram is
  usually included to illustrate the hypothesized causal relationship
  between the theoretical constructs.
\item
  Operationalize constructs. In this step, previous literature and
  theoretical justification are used to build a bridge between
  theoretical constructs and observable measurements.
\item
  Collect data and build models alongside the statistical hypotheses,
  which are operationalized from the research hypotheses.
\item
  Reach research conclusions and recommend policy. The statistical
  conclusions are converted into research conclusions. Policy
  recommendations are often accompanied.
\end{itemize}

\citet{shmueli2010model} defined predictive modeling as the process of
applying a statistical model or data mining algorithm to data for the
purpose of predicting new or future observations. Predictions include
point predictions, interval predictions, regions, distributions, and
rankings of new observations. Predictive model can be any method that
produces predictions.

\subsection{Data Modeling versus Algorithmic
Modeling}\label{data-modeling-versus-algorithmic-modeling}

\citet{breiman2001modeling} discussed two cultures in the use of
statistical modeling to reach conclusions from data: the data modeling
culture and the algorithmic modeling culture. In the data modeling
culture, the data are assumed to be generated by a given stochastic data
model. In the algorithmic modeling culture, the data mechanism is
treated as unknown and algorithmic models are used.

Data modeling gives the statistics field many successes in analyzing
data and getting information about the data mechanisms. However,
\citet{breiman2001modeling} argued that the focus on data models in the
statistical community has led to some side effects such as

\begin{itemize}
\item
  Produced irrelevant theory and questionable scientific conclusions.
\item
  Kept statisticians from using algorithmic models that might be more
  suitable.
\item
  Restricted the ability of statisticians to deal with a wide range of
  problems.
\end{itemize}

Algorithmic modeling was used by industrial statisticians long time ago.
However, the development of algorithmic methods was taken up by a
community outside statistics \citep{breiman2001modeling}. The goal of
algorithmic modeling is predictive accuracy. For some complex prediction
problems, data models are not suitable. These prediction problems
include speech recognition, image recognition, handwriting recognition,
nonlinear time series prediction, and financial market prediction. The
theory in algorithmic modeling focuses on the properties of algorithms,
such as convergence and predictive accuracy.

\subsection{Big Data Analysis}\label{big-data-analysis}

Unlike traditional data analysis, big data analysis employs additional
methods and tools that can extract information rapidly from massive
data. In particular, big data analysis uses the following processing
methods \citep{chen2014b}:

\begin{itemize}
\item
  \textbf{Bloom filter} A bloom filter is a space-efficient
  probabilistic data structure that is used to determine whether an
  element belongs to a set. It has the advantages of high space
  efficiency and high query speed. A drawback of using bloom filter is
  that there is a certain misrecognition rate.
\item
  \textbf{Hashing} Hashing is a method that transforms data into
  fixed-length numerical values through a hash function. It has the
  advantages of rapid reading and writing. However, sound hash functions
  are difficult to find.
\item
  \textbf{Indexing} Indexing refers to a process of partitioning data in
  order to speed up reading. Hashing is a special case of indexing.
\item
  \textbf{Tries} A trie, also called digital tree, is a method to
  improve query efficiency by using common prefixes of character strings
  to reduce comparison on character strings to the greatest extent.
\item
  \textbf{Parallel computing} Parallel computing uses multiple computing
  resources to complete a computation task. Parallel computing tools
  include MPI (Message Passing Interface), MapReduce, and Dryad.
\end{itemize}

Big data analysis can be conducted in the following levels
\citep{chen2014b}: memory-level, business intelligence (BI) level, and
massive level. Memory-level analysis is conducted when the data can be
loaded to the memory of a cluster of computers. Current hardware can
handle hundreds of gigabytes (GB) of data in memory. BI level analysis
can be conducted when the data surpass the memory level. It is common
for BI level analysis products to support data over terabytes (TB).
Massive level analysis is conducted when the data surpass the
capabilities of products for BI level analysis. Usually Hadoop and
MapReduce are used in massive level analysis.

\subsection{Reproducible Analysis}\label{reproducible-analysis}

As mentioned in Section \ref{S:process}, a typical data analysis
workflow includes collecting data, analyzing data, and reporting
results. The data collected are saved in a database or files. The data
are then analyzed by one or more scripts, which may save some
intermediate results or always work on the raw data. Finally a report is
produced to describe the results, which include relevant plots, tables,
and summaries of the data. The workflow may subject to the following
potential issues \citep[Chapter 2]{mailund2017}:

\begin{itemize}
\item
  The data are separated from the analysis scripts.
\item
  The documentation of the analysis is separated from the analysis
  itself.
\end{itemize}

If the analysis is done on the raw data with a single script, then the
first issue is not a major problem. If the analysis consists of multiple
scripts and a script saves intermediate results that are read by the
next script, then the scripts describe a workflow of data analysis. To
reproduce an analysis, the scripts have to be executed in the right
order. The workflow may cause major problems if the order of the scripts
is not documented or the documentation is not updated or lost. One way
to address the first issue is to write the scripts so that any part of
the workflow can be run completely automatically at any time.

If the documentation of the analysis is synchronized with the analysis,
then the second issue is not a major problem. However, the documentation
may become completely useless if the scripts are changed but the
documentation is not updated.

Literate programming is an approach to address the two issues mentioned
above. In literate programming, the documentation of a program and the
code of the program are written together. To do literate programming in
R, one way is to use the R Markdown and the \(\texttt{knitr}\) package.

\subsection{Ethical Issues}\label{ethical-issues}

Analysts may face ethical issues and dilemmas during the data analysis
process. In some fields, for example, ethical issues and dilemmas
include participant consent, benefits, risk, confidentiality, and data
ownership \citep{miles2014}. For data analysis in actuarial science and
insurance in particular, we face the following ethical matters and
issues \citep{miles2014}:

\begin{itemize}
\item
  \textbf{Worthiness of the project} Is the project worth doing? Will
  the project contribute in some significant way to a domain broader
  than my career? If a project is only opportunistic and does not have a
  larger significance, then it might be pursued with less care. The
  result may be looked good but not right.
\item
  \textbf{Competence} Do I or the whole team have the expertise to carry
  out the project? Incompetence may lead to weakness in the analytics
  such as collecting large amounts of data poorly and drawing
  superficial conclusions.
\item
  \textbf{Benefits, costs, and reciprocity} Will each stakeholder gain
  from the project? Are the benefit and the cost equitable? A project
  will likely to fail if the benefit and the cost for a stakeholder do
  not match.
\item
  \textbf{Privacy and confidentiality} How do we make sure that the
  information is kept confidentially? Where raw data and analysis
  results are stored and how will have access to them should be
  documented in explicit confidentiality agreements.
\end{itemize}

\section{Data Analysis Techniques}\label{data-analysis-techniques}

Techniques for data analysis are drawn from different but overlapping
fields such as statistics, machine learning, pattern recognition, and
data mining. Statistics is a field that addresses reliable ways of
gathering data and making inferences based on them
\citep{bandyo2011, bluman2012}. The term machine learning was coined by
Samuel in 1959 \citep{samuel1959ml}. Originally, machine learning refers
to the field of study where computers have the ability to learn without
being explicitly programmed. Nowadays, machine learning has evolved to
the broad field of study where computational methods use experience
(i.e., the past information available for analysis) to improve
performance or to make accurate predictions
\citep{bishop2007, clarke2009, mohri2012, kubat2017}. There are four
types of machine learning algorithms (See
\protect\hyperlink{tab:13.3}{Table 13.3} depending on the type of the
data and the type of the learning tasks.

\[\begin{matrix}
\begin{array}{rll} \hline
& \textbf{Supervised} & \textbf{Unsupervised} \\\hline
\textbf{Discrete Label} & \text{Classification} & \text{Clustering} \\
\textbf{Continuous Label} & \text{Regression} & \text{Dimension reduction} \\
\hline
\end{array}
\end{matrix}
\] \protect\hyperlink{tab:13.3}{Table 13.3}: Types of machine learning
algorithms.

Originating in engineering, pattern recognition is a field that is
closely related to machine learning, which grew out of computer science.
In fact, pattern recognition and machine learning can be considered to
be two facets of the same field \citep{bishop2007}. Data mining is a
field that concerns collecting, cleaning, processing, analyzing, and
gaining useful insights from data \citep{aggarwal2015}.

\subsection{Exploratory Techniques}\label{exploratory-techniques}

Exploratory data analysis techniques include descriptive statistics as
well as many unsupervised learning techniques such as data clustering
and principal component analysis.

\subsubsection{Descriptive Statistics}\label{descriptive-statistics}

In the mass noun sense, descriptive statistics is an area of statistics
that concerns the collection, organization, summarization, and
presentation of data \citep{bluman2012}. In the count noun sense,
descriptive statistics are summary statistics that quantitatively
describe or summarize data.

\[\begin{matrix}
\begin{array}{ll} \hline
& \textbf{Descriptive Statistics} \\\hline
\text{Measures of central tendency} & \text{Mean, median, mode, midrange}\\
\text{Measures of variation} & \text{Range, variance, standard deviation} \\
\text{Measures of position} & \text{Quantile} \\
\hline
\end{array}
\end{matrix}
\] \protect\hyperlink{tab:13.4}{Table 13.4}: Some commonly used
descriptive statistics.

\protect\hyperlink{tab:13.4}{Table 13.4} lists some commonly used
descriptive statistics. In R, we can use the function
\(\texttt{summary}\) to calculate some of the descriptive statistics.
For numeric data, we can visualize the descriptive statistics using a
boxplot.

In addition to these quantitative descriptive statistics, we can also
qualitatively describe shapes of the distributions \citep{bluman2012}.
For example, we can say that a distribution is positively skewed,
symmetric, or negatively skewed. To visualize the distribution of a
variable, we can draw a histogram.

\subsubsection{Principal Component
Analysis}\label{principal-component-analysis}

Principal component analysis (PCA) is a statistical procedure that
transforms a dataset described by possibly correlated variables into a
dataset described by linearly uncorrelated variables, which are called
principal components and are ordered according to their variances. PCA
is a technique for dimension reduction. If the original variables are
highly correlated, then the first few principal components can account
for most of the variation of the original data.

The principal components of the variables are related to the
eigenvectors and eigenvectors of the covariance matrix of the variables.
For \(i=1,2,\ldots,d\), let \((\lambda_i, \textbf{e}_i)\) be the \(i\)th
eigenvalue-eigenvector pair of the covariance matrix \({\Sigma}\) of
\(d\) variables \(X_1,X_2,\ldots,X_d\) such that
\(\lambda_1\ge \lambda_2\ge \ldots\ge \lambda_d\ge 0\) and the
eigenvectors are normalized. Then the \(i\)th principal component is
given by

\[Z_{i} = \textbf{e}_i' \textbf{X} =\sum_{j=1}^d e_{ij} X_j,\] where
\(\textbf{X}=(X_1,X_2,\ldots,X_d)'\). It can be shown that
\(\mathrm{Var~}{(Z_i)} = \lambda_i\). As a result, the proportion of
variance explained by the \(i\)th principal component is calculated as

\[\dfrac{\mathrm{Var~}{(Z_i)}}{ \sum_{j=1}^{d} \mathrm{Var~}{(Z_j)}} = \dfrac{\lambda_i}{\lambda_1+\lambda_2+\cdots+\lambda_d}.\]

For more information about PCA, readers are referred to
\citet{mirkin2011}.

\subsubsection{Cluster Analysis}\label{cluster-analysis}

Cluster analysis (aka data clustering) refers to the process of dividing
a dataset into homogeneous groups or clusters such that points in the
same cluster are similar and points from different clusters are quite
distinct \citep{gan2007, gan2011}. Data clustering is one of the most
popular tools for exploratory data analysis and has found applications
in many scientific areas.

During the past several decades, many clustering algorithms have been
proposed. Among these clustering algorithms, the \(k\)-means algorithm
is perhaps the most well-known algorithm due to its simplicity. To
describe the k-means algorithm, let
\(X=\{\textbf{x}_1,\textbf{x}_2,\ldots,\textbf{x}_n\}\) be a dataset
containing \(n\) points, each of which is described by \(d\) numerical
features. Given a desired number of clusters \(k\), the \(k\)-means
algorithm aims at minimizing the following objective function:

\[P(U,Z) = \sum_{l=1}^k\sum_{i=1}^n u_{il} \Vert \textbf{x}_i-\textbf{z}_l\Vert^2,\]
where \(U=(u_{il})_{n\times k}\) is an \(n\times k\) partition matrix,
\(Z=\{\textbf{z}_1,\textbf{z}_2,\ldots,\textbf{z}_k\}\) is a set of
cluster centers, and \(\Vert\cdot\Vert\) is the \(L^2\) norm or
Euclidean distance. The partition matrix \(U\) satisfies the following
conditions:

\[u_{il}\in \{0,1\},\quad i=1,2,\ldots,n,\:l=1,2,\ldots,k,\]
\[\sum_{l=1}^k u_{il}=1,\quad i=1,2,\ldots,n.\]

The \(k\)-means algorithm employs an iterative procedure to minimize the
objective function. It repeatedly updates the partition matrix \(U\) and
the cluster centers \(Z\) alternately until some stop criterion is met.
For more information about \(k\)-means, readers are referred to
\citet{gan2007} and \citet{mirkin2011}.

\subsection{Confirmatory Techniques}\label{confirmatory-techniques}

Confirmatory data analysis techniques include the traditional
statistical tools of inference, significance, and confidence.

\subsubsection{Linear Models}\label{linear-models}

Linear models, also called linear regression models, aim at using a
linear function to approximate the relationship between the dependent
variable and independent variables. A linear regression model is called
a simple linear regression model if there is only one independent
variable. When more than one independent variables are involved, a
linear regression model is called a multiple linear regression model.

Let \(X\) and \(Y\) denote the independent and the dependent variables,
respectively. For \(i=1,2,\ldots,n\), let \((x_i, y_i)\) be the observed
values of \((X,Y)\) in the \(i\)th case. Then the simple linear
regression model is specified as follows \citep{frees2009}:

\[y_i  = \beta_0 + \beta_1 x_i + \epsilon_i,\quad i=1,2,\ldots,n,\]
where \(\beta_0\) and \(\beta_1\) are parameters and \(\epsilon_i\) is a
random variable representing the error for the \(i\)th case.

When there are multiple independent variables, the following multiple
linear regression model is used:

\[y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_k x_{ik} + \epsilon_i,\]
where \(\beta_0\), \(\beta_1\), \(\ldots\), \(\beta_k\) are unknown
parameters to be estimated.

Linear regression models usually make the following assumptions:

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  \(x_{i1},x_{i2},\ldots,x_{ik}\) are nonstochastic variables.
\item
  \(\mathrm{Var~}(y_i)=\sigma^2\), where \(\mathrm{Var~}(y_i)\) denotes
  the variance of \(y_i\).
\item
  \(y_1,y_2,\ldots,y_n\) are independent random variables.
\end{enumerate}

For the purpose of obtaining tests and confidence statements with small
samples, the following strong normality assumption is also made:

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  \(\epsilon_1,\epsilon_2,\ldots,\epsilon_n\) are normally distributed.
\end{enumerate}

\subsubsection{Generalized Linear
Models}\label{generalized-linear-models}

The generalized linear model (GLM) is a wide family of regression models
that include linear regression models as special cases. In a GLM, the
mean of the response (i.e., the dependent variable) is assumed to be a
function of linear combinations of the explanatory variables, i.e.,

\[\mu_i = E[y_i],\]
\[\eta_i = \textbf{x}_i'\boldsymbol{\beta} = g(\mu_i),\] where
\(\textbf{x}_i=(1,x_{i1}, x_{i2}, \ldots, x_{ik})'\) is a vector of
regressor values, \(\mu_i\) is the mean response for the \(i\)th case,
and \(\eta_i\) is a systematic component of the GLM. The function
\(g(\cdot)\) is known and is called the link function. The mean response
can vary by observations by allowing some parameters to change. However,
the regression parameters \(\boldsymbol{\beta}\) are assumed to be the
same among different observations.

GLMs make the following assumptions:

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  \(x_{i1},x_{i2},\ldots,x_{in}\) are nonstochastic variables.
\item
  \(y_1,y_2,\ldots,y_n\) are independent.
\item
  The dependent variable is assumed to follow a distribution from the
  linear exponential family.
\item
  The variance of the dependent variable is not assumed to be constant
  but is a function of the mean, i.e.,
\end{enumerate}

\[\mathrm{Var~}{(y_i)} = \phi \nu(\mu_i),\] where \(\phi\) denotes the
dispersion parameter and \(\nu(\cdot)\) is a function.

As we can see from the above specification, the GLM provides a unifying
framework to handle different types of dependent variables, including
discrete and continuous variables. For more information about GLMs,
readers are referred to \citet{dejong2008} and \citet{frees2009}.

\subsubsection{Tree-based Models}\label{tree-based-models}

Decision trees, also known as tree-based models, involve dividing the
predictor space (i.e., the space formed by independent variables) into a
number of simple regions and using the mean or the mode of the region
for prediction \citep{breiman1984}. There are two types of tree-based
models: classification trees and regression trees. When the dependent
variable is categorical, the resulting tree models are called
classification trees. When the dependent variable is continuous, the
resulting tree models are called regression trees.

The process of building classification trees is similar to that of
building regression trees. Here we only briefly describe how to build a
regression tree. To do that, the predictor space is divided into
non-overlapping regions such that the following objective function

\[f(R_1,R_2,\ldots,R_J) = \sum_{j=1}^J \sum_{i=1}^n I_{R_j}(\textbf{x}_i)(y_i - \mu_j)^2\]
is minimized, where \(I\) is an indicator function, \(R_j\) denotes the
set of indices of the observations that belong to the \(j\)th box,
\(\mu_j\) is the mean response of the observations in the \(j\)th box,
\(\textbf{x}_i\) is the vector of predictor values for the \(i\)th
observation, and \(y_i\) is the response value for the \(i\)th
observation.

In terms of predictive accuracy, decision trees generally do not perform
to the level of other regression and classification models. However,
tree-based models may outperform linear models when the relationship
between the response and the predictors is nonlinear. For more
information about decision trees, readers are referred to
\citet{breiman1984} and \citet{mitchell1997}.

\section{Some R Functions}\label{some-r-functions}

R is an open-source software for statistical computing and graphics. The
R software can be downloaded from the R project website at
\url{https://www.r-project.org/}. In this section, we give some R
function for data analysis, especially the data analysis tasks mentioned
in previous sections.

\[\begin{matrix}
\begin{array}{lll} \hline
\text{Data Analysis Task} & \text{R package} & \text{R Function} \\\hline
\text{Descriptive Statistics} & \texttt{base} & \texttt{summary}\\
\text{Principal Component Analysis} & \texttt{stats} & \texttt{prcomp} \\
\text{Data Clustering} & \texttt{stats} & \texttt{kmeans}, \texttt{hclust} \\
\text{Fitting Distributions} & \texttt{MASS} & \texttt{fitdistr} \\
\text{Linear Regression Models} & \texttt{stats} & \texttt{lm} \\
\text{Generalized Linear Models} & \texttt{stats} & \texttt{glm} \\
\text{Regression Trees} & \texttt{rpart} & \texttt{rpart} \\
\text{Survival Analysis} & \texttt{survival} & \texttt{survfit} \\
\hline
\end{array}
\end{matrix}
\] \protect\hyperlink{tab:13.5}{Table 13.5}: Some R functions for data
analysis.

\protect\hyperlink{tab:13.5}{Table 13.5} lists a few R functions for
different data analysis tasks. Readers can read the R documentation for
examples of using these functions. There are also other R functions from
other packages to do similar things. However, the functions listed in
this table provide good start points for readers to conduct data
analysis in R. For analyzing large datasets in R in an efficient way,
readers are referred to \citet{daroczi2015}.

\section{Summary}\label{summary}

In this chapter, we gave a high-level overview of data analysis by
introducing data types, data structures, data storages, data sources,
data analysis processes, and data analysis techniques. In particular, we
presented various aspects of data analysis. In addition, we provided
several websites where readers can obtain real-world datasets to horn
their data analysis skills. We also listed some R packages and functions
that can be used to perform various data analysis tasks.

\section{Further Resources and
Contributors}\label{DS:further-reading-and-resources}

\subsubsection*{Contributor}\label{contributor-2}
\addcontentsline{toc}{subsubsection}{Contributor}

\begin{itemize}
\tightlist
\item
  \textbf{Guojun Gan}, University of Connecticut, is the principal
  author of the initial version of this chapter. Email:
  \href{mailto:guojun.gan@uconn.edu}{\nolinkurl{guojun.gan@uconn.edu}}
  for chapter comments and suggested improvements.
\item
  Chapter reviewers include: Min Ji, Toby White.
\end{itemize}

\chapter{Dependence Modeling}\label{C:DependenceModel}

\emph{Chapter Preview}. In practice, there are many types of variables
that one encounter and the first step in dependence modeling is
identifying the type of variable you are dealing with to help direct you
to the appropriate technique. This chapter introduces readers to
variable types and techniques for modeling dependence or association of
multivariate distributions. Section \ref{S:VarTypes} provides an
overview of the types of variables. Section \ref{S:Measures} then
elaborates basic measures for modeling the dependence between variables.

Section \ref{S:Copula} introduces a novel approach to modeling
dependence using Copulas which is reinforced with practical
illustrations in Section \ref{S:CopAppl}. The types of Copula families
and basic properties of Copula functions is explained Section
\ref{S:CopTyp}. The chapter concludes by explaining why the study of
dependence modeling is important in Section \ref{S:CopImp}.

\section{Variable Types}\label{S:VarTypes}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Classify variables as qualitative or quantitative.
\item
  Describe multivariate variables.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

People, firms, and other entities that we want to understand are
described in a dataset by numerical characteristics. As these
characteristics vary by entity, they are commonly known as variables. To
manage insurance systems, it will be critical to understand the
distribution of each variable and how they are associated with one
another. It is common for data sets to have many variables (high
dimensional) and so it useful to begin by classifying them into
different types. As will be seen, these classifications are not strict;
there is overlap among the groups. Nonetheless, the grouping summarized
in \protect\hyperlink{tab:14.1}{Table 14.1} and explained in the
remainder of this section provides a solid first step in framing a data
set.

\[
{\small \begin{matrix}
\begin{array}{l|l} \hline
\textbf{Variable Type} & \textbf{Example} \\\hline
Qualitative &            \\
    \text{Binary} &        \text{Sex} \\
\text{Categorical (Unordered, Nominal)} & \text{Territory (e.g., state/province) in which an insured resides} \\
\text{Ordered Category (Ordinal)} & \text{Claimant satisfaction (five point scale ranging from 1=dissatisfied} \\
& ~~~ \text{to 5 =satisfied)} \\\hline
Quantitative &            \\
\text{Continuous} & \text{Policyholder's age, weight, income} \\
  \text{Discrete} & \text{Amount of deductible  (0, 250, 500, and 1000)} \\
\text{Count} & \text{Number of insurance claims} \\
\text{Combinations of}  & \text{Policy losses, mixture of 0's (for no loss)}  \\
~~~ \text{Discrete and Continuous} & ~~~\text{and positive claim amount} \\
\text{Interval Variable} & \text{Driver Age: 16-24 (young), 25-54 (intermediate),}  \\
& ~~~\text{55 and over (senior)} \\
\text{Circular Data} & \text{Time of day measures of customer arrival} \\ \hline
Multivariate ~ Variable &            \\
\text{High Dimensional Data} & \text{Characteristics of a firm purchasing worker's compensation} \\
& ~~~\text{insurance (location of plants, industry, number of employees,} \\
&~~~\text{and so on)} \\
\text{Spatial Data} & \text{Longitude/latitude of the location an insurance hailstorm claim} \\
\text{Missing Data} & \text{Policyholder's age (continuous/interval) and "-99" for} \\
&~~~ \text{"not reported," that is, missing} \\
\text{Censored and Truncated Data} & \text{Amount of insurance claims in excess of a deductible} \\
\text{Aggregate Claims} & \text{Losses recorded for each claim in a motor vehicle policy.} \\
\text{Stochastic Process Realizations} & \text{The time and amount of each occurrence of an insured loss} \\ \hline
\end{array}
\end{matrix}}
\]

\protect\hyperlink{tab:14.1}{Table 14.1} : Variable types

In data analysis, it is important to understand what type of variable
you are working with. For example, Consider a pair of random variables
\emph{(Coverage,Claim)} from the LGPIF data introduced in chapter 1 as
displayed in Figure \ref{fig:IntroPlot} below. We would like to know
whether the distribution of \emph{Coverage} depends on the distribution
of \emph{Claim} or whether they are statistically independent. We would
also want to know how the \emph{Claim} distribution depends on the
\emph{EntityType} variable. Because the \emph{EntityType} variable
belongs to a different class of variables, modeling the dependence
between \emph{Claim} and \emph{Coverage} may require a different
technique from that of \emph{Claim} and \emph{EntityType}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{LossDataAnalytics_files/figure-latex/IntroPlot-1} 

}

\caption{Scatter plot of *(Coverage,Claim)* from LGPIF data}\label{fig:IntroPlot}
\end{figure}

\subsection{Qualitative Variables}\label{S:QuaVar}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this sub-section, you learn how to:

\begin{itemize}
\tightlist
\item
  Classify qualitative variables as nominal or ordinal
\item
  Describe binary variable
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

A qualitative, or categorical variable is one for which the measurement
denotes membership in a set of groups, or categories. For example, if
you were coding which area of the country an insured resides, you might
use a 1 for the northern part, 2 for southern, and 3 for everything
else. This location variable is an example of a nominal variable, one
for which the levels have no natural ordering. Any analysis of nominal
variables should not depend on the labeling of the categories. For
example, instead of using a 1,2,3 for north, south, other, I should
arrive at the same set of summary statistics if I used a 2,1,3 coding
instead, interchanging north and south.

In contrast, an ordinal variable is a type of categorical variable for
which an ordering does exist. For example, with a survey to see how
satisfied customers are with our claims servicing department, we might
use a five point scale that ranges from 1 meaning dissatisfied to a 5
meaning satisfied. Ordinal variables provide a clear ordering of levels
of a variable but the amount of separation between levels is unknown.

A binary variable is a special type of categorical variable where there
are only two categories commonly taken to be a 0 and a 1. For example,
we might code a variable in a dataset to be a 1 if an insured is female
and a 0 if male.

\subsection{Quantitative Variables}\label{S:QuanVar}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this sub-section, you learn how to:

\begin{itemize}
\tightlist
\item
  Differentiate between continuous and discrete variable
\item
  Use a combination of continuous and discrete variable
\item
  Describe circular data
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Unlike a qualitative variable, a quantitative variable is one in which
numerical level is a realization from some scale so that the distance
between any two levels of the scale takes on meaning. A continuous
variable is one that can take on any value within a finite interval. For
example, it is common to represent a policyholder's age, weight, or
income, as a continuous variable. In contrast, a discrete variable is
one that takes on only a finite number of values in any finite interval.
For example, when examining a policyholder's choice of deductibles, it
may be that values of 0, 250, 500, and 1000 are the only possible
outcomes. Like an ordinal variable, these represent distinct categories
that are ordered. Unlike an ordinal variable, the numerical difference
between levels takes on economic meaning. A special type of discrete
variable is a count variable, one with values on the nonnegative
integers. For example, we will be particularly interested in the number
of claims arising from a policy during a given period.

Some variables are inherently a \emph{combination of discrete and
continuous} components. For example, when we analyze the insured loss of
a policyholder, we will encounter a discrete outcome at zero,
representing no insured loss, and a continuous amount for positive
outcomes, representing the amount of the insured loss. Another
interesting variation is an interval variable,one that gives a range of
possible outcomes.

Circular data represent an interesting category typically not analyzed
by insurers. As an example of circular data, suppose that you monitor
calls to your customer service center and would like to know when is the
peak time of the day for calls to arrive. In this context, one can think
about the time of the day as a variable with realizations on a circle,
e.g., imagine an analog picture of a clock. For circular data, the
distance between observations at 00:15 and 00:45 are just as close as
observations 23:45 and 00:15 (here, we use the convention \emph{HH:MM}
means hours and minutes).

\subsection{Multivariate Variables}\label{multivariate-variables}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this sub-section, you learn how to:

\begin{itemize}
\tightlist
\item
  Differentiate between univariate and multivariate data
\item
  Handle missing variables
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Insurance data typically are multivariate in the sense that we can take
many measurements on a single entity. For example, when studying losses
associated with a firm's worker's compensation plan, we might want to
know the location of its manufacturing plants, the industry in which it
operates, the number of employees, and so forth. The usual strategy for
analyzing multivariate data is to begin by examining each variable in
isolation of the others. This is known as a univariate approach.

In contrast, for some variables, it makes little sense to only look at
one dimensional aspects. For example, insurers typically organize
spatial data by longitude and latitude to analyze the location of
weather related insurance claims due hailstorms. Having only a single
number, either longitude or latitude, provides little information in
understanding geographical location.

Another special case of a multivariate variable, less obvious, involves
coding for missing data. Historically, some statistical packages used a
-99 to report when a variable, such as policyholder's age, was not
available or not reported. This led to many unsuspecting analysts
providing strange statistics when summarizing a set of data. When data
are missing, it is better to think about the variable as two dimensions,
one to indicate whether or not the variable is reported and the second
providing the age (if reported). In the same way, insurance data are
commonly censored and truncated. We refer you to Chapter 4 for more on
censored and truncated data. Aggregate claims can also be coded as
another special type of multivariate variable. We refer you to Chapter 5
for more Aggregate claims.

Perhaps the most complicated type of multivariate variable is a
realization of a stochastic process. You will recall that a stochastic
process is little more than a collection of random variables. For
example, in insurance, we might think about the times that claims arrive
to an insurance company in a one year time horizon. This is a high
dimensional variable that theoretically is infinite dimensional. Special
techniques are required to understand realizations of stochastic
processes that will not be addressed here.

\section{Classic Measures of Scalar Associations}\label{S:Measures}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Estimate correlation using Pearson method
\item
  Use rank based measures like Spearman, Kendall to estimate correlation
\item
  Measure dependence using odds ratio, Pearson chi-square and likelihood
  ratio test statistic
\item
  Use normal-based correlations to quantify associations involving
  ordinal variables
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Association Measures for Quantitative
Variables}\label{association-measures-for-quantitative-variables}

For this section, consider a pair of random variables \((X,Y)\) having
joint distribution function \(F(\cdot)\) and a random sample
\((X_i,Y_i), i=1, \ldots, n\). For the continuous case, suppose that
\(F(\cdot)\) is absolutely continuous with absolutely continuous
marginals.

\subsubsection{Pearson Correlation}\label{pearson-correlation}

Define the sample covariance function
\(\widehat{Cov}(X,Y) = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})\),
where \(\bar{X}\) and \(\bar{Y}\) are the sample means of \(X\) and
\(Y\), respectively. Then, the product-moment (Pearson) correlation can
be written as

\begin{equation*}
r = \frac{\widehat{Cov}(X,Y)}{\sqrt{\widehat{Cov}(X,X)\widehat{Cov}(Y,Y)}}.
\end{equation*}

The correlation statistic \(r\) is widely used to capture linear
association between random variables. It is a (nonparametric) estimator
of the correlation parameter \(\rho\), defined to be the covariance
divided by the product of standard deviations. In this sense, it
captures association for any pair of random variables.

This statistic has several important features. Unlike regression
estimators, it is symmetric between random variables, so the correlation
between \(X\) and \(Y\) equals the correlation between \(Y\) and \(X\).
It is unchanged by linear transformations of random variables (up to
sign changes) so that we can multiply random variables or add constants
as is helpful for interpretation. The range of the statistic is
\([-1,1]\) which does not depend on the distribution of either \(X\) or
\(Y\).

Further, in the case of independence, the correlation coefficient \(r\)
is 0. However, it is well known that zero correlation does not imply
independence, except for normally distributed random variables. The
correlation statistic \(r\) is also a (maximum likelihood) estimator of
the association parameter for bivariate normal distribution. So, for
normally distributed data, the correlation statistic \(r\) can be used
to assess independence. For additional interpretations of this
well-known statistic, readers will enjoy \citep{lee1988thirteen}.

You can obtain the Pearson correlation statistic \(r\) using the
\texttt{cor()} function in \texttt{R} and selecting the \texttt{pearson}
method. This is demonstrated below by using the \emph{Coverage} rating
variable in millions of dollars and \emph{Claim} amount variable in
dollars from the LGPIF data introduced in chapter 1.

From \texttt{R} output above, \(r=0.31\) , which indicates a positive
association between \emph{Claim} and \emph{Coverage}. This means that as
the coverage amount of a policy increases we expect claim to increase.

\subsection{Rank Based Measures}\label{rank-based-measures}

\subsubsection{Spearman's Rho}\label{spearmans-rho}

The Pearson correlation coefficient does have the drawback that it is
not invariant to nonlinear transforms of the data. For example, the
correlation between \(X\) and \(\ln Y\) can be quite different from the
correlation between \(X\) and \(Y\). As we see from the \texttt{R} code
for Pearson correlation statistic above, the correlation statistic \(r\)
between \emph{Coverage} rating variable in logarithmic millions of
dollars and \emph{Claim} amounts variable in dollars is \(0.1\) as
compared to \(0.31\) when we calculate the correlation between
\emph{Coverage} rating variable in millions of dollars and \emph{Claim}
amounts variable in dollars. This limitation is one reason for
considering alternative statistics.

Alternative measures of correlation are based on ranks of the data. Let
\(R(X_j)\) denote the rank of \(X_j\) from the sample
\(X_1, \ldots, X_n\) and similarly for \(R(Y_j)\). Let
\(R(X) = \left(R(X_1), \ldots, R(X_n)\right)'\) denote the vector of
ranks, and similarly for \(R(Y)\). For example, if \(n=3\) and
\(X=(24, 13, 109)\), then \(R(X)=(2,1,3)\). A comprehensive introduction
of rank statistics can be found in, for example,
\citep{hettmansperger1984statistical}. Also, ranks can be used to obtain
the empirical distribution function, refer to section 4.1.1 for more on
the empirical distribution function.

With this, the correlation measure of \citep{spearman1904proof} is
simply the product-moment correlation computed on the ranks:

\begin{equation*}
r_S = \frac{\widehat{Cov}(R(X),R(Y))}{\sqrt{\widehat{Cov}(R(X),R(X))\widehat{Cov}(R(Y),R(Y))}}
= \frac{\widehat{Cov}(R(X),R(Y))}{(n^2-1)/12} .
\end{equation*}

You can obtain the Spearman correlation statistic \(r_S\) using the
\texttt{cor()} function in \texttt{R} and selecting the
\texttt{spearman} method. From below, the Spearman correlation between
the \emph{Coverage} rating variable in millions of dollars and
\emph{Claim} amount variable in dollars is \(0.41\).

We can show that the Spearman correlation statistic is invariant under
strictly increasing transformations. From the \texttt{R} Code for
Spearman correlation statistic above, \(r_S=0.41\) between the
\emph{Coverage} rating variable in logarithmic millions of dollars and
\emph{Claim} amount variable in dollars.

\subsubsection{Kendall's Tau}\label{kendalls-tau}

An alternative measure that uses ranks is based on the concept of
\emph{concordance}. An observation pair \((X,Y)\) is said to be
concordant (discordant) if the observation with a larger value of \(X\)
has also the larger (smaller) value of \(Y\). Then
\(\Pr(concordance) = \Pr[ (X_1-X_2)(Y_1-Y_2) >0 ]\) ,
\(\Pr(discordance) = \Pr[ (X_1-X_2)(Y_1-Y_2) <0 ]\),
\(\Pr(tie) = \Pr[ (X_1-X_2)(Y_1-Y_2) =0 ]\) and

\begin{eqnarray*}
\tau(X,Y)= \Pr(concordance) - \Pr(discordance) = 2\Pr(concordance) - 1 + \Pr(tie).
\end{eqnarray*}

To estimate this, the pairs \((X_i,Y_i)\) and \((X_j,Y_j)\) are said to
be concordant if the product \(sgn(X_j-X_i)sgn(Y_j-Y_i)\) equals 1 and
discordant if the product equals -1. Here, \(sgn(x)=1,0,-1\) as \(x>0\),
\(x=0\), \(x<0\), respectively. With this, we can express the
association measure of \citep{kendall1938new}, known as Kendall's tau,
as

\begin{equation*}
\begin{array}{rl}
\tau &= \frac{2}{n(n-1)} \sum_{i<j}sgn(X_j-X_i)sgn(Y_j-Y_i)\\
&= \frac{2}{n(n-1)} \sum_{i<j}sgn(R(X_j)-R(X_i))sgn(R(Y_j)-R(Y_i)) 
\end{array}.
\end{equation*}

Interestingly, \citep{hougaard2000analysis}, page 137, attributes the
original discovery of this statistic to
\citep{fechnerkollektivmasslehre}, noting that Kendall's discovery was
independent and more complete than the original work.

You can obtain the Kendall's tau, using the \texttt{cor()} function in
\texttt{R} and selecting the \texttt{kendall} method. From below,
\(\tau=0.32\) between the \emph{Coverage} rating variable in millions of
dollars and \emph{Claim} amount variable in dollars. When there are ties
in the data, the \texttt{cor()} function computes \emph{Kendall's
tau\_b}, as proposed by \citep{kendall1945}.

Also,to show that the Kendall's tau is invariate under strictly
increasing transformations , \(\tau=0.32\) between the \emph{Coverage}
rating variable in logarithmic millions of dollars and \emph{Claim}
amount variable in dollars.

\subsection{Nominal Variables}\label{nominal-variables}

\subsubsection{Bernoulli Variables}\label{bernoulli-variables}

To see why dependence measures for continuous variables may not be the
best for discrete variables, let us focus on the case of Bernoulli
variables that take on simple binary outcomes, 0 and 1. For notation,
let \(\pi_{jk} = \Pr(X=j, Y=k)\) for \(j,k=0,1\) and let
\(\pi_X=\Pr(X=1)\) and similarly for \(\pi_Y\). Then, the population
version of the product-moment (Pearson) correlation can be easily seen
to be

\begin{eqnarray*}
\rho = \frac{\pi_{11} - \pi_X \pi_Y}{\sqrt{\pi_X(1-\pi_X)\pi_Y(1-\pi_Y)}} .
\end{eqnarray*}

Unlike the case for continuous data, it is not possible for this measure
to achieve the limiting boundaries of the interval \([-1,1]\). To see
this, students of probability may recall the
Fr\(\acute{e}\)chet-H\(\ddot{o}\)effding bounds for a joint distribution
that turn out to be
\(\max\{0, \pi_X+\pi_Y-1\} \le \pi_{11} \le \min\{\pi_X,\pi_Y\}\) for
this joint probability. This limit on the joint probability imposes an
additional restriction on the Pearson correlation. As an illustration,
assume equal probabilities \(\pi_X =\pi_Y = \pi > 1/2\). Then, the lower
bound is

\begin{eqnarray*}
\frac{2\pi - 1 - \pi^2}{\pi(1-\pi)} = -\frac{1-\pi}{\pi} .
\end{eqnarray*}

For example, if \(\pi=0.8\), then the smallest that the Pearson
correlation could be is -0.25. More generally, there are bounds on
\(\rho\) that depend on \(\pi_X\) and \(\pi_Y\) that make it difficult
to interpret this measure.

As noted by \citep{bishop1975discrete} (page 382), squaring this
correlation coefficient yields the Pearson chi-square statistic
(introduced in chapter 2) . Despite the boundary problems described
above, this feature makes the Pearson correlation coefficient a good
choice for describing dependence with binary data. The other is the odds
ratio, described as follows.

As an alternative measure for Bernoulli variables, the odds ratio is
given by

\begin{eqnarray*}
OR(\pi_{11}) = \frac{\pi_{11} \pi_{00}}{\pi_{01} \pi_{10}} = \frac{\pi_{11} \left( 1+\pi_{11}-\pi_X -\pi_Y\right)}{(\pi_X-\pi_{11})(\pi_Y- \pi_{11})} .
\end{eqnarray*}

Pleasant calculations show that \(OR(z)\) is \(0\) at the lower
Fr\(\acute{e}\)chet-H\(\ddot{o}\)effding bound
\(z= \max\{0, \pi_X+\pi_Y-1\}\) and is \(\infty\) at the upper bound
\(z=\min\{\pi_X,\pi_Y\}\). Thus, the bounds on this measure do not
depend on the marginal probabilities \(\pi_X\) and \(\pi_Y\), making it
easier to interpret this measure.

As noted by \citep{yule1900association}, odds ratios are invariant to
the labeling of 0 and 1. Further, they are invariant to the marginals in
the sense that one can rescale \(\pi_X\) and \(\pi_Y\) by positive
constants and the odds ratio remains unchanged. Specifically, suppose
that \(a_i\), \(b_j\) are sets of positive constants and that

\begin{eqnarray*}
\pi_{ij}^{new} &=& a_i b_j \pi_{ij}
\end{eqnarray*}

and \(\sum_{ij} \pi_{ij}^{new}=1.\) Then,

\begin{eqnarray*}
OR^{new} = \frac{(a_1 b_1 \pi_{11})( a_0 b_0 \pi_{00})}{(a_0 b_1 \pi_{01})( a_1 b_0\pi_{10})}
= \frac{\pi_{11} \pi_{00}}{\pi_{01} \pi_{10}} =OR^{old} .
\end{eqnarray*}

For additional help with interpretation, Yule proposed two transforms
for the odds ratio, the first in \citep{yule1900association},

\begin{eqnarray*}
\frac{OR-1}{OR+1},
\end{eqnarray*}

and the second in \citep{yule1912methods},

\begin{eqnarray*}
\frac{\sqrt{OR}-1}{\sqrt{OR}+1}.
\end{eqnarray*}

Although these statistics provide the same information as is the
original odds ration \(OR\), they have the advantage of taking values in
the interval \([-1,1]\), making them easier to interpret.

In a later section, we will also see that the marginal distributions
have no effect on the Fr\(\acute{e}\)chet-H\(\ddot{o}\)effding of the
tetrachoric correlation, another measure of association, see also,
\citep{joe2014dependence}, page 48.

\[
{\small \begin{matrix}
\begin{array}{l|rr|r} 
    \hline
                  & \text{Fire5} & & \\
\text{NoClaimCredit} & 0     & 1     & \text{Total} \\
  \hline
           0  & 1611  & 2175  & 3786 \\
           1  & 897   & 956   & 1853 \\
    \hline
    \text{Total}    & 2508  & 3131  & 5639 \\
   \hline
\end{array}
\end{matrix}}
\]

\protect\hyperlink{tab:14.2}{Table 14.2} : 2 \(\times\) 2 table of
counts for \emph{Fire5} and \emph{NoClaimCredit} variables from LGPIF
data.

From \protect\hyperlink{tab:14.2}{Table 14.2},
\(OR(\pi_{11})=\frac{1611(956)}{897(2175)}=0.79\). You can obtain the
\(OR(\pi_{11})\), using the \texttt{oddsratio()} function from the
\texttt{epitools} library in \texttt{R}. From the output below,
\(OR(\pi_{11})=0.79\) for the binary variables \emph{NoClaimCredit} and
\emph{Fier5} from the LGPIF data.

\subsubsection{Categorical Variables}\label{categorical-variables}

More generally, let \((X,Y)\) be a bivariate pair having \(ncat_X\) and
\(ncat_Y\) numbers of categories, respectively. For a two-way table of
counts, let \(n_{jk}\) be the number in the \(j\)th row, \(k\) column.
Let \(n_{j\centerdot}\) be the row margin total, \(n_{\centerdot k}\) be
the column margin total and \(n=\sum_{j,k} n_{j,k}\). Define Pearson
chi-square statistic as

\begin{eqnarray*}
\chi^2 = \sum_{jk} \frac{(n_{jk}- n_{j\centerdot}n_{\centerdot k}/n)^2}{n_{j\centerdot}n_{\centerdot k}/n} .
\end{eqnarray*}

The likelihood ratio test statistic is

\begin{eqnarray*}
G^2 = 2 \sum_{jk} n_{jk} \ln\frac{n_{jk}}{n_{j\centerdot}n_{\centerdot k}/n} .
\end{eqnarray*}

Under the assumption of independence, both \(\chi^2\) and \(G^2\) have
an asymptotic chi-square distribution with \((ncat_X-1)(ncat_Y-1)\)
degrees of freedom.

To help see what these statistics are estimating, let
\(\pi_{jk} = \Pr(X=j, Y=k)\) and let \(\pi_{X,j}=\Pr(X=j)\) and
similarly for \(\pi_{Y,k}\). Assuming that \(n_{jk}/n \approx \pi_{jk}\)
for large \(n\) and similarly for the marginal probabilities, we have

\begin{eqnarray*}
\frac{\chi^2}{n} \approx \sum_{jk} \frac{(\pi_{jk}- \pi_{X,j}\pi_{Y,k})^2}{\pi_{X,j}\pi_{Y,k}}
\end{eqnarray*}

and

\begin{eqnarray*}
\frac{G^2}{n} \approx 2 \sum_{jk} \pi_{jk} \ln\frac{\pi_{jk}}{\pi_{X,j}\pi_{Y,k}} .
\end{eqnarray*}

Under the null hypothesis of independence, we have
\(\pi_{jk} =\pi_{X,j}\pi_{Y,k}\) and it is clear from these
approximations that we anticipate that these statistics will be small
under this hypothesis.

Classical approaches, as described in \citep{bishop1975discrete} (page
374), distinguish between tests of independence and measures of
associations. The former are designed to detect whether a relationship
exists whereas the latter are meant to assess the type and extent of a
relationship. We acknowledge these differing purposes but also less
concerned with this distinction for actuarial applications.

\[
{\small \begin{matrix}
\begin{array}{l|rr} 
    \hline
                  & \text{NoClaimCredit} &  \\
       \text{EntityType} & 0     & 1      \\
  \hline
            \text{City}    & 644  & 149 \\
          \text{County}    & 310  &  18 \\
            \text{Misc}    & 336  & 273 \\
          \text{School}    & 1103 & 494 \\
           \text{Town}     & 492  & 479 \\
         \text{Village}    & 901  & 440 \\
   \hline
\end{array}
\end{matrix}}
\]

\protect\hyperlink{tab:14.3}{Table 14.3} : Two-way table of counts for
\emph{EntityType} and \emph{NoClaimCredit} variables from LGPIF data.

You can obtain the Pearson chi-square statistic, using the
\texttt{chisq.test()} function from the \texttt{MASS} library in
\texttt{R}. Here, we test whether the \emph{EntityType} variable is
independent of \emph{NoClaimCredit} variable using
\protect\hyperlink{tab:14.3}{Table 14.3}.

As the p-value is less than the .05 significance level, we reject the
null hypothesis that the \emph{EntityType} is independent of
\emph{NoClaimCredit}.

Furthermore, you can obtain the likelihood ratio test statistic , using
the \texttt{likelihood.test()} function from the \texttt{Deducer}
library in \texttt{R}. From below, we test whether the \emph{EntityType}
variable is independent of \emph{NoClaimCredit} variable from the LGPIF
data. Same conclusion is drawn as the Pearson chi-square test.

\subsection{Ordinal Variables}\label{ordinal-variables}

As the analyst moves from the continuous to the nominal scale, there are
two main sources of loss of information \citep{bishop1975discrete} (page
343). The first is breaking the precise continuous measurements into
groups. The second is losing the ordering of the groups. So, it is
sensible to describe what we can do with variables that in discrete
groups but where the ordering is known.

As described in Section \ref{S:QuaVar}, ordinal variables provide a
clear ordering of levels of a variable but distances between levels are
unknown. Associations have traditionally been quantified parametrically
using normal-based correlations and nonparametrically using Spearman
correlations with tied ranks.

\subsubsection{Parametric Approach Using Normal Based
Correlations}\label{parametric-approach-using-normal-based-correlations}

Refer to page 60, Section 2.12.7 of \citep{joe2014dependence}. Let
\((y_1,y_2)\) be a bivariate pair with discrete values on
\(m_1, \ldots, m_2\). For a two-way table of ordinal counts, let
\(n_{st}\) be the number in the \(s\)th row, \(t\) column. Let
\((n_{m_1\centerdot}, \ldots, n_{m_2\centerdot})\) be the row margin
total, \((n_{\centerdot m_1}, \ldots, n_{\centerdot m_2})\) be the
column margin total and \(n=\sum_{s,t} n_{s,t}\).

Let \(\hat{\xi}_{1s} = \Phi^{-1}((n_{m_1}+\cdots+n_{s\centerdot})/n)\)
for \(s=m_1, \ldots, m_2\) be a cutpoint and similarly for
\(\hat{\xi}_{2t}\). The polychoric correlation, based on a two-step
estimation procedure, is

\begin{eqnarray*}
\begin{array}{cr}
  \hat{\rho_N} &=\text{argmax}_{\rho}
  \sum_{s=m_1}^{m_2} \sum_{t=m_1}^{m_2} n_{st} \log\left\{
    \Phi_2(\hat{\xi}_{1s}, \hat{\xi}_{2t};\rho)
    -\Phi_2(\hat{\xi}_{1,s-1}, \hat{\xi}_{2t};\rho) \right.\\
   & \left. -\Phi_2(\hat{\xi}_{1s}, \hat{\xi}_{2,t-1};\rho)
    +\Phi_2(\hat{\xi}_{1,s-1}, \hat{\xi}_{2,t-1};\rho)
    \right\}
\end{array}
\end{eqnarray*}

It is called a tetrachoric correlation for binary variables.

\[
{\small \begin{matrix}
\begin{array}{l|rr} 
    \hline
                  & \text{NoClaimCredit} &  \\
\text{AlarmCredit} & 0     & 1      \\
  \hline
          1  & 1669  &  942   \\
          2  &    121 &  118 \\
          3  &  195  &   132 \\
          4 &  1801  &   661 \\
   \hline
\end{array}
\end{matrix}}
\]

\protect\hyperlink{tab:14.4}{Table 14.4} : Two-way table of counts for
\emph{AlarmCredit} and \emph{NoClaimCredit} variables from LGPIF data.

You can obtain the polychoric or tetrachoric correlation using the
\texttt{polychoric()} or \texttt{tetrachoric()} function from the
\texttt{psych} library in \texttt{R}. The polychoric correlation is
illustrated using \protect\hyperlink{tab:14.4}{Table 14.4}.
\(\hat{\rho}_N=-0.14\), which means that there is a negative
relationship between \emph{AlarmCredit} and \emph{NoClaimCredit}.

\subsection{Interval Variables}\label{interval-variables}

As described in Section \ref{S:QuanVar}, interval variables provide a
clear ordering of levels of a variable and the numerical distance
between any two levels of the scale can be readily interpretable. For
example, drivers age group variable is an interval variable.

For measuring association, both the continuous variable and ordinal
variable approaches make sense. The former takes advantage of knowledge
of the ordering although assumes continuity. The latter does not rely on
the continuity but also does not make use of the information given by
the distance between scales.

\subsection{Discrete and Continuous
Variables}\label{discrete-and-continuous-variables}

The polyserial correlation is defined similarly, when one variable
(\(y_1\)) is continuous and the other (\(y_2\)) ordinal. Define \(z\) to
be the normal score of \(y_1\). The polyserial correlation is

\begin{eqnarray*}
\hat{\rho_N} = \text{argmax}_{\rho}
\sum_{i=1}^n \log\left\{ \phi(z_{i1})\left[
\Phi(\frac{\hat{\xi}_{2,y_{i2}} - \rho z_{i1}}
{(1-\rho^2)^{1/2}})
-\Phi(\frac{\hat{\xi}_{2,y_{i2-1}} - \rho z_{i1}}
{(1-\rho^2)^{1/2}})
\right]
\right\}
\end{eqnarray*}

The biserial correlation is defined similarly, when one variable is
continuous and the other binary.

\[
{\small \begin{matrix}
\begin{array}{l|r|r} 
    \hline
\text{NoClaimCredit} & \text{Mean}     &\text{Total}       \\
 & \text{Claim}     &\text{Claim}       \\
  \hline
          0  & 22,505  &  85,200,483   \\
          1  &    6,629 &  12,282,618 \\
   \hline
\end{array}
\end{matrix}}
\]

\protect\hyperlink{tab:14.5}{Table 14.5} : Summary of \emph{Claim} by
\emph{NoClaimCredit} variable from LGPIF data.

You can obtain the polyserial or biserial correlation using the
\texttt{polyserial()} or \texttt{biserial()} function from the
\texttt{psych} library in \texttt{R}. \protect\hyperlink{tab:14.5}{Table
14.5} gives the summary of \emph{Claim} by \emph{NoClaimCredit} and the
biserial correlation is illustrated using \texttt{R} code below. The
\(\hat{\rho}_N=-0.04\) which means that there is a negative correlation
between \emph{Claim} and \emph{NoClaimCredit}.

\section{Introduction to Copulas}\label{S:Copula}

Copula functions are widely used in statistics and actuarial science
literature for dependency modeling.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Describe a multivariate distribution function in terms of a copula
  function.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

A copula is a multivariate distribution function with uniform marginals.
Specifically, let \(U_1, \ldots, U_p\) be \(p\) uniform random variables
on \((0,1)\). Their distribution function
\[{C}(u_1, \ldots, u_p) = \Pr(U_1 \leq u_1, \ldots, U_p \leq u_p),\]

is a copula. We seek to use copulas in applications that are based on
more than just uniformly distributed data. Thus, consider arbitrary
marginal distribution functions \({F}_1(y_1)\),\ldots{},\({F}_p(y_p)\).
Then, we can define a multivariate distribution function using the
copula such that \[{F}(y_1, \ldots, y_p)= {C}({F}_1(y_1), \ldots,
{F}_p(y_p)).\]

Here, \(F\) is a multivariate distribution function in this equation.
Sklar (1959) showed that \(any\) multivariate distribution function
\(F\), can be written in the form of this equation, that is, using a
copula representation.

Sklar also showed that, if the marginal distributions are continuous,
then there is a unique copula representation. In this chapter we focus
on copula modeling with continuous variables. For discrete case, readers
can see \citep{joe2014dependence} and \citep{genest2007methods}.

For bivariate case, \(p=2\) , the distribution function of two random
variables can be written by the bivariate copula function:
\[{C}(u_1, \, u_2) = \Pr(U_1 \leq u_1, \, U_2 \leq
u_2),\]

\[{F}(y_1, \, y_2)= {C}({F}_1(y_1), \,
{F}_p(y_2)).\]

To give an example for bivariate copula, we can look at Frank's (1979)
copula. The equation is

\[{C}(u_1,u_2) = \frac{1}{\theta} \ln \left( 1+ \frac{ (\exp(\theta
u_1) -1)(\exp(\theta u_2) -1)} {\exp(\theta) -1} \right).\]

This is a bivariate distribution function with its domain on the unit
square \([0,1]^2.\) Here \(\theta\) is dependence parameter and the
range of dependence is controlled by the parameter \(\theta\). Positive
association increases as \(\theta\) increases and this positive
association can be summarized with Spearman's rho (\(\rho\)) and
Kendall's tau (\(\tau\)). Frank's copula is one of the commonly used
copula functions in the copula literature. We will see other copula
functions in Section \ref{S:CopTyp}.

\section{Application Using Copulas}\label{S:CopAppl}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Discover dependence structure between random variables
\item
  Model the dependence with a copula function
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

This section analyzes the insurance \emph{losses } and \emph{expenses }
data with the statistical programming \texttt{R}. This data set was
introduced in \citep{frees1998understanding} and is now readily
available in the \texttt{copula} package. The model fitting process is
started by marginal modeling of two variables (\(loss\) and
\(expense\)). Then we model the joint distribution of these marginal
outcomes.

\subsection{Data Description}\label{data-description}

We start with getting a sample (\(n = 1500\)) from the whole data. We
consider first two variables of the data; \emph{losses} and
\emph{expenses}.

\begin{itemize}
\tightlist
\item
  \emph{losses }: general liability claims from Insurance Services
  Office, Inc. (ISO)
\item
  \emph{expenses }: ALAE, specifically attributable to the settlement of
  individual claims (e.g.~lawyer's fees, claims investigation expenses)
\end{itemize}

To visualize the relationship between \emph{losses } and \emph{expenses
} (ALAE), scatterplots in figure \ref{fig:Scatter} are created on the
real dollar scale and on the log scale.

\subsection{Marginal Models}\label{marginal-models}

We first examine the marginal distributions of \emph{losses } and
\emph{expenses } before going through the joint modeling. The histograms
show that both \emph{losses } and \emph{expenses } are right-skewed and
fat-tailed.

For marginal distributions of losses and expenses, we consider a
Pareto-type distribution, namely a Pareto type II with distribution
function

\[ F(y)=1- \left( 1 + \frac{y}{\theta} \right) ^{-\alpha},\] where
\(\theta\) is the scale parameter and \(\alpha\) is the shape parameter.

The marginal distributions of losses and expenses are fitted with
maximum likelihood. Specifically, we use the \(vglm\) function from the
\texttt{R\ VGAM} package. Firstly, we fit the marginal distribution of
\emph{expenses }.

We repeat this procedure to fit the marginal distribution of the
\emph{loss} variable. Because the loss data also seems right-skewed and
heavy-tail data, we also model the marginal distribution with Pareto II
distribution.

To visualize the fitted distribution of \emph{expenses } and \emph{loss}
variables, we use the estimated parameters and plot the corresponding
distribution function and density function. For more details on marginal
model selection, see Chapter \ref{C:ModelSelection}.

\subsection{Probability Integral
Transformation}\label{probability-integral-transformation}

The probability integral transformation shows that any continuous
variable can be mapped to a \(U(0,1)\) random variable via its
distribution function.

Given the fitted Pareto II distribution, the variable \emph{expenses} is
transformed to the variable \(u_1\), which follows a uniform
distribution on \([0,1]\):

\[u_1 = 1 - \left( 1 + \frac{ALAE}{\hat{\theta}} \right)^{-\hat{\alpha}}.\]

After applying the probability integral transformation to \emph{expenses
} variable, we plot the histogram of \emph{Transformed Alae } in Figure
\ref{fig:Hist}.

\begin{figure}

{\centering \includegraphics{LossDataAnalytics_files/figure-latex/Hist-1} 

}

\caption{Histogram of Transformed Alae}\label{fig:Hist}
\end{figure}

After fitting process,the variable \emph{loss} is also transformed to
the variable \(u_2\), which follows a uniform distribution on \([0,1]\).
We plot the histogram of \emph{Transformed Loss }. As an alternative,
the variable \emph{loss} is transformed to \(normal\) \(scores\) with
the quantile function of standard normal distribution. As we see in
Figure \ref{fig:Hist2}, normal scores of the variable \emph{loss} are
approximately marginally standard normal.

\begin{figure}
\centering
\includegraphics{LossDataAnalytics_files/figure-latex/Hist2-1.pdf}
\caption{\label{fig:Hist2}Histogram of Transformed Loss. The left-hand panel
shows the distribution of probability integral transformed losses. The
right-hand panel shows the distribution for the corresponding normal
scores.}
\end{figure}

\subsection{Joint Modeling with Copula
Function}\label{joint-modeling-with-copula-function}

Before jointly modeling losses and expenses, we draw the scatterplot of
transformed variables \((u_1, u_2)\) and the scatterplot of normal
scores in Figure \ref{fig:Scatter2}.

Then we calculate the Spearman's rho between these two uniform random
variables.

\begin{figure}
\centering
\includegraphics{LossDataAnalytics_files/figure-latex/Scatter2-1.pdf}
\caption{\label{fig:Scatter2}Left: Scatter plot for transformed variables.
Right:Scatter plot for normal scores}
\end{figure}

Scatter plots and Spearman's rho correlation value (0.451) shows us
there is a positive dependency between these two uniform random
variables. It is more clear to see the relationship with normal scores
in the second graph. To learn more details about normal scores and their
applications in copula modeling, see \citep{joe2014dependence}.

\((U_1, U_2)\), (\(U_1 = F_1(ALAE)\) and \(U_2=F_2(LOSS)\)), is fit to
Frank's copula with maximum likelihood method.

The fitted model implies that losses and expenses are positively
dependent and their dependence is significant.

We use the fitted parameter to update the Frank's copula. The Spearman's
correlation corresponding to the fitted copula parameter(3.114) is
calculated with the \texttt{rho} function. In this case, the Spearman's
correlation coefficient is 0.462, which is very close to the sample
Spearman's correlation coefficient; 0.452.

To visualize the fitted Frank's copula, the distribution function and
density function perspective plots are drawn in Figure
\ref{fig:FrankCop}.

\begin{figure}
\centering
\includegraphics{LossDataAnalytics_files/figure-latex/FrankCop-1.pdf}
\caption{\label{fig:FrankCop}Left: Plot for distribution function for Franks
Copula. Right:Plot for density function for Franks Copula}
\end{figure}

Frank's copula models positive dependence for this data set, with
\(\theta=3.114\). For Frank's copula, the dependence is related to
values of \(\theta\). That is:

\begin{itemize}
\tightlist
\item
  \(\theta=0\): independent copula
\item
  \(\theta>0\): positive dependence
\item
  \(\theta<0\): negative dependence
\end{itemize}

\section{Types of Copulas}\label{S:CopTyp}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to:

\begin{itemize}
\tightlist
\item
  Define the basic families of the copula functions
\item
  Calculate the association coefficients by the help of copula functions
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

There are several families of copulas have been described in the
literature. Two main families of the copula families are the
\textbf{Archimedian} and \textbf{Elliptical} copulas.

\subsection{Elliptical Copulas}\label{elliptical-copulas}

Elliptical copulas are constructed from elliptical distributions. This
copula decompose (multivariate) elliptical distributions into their
univariate elliptical marginal distributions by Sklar's theorem
\citep{hofertelements}.

Properties of elliptical copulas are typically obtained from the
properties of corresponding elliptical distributions
\citep{hofertelements}.

For example, the normal distribution is a special type of elliptical
distribution. To introduce the elliptical class of copulas, we start
with the familiar multivariate normal distribution with probability
density function
\[\phi_N (\mathbf{z})= \frac{1}{(2 \pi)^{p/2}\sqrt{\det \boldsymbol \Sigma}}
\exp\left( -\frac{1}{2} \mathbf{z}^{\prime} \boldsymbol
\Sigma^{-1}\mathbf{z}\right).\]

Here, \(\boldsymbol \Sigma\) is a correlation matrix, with ones on the
diagonal. Let \(\Phi\) and \(\phi\) denote the standard normal
distribution and density functions. We define the Gaussian (normal)
copula density function as

\[{c}_N(u_1,  \ldots, u_p) = \phi_N \left(\Phi^{-1}(u_1), \ldots, \Phi^{-1}(u_p) \right) \prod_{j=1}^p \frac{1}{\phi(\Phi^{-1}(u_j))}.\]

As with other copulas, the domain is the unit cube \([0,1]^p\).

Specifically, a \(p\)-dimensional vector \({z}\) has an \({elliptical}\)
\({distribution}\) if the density can be written as
\[h_E (\mathbf{z})= \frac{k_p}{\sqrt{\det \boldsymbol \Sigma}}
g_p \left( \frac{1}{2} (\mathbf{z}- \boldsymbol \mu)^{\prime}
\boldsymbol \Sigma^{-1}(\mathbf{z}- \boldsymbol \mu) \right).\]

We will use elliptical distributions to generate copulas. Because
copulas are concerned primarily with relationships, we may restrict our
considerations to the case where \(\mu = \mathbf{0}\) and
\(\boldsymbol \Sigma\) is a correlation matrix. With these restrictions,
the marginal distributions of the multivariate elliptical copula are
identical; we use \(H\) to refer to this marginal distribution function
and \(h\) is the corresponding density. This marginal density is
\(h(z) = k_1 g_1(z^2/2).\)

We are now ready to define the \(elliptical\) \(copula\), a function
defined on the unit cube \([0,1]^p\) as

\[{c}_E(u_1,  \ldots, u_p) = h_E \left(H^{-1}(u_1), \ldots,
H^{-1}(u_p) \right) \prod_{j=1}^p \frac{1}{h(H^{-1}(u_j))}.\]

In the elliptical copula family, the function \(g_p\) is known as a
\emph{generator} in that it can be used to generate alternative
distributions.

\[
\small\begin{array}{lc}
\hline & Generator \\
 Distribution &  \mathrm{g}_p(x)  \\
\hline
 \text{Normal distribution} &  e^{-x}\\
 \text{t-distribution with r degrees of freedom} &   (1+2x/r)^{-(p+r)/2}\\
 \text{Cauchy} &  (1+2x)^{-(p+1)/2}\\
\text{Logistic} &  e^{-x}/(1+e^{-x})^2\\
 \text{Exponential power} &   \exp(-rx^s)\\
\hline
\end{array}
\]

\protect\hyperlink{tab:14.6}{Table 14.6} : Distribution and Generator
Functions (\(\mathrm{g}_p(x)\)) for Selected Elliptical Copulas

Most empirical work focuses on the normal copula and \(t\)-copula. That
is, \(t\)-copulas are useful for modeling the dependency in the tails of
bivariate distributions, especially in financial risk analysis
applications.

The \(t\)-copulas with same association parameter in varying the degrees
of freedom parameter show us different tail dependency structures. For
more information on about \(t\)-copulas readers can see
\citep{joe2014dependence}, \citep{hofertelements}.

\subsection{Archimedian Copulas}\label{archimedian-copulas}

This class of copulas are constructed from a \(generator\)
function,which is \(\mathrm{g}(\cdot)\) is a convex, decreasing function
with domain {[}0,1{]} and range \([0, \infty)\) such that
\(\mathrm{g}(0)=0\). Use \(\mathrm{g}^{-1}\) for the inverse function of
\(\mathrm{g}\). Then the function

\[\mathrm{C}_{\mathrm{g}}(u_1, \ldots, u_p) = \mathrm{g}^{-1} \left(\mathrm{g}(u_1)+ \cdots + \mathrm{g}(u_p) \right)\]

is said to be an \emph{Archimedean} copula. The function \(\mathrm{g}\)
is known as the \emph{generator} of the copula
\(\mathrm{C}_{\mathrm{g}}\).

For bivariate case; \(p=2\) , Archimedean copula function can be written
by the function

\[\mathrm{C}_{\mathrm{g}}(u_1, \, u_2) = \mathrm{g}^{-1} \left(\mathrm{g}(u_1) + \mathrm{g}(u_2) \right).\]

Some important special cases of Archimedean copulas are Frank copula,
Clayton/Cook-Johnson copula, Gumbel/Hougaard copula. This copula classes
are derived from different generator functions.

We can remember that we mentioned about Frank's copula with details in
Section \ref{S:Copula} and in Section \ref{S:CopAppl}. Here we will
continue to express the equations for Clayton copula and Gumbel/Hougaard
copula.

\subsubsection{Clayton Copula}\label{clayton-copula}

For \(p=2\), the Clayton copula is parameterized by
\(\theta \in [-1,\infty)\) is defined by
\[C_{\theta}^C(u)=\max\{u_1^{-\theta}+u_2^{-\theta}-1,0\}^{1/\theta}, \quad u\in[0,1]^2.\]

This is a bivariate distribution function of Clayton copula defined in
unit square \([0,1]^2.\) The range of dependence is controlled by the
parameter \(\theta\) as the same as Frank copula.

\subsubsection{Gumbel-Hougaard copula}\label{gumbel-hougaard-copula}

The Gumbel-Hougaarg copula is parametrized by \(\theta \in [1,\infty)\)
and defined by
\[C_{\theta}^{GH}(u)=\exp\left(-\left(\sum_{i=1}^2 (-\log u_i)^{\theta}\right)^{1/\theta}\right), \quad u\in[0,1]^2.\]

Readers seeking deeper background on Archimedean copulas can see
\citet{joe2014dependence}, \citet{frees1998understanding}, and
\citet{genest1986bivariate}.

\subsection{Properties of Copulas}\label{properties-of-copulas}

\subsubsection{Bounds on Association}\label{bounds-on-association}

Like all multivariate distribution functions, copulas are bounded. The
Fr\('{e}\)chet-Hoeffding bounds are

\[\max( u_1 +\cdots+ u_p + p -1, 0) \leq  \mathrm{C}(u_1,  \ldots, u_p) \leq \min (u_1,  \ldots,u_p).\]

To see the right-hand side of the equation, note that
\[\mathrm{C}(u_1,\ldots, u_p) = \Pr(U_1 \leq u_1, \ldots, U_p \leq u_p) \leq  \Pr(U_j \leq u_j)\],
for \(j=1,\ldots,p\). The bound is achieved when \(U_1 = \cdots = U_p\).
To see the left-hand side when \(p=2\), consider \(U_2=1-U_1\). In this
case, if \(1-u_2 < u_1\) then
\(\Pr(U_1 \leq u_1, U_2 \leq u_2) = \Pr ( 1-u_2 \leq U_1 < u_1) =u_1+u_2-1.\)
\citep{nelsen1997introduction}

The product copula is \(\mathrm{C}(u_1,u_2)=u_1u_2\) is the result of
assuming independence between random variables.

The lower bound is achieved when the two random variables are perfectly
negatively related (\(U_2=1-U_1\)) and the upper bound is achieved when
they are perfectly positively related (\(U_2=U_1\)).

We can see The Frechet-Hoeffding bounds for two random variables in the
Figure \ref{fig:Bounds}.

\begin{figure}
\centering
\includegraphics{LossDataAnalytics_files/figure-latex/Bounds-1.pdf}
\caption{\label{fig:Bounds}Perfect Positive and Perfect negative dependence
plots}
\end{figure}

\subsubsection{Measures of Association}\label{measures-of-association}

Schweizer and Wolff (1981) established that the copula accounts for all
the dependence between two random variables, \(Y_1\) and \(Y_2\), in the
following sense. Consider m\(_1\) and m\(_2\), strictly increasing
functions. Thus, the manner in which \(Y_1\) and \(Y_2\) ``move
together'' is captured by the copula, regardless of the scale in which
each variable is measured.

Schweizer and Wolff also showed the two standard nonparametric measures
of association could be expressed solely in terms of the copula
function. Spearman's correlation coefficient is given by

\[= 12 \int \int \left\{\mathrm{C}(u,v) - uv \right\} du dv.\]

Kendall's tau is given by

\[= 4 \int \int \mathrm{C}(u,v)d\mathrm{C}(u,v) - 1 .\]

For these expressions, we assume that \(Y_1\) and \(Y_2\) have a jointly
continuous distribution function. Further, the definition of Kendall's
tau uses an independent copy of (\(Y_1\), \(Y_2\)), labeled
(\(Y_1^{\ast}\), \(Y_2^{\ast}\)), to define the measure of
``concordance.'' the widely used Pearson correlation depends on the
margins as well as the copula. Because it is affected by non-linear
changes of scale.

\subsubsection{Tail Dependency}\label{tail-dependency}

There are some applications in which it is useful to distinguish by the
part of the distribution in which the association is strongest. For
example, in insurance it is helpful to understand association among the
largest losses, that is, association in the right tails of the data.

To capture this type of dependency, we use the right-tail concentration
function. The function is

\[R(z) = \frac{\Pr(U_1 >z, U_2 > z)}{1-z} =\Pr(U_1 > z | U_2 > z) =\frac{1 - 2z + \mathrm{C}(z,z)}{1-z} .\]

From this equation , \(R(z)\) will equal to \(z\) under independence.
Joe (1997) uses the term ``upper tail dependence parameter'' for
\(R = \lim_{z \rightarrow 1} R(z)\). Similarly, the left-tail
concentration function is

\[L(z) = \frac{\Pr(U_1 \leq z, U_2 \leq z)}{z}=\Pr(U_1 \leq z | U_2 \leq z) =\frac{ \mathrm{C}(z,z)}{1-z}.\]

Tail dependency concentration function captures the probability of two
random variables both catching up extreme values.

We calculate the left and right tail concentration functions for four
different types of copulas; Normal, Frank,Gumbel and t copula. After
getting tail concentration functions for each copula, we show
concentration function's values for these four copulas in
\protect\hyperlink{tab:14.7}{Table 14.7}. As in \citet{venter2002tails},
we show \(L(z)\) for \(z\leq 0.5\) and \(R(z)\) for \(z>0.5\) in the
tail dependence plot in Figure \ref{fig:DepTails}. We interpret the tail
dependence plot, to mean that both the Frank and Normal copula exhibit
no tail dependence whereas the \(t\) and the Gumbel may do so. The \(t\)
copula is symmetric in its treatment of upper and lower tails.

\[
{\small \begin{matrix}
\begin{array}{l|rr} 
    \hline
\text{Copula} & \text{Lower}    & \text{Upper}     \\
\hline
\text{Frank}  & 0  & 0   \\
\text{Gumbel}  & 0   & 0.74    \\
\text{Normal}  & 0   & 0    \\
\text{t}  & 0.10   & 0.10    \\
   \hline
\end{array}
\end{matrix}}
\]

\protect\hyperlink{tab:14.7}{Table 14.7} : Tail concentration function
values for different copulas

\begin{figure}
\centering
\includegraphics{LossDataAnalytics_files/figure-latex/DepTails-1.pdf}
\caption{\label{fig:DepTails}Tail dependence plots}
\end{figure}

\section{Why is Dependence Modeling Important?}\label{S:CopImp}

Dependence Modeling is important because it enables us to understand the
dependence structure by defining the relationship between variables in a
dataset. In insurance, ignoring dependence modeling may not impact
pricing but could lead to misestimation of required capital to cover
losses. For instance, from Section \ref{S:CopAppl} , it is seen that
there was a positive relationship between \emph{Loss} and
\emph{Expense}. This means that, if there is a large loss then we expect
expenses to be large as well and ignoring this relationship could lead
to misestimation of reserves.

To illustrate the importance of dependence modeling, we refer you back
to Portfolio Management example in Chapter 6 that assumed that the
property and liability risks are independent. Here, we incorporate
dependence by allowing the 4 lines of business to depend on one another
through a Gaussian copula. In \protect\hyperlink{tab:14.8}{Table 14.8},
we show that dependence affects the portfolio quantiles (\(VaR_q\)),
although not the expect value. For instance , the \(VaR_{0.99}\) for
total risk which is the amount of capital required to ensure, with a
\(99\%\) degree of certainty that the firm does not become technically
insolvent is higher when we incorporate dependence. This leads to less
capital being allocated when dependence is ignored and can cause
unexpected solvency problems.

\[
{\small \begin{matrix}
\begin{array}{l|rrrr} 
    \hline
 \text{Independent} &\text{Expected}   & VaR_{0.9}  & VaR_{0.95}  & VaR_{0.99}  \\
                   &\text{Value}      &            &             &             \\
     \hline              
\text{Retained}    & 269              &  300       & 300         & 300         \\
\text{Insurer}     & 2,274            &  4,400     & 6,173       & 11,859      \\
\text{Total}       & 2,543            &  4,675     & 6,464       & 12,159      \\
   \hline
\text{Gaussian Copula}&\text{Expected}& VaR_{0.9}  & VaR_{0.95}  & VaR_{0.99}  \\
                      &\text{Value}    &           &             &              \\
     \hline                      
\text{Retained}       & 269            &  300      & 300         &  300         \\
\text{Insurer}        & 2,340          &  4,988    & 7,339       & 14,905       \\
\text{Total}          & 2,609          &  5,288    & 7,639       & 15,205       \\
   \hline
\end{array}
\end{matrix}}
\]

\protect\hyperlink{tab:14.8}{Table 14.8} : Results for portfolio
expected value and quantiles (\(VaR_q\))

\section{Further Resources and
Contributors}\label{Dep:further-reading-and-resources}

\subsubsection*{Contributors}\label{contributors-8}
\addcontentsline{toc}{subsubsection}{Contributors}

\begin{itemize}
\tightlist
\item
  \textbf{Edward W. (Jed) Frees} and \textbf{Nii-Armah Okine},
  University of Wisconsin-Madison, and \textbf{Emine Selin Sarıdaş},
  Mimar Sinan University, are the principal authors of the initial
  version of this chapter. Email:
  \href{mailto:jfrees@bus.wisc.edu}{\nolinkurl{jfrees@bus.wisc.edu}} for
  chapter comments and suggested improvements.
\end{itemize}

\subsection*{TS 14.A. Other Classic Measures of Scalar
Associations}\label{ts-14.a.-other-classic-measures-of-scalar-associations}
\addcontentsline{toc}{subsection}{TS 14.A. Other Classic Measures of
Scalar Associations}

\subsubsection*{TS 14.A.1. Blomqvist's
Beta}\label{ts-14.a.1.-blomqvists-beta}
\addcontentsline{toc}{subsubsection}{TS 14.A.1. Blomqvist's Beta}

\citet{blomqvist1950measure} developed a measure of dependence now known
as Blomqvist's beta, also called the \emph{median concordance
coefficient} and the \emph{medial correlation coefficient}. Using
distribution functions, this parameter can be expressed as

\begin{equation*}
\beta = 4F\left(F^{-1}_X(1/2),F^{-1}_Y(1/2) \right) - 1.
\end{equation*}

That is, first evaluate each marginal at its median (\(F^{-1}_X(1/2)\)
and \(F^{-1}_Y(1/2)\), respectively). Then, evaluate the bivariate
distribution function at the two medians. After rescaling (multiplying
by 4 and subtracting 1), the coefficient turns out to have a range of
\([-1,1]\), where 0 occurs under independence.

Like Spearman's rho and Kendall's tau, an estimator based on ranks is
easy to provide. First write
\(\beta = 4C(1/2,1/2)-1 = 2\Pr((U_1-1/2)(U_2-1/2))-1\) where
\(U_1, U_2\) are uniform random variables. Then, define

\begin{equation*}
\hat{\beta} = \frac{2}{n} \sum_{i=1}^n I\left( (R(X_{i})-\frac{n+1}{2})(R(Y_{i})-\frac{n+1}{2}) \ge 0 \right)-1 .
\end{equation*}

See, for example, \citep{joe2014dependence}, page 57 or
\citep{hougaard2000analysis}, page 135, for more details.

Because Blomqvist's parameter is based on the center of the
distribution, it is particularly useful when data are censored; in this
case, information in extreme parts of the distribution are not always
reliable. How does this affect a choice of association measures? First,
recall that association measures are based on a bivariate distribution
function. So, if one has knowledge of a good approximation of the
distribution function, then calculation of an association measure is
straightforward in principle. Second, for censored data, bivariate
extensions of the univariate Kaplan-Meier distribution function
estimator are available. For example, the version introduced in
\citep{dabrowska1988kaplan} is appealing. However, because of instances
when large masses of data appear at the upper range of the data, this
and other estimators of the bivariate distribution function are
unreliable. This means that, summary measures of the estimated
distribution function based on Spearman's rho or Kendall's tau can be
unreliable. For this situation, Blomqvist's beta appears to be a better
choice as it focuses on the center of the distribution.
\citep{hougaard2000analysis}, Chapter 14, provides additional
discussion.

You can obtain the Blomqvist's beta, using the \texttt{betan()} function
from the \texttt{copula} library in \texttt{R}. From below,
\(\beta=0.3\) between the \emph{Coverage} rating variable in millions of
dollars and \emph{Claim} amount variable in dollars.

In addition,to show that the Blomqvist's beta is invariate under
strictly increasing transformations , \(\beta=0.3\) between the
\emph{Coverage} rating variable in logarithmic millions of dollars and
\emph{Claim} amount variable in dollars.

\subsubsection*{TS 14.A.2. Nonparametric Approach Using Spearman
Correlation with Tied
Ranks}\label{ts-14.a.2.-nonparametric-approach-using-spearman-correlation-with-tied-ranks}
\addcontentsline{toc}{subsubsection}{TS 14.A.2. Nonparametric Approach
Using Spearman Correlation with Tied Ranks}

For the first variable, the average rank of observations in the \(s\)th
row is

\begin{equation*}
r_{1s} = n_{m_1\centerdot}+ \cdots+ n_{s-1,\centerdot}+ \frac{1}{2} \left(1+ n_{s\centerdot}\right)
\end{equation*}

and similarly
\(r_{2t} = \frac{1}{2} \left[(n_{\centerdot m_1}+ \cdots+ n_{\centerdot,s-1}+1)+ (n_{\centerdot m_1}+ \cdots+ n_{\centerdot s})\right]\).
With this, we have Spearman's rho with tied rank is

\begin{equation*}
\hat{\rho}_S = \frac{\sum_{s=m_1}^{m_2} \sum_{t=m_1}^{m_2} n_{st}(r_{1s} - \bar{r})(r_{2t} - \bar{r})}
{\left[\sum_{s=m_1}^{m_2}n_{s \centerdot}(r_{1s} - \bar{r})^2 \sum_{t=m_1}^{m_2} n_{\centerdot t}(r_{2t} - \bar{r})^2
\right]^2}
\end{equation*}

where the average rank is \(\bar{r} = (n+1)/2\).

Special Case: Binary Data. Here, \(m_1=0\) and \(m_2=1\). For the first
variable ranks, we have \(r_{10} = (1+n_{0\centerdot})/2\) and
\(r_{11} = (n_{0\centerdot}+1+n)/2\). Thus,
\(r_{10} -\bar{r}= (n_{0\centerdot}-n)/2\) and
\(r_{11}-\bar{r} = n_{0\centerdot}/2\). This means that we have
\(\sum_{s=0}^{1}n_{s\centerdot}(r_{1s} - \bar{r})^2 = n (n-n_{0\centerdot})n_{0\centerdot}/4\)
and similarly for the second variable. For the numerator, we have

\begin{eqnarray*}
\sum_{s=0}^{1}  \sum_{t=0}^{1} && n_{st}(r_{1s} - \bar{r})(r_{2t} - \bar{r})\\
&=& n_{00} \frac{n_{0\centerdot}-n}{2} \frac{n_{\centerdot 0}-n}{2}
+n_{01} \frac{n_{0\centerdot}-n}{2} \frac{n_{\centerdot 0}}{2}
+n_{10} \frac{n_{0\centerdot}}{2} \frac{n_{\centerdot 0}-n}{2}
+n_{11} \frac{n_{0\centerdot}}{2} \frac{n_{\centerdot 0}}{2} \\
&=& \frac{1}{4}(n_{00} (n_{0\centerdot}-n) (n_{\centerdot 0}-n)
+(n_{0\centerdot}-n_{00}) (n_{0\centerdot}-n)n_{\centerdot 0} \\
&&  ~ ~ ~ +(n_{\centerdot 0}-n_{00})  n_{0\centerdot}(n_{\centerdot 0}-n)
+(n-n_{\centerdot 0}-n_{0\centerdot}+n_{00}) n_{0\centerdot}n_{\centerdot 0} ) \\
&=& \frac{1}{4}(n_{00} n^2
- n_{0\centerdot} (n_{0\centerdot}-n)n_{\centerdot 0} \\
&& ~ ~ ~ +n_{\centerdot 0}  n_{0\centerdot}(n_{\centerdot 0}-n)
+(n-n_{\centerdot 0}-n_{0\centerdot}) n_{0\centerdot}n_{\centerdot 0} ) \\
&=& \frac{1}{4}(n_{00} n^2
- n_{0\centerdot}n_{\centerdot 0} (n_{0\centerdot}-n +n_{\centerdot 0}-n
+n-n_{\centerdot 0}-n_{0\centerdot}) \\
&=& \frac{n}{4}(n n_{00} - n_{0\centerdot}n_{\centerdot 0}) .
\end{eqnarray*}

This yields

\begin{eqnarray*}
\hat{\rho}_S &=& \frac{n(n n_{00} - n_{0\centerdot}n_{\centerdot 0})}
{4\sqrt{(n (n-n_{0\centerdot})n_{0\centerdot}/4)(n (n-n_{\centerdot 0})n_{\centerdot 0}/4)}} \\
&=& \frac{n n_{00} - n_{0\centerdot}n_{\centerdot 0}}
{\sqrt{ n_{0\centerdot} n_{\centerdot 0}(n-n_{0\centerdot}) (n-n_{\centerdot 0})}} \\
&=& \frac{n_{00} - n (1-\hat{\pi}_X)(1- \hat{\pi}_Y)}
{\sqrt{\hat{\pi}_X(1-\hat{\pi}_X)\hat{\pi}_Y(1-\hat{\pi}_Y) }}
\end{eqnarray*}

where \(\hat{\pi}_X = (n-n_{0\centerdot})/n\) and similarly for
\(\hat{\pi}_Y\). Note that this is same form as the Pearson measure.
From this, we see that the joint count \(n_{00}\) drives this
association measure.

\bigskip

You can obtain the ties-corrected Spearman correlation statistic \(r_S\)
using the \texttt{cor()} function in \texttt{R} and selecting the
\texttt{spearman} method. From below \(\hat{\rho}_S=-0.09\).

\chapter{Appendix A: Review of Statistical Inference}\label{C:AppA}

\emph{Chapter preview}. The appendix gives an overview of concepts and
methods related to statistical inference on the population of interest,
using a random sample of observations from the population. In the
appendix, Section \ref{S:AppA:BASIC} introduces the basic concepts
related to the population and the sample used for making the inference.
Section \ref{S:AppA:PE} presents the commonly used methods for point
estimation of population characteristics. Section \ref{S:AppA:IE}
demonstrates interval estimation that takes into consideration the
uncertainty in the estimation, due to use of a random sample from the
population. Section \ref{S:AppA:HT} introduces the concept of hypothesis
testing for the purpose of variable and model selection.

\section{Basic Concepts}\label{S:AppA:BASIC}

In this section, you learn the following concepts related to statistical
inference.

\begin{itemize}
\tightlist
\item
  Random sampling from a population that can be summarized using a list
  of items or individuals within the population
\item
  Sampling distributions that characterize the distributions of possible
  outcomes for a statistic calculated from a random sample
\item
  The central limit theorem that guides the distribution of the mean of
  a random sample from the population
\end{itemize}

\textbf{Statistical inference} is the process of making conclusions on
the characteristics of a large set of items/individuals (i.e., the
\textbf{population}), using a representative set of data (e.g., a
\textbf{random sample}) from a list of items or individuals from the
population that can be sampled. While the process has a broad spectrum
of applications in various areas including science, engineering, health,
social, and economic fields, statistical inference is important to
insurance companies that use data from their existing policy holders in
order to make inference on the characteristics (e.g., risk profiles) of
a specific segment of target customers (i.e., the population) whom the
insurance companies do not directly observe.

\textbf{Example -- Wisconsin Property Fund.} Assume there are 1,377
\emph{individual} claims from the 2010 experience.

\begin{longtable}[]{@{}rrrrrrrr@{}}
\toprule
& Minimum & First Quartile & Median & Mean & Third Quartile & Maximum &
Standard Deviation\tabularnewline
\midrule
\endhead
Claims & 1 & 788 & 2,250 & 26,620 & 6,171 & 12,920,000 &
368,030\tabularnewline
Logarithmic Claims & 0 & 6.670 & 7.719 & 7.804 & 8.728 & 16.370 &
1.683\tabularnewline
\bottomrule
\end{longtable}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{LossDataAnalytics_files/figure-latex/ClaimDistn1-1} 

}

\caption{Distribution of Claims}\label{fig:ClaimDistn1}
\end{figure}

\begin{verbatim}
## Sample size:  1377
\end{verbatim}

Using the 2010 claim experience (the sample), the Wisconsin Property
Fund may be interested in assessing the severity of all claims that
could potentially occur, such as 2010, 2011, and so forth (the
population). This process is important in the contexts of ratemaking or
claim predictive modeling. In order for such inference to be valid, we
need to assume that

\begin{itemize}
\tightlist
\item
  the set of 2010 claims is a \emph{random sample} that is
  representative of the population,
\item
  the \emph{sampling distribution} of the average claim amount can be
  estimated, so that we can quantify the bias and uncertainty in the
  esitmation due to use of a finite sample.
\end{itemize}

\subsection{Random Sampling}\label{random-sampling}

In statistics, a sampling \textbf{error} occurs when the
\textbf{sampling frame}, the list from which the sample is drawn, is not
an adequate approximation of the population of interest. A sample must
be a representative subset of a population, or universe, of interest. If
the sample is not representative, taking a larger sample does not
eliminate bias, as the same mistake is repeated over again and again.
Thus, we introduce the concept for random sampling that gives rise to a
simple \textbf{random sample} that is representative of the population.

We assume that the random variable \(X\) represents a draw from a
population with a distribution function \(F(\cdot)\) with mean
\(\mathrm{E}[X]=\mu\) and variance
\(\mathrm{Var}[X]=\mathrm{E}[(X-\mu)^2]\), where \(E(\cdot)\) denotes
the expectation of a random variable. In \textbf{random sampling}, we
make a total of \(n\) such draws represented by \(X_1, \ldots, X_n\),
each unrelated to one another (i.e., \emph{statistically independent}).
We refer to \(X_1, \ldots, X_n\) as a \textbf{random sample} (\emph{with
replacement}) from \(F(\cdot)\), taking either a parametric or
nonparametric form. Alternatively, we may say that \(X_1, \ldots, X_n\)
are identically and independently distributed (\emph{iid}) with
distribution function \(F(\cdot)\).

\subsection{Sampling Distribution}\label{sampling-distribution}

Using the random sample \(X_1, \ldots, X_n\), we are interested in
making a conclusion on a specific attribute of the population
distribution \(F(\cdot)\). For example, we may be interested in making
an inference on the population mean, denoted \(\mu\). It is natural to
think of the \textbf{sample mean}, \(\bar{X}=\sum_{i=1}^nX_i\), as an
estimate of the population mean \(\mu\). We call the sample mean as a
\textbf{statistic} calculated from the random sample
\(X_1, \ldots, X_n\). Other commonly used summary statistics include
sample standard deviation and sample quantiles.

When using a statistic (e.g., the sample mean \(\bar{X}\)) to make
statistical inference on the population attribute (e.g., population mean
\(\mu\)), the quality of inference is determined by the bias and
uncertainty in the estimation, owing to the use of a sample in place of
the population. Hence, it is important to study the distribution of a
statistic that quantifies the bias and variability of the statistic. In
particular, the distribution of the sample mean, \(\bar{X}\) (or any
other statistic), is called the \textbf{sampling distribution}. The
sampling distribution depends on the sampling process, the statistic,
the sample size \(n\) and the population distribution \(F(\cdot)\). The
central limit theorem gives the large-sample (sampling) distribution of
the sample mean under certain conditions.

\subsection{Central Limit Theorem}\label{central-limit-theorem}

In statistics, there are variations of the central limit theorem (CLT)
ensuring that, under certain conditions, the sample mean will approach
the population mean with its sampling distribution approaching the
normal distribution as the sample size goes to infinity. We give the
Lindeberg--Levy CLT that establishes the asymptotic sampling
distribution of the sample mean \(\bar{X}\) calculated using a random
sample from a universe population having a distribution \(F(\cdot)\).

\textbf{Lindeberg--Levy CLT.} Let \(X_1, \ldots, X_n\) be a random
sample from a population distribution \(F(\cdot)\) with mean \(\mu\) and
variance \(\sigma^2<\infty\). The difference between the sample mean
\(\bar{X}\) and \(\mu\), when multiplied by \(\sqrt{n}\), converges in
distribution to a normal distribution as the sample size goes to
infinity. That is,
\[\sqrt{n}(\bar{X}-\mu)\xrightarrow[]{d}N(0,\sigma).\]

Note that the CLT does not require a parametric form for \(F(\cdot)\).
Based on the CLT, we may perform statistical inference on the population
mean (we \emph{infer}, not \emph{deduce}). The types of inference we may
perform include \textbf{estimation} of the population,
\textbf{hypothesis testing} on whether a null statement is true, and
\textbf{prediction} of future samples from the population.

\section{Point Estimation and Properties}\label{S:AppA:PE}

In this section, you learn how to

\begin{itemize}
\tightlist
\item
  estimate population parameters using method of moments estimation
\item
  estimate population parameters based on maximum likelihood estimation
\end{itemize}

The population distribution function \(F(\cdot)\) can usually be
characterized by a limited (finite) number of terms called
\textbf{parameters}, in which case we refer to the distribution as a
\textbf{parametric distribution}. In contrast, in \textbf{nonparametric}
analysis, the attributes of the sampling distribution are not limited to
a small number of parameters.

For obtaining the population characteristics, there are different
attributes related to the population distribution \(F(\cdot)\). Such
measures include the mean, median, percentiles (i.e., 95th percentile),
and standard deviation. Because these summary measures do not depend on
a specific parametric reference, they are \textbf{nonparametric} summary
measures.

In \textbf{parametric} analysis, on the other hand, we may assume
specific families of distributions with specific parameters. For
example, people usually think of logarithm of claim amounts to be
normally distributed with mean \(\mu\) and standard deviation
\(\sigma\). That is, we assume that the claims have a \emph{lognormal}
distribution with parameters \(\mu\) and \(\sigma\). Alternatively,
insurance companies commonly assume that claim severity follows a gamma
distribution with a shape parameter \(\alpha\) and a scale parameter
\(\theta\). Here, the normal, lognormal, and gamma distributions are
examples of parametric distributions. In the above examples, the
quantities of \(\mu\), \(\sigma\), \(\alpha\), and \(\theta\) are known
as \emph{parameters}. For a given parametric distribution family, the
distribution is uniquely determined by the values of the parameters.

One often uses \(\theta\) to denote a summary attribute of the
population. In parametric models, \(\theta\) can be a parameter or a
function of parameters from a distribution such as the normal mean and
variance parameters. In nonparametric analysis, it can take a form of a
nonparametric summary such as the population mean or standard deviation.
Let \(\hat{\theta} =\hat{\theta}(X_1, \ldots, X_n)\) be a function of
the sample that provides a proxy, or an \textbf{estimate}, of
\(\theta\). It is referred to as a \textbf{statistic}, a function of the
sample \(X_1, \ldots, X_n\).

\textbf{Example -- Wisconsin Property Fund.} The sample mean 7.804 and
the sample standard deviation 1.683 can be either deemed as
nonparametric estimates of the population mean and standard deviation,
or as parametric estimates of \(\mu\) and \(\sigma\) of the normal
distribution concerning the logarithmic claims. Using results from the
lognormal distribution, we may estimate the expected claim, the
lognormal mean, as 10,106.8 ( \(=\exp(7.804+1.683^2/2)\) ).

For the Wisconsin Property Fund data, we may denote \(\hat{\mu} =7.804\)
and \(\hat{\sigma} = 1.683\), with the hat notation denoting an
\textbf{estimate} of the parameter based on the sample. In particular,
such an estimate is referred to as a \textbf{point estimate}, a single
approximation of the corresponding parameter. For point estimation, we
introduce the two commonly used methods called the method of moments
estimation and maximum likelihood estimation.

\subsection{Method of Moments
Estimation}\label{method-of-moments-estimation}

Before defining the method of moments estimation, we define the the
concept of \textbf{moments}. Moments are population attributes that
characterize the distribution function \(F(\cdot)\). Given a random draw
\(X\) from \(F(\cdot)\), the expectation \(\mu_k=\mathrm{E}[X^k]\) is
called the \textbf{\(k\)th moment} of \(X\), \(k=1,2,3,\ldots\) For
example, the population mean \(\mu\) is the \emph{first} moment.
Furthermore, the expectation \(\mathrm{E}[(X-\mu)^k]\) is called a
\textbf{\(k\)th central moment}. Thus, the variance is the second
central moment.

Using the random sample \(X_1, \ldots, X_n\), we may construct the
corresponding sample moment, \(\hat{\mu}_k=(1/n)\sum_{i=1}^n X_i^k\),
for estimating the population attribute \(\mu_k\). For example, we have
used the sample mean \(\bar{X}\) as an estimator for the population mean
\(\mu\). Similarly, the second central moment can be estimated as
\((1/n)\sum_{i=1}^n(X_i-\bar{X})^2\). Without assuming a parametric form
for \(F(\cdot)\), the sample moments constitute nonparametric estimates
of the corresponding population attributes. Such an estimator based on
matching of the corresponding sample and population moments is called a
\textbf{method of moments estimator} (MME).

While the MME works naturally in a nonparametric model, it can be used
to estimate parameters when a specific parametric family of distribution
is assumed for \(F(\cdot)\). Denote by
\(\boldsymbol{\theta}=(\theta_1,\cdots,\theta_m)\) the vector of
parameters corresponding to a parametric distribution \(F(\cdot)\).
Given a distribution family, we commonly know the relationships between
the parameters and the moments. In particular, we know the specific
forms of the functions \(h_1(\cdot),h_2(\cdot),\cdots,h_m(\cdot)\) such
that
\(\mu_1=h_1(\boldsymbol{\theta}),\,\mu_2=h_2(\boldsymbol{\theta}),\,\cdots,\,\mu_m=h_m(\boldsymbol{\theta})\).
Given the MME \(\hat{\mu}_1, \ldots, \hat{\mu}_m\) from the random
sample, the MME of the parameters
\(\hat{\theta}_1,\cdots,\hat{\theta}_m\) can be obtained by solving the
equations of \[\hat{\mu}_1=h_1(\hat{\theta}_1,\cdots,\hat{\theta}_m);\]
\[\hat{\mu}_2=h_2(\hat{\theta}_1,\cdots,\hat{\theta}_m);\] \[\cdots\]
\[\hat{\mu}_m=h_m(\hat{\theta}_1,\cdots,\hat{\theta}_m).\]

\textbf{Example -- Wisconsin Property Fund.} Assume that the claims
follow a lognormal distribution, so that logarithmic claims follow a
normal distribution. Specifically, assume \(\ln(X)\) has a normal
distribution with mean \(\mu\) and variance \(\sigma^2\), denoted as
\(\ln(X) \sim N(\mu, \sigma^2)\). It is straightforward that the MME
\(\hat{\mu}=\bar{X}\) and
\(\hat{\sigma}=\sqrt{(1/n)\sum_{i=1}^n(X_i-\bar{X})^2}\). For the
Wisconsin Property Fund example, the method of moments estimates are
\(\hat{\mu} =7.804\) and \(\hat{\sigma} = 1.683\).

\subsection{Maximum Likelihood Estimation}\label{S:AppA:MLE}

When \(F(\cdot)\) takes a parametric form, the maximum likelihood method
is widely used for estimating the population parameters
\(\boldsymbol{\theta}\). Maximum likelihood estimation is based on the
likelihood function, a function of the parameters given the observed
sample. Denote by \(f(x_i|\boldsymbol{\theta})\) the probability
function of \(X_i\) evaluated at \(X_i=x_i\) \((i=1,2,\cdots,n)\); it is
the probability mass function in the case of a discrete \(X\) and the
probability density function in the case of a continuous \(X\). Assuming
independence, the \textbf{likelihood function} of
\(\boldsymbol{\theta}\) associated with the observation
\((X_1,X_2,\cdots,X_n)=(x_1,x_2,\cdots,x_n)=\mathbf{x}\) can be written
as
\[L(\boldsymbol{\theta}|\mathbf{x})=\prod_{i=1}^nf(x_i|\boldsymbol{\theta}),\]
with the corresponding \textbf{log-likelihood function} given by
\[l(\boldsymbol{\theta}|\mathbf{x})=\ln(L(\boldsymbol{\theta}|\mathbf{x}))=\sum_{i=1}^n\ln f(x_i|\boldsymbol{\theta}).\]
The maximum likelihood estimator (MLE) of \(\boldsymbol{\theta}\) is the
set of values of \(\boldsymbol{\theta}\) that maximize the likelihood
function (log-likelihood function), given the observed sample. That is,
the MLE \(\hat{\boldsymbol{\theta}}\) can be written as
\[\hat{\boldsymbol{\theta}}={\mbox{argmax}}_{\boldsymbol{\theta}\in\Theta}l(\boldsymbol{\theta}|\mathbf{x}),\]
where \(\Theta\) is the parameter space of \(\boldsymbol{\theta}\), and
\({\mbox{argmax}}_{\boldsymbol{\theta}\in\Theta}l(\boldsymbol{\theta}|\mathbf{x})\)
is defined as the value of \(\boldsymbol{\theta}\) at which the function
\(l(\boldsymbol{\theta}|\mathbf{x})\) reachs its maximum.

Given the analytical form of the likelihood function, the MLE can be
obtained by taking the first derivative of the log-likelihood function
with respect to \(\boldsymbol{\theta}\), and setting the values of the
partial derivatives to zero. That is, the MLE are the solutions of the
equations of
\[\frac{\partial l(\hat{\boldsymbol{\theta}}|\mathbf{x})}{\partial\hat{\theta}_1}=0;\]
\[\frac{\partial l(\hat{\boldsymbol{\theta}}|\mathbf{x})}{\partial\hat{\theta}_2}=0;\]
\[\cdots\]
\[\frac{\partial l(\hat{\boldsymbol{\theta}}|\mathbf{x})}{\partial\hat{\theta}_m}=0,\]
provided that the second partial derivatives are negative.

For parametric models, the MLE of the parameters can be obtained either
analytically (e.g., in the case of normal distributions and linear
estimators), or numerically through iterative algorithms such as the
Newton-Raphson method and its adaptive versions (e.g., in the case of
generalized linear models with a non-normal response variable).

\textbf{Normal distribution.} Assume \((X_1,X_2,\cdots,X_n)\) to be a
random sample from the normal distribution \(N(\mu, \sigma^2)\). With an
observed sample \((X_1,X_2,\cdots,X_n)=(x_1,x_2,\cdots,x_n)\), we can
write the likelihood function of \(\mu,\sigma^2\) as
\[L(\mu,\sigma^2)=\prod_{i=1}^n\left[\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{\left(x_i-\mu\right)^2}{2\sigma^2}}\right],\]
with the corresponding log-likelihood function given by
\[l(\mu,\sigma^2)=-\frac{n}{2}[\ln(2\pi)+\ln(\sigma^2)]-\frac{1}{2\sigma^2}\sum_{i=1}^n\left(x_i-\mu\right)^2.\]

By solving
\[\frac{\partial l(\hat{\mu},\sigma^2)}{\partial \hat{\mu}}=0,\] we
obtain \(\hat{\mu}=\bar{x}=(1/n)\sum_{i=1}^nx_i\). It is straightforward
to verify that
\(\frac{\partial l^2(\hat{\mu},\sigma^2)}{\partial \hat{\mu}^2}\left|_{\hat{\mu}=\bar{x}}\right.<0\).
Since this works for arbitrary \(x\), \(\hat{\mu}=\bar{X}\) is the MLE
of \(\mu\). Similarly, by solving
\[\frac{\partial l(\mu,\hat{\sigma}^2)}{\partial \hat{\sigma}^2}=0,\] we
obtain \(\hat{\sigma}^2=(1/n)\sum_{i=1}^n(x_i-\mu)^2\). Further
replacing \(\mu\) by \(\hat{\mu}\), we derive the MLE of \(\sigma^2\) as
\(\hat{\sigma}^2=(1/n)\sum_{i=1}^n(X_i-\bar{X})^2\).

Hence, the sample mean \(\bar{X}\) and \(\hat{\sigma}^2\) are both the
\emph{MME} and MLE for the mean \(\mu\) and variance \(\sigma^2\), under
a normal population distribution \(F(\cdot)\). More details regarding
the properties of the likelihood function, and the derivation of MLE
under parametric distributions other than the normal distribution are
given in Appendix Chapter \ref{C:AppB}.

\section{Interval Estimation}\label{S:AppA:IE}

In this section, you learn how to

\begin{itemize}
\tightlist
\item
  derive the exact sampling distribution of the MLE of the normal mean
\item
  obtain the large-sample approximation of the sampling distribution
  using the large sample properties of the MLE
\item
  construct a confidence interval of a parameter based on the large
  sample properties of the MLE
\end{itemize}

Now that we have introduced the MME and MLE, we may perform the first
type of statistical inference, \textbf{interval estimation} that
quantifies the uncertainty resulting from the use of a finite sample. By
deriving the sampling distribution of MLE, we can estimate an interval
(a confidence interval) for the parameter. Under the frequentist
approach (e.g., that based on maximum likelihood estimation), the
confidence intervals generated from the same random sampling frame will
cover the true value the majority of times (e.g., 95\% of the times), if
we repeat the sampling process and re-calculate the interval over and
over again. Such a process requires the derivation of the sampling
distribution for the MLE.

\subsection{Exact Distribution for Normal Sample
Mean}\label{S:AppA:IE:ED}

Due to the \textbf{additivity} property of the normal distribution
(i.e., a sum of normal random variables that follows a multivariate
normal distribution still follows a normal distribution) and that the
normal distribution belongs to the \textbf{location--scale family}
(i.e., a location and/or scale transformation of a normal random
variable has a normal distribution), the sample mean \(\bar{X}\) of a
random sample from a normal \(F(\cdot)\) has a normal sampling
distribution for any finite \(n\). Given
\(X_i\sim^{iid} N(\mu,\sigma^2)\), \(i=1,\dots,n\), the MLE of \(\mu\)
has an exact distribution
\[\bar{X}\sim N\left(\mu,\frac{\sigma^2}{n}\right).\] Hence, the sample
mean is an unbiased estimator of \(\mu\). In addition, the uncertainty
in the estimation can be quantified by its variance \(\sigma^2/n\), that
decreases with the sample size \(n\). When the sample size goes to
infinity, the sample mean will approach a single mass at the true value.

\subsection{Large-sample Properties of
MLE}\label{large-sample-properties-of-mle}

For the MLE of the mean parameter and any other parameters of other
parametric distribution families, however, we usually cannot derive an
exact sampling distribution for finite samples. Fortunately, when the
sample size is sufficiently large, MLEs can be approximated by a normal
distribution. Due to the general maximum likelihood theory, the MLE has
some nice large-sample properties.

\begin{itemize}
\item
  The MLE \(\hat{\theta}\) of a parameter \(\theta\), is a
  \textbf{consistent} estimator. That is, \(\hat{\theta}\) converges in
  probability to the true value \(\theta\), as the sample size \(n\)
  goes to infinity.
\item
  The MLE has the \textbf{asymptotic normality} property, meaning that
  the estimator will converge in distribution to a normal distribution
  centered around the true value, when the sample size goes to infinity.
  Namely,
  \[\sqrt{n}(\hat{\theta}-\theta)\rightarrow_d N\left(0,\,V\right),\quad \mbox{as}\quad n\rightarrow \infty,\]
  where \(V\) is the inverse of the Fisher Information. Hence, the MLE
  \(\hat{\theta}\) approximately follows a normal distribution with mean
  \(\theta\) and variance \(V/n\), when the sample size is large.
\item
  The MLE is \textbf{efficient}, meaning that it has the smallest
  asymptotic variance \(V\), commonly referred to as the
  \textbf{Cramer--Rao lower bound}. In particular, the Cramer--Rao lower
  bound is the inverse of the Fisher information defined as
  \(\mathcal{I}(\theta)=-\mathrm{E}(\partial^2\ln f(X;\theta)/\partial \theta^2)\).
  Hence, \(\mathrm{Var}(\hat{\theta})\) can be estimated based on the
  observed Fisher information that can be written as
  \(-\sum_{i=1}^n \partial^2\ln f(X_i;\theta)/\partial \theta^2\).
\end{itemize}

For many parametric distributions, the Fisher information may be derived
analytically for the MLE of parameters. For more sophisticated
parametric models, the Fisher information can be evaluated numerically
using numerical integration for continuous distributions, or numerical
summation for discrete distributions.

\subsection{Confidence Interval}\label{confidence-interval}

Given that the MLE \(\hat{\theta}\) has either an exact or an
approximate normal distribution with mean \(\theta\) and variance
\(\mathrm{Var}(\hat{\theta})\), we may take the square root of the
variance and plug-in the estimate to define
\(se(\hat{\theta}) = \sqrt{\mathrm{Var}(\hat{\theta})}\). A
\textbf{standard error} is an estimated standard deviation that
quantifies the uncertainty in the estimation resulting from the use of a
finite sample. Under some regularity conditions governing the population
distribution, we may establish that the statistic
\[\frac{\hat{\theta}-\theta}{se(\hat{\theta})}\] converges in
distribution to a Student-\(t\) distribution with degrees of freedom (a
parameter of the distribution) \({n-p}\), where \(p\) is the number of
parameters in the model other than the variance. For example, for the
normal distribution case, we have \(p=1\) for the parameter \(\mu\); for
a linear regression model with an independent variable, we have \(p=2\)
for the parameters of the intercept and the independent variable. Denote
by \(t_{n-p}(1-\alpha/2)\) the \(100\times(1-\alpha/2)\)-th percentile
of the Student-\(t\) distribution that satisfies
\(\Pr\left[t< t_{n-p}\left(1-{\alpha}/{2}\right) \right]= 1-{\alpha}/{2}\).
We have,
\[\Pr\left[-t_{n-p}\left(1-\frac{\alpha}{2}\right)<\frac{\hat{\theta}-\theta}{se(\hat{\theta})}< t_{n-p}\left(1-\frac{\alpha}{2}\right) \right]= 1-{\alpha},\]
from which we can derive a \textbf{confidence interval} for \(\theta\).
From the above equation we can derive a pair of statistics,
\(\hat{\theta}_1\) and \(\hat{\theta}_2\), that provide an interval of
the form \([\hat{\theta}_1, \hat{\theta}_2]\). This interval is a
\(1-\alpha\) confidence interval for \(\theta\) such that
\(\Pr\left(\hat{\theta}_1 \le \theta \le \hat{\theta}_2\right) = 1-\alpha,\)
where the probability \(1-\alpha\) is referred to as the
\textbf{confidence level}. Note that the above confidence interval is
not valid for small samples, except for the case of the normal mean.

\textbf{Normal distribution.} For the normal population mean \(\mu\),
the MLE has an exact sampling distribution
\(\bar{X}\sim N(\mu,\sigma/\sqrt{n})\), in which we can estimate
\(se(\hat{\theta})\) by \(\hat{\sigma}/\sqrt{n}\). Based on the
\textbf{Cochran's theorem}, the resulting statistic has an exact
Student-\(t\) distribution with degrees of freedom \(n-1\). Hence, we
can derive the lower and upper bounds of the confidence interval as
\[\hat{\mu}_1 = \hat{\mu} - t_{n-1}\left(1-\frac{\alpha}{2}\right)\frac{ \hat{\sigma}}{\sqrt{n}}\]
and
\[\hat{\mu}_2 = \hat{\mu} + t_{n-1}\left(1-\frac{\alpha}{2}\right)\frac{ \hat{\sigma}}{\sqrt{n}}.\]
When \(\alpha = 0.05\), \(t_{n-1}(1-\alpha/2) \approx 1.96\) for large
values of \(n\). Based on the Cochran's theorem, the confidence interval
is valid regardless of the sample size.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example -- Wisconsin Property Fund.} For the lognormal claim
model, (7.715235, 7.893208) is a 95\% confidence interval for \(\mu\).

More details regarding interval estimation based the MLE of other
parameters and distribution families are given in Appendix Chapter
\ref{C:AppC}.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{Hypothesis Testing}\label{S:AppA:HT}

In this section, you learn how to

\begin{itemize}
\tightlist
\item
  understand the basic concepts in hypothesis testing including the
  level of significance and the power of a test
\item
  perform hypothesis testing such as a Student-\(t\) test based on the
  properties of the MLE
\item
  construct a likelihood ratio test for a single parameter or multiple
  parameters from the same statistical model
\item
  use information criteria such as the Akaike's information criterion or
  the Bayesian information criterion to perform model selection
\end{itemize}

For the parameter(s) \(\boldsymbol{\theta}\) from a parametric
distribution, an alternative type of statistical inference is called
\textbf{hypothesis tesing} that verifies whether a hypothesis regarding
the parameter(s) is true, under a given probability called the
\textbf{level of significance} \(\alpha\) (e.g., 5\%). In hypothesis
testing, we reject the null hypothesis, a restrictive statement
concerning the parameter(s), if the probability of observing a random
sample as extremal as the observed one is smaller than \(\alpha\), if
the null hypothesis were true.

\subsection{Basic Concepts}\label{basic-concepts}

In a statistical test, we are usually interested in testing whether a
statement regarding some parameter(s), a \textbf{null hypothesis}
(denoted \(H_0\)), is true given the observed data. The null hypothesis
can take a general form \(H_0:\theta\in\Theta_0\), where \(\Theta_0\) is
a subset of the parameter space \(\Theta\) of \(\theta\) that may
contain multiple parameters. For the case with a single parameter
\(\theta\), the null hypothesis usually takes either the form
\(H_0:\theta=\theta_0\) or \(H_0:\theta\leq\theta_0\). The opposite of
the null hypothesis is called the \textbf{alternative hypothesis} that
can be written as \(H_a:\theta\neq\theta_0\) or \(H_a:\theta>\theta_0\).
The statistical test on \(H_0:\theta=\theta_0\) is called a
\textbf{two-sided} as the alternative hypothesis contains two
ineqalities of \(H_a:\theta<\theta_0\) or \(\theta>\theta_0\). In
contrast, the statistical test on either \(H_0:\theta\leq\theta_0\) or
\(H_0:\theta\geq\theta_0\) is called a \textbf{one-sided} test.

A statistical test is usually constructed based on a statistic \(T\) and
its exact or large-sample distribution. The test typically rejects a
two-sided test when either \(T > c_1\) or \(T < c_2\), where the two
constants \(c_1\) and \(c_2\) are obtained based on the sampling
distribution of \(T\) at a probability level \(\alpha\) called the
\textbf{level of significance}. In particular, the level of significance
\(\alpha\) satisfies
\[\alpha=\Pr(\mbox{reject }H_0|H_0\mbox{ is true}),\] meaning that if
the null hypothesis were true, we would reject the null hypothesis only
5\% of the times, if we repeat the sampling process and perform the test
over and over again.

Thus, the level of significance is the probability of making a
\textbf{type I error} (error of the first kind), the error of
incorrectly rejecting a true null hypothesis. For this reason, the level
of significance \(\alpha\) is also referred to as the type I error rate.
Another type of error we may make in hypothesis testing is the
\textbf{type II error} (error of the second kind), the error of
incorrectly accepting a false null hypothesis. Similarly, we can define
the \textbf{type II error rate} as the probability of not rejecting
(accepting) a null hypothesis given that it is not true. That is, the
type II error rate is given by
\[\Pr(\mbox{accept }H_0|H_0\mbox{ is false}).\] Another important
quantity concerning the quality of the statistical test is called the
\textbf{power} of the test \(\beta\), defined as the probability of
rejecting a false null hypothesis. The mathematical definition of the
power is \[\beta=\Pr(\mbox{reject }H_0|H_0\mbox{ is false}).\] Note that
the power of the test is typically calculated based on a specific
alternative value of \(\theta=\theta_a\), given a specific sampling
distribution and a given sample size. In real experimental studies,
people usually calculate the required sample size in order to choose a
sample size that will ensure a large chance of obtaining a statistically
significant test (i.e., with a prespecified statistical power such as
85\%).

\subsection{\texorpdfstring{Student-\(t\) test based on
MLE}{Student-t test based on MLE}}\label{student-t-test-based-on-mle}

Based on the results from Section \ref{S:AppA:IE:ED}, we can define a
Student \(t\) test for testing \(H_0:\theta=\theta_0\). In particular,
we define the test statistic as
\[t\text{-stat}=\frac{\hat{\theta}-\theta_0}{se(\hat{\theta})},\] which
has a large-sample distribution of a Student-\(t\) distribution with
degrees of freedom \({n-p}\), when the null hypothesis is true (i.e.,
when \(\theta=\theta_0\)).

For a given \textbf{level of significance} \(\alpha\), say 5\%, we
reject the null hypothesis if the event
\(t\text{-stat}<-t_{n-p}\left(1-{\alpha}/{2}\right)\) or
\(t\text{-stat}> t_{n-p}\left(1-{\alpha}/{2}\right)\) occurs (the
\textbf{rejection region}). Under the null hypothesis \(H_0\), we have
\[\Pr\left[t\text{-stat}<-t_{n-p}\left(1-\frac{\alpha}{2}\right)\right]=\Pr\left[t\text{-stat}> t_{n-p}\left(1-\frac{\alpha}{2}\right) \right]= \frac{\alpha}{2}.\]
In addition to the concept of rejection region, we may reject the test
based on the \(p\)\textbf{-value} defined as \(2\Pr(T>|t\text{-stat}|)\)
for the aforementioned two-sided test, where the random variable
\(T\sim T_{n-p}\). We reject the null hypothesis if \(p\)-value is
smaller than and equal to \(\alpha\). For a given sample, a \(p\)-value
is defined to be the smallest significance level for which the null
hypothesis would be rejected.

Similarly, we can construct a one-sided test for the null hypothesis
\(H_0:\theta\leq\theta_0\) (or \(H_0:\theta\geq\theta_0\)). Using the
same test statistic, we reject the null hypothesis when
\(t\text{-stat}> t_{n-p}\left(1-{\alpha}\right)\) (or
\(t\text{-stat}<- t_{n-p}\left(1-{\alpha}\right)\) for the test on
\(H_0:\theta\geq\theta_0\)). The corresponding \(p\)-value is defined as
\(\Pr(T>|t\text{-stat}|)\) (or \(\Pr(T<|t\text{-stat}|)\) for the test
on \(H_0:\theta\geq\theta_0\)). Note that the test is not valid for
small samples, except for the case of the test on the normal mean.

\textbf{One-sample \(t\) Test for Normal Mean.} For the test on the
normal mean of the form \(H_0:\mu=\mu_0\), \(H_0:\mu\leq\mu_0\) or
\(H_0:\mu\geq\mu_0\), we can define the test statistic as
\[t\text{-stat}=\frac{\bar{X}-\mu_0}{{\hat{\sigma}}/{\sqrt{n}}},\] for
which we have an exact sampling distribution
\(t\text{-stat}\sim T_{n-1}\) from the Cochran's theorem, with
\(T_{n-1}\) denoting a Student-\(t\) distribution with degrees of
freedom \(n-1\). According to the Cochran's theorem, the test is valid
for both small and large samples.

\textbf{Example -- Wisconsin Property Fund.} Assume that mean
logarithmic claims have historically been approximately by
\(\mu_0 = \ln(5000)= 8.517\). We might want to use the 2010 data to
assess whether the mean of the distribution has changed (a two-sided
test), or whether it has increased (a one-sided test). Given the actual
2010 average \(\hat{\mu} =7.804\), we may use the one-sample \(t\) test
to assess whether this is a significant departure from \(\mu_0 = 8.517\)
(i.e., in testing \(H_0:\mu=8.517\)). The test statistic
\(t\text{-stat}=(8.517-7.804)/(1.683/\sqrt{1377}) = 15.72>t_{1376}\left(0.975\right)\).
Hence, we reject the two-sided test at \(\alpha=5\%\). Similarly, we
will reject the one-sided test at \(\alpha=5\%\).

\textbf{Example -- Wisconsin Property Fund.} For numerical stability and
extensions to regression applications, statistical packages often work
with transformed versions of parameters. The following estimates are
from the \textbf{R} package \textbf{VGAM} (the function). More details
on the MLE of other distribution families are given in Appendix Chapter
\ref{C:AppC}.

\begin{longtable}[]{@{}rrrr@{}}
\toprule
Distribution & Parameter & Standard & \(t\)-stat\tabularnewline
& Estimate & Error &\tabularnewline
Gamma & 10.190 & 0.050 & 203.831\tabularnewline
& -1.236 & 0.030 & -41.180\tabularnewline
Lognormal & 7.804 & 0.045 & 172.089\tabularnewline
& 0.520 & 0.019 & 27.303\tabularnewline
Pareto & 7.733 & 0.093 & 82.853\tabularnewline
& -0.001 & 0.054 & -0.016\tabularnewline
GB2 & 2.831 & 1.000 & 2.832\tabularnewline
& 1.203 & 0.292 & 4.120\tabularnewline
& 6.329 & 0.390 & 16.220\tabularnewline
& 1.295 & 0.219 & 5.910\tabularnewline
\bottomrule
\end{longtable}

\subsection{Likelihood Ratio Test}\label{S:AppA:HT:LRT}

In the previous subsection, we have introduced the Student-\(t\) test on
a single parameter, based on the properties of the MLE. In this section,
we define an alternative test called the \textbf{likelihood ratio test}
(LRT). The LRT may be used to test multiple parameters from the same
statistical model.

Given the likelihood function \(L(\theta|\mathbf{x})\) and
\(\Theta_0 \subset \Theta\), the likelihood ratio test statistic for
testing \(H_0:\theta\in\Theta_0\) against \(H_a:\theta\notin\Theta_0\)
is given by
\[L=\frac{\sup_{\theta\in\Theta_0}L(\theta|\mathbf{x})}{\sup_{\theta\in\Theta}L(\theta|\mathbf{x})},\]
and that for testing \(H_0:\theta=\theta_0\) versis
\(H_a:\theta\neq\theta_0\) is
\[L=\frac{L(\theta_0|\mathbf{x})}{\sup_{\theta\in\Theta}L(\theta|\mathbf{x})}.\]
The LRT rejects the null hypothesis when \(L < c\), with the threshold
depending on the level of significance \(\alpha\), the sample size
\(n\), and the number of parameters in \(\theta\). Based on the
\textbf{Neyman--Pearson Lemma}, the LRT is the \textbf{uniformly most
powerful} (UMP) test for testing \(H_0:\theta=\theta_0\) versis
\(H_a:\theta=\theta_a\). That is, it provides the largest power
\(\beta\) for a given \(\alpha\) and a given alternative value
\(\theta_a\).

Based on the \textbf{Wilks's Theorem}, the likelihood ratio test
statistic \(-2\ln(L)\) converges in distribution to a Chi-square
distribution with the degree of freedom being the difference between the
dimensionality of the parameter spaces \(\Theta\) and \(\Theta_0\), when
the sample size goes to infinity and when the null model is nested
within the alternative model. That is, when the null model is a special
case of the alternative model containing a restricted sample space, we
may approximate \(c\) by \(\chi^2_{p_1 - p_2}(1-\alpha)\), the
\(100\times(1-\alpha)\) th percentile of the Chi-square distribution,
with \(p_1-p_2\) being the degrees of freedom, and \(p_1\) and \(p_2\)
being the numbers of parameters in the alternative and null models,
respectively. Note that the LRT is also a large-sample test that will
not be valid for small samples.

\subsection{Information Criteria}\label{S:AppA:HT:IC}

In real-life applications, the LRT has been commonly used for comparing
two nested models. The LRT approach as a model selection tool, however,
has two major drawbacks: 1) It typically requires the null model to be
nested within the alternative model; 2) models selected from the LRT
tends to provide in-sample over-fitting, leading to poor out-of-sample
prediction. In order to overcome these issues, model selection based on
information criteria, applicable to non-nested models while taking into
consideration the model complexity, is more widely used for model
selection. Here, we introduce the two most widely used criteria, the
Akaike's information criterion and the Bayesian information criterion.

In particular, the \textbf{Akaike's information criterion} (\(AIC\)) is
defined as \[AIC = -2\ln L(\hat{\boldsymbol \theta}) + 2p,\] where
\(\hat{\boldsymbol \theta}\) denotes the MLE of
\({\boldsymbol \theta}\), and \(p\) is the number of parameters in the
model. The additional term \(2 p\) represents a penalty for the
complexity of the model. That is, with the same maximized likelihood
function, the \(AIC\) favors model with less parameters. We note that
the \(AIC\) does not consider the impact from the sample size \(n\).

Alternatively, people use the \textbf{Bayesian information criterion}
(\(BIC\)) that takes into consideration the sample size. The \(BIC\) is
defined as \[BIC = -2\ln L(\hat{\boldsymbol \theta}) + p\,\ln(n).\] We
observe that the \(BIC\) generally puts a higher weight on the number of
parameters. With the same maximized likelihood function, the \(BIC\)
will suggest a more parsimonious model than the \(AIC\).

\textbf{Example -- Wisconsin Property Fund.} Both the \(AIC\) and
\(BIC\) statistics suggest that the \emph{GB2} is the best fitting model
whereas gamma is the worst.

\begin{longtable}[]{@{}rrr@{}}
\toprule
Distribution & AIC & BIC\tabularnewline
\midrule
\endhead
Gamma & 28,305.2 & 28,315.6\tabularnewline
Lognormal & 26,837.7 & 26,848.2\tabularnewline
Pareto & 26,813.3 & 26,823.7\tabularnewline
GB2 & 26,768.1 & 26,789.0\tabularnewline
\bottomrule
\end{longtable}

In this graph,

\begin{itemize}
\item
  black represents actual (smoothed) logarithmic claims
\item
  Best approximated by green which is fitted GB2
\item
  Pareto (purple) and Lognormal (lightblue) are also pretty good
\item
  Worst are the exponential (in red) and gamma (in dark blue)
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{LossDataAnalytics_files/figure-latex/FitClaimDistn-1} 

}

\caption{Fitted Claims Distribution}\label{fig:FitClaimDistn}
\end{figure}

\begin{verbatim}
## Sample size:  6258
\end{verbatim}

R Code for Fitted Claims Distributions

\subsubsection*{Contributors}\label{contributors-9}
\addcontentsline{toc}{subsubsection}{Contributors}

\begin{itemize}
\tightlist
\item
  \textbf{Lei (Larry) Hua}, Northern Illinois University, and
  \textbf{Edward W. (Jed) Frees}, University of Wisconsin-Madison, are
  the principal authors of the initial version of this chapter. Email:
  \href{mailto:lhua@niu.edu}{\nolinkurl{lhua@niu.edu}} or
  \href{mailto:jfrees@bus.wisc.edu}{\nolinkurl{jfrees@bus.wisc.edu}} for
  chapter comments and suggested improvements.
\end{itemize}

\chapter{Appendix B: Iterated Expectations}\label{C:AppB}

This appendix introduces the laws related to iterated expectations. In
particular, Section \ref{S:AppB:CD} introduces the concepts of
conditional distribution and conditional expectation. Section
\ref{S:AppB:IE} introduces the Law of Iterated Expectations and the Law
of Total Variance.

In some situations, we only observe a single outcome but can
conceptualize an outcome as resulting from a two (or more) stage
process. Such types of statistical models are called \textbf{two-stage},
or \textbf{hierarchical} models. Some special cases of hierarchical
models include:

\begin{itemize}
\item
  models where the parameters of the distribution are random variables;
\item
  mixture distribution, where Stage 1 represents the draw of a
  sub-population and Stage 2 represents a random variable from a
  distribution that is determined by the sub-population drew in Stage 1;
\item
  an aggregate distribution, where Stage 1 represents the draw of the
  number of events and Stage two represents the loss amount occurred per
  event.
\end{itemize}

In these situations, the process gives rise to a conditional
distribution of a random variable (the Stage 2 outcome) given the other
(the Stage 1 outcome). The Law of Iterated Expectations can be useful
for obtaining the unconditional expectation or variance of a random
variable in such cases.

\section{Conditional Distribution and Conditional
Expectation}\label{S:AppB:CD}

In this section, you learn

\begin{itemize}
\tightlist
\item
  the concepts related to the conditional distribution of a random
  variable given another
\item
  how to define the conditional expectation and variance based on the
  conditional distribution function
\end{itemize}

The iterated expectations are the laws regarding calculation of the
expectation and variance of a random variable using a conditional
distribution of the variable given another variable. Hence, we first
introduce the concepts related to the conditional distribution, and the
calculation of the conditional expectation and variable based on a given
conditional distribution.

\subsection{Conditional Distribution}\label{conditional-distribution}

Here we introduce the concept of conditional distribution respectively
for discrete and continuous random variables.

\subsubsection{Discrete Case}\label{discrete-case}

Suppose that \(X\) and \(Y\) are both discrete random variables, meaning
that they can take a finite or countable number of possible values with
a positive probability. The \textbf{joint probability (mass) function}
of (\(X\), \(Y\)) is defined as \[p(x,y) = \Pr[X=x, Y=y]\].

When \(X\) and \(Y\) are \textbf{independent} (the value of \(X\) does
not depend on that of \(Y\)), we have \[p(x,y)=p(x)p(y),\] with
\(p(x)=\Pr[X=x]\) and \(p(y)=\Pr[Y=y]\) being the \textbf{marginal
probability function} of \(X\) and \(Y\), respectively.

Given the joint probability function, we may obtain the marginal
probability functions of \(Y\) as \[p(y)=\sum_x p(x,y),\] where the
summation is over all possible values of \(x\), and the marginal
probability function of \(X\) can be obtained in a similar manner.

The \textbf{conditional probability (mass) function} of \((Y|X)\) is
defined as \[p(y|x) =\Pr[Y=y|X=x]= \frac{p(x,y)}{\Pr[X=x]},\] where we
may obtain the conditional probability function of \((X|Y)\) in a
similar manner. In particular, the above conditional probability
represents the probability of the event \(Y=y\) given the event \(X=x\).
Hence, even in cases where \(\Pr[X=x]=0\), the function may be given as
a particular form, in real applications.

\subsubsection{Continuous Case}\label{continuous-case}

For continuous random variables \(X\) and \(Y\), we may define their
joint probability (density) function based on the joint cumulative
distribution function. The \textbf{joint cumulative distribution
function} of (\(X\), \(Y\)) is defined as
\[F(x,y) = \Pr[X\leq x, Y\leq y].\]

When \(X\) and \(Y\) are \emph{independent}, we have
\[F(x,y)=F(x)F(y),\] with \(F(x)=\Pr[X\leq x]\) and
\(F(y)=\Pr[Y\leq y]\) being the \textbf{cumulative distribution
function} (cdf) of \(X\) and \(Y\), respectively. The random variable
\(X\) is referred to as a \textbf{continuous} random variable if its cdf
is continuous on \(x\).

When the cdf \(F(x)\) is continuous on \(x\), then we define
\(f(x)=\partial F(x)/\partial x\) as the \textbf{(marginal) probability
density function} (pdf) of \(X\). Similarly, if the joint cdf \(F(x,y)\)
is continuous on both \(x\) and \(y\), we define
\[f(x,y)=\frac{\partial^2 F(x,y)}{\partial x\partial y}\] as the
\textbf{joint probability density function} of (\(X\), \(Y\)), in which
case we refer to the random variables as \textbf{jointly continuous}.

When \(X\) and \(Y\) are \emph{independent}, we have
\[f(x,y)=f(x)f(y).\]

Given the joint density function, we may obtain the marginal density
function of \(Y\) as \[f(y)=\int_x f(x,y)\,dx,\] where the integral is
over all possible values of \(x\), and the marginal probability function
of \(X\) can be obtained in a similar manner.

Based on the joint pdf and the marginal pdf, we define the
\textbf{conditional probability density function} of \((Y|X)\) as

\[f(y|x) = \frac{f(x,y)}{f(x)},\] where we may obtain the conditional
probability function of \((X|Y)\) in a similar manner. Here, the
conditional density function is the density function of \(y\) given
\(X=x\). Hence, even in cases where \(\Pr[X=x]=0\) or when \(f(x)\) is
not defined, the function may be given in a particular form in real
applications.

\subsection{Conditional Expectation and Conditional
Variance}\label{conditional-expectation-and-conditional-variance}

Now we define the conditional expectation and variance based on the
conditional distribution defined in the previous subsection.

\subsubsection{Discrete Case}\label{discrete-case-1}

For a discrete random variable \(Y\), its \textbf{expectation} is
defined as \(\mathrm{E}[Y]=\sum_y y\,p(y)\) if its value is finite, and
its \textbf{variance} is defined as
\(\mathrm{Var}[Y]=\mathrm{E}\{(Y-\mathrm{E}[Y])^2\}=\sum_y y^2\,p(y)-\{\mathrm{E}[Y]\}^2\)
if its value is finite.

For a discrete random variable \(Y\), the \textbf{conditional
expectation} of the random variable \(Y\) given the event \(X=x\) is
defined as \[\mathrm{E}[Y|X=x]=\sum_y y\,p(y|x),\] where \(X\) does not
have to be a discrete variable, as far as the conditional probability
function \(p(y|x)\) is given.

Note that the conditional expectation \(\mathrm{E}[Y|X=x]\) is a fixed
number. When we replace \(x\) with \(X\) on the right hand side of the
above equation, we can define the expectation of \(Y\) given the random
variable \(X\) as \[\mathrm{E}[Y|X]=\sum_y y\,p(y|X),\] which is still a
\emph{random variable}, and the randomness comes from \(X\).

In a similar manner, we can define the \textbf{conditional variance} of
the random variable \(Y\) given the event \(X=x\) as
\[\mathrm{Var}[Y|X=x]=\mathrm{E}[Y^2|X=x]-\{\mathrm{E}[Y|X=x]\}^2=\sum_y y^2\,p(y|x)-\{\mathrm{E}[Y|X=x]\}^2.\]

The variance of \(Y\) given \(X\), \(\mathrm{Var}[Y|X]\) can be defined
by replacing \(x\) by \(X\) in the above equation, and
\(\mathrm{Var}[Y|X]\) is still a random variable and the randomness
comes from \(X\).

\subsubsection{Continuous Case}\label{continuous-case-1}

For a continuous random variable \(Y\), its \textbf{expectation} is
defined as \(\mathrm{E}[Y]=\int_y y\,f(y)dy\) if the integral exists,
and its \textbf{variance} is defined as
\(\mathrm{Var}[Y]=\mathrm{E}\{(X-\mathrm{E}[Y])^2\}=\int_y y^2\,f(y)dy-\{\mathrm{E}[Y]\}^2\)
if its value is finite.

For jointly continuous random variables \(X\) and \(Y\), the
\textbf{conditional expectation} of the random variable \(Y\) given
\(X=x\) is defined as \[\mathrm{E}[Y|X=x]=\int_y y\,f(y|x)dy.\] where
\(X\) does not have to be a continuous variable, as far as the
conditional probability function \(f(y|x)\) is given.

Similarly, the conditional expectation \(\mathrm{E}[Y|X=x]\) is a fixed
number. When we replace \(x\) with \(X\) on the right-hand side of the
above equation, we can define the expectation of \(Y\) given the random
variable \(X\) as \[\mathrm{E}[Y|X]=\int_y y\,p(y|X)\,dy,\] which is
still a \emph{random variable}, and the randomness comes from \(X\).

In a similar manner, we can define the \textbf{conditional variance} of
the random variable \(Y\) given the event \(X=x\) as
\[\mathrm{Var}[Y|X=x]=\mathrm{E}[Y^2|X=x]-\{\mathrm{E}[Y|X=x]\}^2=\int_y y^2\,f(y|x)\,dy-\{\mathrm{E}[Y|X=x]\}^2.\]

The variance of \(Y\) given \(X\), \(\mathrm{Var}[Y|X]\) can then be
defined by replacing \(x\) by \(X\) in the above equation, and similarly
\(\mathrm{Var}[Y|X]\) is also a random variable and the randomness comes
from \(X\).

\section{Iterated Expectations and Total Variance}\label{S:AppB:IE}

In this section, you learn

\begin{itemize}
\tightlist
\item
  the Law of Iterated Expectations for calculating the expectation of a
  random variable based on its conditional distribution given another
  random variable
\item
  the Law of Total Variance for calculating the variance of a random
  variable based on its conditional distribution given another random
  variable
\item
  how to calculate the expectation and variance based on an example of a
  two-stage model
\end{itemize}

\subsection{Law of Iterated
Expectations}\label{law-of-iterated-expectations}

Consider two random variables \(X\) and \(Y\), and \(h(X,Y)\), a random
variable depending on the function \(h\), \(X\) and \(Y\).

Assuming all the expectations exist and are finite, the \textbf{Law of
Iterated Expectations} states that
\[\mathrm{E}[h(X,Y)]= \mathrm{E} \left\{ \mathrm{E} \left[ h(X,Y) | X \right] \right \},\]
where the first (inside) expectation is taken with respect to the random
variable \(Y\) and the second (outside) expectation is taken with
respect to \(X\).

For the Law of Iterated Expectations, the random variables may be
discrete, continuous, or a hybrid combination of the two. We use the
example of discrete variables of \(X\) and \(Y\) to illustrate the
calculation of the unconditional expectation using the Law of Iterated
Expectations. For continuous random variables, we only need to replace
the summation with the integral, as illustrated earlier in the appendix.

Given \(p(y|x)\) the joint pmf of \(X\) and \(Y\), the conditional
expectation of \(h(X,Y)\) given the event \(X=x\) is defined as
\[\mathrm{E} \left[ h(X,Y) | X=x \right] = \sum_y h(x,y) p(y|x),\] and
the conditional expectation of \(h(X,Y)\) given \(X\) being a
\emph{random variable} can be written as
\[\mathrm{E} \left[ h(X,Y) | X \right] = \sum_y h(X,y) p(y|X).\]

The unconditional expectation of \(h(X,Y)\) can then be obtained by
taking the expectation of \(\mathrm{E} \left[ h(X,Y) | X \right]\) with
respect to the random variable \(X\). That is, we can obtain
\(\mathrm{E}[ h(X,Y)]\) as \[\begin{aligned}
     \mathrm{E} \left\{ \mathrm{E} \left[ h(X,Y) | X \right] \right \}
    &= \sum_x  \left\{\sum_y h(x,y) p(y|x) \right \} p(x) \\
    &= \sum_x  \sum_y h(x,y) p(y|x)p(x) \\
    &=  \sum_x  \sum_y h(x,y) p(x,y)
    =  \mathrm{E}[h(X,Y)] \end{aligned}.\]

The Law of Iterated Expectations for the continuous and hybrid cases can
be proved in a similar manner, by replacing the corresponding
summation(s) by integral(s).

\subsection{Law of Total Variance}\label{law-of-total-variance}

Assuming that all the variances exist and are finite, the \textbf{Law of
Total Variance} states that
\[\mathrm{Var}[h(X,Y)]= \mathrm{E} \left\{ \mathrm{Var} \left[h(X,Y) | X \right] \right \}
    +\mathrm{Var} \left\{ \mathrm{E} \left[ h(X,Y) | X \right] \right \},\]
where the first (inside) expectation/variance is taken with respect to
the random variable \(Y\) and the second (outside) expectation/variance
is taken with respect to \(X\). Thus, the unconditional variance equals
to the expectation of the conditional variance plus the variance of the
conditional expectation.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In order to verify this rule, first note that we can calculate a
conditional variance as
\[\mathrm{Var} \left[ h(X,Y) | X \right]  = \mathrm{E} [ h(X,Y)^2 | X ] -\left\{\mathrm{E} \left[ h(X,Y) | X \right] \right\}^2.\]

From this, the expectation of the conditional variance is

\begin{align}
    \mathrm{E}\{\mathrm{Var} \left[ h(X,Y) | X \right] \} &=
    \mathrm{E}\left\{\mathrm{E} \left[ h(X,Y)^2 | X \right] \right\} - \mathrm{E}\left(\left\{\mathrm{E} \left[ h(X,Y) | X \right] \right\}^2\right) \notag \\
    &=\mathrm{E} \left[ h(X,Y)^2\right] - \mathrm{E}\left(\left\{\mathrm{E} \left[ h(X,Y) | X \right] \right\}^2\right).\label{eq:AppBEV1}
\end{align}

Further, note that the conditional expectation,
\(\mathrm{E} \left[ h(X,Y) | X \right]\), is a function of \(X\),
denoted \(g(X)\). Thus, \(g(X)\) is a random variable with mean
\(\mathrm{E}[h(X,Y)]\) and variance

\begin{align}
    \mathrm{Var} \left\{ \mathrm{E} \left[ h(X,Y) | X \right] \right \} &=\mathrm{Var}[g(X)]  \notag \\
    &= \mathrm{E}[g(X)^2]\ - \left\{\mathrm{E}[g(X)]\right\}^2 \nonumber\\
    &= \mathrm{E}\left(\left\{\mathrm{E} \left[ h(X,Y) | X \right] \right\}^2\right)
    - \left\{\mathrm{E}[h(X,Y)]\right\}^2.\label{eq:AppBVE2}
\end{align}

Thus, adding Equations \eqref{eq:AppBEV1} and \eqref{eq:AppBVE2} leads to
the unconditional variance \(\mathrm{Var} \left[ h(X,Y) \right]\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Application}\label{application}

To apply the Law of Iterated Expectations and the Law of Total Variance,
we generally adopt the following procedure.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Identify the random variable that is being conditioned upon, typically
  a stage 1 outcome (that is not observed).
\item
  Conditional on the stage 1 outcome, calculate summary measures such as
  a mean, variance, and the like.
\item
  There are several results of the step 2, one for each stage 1 outcome.
  Then, combine these results using the iterated expectations or total
  variance rules.
\end{enumerate}

\textbf{Mixtures of Finite Populations.} Suppose that the random
variable \(N_1\) represents a realization of the number of claims in a
policy year from the population of good drivers and \(N_2\) represents
that from the population of bad drivers. For a specific driver, there is
a probability \(\alpha\) that (s)he is a good driver. For a specific
draw \(N\), we have \[N =
    \begin{cases}
    N_1,  &  \text{if (s)he is a good driver;}\\
    N_2,  &   \text{otherwise}.\\
    \end{cases}\]

Let \(T\) be the indicator whether (s)he is a good driver, with \(T=1\)
representing that the driver is a good driver with \(\Pr[T=1]=\alpha\)
and \(T=2\) representing that the driver is a bad driver with
\(\Pr[T=2]=1-\alpha\).

From the Law of Iterated Expectations, we can obtain the expected number
of claims as \[
    \mathrm{E}[N]= \mathrm{E} \left\{ \mathrm{E} \left[ N | T \right] \right \}= \mathrm{E}[N_1] \times \alpha +  \mathrm{E}[N_2] \times (1-\alpha).\]

From the Law of Total Variance, we can obtain the variance of \(N\) as
\[\mathrm{Var}[N]= \mathrm{E} \left\{ \mathrm{Var} \left[ N | T \right] \right \}
    +\mathrm{Var} \left\{ \mathrm{E} \left[ N | T \right] \right \}.\]

To be more concrete, suppose that \(N_j\) follows a Poisson distribution
with the mean \(\lambda_j\), \(j=1,2\). Then we have
\[\mathrm{Var}[N|T=j]= \mathrm{E}[N|T=j] = \lambda_j, \quad j = 1,2.\]

Thus, we can derive the expectation of the conditional variance as
\[\mathrm{E} \left\{ \mathrm{Var} \left[ N | T \right] \right \} = \alpha \lambda_1+ (1-\alpha) \lambda_2\]
and the variance of the conditional expectation as
\[\mathrm{Var} \left\{ \mathrm{E} \left[ N | T \right] \right \} = (\lambda_1-\lambda_2)^2 \alpha (1-\alpha).\]
Note that the later is the variance for a Bernoulli with outcomes
\(\lambda_1\) and \(\lambda_2\), and the binomial probability
\(\alpha\).

Based on the Law of Total Variance, the unconditional variance of \(N\)
is given by
\[\mathrm{Var}[N]= \alpha \lambda_1+ (1-\alpha) \lambda_2 + (\lambda_1-\lambda_2)^2 \alpha (1-\alpha).\]

\section{Conjugate Distributions}\label{S:AppConjugateDistributions}

As described in Section \ref{S:IntroBayes}, for conjugate distributions
the posterior and the prior come from the same family of distributions.
In insurance applications, this broadly occurs in a ``family of
distribution familes'' known as the linear exponential family which we
introduce first.

\subsection{Linear Exponential Family}\label{linear-exponential-family}

\textbf{Definition.} The distribution of the \emph{linear exponential
family} is

\begin{equation}
f( x; \gamma ,\theta ) =
\exp \left( \frac{x\gamma -b(\gamma )}{\theta} +S\left( x,\theta \right) \right).
\end{equation}

Here, \(x\) is a dependent variable and \(\gamma\) is the parameter of
interest. The quantity \(\theta\) is a scale parameter. The term
\(b(\gamma)\) depends only on the parameter \(\gamma\), not the
dependent variable. The statistic \(S\left(x,\theta \right)\) is a
function of the dependent variable and the scale parameter, not the
parameter \(\gamma\).

The dependent variable \(x\) may be discrete, continuous or a hybrid
combination of the two. Thus, \(f\left( .\right)\) may be interpreted to
be a density or mass function, depending on the application. The
following table provides several examples, including the normal,
binomial and Poisson distributions.

\[
{\small
\begin{matrix}
\text{Selected Distributions of the Linear Exponential Family}\\
\begin{array}{l|ccccc}
\hline
             &             & \text{Density or} & & & \\
\text{Distribution} & \text{Parameters} & \text{Mass Function} & \text{Components} \\
\hline \text{General} & \gamma,~ \theta & 
\exp \left( \frac{x\gamma -b(\gamma )}{\theta} +S\left( x,\theta \right) \right) & 
\gamma,~ \theta, b(\gamma), S(x, \theta)\\
 \text{Normal} & \mu, \sigma^2  &
\frac{1}{\sigma \sqrt{2\pi }}\exp \left(-\frac{(x-\mu )^{2}}{2\sigma ^{2}}\right) & 
\mu, \sigma^2, \frac{\gamma^2}{2}, - \left(\frac{x^2}{2\theta} + \frac{\ln(2 \pi
\theta)}{2} \right) \\
\text{Binomal} & \pi & 
{n \choose x} \pi ^x (1-\pi)^{n-x} & 
\ln
\left(\frac{\pi}{1-\pi} \right), 1, n \ln(1+e^{\gamma} ),  \\
&  &  &  \ln {n \choose x} \\
\text{Poisson} & \lambda & 
\frac{\lambda^x}{x!} \exp(-\lambda)  & 
\ln \lambda, 1, e^{\gamma}, - \ln (x!)  \\
\text{Negative } & 
r,p &  \frac{\Gamma(x+r)}{x!\Gamma(r)} p^r ( 1-p)^x & 
\ln(1-p), 1, -r \ln(1-e^{\gamma}), \\
~~~\text{Binomial}^{\ast} & & & ~~~\ln \left[ \frac{\Gamma(x+r)}{x!
\Gamma(r)} \right] \\
\text{Gamma} & \alpha, \theta  & \frac{1}{\Gamma (\alpha)\theta ^ \alpha}
x^{\alpha -1 }\exp(-x/ \theta)  & - \frac{1}{\alpha \gamma},
\frac{1}{\alpha}, - \ln ( - \gamma), -\theta^{-1} \ln \theta \\
& & &  - \ln \left( \Gamma(\theta ^{-1}) \right) +
(\theta^{-1} - 1) \ln x & & \\ \hline
\end{array}\\
^{\ast} \text{This assumes that the parameter r is fixed but need not be an integer.}\\ 
\end{matrix}
}
\]

The Tweedie (see Section \ref{S:TweedieDistribution}) and inverse
Gaussian distributions are also members of the linear exponential
family. The linear exponential family of distribution families is
extensively used as the basis of generalized linear models as described
in, for example, \citet{frees2009regression}.

\subsection{Conjugate Distributions}\label{conjugate-distributions}

Now assume that the parameter \(\gamma\) is random with distribution
\(\pi(\gamma, \tau)\), where \(\tau\) is a vector of parameters that
describe the distribution of \(\gamma\). In Bayesian models, the
distribution \(\pi\) is known as the prior and reflects our belief or
information about \(\gamma\). The likelihood \(f(x|\gamma)\) is a
probability conditional on \(\gamma\). The distribution of \(\gamma\)
with knowledge of the random variables, \(\pi(\gamma,\tau| x)\), is
called the posterior distribution. For a given likelihood distribution,
priors and posteriors that come from the same parametric family are
known as conjugate families of distributions.

For a linear exponential likelihood, there exists a natural conjugate
family. Specifically, consider a likelihood of the form
\(f(x|\gamma) = \exp \left\{(x\gamma -b(\gamma))/\theta\right\} \exp \left\{S\left( x,\theta \right) \right\}\).
For this likelihood, define the prior distribution \[
\pi(\gamma,\tau) = C \exp\left\{
\gamma a_1(\tau) - b(\gamma)a_2(\tau))\right\},
\] where \(C\) is a normalizing constant. Here, \(a_1(\tau)=a_1\) and
\(a_2(\tau)=a_2\) are functions of the parameters \(\tau\) although we
simplify the notation by dropping explicit dependence on \(\tau\). The
joint distribution of \(x\) and \(\gamma\) is given by
\(f(x,\gamma) = f(x|\gamma) \pi(\gamma,\tau)\). Using Bayes Theorem, the
posterior distribution is \[
\pi(\gamma,\tau|x) = C_1 \exp\left\{
\gamma \left( a_1+\frac{x}{\theta}\right) - b(\gamma)\left( a_2+\frac{1}{\theta}\right)\right\},
\] where \(C_1\) is a normalizing constant. Thus, we see that
\(\pi(\gamma,\tau|x)\) has the same form as \(\pi(\gamma,\tau)\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Special case. Poisson-Gamma Model.} Consider a Poisson
likelihood so that \(b(\gamma) = e^{\gamma}\) and scale parameter
(\(\theta\)) equals one. Thus, we have \[
\pi(\gamma,\tau) = C \exp\left\{
\gamma a_1 - a_2 e^{\gamma} \right\}=
C ~ 
\left(
e^{\gamma}\right)^{a_1}
\exp\left(-a_2e^{\gamma} \right).
\] From the table of exponential family distributions, we recognize this
to be a gamma distribution. That is, we have that the prior distribution
of \(\lambda = e^{\gamma}\) is a gamma distribution with parameters
\(\alpha_{prior} = a_1+1\) and \(\theta_{prior}^{-1}=a_2\). The
posterior distribution is a gamma distribution with parameters
\(\alpha_{post} =a_1+x+1=\alpha_{prior}+x\) and
\(\theta_{post}^{-1}=a_2+1 = \theta_{prior}^{-1}+1\). This is consistent
with our Section \ref{S:PosteriorDistribution} result.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Special case. Normal-Normal Model.} Consider a normal likelihood
so that \(b(\gamma) = \gamma^2/2\) and the scale parameter is
\(\sigma^2\). Thus, we have \[
\pi(\gamma,\tau) = C \exp\left\{
\gamma a_1 - \frac{\gamma^2}{2}a_2\right\}=
C_1(\tau)\exp\left\{ - \frac{a_2}{2}\left(\gamma -\frac{a_1}{a_2}\right)^2\right\},
\] The prior distribution of \(\gamma\) is normal with mean \(a_1/a_2\)
and variance \(a_2^{-1}\). The posterior distribution of \(\gamma\)
given \(x\) is normal with mean \((a_1+x/\sigma^2)/(a_2+\sigma^{-2})\)
and variance \((a_2+\sigma^{-2})^{-1}\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Special case. Beta-Binomial Model.} Consider a binomial
likelihood so that \(b(\gamma) = n \ln(1+e^{\gamma})\) and scale
parameter equals one. Thus, we have \[
\pi(\gamma,\tau) = C \exp\left\{
\gamma a_1 - n a_2 \ln(1+e^{\gamma}) \right\}=
C ~ 
\left(
\frac{e^{\gamma}}{1+e^{\gamma}}\right)^{a_1}
\left(1-\frac{e^{\gamma}}{1+e^{\gamma}}\right)^{-na_2+a_1}.
\]

This is a beta distribution. As in the other cases, prior parameters
\(a_1\) and \(a_2\) are updated to become posterior parameters \(a_1+x\)
and \(a_2+1\).

\subsubsection*{Contributors}\label{contributors-10}
\addcontentsline{toc}{subsubsection}{Contributors}

\begin{itemize}
\tightlist
\item
  \textbf{Lei (Larry) Hua}, Northern Illinois University, and
  \textbf{Edward W. (Jed) Frees}, University of Wisconsin-Madison, are
  the principal authors of the initial version of this chapter. Email:
  \href{mailto:lhua@niu.edu}{\nolinkurl{lhua@niu.edu}} or
  \href{mailto:jfrees@bus.wisc.edu}{\nolinkurl{jfrees@bus.wisc.edu}} for
  chapter comments and suggested improvements.
\end{itemize}

\chapter{Appendix C: Maximum Likelihood Theory}\label{C:AppC}

\emph{Chapter preview}. Appendix Chapter \ref{C:AppA} introduced the
maximum likelihood theory regarding estimation of parameters from a
parametric family. This appendix gives more specific examples and
expands some of the concepts. Section \ref{S:AppC:LF} reviews the
definition of the likelihood function, and introduces its properties.
Section \ref{S:AppC:MLE} reviews the maximum likelihood estimators, and
extends their large-sample properties to the case where there are
multiple parameters in the model. Section \ref{S:AppC:SI} reviews
statistical inference based on maximum likelihood estimators, with
specific examples on cases with multiple parameters.

\section{Likelihood Function}\label{S:AppC:LF}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn

\begin{itemize}
\tightlist
\item
  the definitions of the likelihood function and the log-likelihood
  function
\item
  the properties of the likelihood function.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

From Appendix \ref{C:AppA}, the likelihood function is a function of
parameters given the observed data. Here, we review the concepts of the
likelihood function, and introduces its properties that are bases for
maximum likelihood inference.

\subsection{Likelihood and Log-likelihood
Functions}\label{likelihood-and-log-likelihood-functions}

Here, we give a brief review of the likelihood function and the
log-likelihood function from Appendix \ref{C:AppA}. Let
\(f(\cdot|\boldsymbol\theta)\) be the probability function of \(X\), the
probability mass function (pmf) if \(X\) is discrete or the probability
density function (pdf) if it is continuous. The likelihood is a function
of the parameters (\(\boldsymbol \theta\)) given the data
(\(\mathbf{x}\)). Hence, it is a function of the parameters with the
data being fixed, rather than a function of the data with the parameters
being fixed. The vector of data \(\mathbf{x}\) is usually a realization
of a \emph{random sample} as defined in Appendix \ref{C:AppA}.

Given a realized of a random sample \(\mathbf{x}=(x_1,x_2,\cdots,x_n)\)
of size \(n\), the \textbf{likelihood function} is defined as
\[L(\boldsymbol{\theta}|\mathbf{x})=f(\mathbf{x}|\boldsymbol{\theta})=\prod_{i=1}^nf(x_i|\boldsymbol{\theta}),\]
with the corresponding \textbf{log-likelihood function} given by
\[l(\boldsymbol{\theta}|\mathbf{x})=\ln L(\boldsymbol{\theta}|\mathbf{x})=\sum_{i=1}^n\ln f(x_i|\boldsymbol{\theta}),\]
where \(f(\mathbf{x}|\boldsymbol{\theta})\) denotes the joint
probability function of \(\mathbf{x}\). The log-likelihood function
leads to an additive structure that is easy to work with.

In Appendix \ref{C:AppA}, we have used the normal distribution to
illustrate concepts of the likelihood function and the log-likelihood
function. Here, we derive the likelihood and corresponding
log-likelihood functions when the population distribution is from the
Pareto distribution family.

\textbf{Example -- Pareto Distribution.} Suppose that
\(X_1, \ldots, X_n\) represents a random sample from a single-parameter
Pareto distribution with the \textbf{cumulative distribution function}
given by
\[F(x) = \Pr(X_i\leq x)=1- \left(\frac{500}{x}\right)^{\alpha}, ~~~~ x>500,\]
where the parameter \(\theta = \alpha\).

The corresponding probability density function is
\(f(x) = 500^{\alpha} \alpha x^{-\alpha-1}\) and the log-likelihood
function can be derived as
\[l(\boldsymbol \alpha|\mathbf{x}) = \sum_{i=1}^n \ln f(x_i;\alpha) = n \alpha \ln 500 +n \ln \alpha -(\alpha+1)  \sum_{i=1}^n \ln x_i .\]

\subsection{Properties of Likelihood
Functions}\label{properties-of-likelihood-functions}

In mathematical statistics, the first derivative of the log-likelihood
function with respect to the parameters,
\(u(\boldsymbol\theta)=\partial l(\boldsymbol \theta|\mathbf{x})/\partial \boldsymbol \theta\),
is referred to as the \textbf{score function}, or the \textbf{score
vector} when there are multiple parameters in \(\boldsymbol\theta\). The
score function or score vector can be written as
\[u(\boldsymbol\theta)=\frac{ \partial}{\partial \boldsymbol \theta} l(\boldsymbol \theta|\mathbf{x})
    =\frac{ \partial}{\partial \boldsymbol \theta} \ln \prod_{i=1}^n
    f(x_i;\boldsymbol \theta ) =\sum_{i=1}^n \frac{
    \partial}{\partial \boldsymbol \theta}
    \ln f(x_i;\boldsymbol \theta ),\] where
\(u(\boldsymbol\theta)=(u_1(\boldsymbol\theta),u_2(\boldsymbol\theta),\cdots,u_p(\boldsymbol\theta))\)
when \(\boldsymbol\theta=(\theta_1,\cdots,\theta_p)\) contains \(p>2\)
parameters, with the element
\(u_k(\boldsymbol\theta)=\partial l(\boldsymbol \theta|\mathbf{x})/\partial \theta_k\)
being the partial derivative with respect to \(\theta_k\)
(\(k=1,2,\cdots,p\)).

The likelihood function has the following properties:

\begin{itemize}
\tightlist
\item
  One basic property of the likelihood function is that the expectation
  of the score function with respect to \(\mathbf{x}\) is 0. That is,
  \[\mathrm{E}[u(\boldsymbol\theta)]=\mathrm{E} \left[ \frac{ \partial}{\partial \boldsymbol \theta}
  l(\boldsymbol \theta|\mathbf{x}) \right] = \mathbf 0\]
\end{itemize}

To illustrate this, we have

\[\begin{aligned}
    \mathrm{E} \left[ \frac{ \partial}{\partial \boldsymbol \theta} l(\boldsymbol \theta|\mathbf{x}) \right]
    &= \mathrm{E} \left[ \frac{\frac{\partial}{\partial \boldsymbol \theta}f(\mathbf{x};\boldsymbol \theta)}{f(\mathbf{x};\boldsymbol \theta )}  \right]
    = \int\frac{\partial}{\partial \boldsymbol \theta} f(\mathbf{y};\boldsymbol \theta ) d \mathbf y \\
    &= \frac{\partial}{\partial \boldsymbol \theta} \int f(\mathbf{y};\boldsymbol \theta ) d \mathbf y
    = \frac{\partial}{\partial \boldsymbol \theta} 1 = \mathbf 0.\end{aligned}\]

\begin{itemize}
\item
  Denote by
  \({ \partial^2 l(\boldsymbol \theta|\mathbf{x}) }/{\partial \boldsymbol \theta\partial \boldsymbol \theta^{\prime}}={ \partial^2 l(\boldsymbol \theta|\mathbf{x}) }/{\partial \boldsymbol \theta^{2}}\)
  the second derivative of the log-likelihood function when
  \(\boldsymbol\theta\) is a single parameter, or by
  \({ \partial^2 l(\boldsymbol \theta|\mathbf{x}) }/{\partial \boldsymbol \theta\partial \boldsymbol \theta^{\prime}}=(h_{jk})=({ \partial^2 l(\boldsymbol \theta|\mathbf{x}) }/\partial x_j\partial x_k)\)
  the hessian matrix of the log-likelihood function when it contains
  multiple parameters. Denote
  \([{ \partial l(\boldsymbol \theta|\mathbf{x})}{\partial\boldsymbol \theta}][{ \partial l(\boldsymbol \theta|\mathbf{x})}{\partial\boldsymbol \theta'}]=u^2(\boldsymbol \theta)\)
  when \(\boldsymbol\theta\) is a single parameter, or let
  \([{ \partial l(\boldsymbol \theta|\mathbf{x})}{\partial\boldsymbol \theta}][{ \partial l(\boldsymbol \theta|\mathbf{x})}{\partial\boldsymbol \theta'}]=(uu_{jk})\)
  be a \(p\times p\) matrix when \(\boldsymbol\theta\) contains a total
  of \(p\) parameters, with each element
  \(uu_{jk}=u_j(\boldsymbol \theta)u_k(\boldsymbol \theta)\) and
  \(u_j(\boldsymbol \theta)\) being the \(k\)th element of the score
  vector as defined earlier. Another basic property of the likelihood
  function is that sum of the expectation of the hessian matrix and the
  expectation of the kronecker product of the score vector and its
  transpose is \(\mathbf 0\). That is,
  \[\mathrm{E} \left( \frac{ \partial^2 }{\partial \boldsymbol \theta\partial \boldsymbol \theta^{\prime}} l(\boldsymbol \theta|\mathbf{x}) \right) + \mathrm{E} \left( \frac{ \partial l(\boldsymbol \theta|\mathbf{x})}{\partial\boldsymbol \theta} \frac{ \partial l(\boldsymbol \theta|\mathbf{x})}{\partial\boldsymbol \theta^{\prime}}\right) = \mathbf 0.\]
\item
  Define the \textbf{Fisher information matrix} as \[
  \mathcal{I}(\boldsymbol \theta) = \mathrm{E} \left( \frac{ \partial
  l(\boldsymbol \theta|\mathbf{x})}{\partial \boldsymbol \theta} \frac{ \partial
  l(\boldsymbol \theta|\mathbf{x})}{\partial \boldsymbol \theta^{\prime}}
   \right) = -\mathrm{E} \left( \frac{ \partial^2}{\partial \boldsymbol \theta
  \partial \boldsymbol \theta^{\prime}} l(\boldsymbol \theta|\mathbf{x}) \right).\]
\end{itemize}

As the sample size \(n\) goes to infinity, the score function (vector)
converges in distribution to a \textbf{normal distribution} (or
\textbf{multivariate normal distribution} when \(\boldsymbol \theta\)
contains multiple parameters) with mean \textbf{0} and variance (or
covariance matrix in the multivariate case) given by
\(\mathcal{I}(\boldsymbol \theta)\).

\section{Maximum Likelihood Estimators}\label{S:AppC:MLE}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn

\begin{itemize}
\tightlist
\item
  the definition and derivation of the maximum likelihood estimator
  (MLE) for parameters from a specific distribution family
\item
  the properties of maximum likelihood estimators that ensure valid
  large-sample inference of the parameters
\item
  why using the MLE-based method, and what caution that needs to be
  taken.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In statistics, maximum likelihood estimators are values of the
parameters \(\boldsymbol \theta\) that are most likely to have been
produced by the data.

\subsection{Definition and Derivation of
MLE}\label{definition-and-derivation-of-mle}

Based on the definition given in Appendix \ref{C:AppA}, the value of
\(\boldsymbol \theta\), say \(\hat{\boldsymbol \theta}_{MLE}\), that
maximizes the likelihood function, is called the \emph{maximum
likelihood estimator} (MLE) of \(\boldsymbol \theta\).

Because the log function \(\ln(\cdot)\) is a one-to-one function, we can
also determine \(\hat{\boldsymbol{\theta}}_{MLE}\) by maximizing the
log-likelihood function, \(l(\boldsymbol \theta|\mathbf{x})\). That is,
the MLE is defined as
\[\hat{\boldsymbol \theta}_{MLE}={\mbox{argmax}}_{\boldsymbol{\theta}\in\Theta}l(\boldsymbol{\theta}|\mathbf{x}).\]

Given the analytical form of the likelihood function, the MLE can be
obtained by taking the first derivative of the log-likelihood function
with respect to \(\boldsymbol{\theta}\), and setting the values of the
partial derivatives to zero. That is, the MLE are the solutions of the
equations of
\[\frac{\partial l(\hat{\boldsymbol{\theta}}|\mathbf{x})}{\partial\hat{\boldsymbol{\theta}}}=\mathbf 0.\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example. Course C/Exam 4. May 2000, 21.} You are given the
following five observations: 521, 658, 702, 819, 1217. You use the
single-parameter Pareto with cumulative distribution function:
\[F(x) = 1- \left(\frac{500}{x}\right)^{\alpha}, ~~~~ x>500 .\]
Calculate the maximum likelihood estimate of the parameter \(\alpha\).

\emph{Solution}. With \(n=5\), the log-likelihood function is
\[l(\alpha|\mathbf{x} ) =  \sum_{i=1}^5 \ln f(x_i;\alpha ) =  5 \alpha \ln 500 + 5 \ln \alpha
-(\alpha+1) \sum_{i=1}^5 \ln x_i.\] Solving for the root of the score
function yields
\[\frac{ \partial}{\partial \alpha } l(\alpha |\mathbf{x}) =    5  \ln 500 + 5 / \alpha -  \sum_{i=1}^5 \ln x_i
=_{set} 0 \Rightarrow \hat{\alpha}_{MLE} = \frac{5}{\sum_{i=1}^5 \ln x_i - 5  \ln 500 } = 2.453 .\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Asymptotic Properties of
MLE}\label{asymptotic-properties-of-mle}

From Appendix \ref{C:AppA}, the MLE has some nice large-sample
properties, under certain regularity conditions. We presented the
results for a single parameter in Appendix \ref{C:AppA}, but results are
true for the case when \(\boldsymbol{\theta}\) contains multiple
parameters. In particular, we have the following results, in a general
case when \(\boldsymbol{\theta}=(\theta_1,\theta_2,\cdots,\theta_p)\).

\begin{itemize}
\item
  The MLE of a parameter \(\boldsymbol{\theta}\),
  \(\hat{\boldsymbol{\theta}}_{MLE}\), is a \textbf{consistent}
  estimator. That is, the MLE \(\hat{\boldsymbol{\theta}}_{MLE}\)
  converges in probability to the true value \(\boldsymbol{\theta}\), as
  the sample size \(n\) goes to infinity.
\item
  The MLE has the \textbf{asymptotic normality} property, meaning that
  the estimator will converge in distribution to a multivariate normal
  distribution centered around the true value, when the sample size goes
  to infinity. Namely,
  \[\sqrt{n}(\hat{\boldsymbol{\theta}}_{MLE}-\boldsymbol{\theta})\rightarrow N\left(\mathbf 0,\,\boldsymbol{V}\right),\quad \mbox{as}\quad n\rightarrow \infty,\]
  where \(\boldsymbol{V}\) denotes the asymptotic variance (or
  covariance matrix) of the estimator. Hence, the MLE
  \(\hat{\boldsymbol{\theta}}_{MLE}\) has an approximate normal
  distribution with mean \(\boldsymbol{\theta}\) and variance
  (covariance matrix when \(p>1\)) \(\boldsymbol{V}/n\), when the sample
  size is large.
\item
  The MLE is \textbf{efficient}, meaning that it has the smallest
  asymptotic variance \(\boldsymbol{V}\), commonly referred to as the
  \textbf{Cramer--Rao lower bound}. In particular, the Cramer--Rao lower
  bound is the inverse of the Fisher information (matrix)
  \(\mathcal{I}(\boldsymbol{\theta})\) defined earlier in this appendix.
  Hence, \(\mathrm{Var}(\hat{\boldsymbol{\theta}}_{MLE})\) can be
  estimated based on the observed Fisher information.
\end{itemize}

Based on the above results, we may perform statistical inference based
on the procedures defined in Appendix \ref{C:AppA}.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Example. Course C/Exam 4. Nov 2000, 13.} A sample of ten
observations comes from a parametric family
\(f(x,; \theta_1, \theta_2)\) with log-likelihood function
\[l(\theta_1, \theta_2)= \sum_{i=1}^{10} f(x_i; \theta_1, \theta_2) = -2.5 \theta_1^2 - 3
    \theta_1 \theta_2 - \theta_2^2 + 5 \theta_1 + 2 \theta_2 + k,\]
where \(k\) is a constant. Determine the estimated covariance matrix of
the maximum likelihood estimator, \(\hat{\theta_1}, \hat{\theta_2}\).

\emph{Solution}. Denoting \(l=l(\theta_1, \theta_2)\), the hessian
matrix of second derivatives is \[\left(
\begin{array}{cc}
  \frac{ \partial ^2}{\partial \theta_1 ^2 } l & \frac{ \partial ^2}{\partial \theta_1 \partial \theta_2 } l  \\
  \frac{ \partial ^2}{\partial \theta_1 \partial \theta_2 } l & \frac{ \partial ^2}{\partial \theta_1 ^2 } l
\end{array} \right) =
\left(
\begin{array}{cc}
  -5 & -3  \\
  -3 & -2
\end{array} \right)\] Thus, the information matrix is:
\[\mathcal{I}(\theta_1, \theta_2) = -\mathrm{E} \left( \frac{ \partial^2}{\partial \boldsymbol \theta
\partial \boldsymbol \theta^{\prime}} l(\boldsymbol \theta|\mathbf{x}) \right) = \left(
\begin{array}{cc}
  5 & 3  \\
  3 & 2
\end{array} \right)\] and
\[\mathcal{I}^{-1}(\theta_1, \theta_2) = \frac{1}{5(2) - 3(3)}\left(
\begin{array}{cc}
  2 & -3  \\
  -3 & 5
\end{array} \right) = \left(
\begin{array}{cc}
  2 & -3  \\
  -3 & 5
\end{array} \right) .\]

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Use of Maximum Likelihood
Estimation}\label{use-of-maximum-likelihood-estimation}

The method of maximum likelihood has many advantages over alternative
methods such as the method of moment method introduced in Appendix
\ref{C:AppA}.

\begin{itemize}
\tightlist
\item
  It is a general tool that works in many situations. For example, we
  may be able to write out the closed-form likelihood function for
  censored and truncated data. Maximum likelihood estimation can be used
  for regression models including covariates, such as survival
  regression, generalized linear models and mixed models, that may
  include covariates that are time-dependent.
\item
  From the efficiency of the MLE, it is optimal, the best, in the sense
  that it has the smallest variance among the class of all unbiased
  estimators for large sample sizes.
\item
  From the results on the asymptotic normality of the MLE, we can obtain
  a large-sample distribution for the estimator, allowing users to
  assess the variability in the estimation and perform statistical
  inference on the parameters. The approach is less computationally
  extensive than re-sampling methods that require a large of fittings of
  the model.
\end{itemize}

Despite its numerous advantages, MLE has its drawback in cases such as
generalized linear models when it does not have a closed analytical
form. In such cases, maximum likelihood estimators are computed
iteratively using numerical optimization methods. For example, we may
use the Newton-Raphson iterative algorithm or its variations for
obtaining the MLE. Iterative algorithms require starting values. For
some problems, the choice of a close starting value is critical,
particularly in cases where the likelihood function has local minimums
or maximums. Hence, there may be a convergence issue when the starting
value is far from the maximum. Hence, it is important to start from
different values across the parameter space, and compare the maximized
likelihood or log-likelihood to make sure the algorithms have converged
to a global maximum.

\section{Statistical Inference Based on Maximum Likelhood
Estimation}\label{S:AppC:SI}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this section, you learn how to

\begin{itemize}
\tightlist
\item
  perform hypothesis testing based on MLE for cases where there are
  multiple parameters in \(\boldsymbol\theta\)
\item
  perform likelihood ratio test for cases where there are multiple
  parameters in \(\boldsymbol\theta\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In Appendix \ref{C:AppA}, we have introduced maximum likelihood-based
methods for statistical inference when \(\boldsymbol\theta\) contains a
single parameter. Here, we will extend the results to cases where there
are multiple parameters in \(\boldsymbol\theta\).

\subsection{Hypothesis Testing}\label{hypothesis-testing}

In Appendix \ref{C:AppA}, we defined hypothesis testing concerning the
null hypothesis, a statement on the parameter(s) of a distribution or
model. One important type of inference is to assess whether a parameter
estimate is statistically significant, meaning whether the value of the
parameter is zero or not.

We have learned earlier that the MLE \(\hat{\boldsymbol{\theta}}_{MLE}\)
has a large-sample normal distribution with mean \(\boldsymbol \theta\)
and the variance covariance matrix
\(\mathcal{I}^{-1}(\boldsymbol \theta)\). Based on the multivariate
normal distribution, the \(j\)th element of
\(\hat{\boldsymbol{\theta}}_{MLE}\), say \(\hat{\theta}_{MLE,j}\), has a
large-sample univariate normal distribution.

Define \(se(\hat{\theta}_{MLE,j})\), the standard error (estimated
standard deviation) to be the square root of the \(j\)th diagonal
element of \(\mathcal{I}^{-1}(\boldsymbol \theta)_{MLE}\). To assess the
null hypothesis that \(\theta_j=\theta_0\), we define the
\(t\)-statistic or \(t\)-ratio to be
\(t(\hat{\theta}_{MLE,j})=(\hat{\theta}_{MLE,j}-\theta_0)/se(\hat{\theta}_{MLE,j})\).

Under the null hypothesis, it has a Student-\(t\) distribution with
degrees of freedom equal to \(n-p\), with \(p\) being the dimension of
\(\boldsymbol{\theta}\).

For most actuarial applications, we have a large sample size \(n\), so
the \(t\)-distribution is very close to the (standard) normal
distribution. In the case when \(n\) is very large or when the standard
error is known, the \(t\)-statistic can be referred to as a
\(z\)-statistic or \(z\)-score.

Based on the results from Appendix \ref{C:AppA}, if the \(t\)-statistic
\(t(\hat{\theta}_{MLE,j})\) exceeds a cut-off (in absolute value), then
the test for the \(j\) parameter \(\theta_j\) is said to be
statistically significant. If \(\theta_j\) is the regression coefficient
of the \(j\) th independent variable, then we say that the \(j\)th
variable is statistically significant.

For example, if we use a 5\% significance level, then the cut-off value
is 1.96 using a normal distribution approximation for cases with a large
sample size. More generally, using a \(100 \alpha \%\) significance
level, then the cut-off is a \(100(1-\alpha/2)\%\) quantile from a
Student-\(t\) distribution with the degree of freedom being \(n-p\).

Another useful concept in hypothesis testing is the \(p\)-value,
shorthand for probability value. From the mathematical definition in
Appendix \ref{C:AppA}, a \(p\)-value is defined as the smallest
significance level for which the null hypothesis would be rejected.
Hence, the \(p\)-value is a useful summary statistic for the data
analyst to report because it allows the reader to understand the
strength of statistical evidence concerning the deviation from the null
hypothesis.

\subsection{MLE and Model Validation}\label{S:AppC:MLEModelVal}

In addition to hypothesis testing and interval estimation introduced in
Appendix \ref{C:AppA} and the previous subsection, another important
type of inference is selection of a model from two choices, where one
choice is a special case of the other with certain parameters being
restricted. For such two models with one being nested in the other, we
have introduced the likelihood ratio test (LRT) in Appendix
\ref{C:AppA}. Here, we will briefly review the process of performing a
LRT based on a specific example of two alternative models.

Suppose that we have a (large) model under which we derive the maximum
likelihood estimator, \(\hat{\boldsymbol{\theta}}_{MLE}\). Now assume
that some of the \(p\) elements in \(\boldsymbol \theta\) are equal to
zero and determine the maximum likelihood estimator over the remaining
set, with the resulting estimator denoted
\(\hat{\boldsymbol{\theta}}_{Reduced}\).

Based on the definition in Appendix \ref{C:AppA}, the statistic,
\(LRT= 2 \left( l(\hat{\boldsymbol{\theta}}_{MLE}) - l(\hat{\boldsymbol{\theta}}_{Reduced}) \right)\),
is called the likelihood ratio statistic. Under the null hypothesis that
the reduced model is correct, the likelihood ratio has a Chi-square
distribution with degrees of freedom equal to \(d\), the number of
variables set to zero.

Such a test allows us to judge which of the two models is more likely to
be correct, given the observed data. If the statistic \(LRT\) is large
relative to the critical value from the chi-square distribution, then we
reject the reduced model in favor of the larger one. Details regarding
the critical value and alternative methods based on information criteria
are given in Appendix \ref{C:AppA}.

\subsubsection*{Contributors}\label{contributors-11}
\addcontentsline{toc}{subsubsection}{Contributors}

\begin{itemize}
\tightlist
\item
  \textbf{Lei (Larry) Hua}, Northern Illinois University, and
  \textbf{Edward W. (Jed) Frees}, University of Wisconsin-Madison, are
  the principal authors of the initial version of this chapter. Email:
  \href{mailto:lhua@niu.edu}{\nolinkurl{lhua@niu.edu}} or
  \href{mailto:jfrees@bus.wisc.edu}{\nolinkurl{jfrees@bus.wisc.edu}} for
  chapter comments and suggested improvements.
\end{itemize}

\chapter{Appendix D: Summary of
Distributions}\label{C:SummaryDistributions}

\textbf{User Notes}

\begin{itemize}
\tightlist
\item
  The \texttt{R} functions are from the packages \texttt{actuar} and
  \texttt{invgamma}.
\item
  Tables appear when first loaded by the browser. To hide them, click on
  one of the distributions, e.g., \emph{Poisson}, and then click on the
  \emph{Hide} button.
\item
  More information on the \texttt{R} codes is available at the
  \href{https://ewfrees.github.io/LDARcode/index.html}{R Codes for Loss
  Data Analytics} site.
\end{itemize}

\section{Discrete Distributions}\label{discrete-distributions}

\textbf{Overview.} This section summarizes selected discrete probability
distributions used throughout \emph{Loss Data Analytics}. Relevant
functions and \texttt{R} code are provided.

\subsection{The (a,b,0) Class}\label{the-ab0-class}

Poisson

Geometric

Binomial

Negative Binomial

\hypertarget{disA}{}
Hide

Poisson

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
      \hline
  \small{\text{Parameter assumptions}} & \lambda>0 \\
\hline
  ~~p_0 & e^{-\lambda} \\
\hline
  \small{\text{Probability mass function}} & \frac{e^{-\lambda}\lambda^k}{k!} \\
  ~~p_k & \\
\hline
  \small{\text{Expected value}} & \lambda \\
  ~~\mathrm{E}[N] & \\
\hline
  \small{\text{Variance}} & \lambda \\
\hline
  \small{\text{Probability generating function}} & e^{\lambda(z-1)} \\
  ~~P(z) & \\
  \hline
  a \small{\text{ and }} b \small{\text{ for recursion}} & a=0 \\
   & b=\lambda \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Probability mass function}} & \text{dpois}(x=, lambda=\lambda) \\
\hline
  \small{\text{Distribution function}} & \text{ppois}(p=, lambda=\lambda) \\
\hline
  \small{\text{Quantile function}} & \text{qpois}(q=, lambda=\lambda) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rpois}(n=, lambda=\lambda) \\
\hline
\end{array}
\end{matrix}
\]

\hypertarget{disB}{}
Hide

Geometric

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
      \hline
  \small{\text{Parameter assumptions}} & \beta>0 \\
\hline
  ~~p_0 & \frac{1}{1+\beta} \\
\hline
  \small{\text{Probability mass function}} & \frac{\beta^k}{(1+\beta)^{k+1}} \\
  ~~p_k & \\
\hline
  \small{\text{Expected value}} & \beta \\
  ~~\mathrm{E}[N] & \\
\hline
  \small{\text{Variance}} & \beta(1+\beta) \\
\hline
  \small{\text{Probability generating function}} & [1-\beta(z-1)]^{-1} \\
  ~~P(z) & \\
  \hline
  a \small{\text{ and }} b \small{\text{ for recursion}} & a=\frac{\beta}{1+\beta} \\
   & b=0 \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Probability mass function}} & \text{dgeom}(x=, prob=\frac{1}{1+\beta}) \\
\hline
  \small{\text{Distribution function}} & \text{pgeom}(p=, prob=\frac{1}{1+\beta}) \\
\hline
  \small{\text{Quantile function}} & \text{qgeom}(q=, prob=\frac{1}{1+\beta}) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rgeom}(n=, prob=\frac{1}{1+\beta}) \\
\hline
\end{array}
\end{matrix}
\]

\hypertarget{disC}{}
Hide

Binomial

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & 0<q<1,~\text{m is an integer} \\
   & 0 \leq k \leq m\\
\hline
  ~~p_0 &(1-q)^m \\
\hline
  \small{\text{Probability mass function}} & \binom{m}{k}q^k(1-q)^{m-k} \\
  ~~p_k & \\
\hline
  \small{\text{Expected value}} & mq \\
  ~~\mathrm{E}[N] & \\
\hline
  \small{\text{Variance}} & mq(1-q) \\
\hline
  \small{\text{Probability generating function}} & [1+q(z-1)]^m \\
  ~~P(z) & \\
  \hline
  a \small{\text{ and }} b \small{\text{ for recursion}} & a=\frac{-q}{1-q} \\
   & b=\frac{(m+1)q}{1-q} \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Probability mass function}} & \text{dbinom}(x=, size=m, prob=p) \\
\hline
  \small{\text{Distribution function}} & \text{pbinom}(p=, size=m, prob=p) \\
\hline
  \small{\text{Quantile function}} & \text{qbinom}(q=, size=m, prob=p) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rbinom}(n=, size=m, prob=p) \\
\hline
\end{array}
\end{matrix}
\]

\hypertarget{disD}{}
Hide

Negative Binomial

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
      \hline
  \small{\text{Parameter assumptions}} & r>0, \beta>0 \\
\hline
  ~~p_0 & (1+\beta)^{-r} \\
\hline
  \small{\text{Probability mass function}} & \frac{r(r+1)\cdots(r+k-1)\beta^k}{k!(1+\beta)^{r+k}} \\
  ~~p_k & \\
\hline
  \small{\text{Expected value}} & r\beta \\
  ~~\mathrm{E}[N] & \\
\hline
  \small{\text{Variance}} & r\beta(1+\beta) \\
\hline
  \small{\text{Probability generating function}} & [1-\beta(z-1)]^{-r} \\
  ~~P(z) & \\
  \hline
  a \small{\text{ and }} b \small{\text{ for recursion}} & a=\frac{\beta}{1+\beta} \\
   & b=\frac{(r-1)\beta}{1+\beta} \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Probability mass function}} & \text{dnbinom}(x=, size=r, prob=\frac{1}{1+\beta}) \\
\hline
  \small{\text{Distribution function}} & \text{pnbinom}(p=, size=r, prob=\frac{1}{1+\beta}) \\
\hline
  \small{\text{Quantile function}} & \text{qnbinom}(q=, size=r, prob=\frac{1}{1+\beta}) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rnbinom}(n=, size=r, prob=\frac{1}{1+\beta}) \\
\hline
\end{array}
\end{matrix}
\]

\subsection{The (a,b,1) Class}\label{the-ab1-class}

Zero Truncated Poisson

Zero Truncated Geometric

Zero Truncated Binomial

Zero Truncated Negative Binomial

Logarithmic

\hypertarget{ztA}{}
Hide

Zero Truncated Poisson

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
      \hline
  \small{\text{Parameter assumptions}} & \lambda>0 \\
\hline
  ~~p^T_1 & \frac{\lambda}{e^\lambda-1} \\
\hline
  \small{\text{Probability mass function}} & \frac{\lambda^k}{k!(e^\lambda-1)} \\
  ~~p^T_k & \\
\hline
  \small{\text{Expected value}} & \frac{\lambda}{1-e^{-\lambda}} \\
  ~~\mathrm{E}[N] & \\
\hline
  \small{\text{Variance}} & \frac{\lambda[1-(\lambda+1)e^{-\lambda}]}{(1-e^{-\lambda})^2} \\
\hline
  \small{\text{Probability generating function}} & \frac{e^{\lambda z}-1}{e^\lambda-1} \\
  ~~P(z) & \\
\hline
  a \small{\text{ and }} b \small{\text{ for recursion}} & a=0 \\
   & b=\lambda \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Probability mass function}} & \text{dztpois}(x=, lambda=\lambda) \\
\hline
  \small{\text{Distribution function}} & \text{pztpois}(p=, lambda=\lambda) \\
\hline
  \small{\text{Quantile function}} & \text{qztpois}(q=, lambda=\lambda) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rztpois}(n=, lambda=\lambda) \\
\hline
\end{array}
\end{matrix}
\]

\hypertarget{ztB}{}
Hide

Zero Truncated Geometric

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
      \hline
  \small{\text{Parameter assumptions}} & \beta>0 \\
\hline
  ~~p^T_1 & \frac{1}{1+\beta} \\
\hline
  \small{\text{Probability mass function}} & \frac{\beta^{k-1}}{(1+\beta)^k} \\
  ~~p^T_k & \\
\hline
  \small{\text{Expected value}} & 1+\beta \\
  ~~\mathrm{E}[N] & \\
\hline
  \small{\text{Variance}} & \beta(1+\beta) \\
\hline
  \small{\text{Probability generating function}} & \frac{[1-\beta(z-1)]^{-1}-(1+\beta)^{-1}}{1-(1+\beta)^{-1}} \\
  ~~P(z) & \\
\hline
  a \small{\text{ and }} b \small{\text{ for recursion}} & a=\frac{\beta}{1+\beta} \\
   & b=0 \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Probability mass function}} & \text{dztgeom}(x=, prob=\frac{1}{1+\beta}) \\
\hline
  \small{\text{Distribution function}} & \text{pztgeom}(p=, prob=\frac{1}{1+\beta}) \\
\hline
  \small{\text{Quantile function}} & \text{qztgeom}(q=, prob=\frac{1}{1+\beta}) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rztgeom}(n=, prob=\frac{1}{1+\beta}) \\
\hline
\end{array}
\end{matrix}
\]

\hypertarget{ztC}{}
Hide

Zero Truncated Binomial

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & 0<q<1,~\text{m is an integer} \\
   & 0 \leq k \leq m\\
\hline
  ~~p^T_1 & \frac{m(1-q)^{m-1}q}{1-(1-q)^m} \\
\hline
  \small{\text{Probability mass function}} & \frac{\binom{m}{k}q^k(1-q)^{m-k}}{1-(1-q)^m} \\
  ~~p^T_k & \\
\hline
  \small{\text{Expected value}} & \frac{mq}{1-(1-q)^m} \\
  ~~\mathrm{E}[N] & \\
\hline
  \small{\text{Variance}} & \frac{mq[(1-q)-(1-q+mq)(1-q)^m]}{[1-(1-q)^m]^2} \\
\hline
  \small{\text{Probability generating function}} & \frac{[1+q(z-1)^m]-(1-q)^m}{1-(1-q)^m} \\
  ~~P(z) & \\
\hline
  a \small{\text{ and }} b \small{\text{ for recursion}} & a=\frac{-q}{1-q} \\
   & b=\frac{(m+1)q}{1-q} \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commmands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Probability mass function}} & \text{dztbinom}(x=, size=m, prob=p) \\
\hline
  \small{\text{Distribution function}} & \text{pztbinom}(p=, size=m, prob=p) \\
\hline
  \small{\text{Quantile function}} & \text{qztbinom}(q=, size=m, prob=p) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rztbinom}(n=, size=m, prob=p) \\
\hline
\end{array}
\end{matrix}
\]

\hypertarget{ztD}{}
Hide

Zero Truncated Negative Binomial

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & r>-1, r\neq0 \\
\hline
  ~~p^T_1 & \frac{r\beta}{(1+\beta)^{r+1}-(1+\beta)} \\
\hline
  \small{\text{Probability mass function}} & \frac{r(r+1)\cdots(r+k-1)}{k![(1+\beta)^r-1]}(\frac{\beta}{1+\beta})^k \\
  ~~p^T_k & \\
\hline
  \small{\text{Expected value}} & \frac{r\beta}{1-(1+\beta)^{-r}} \\
  ~~\mathrm{E}[N] & \\
\hline
  \small{\text{Variance}} & \frac{r\beta[(1+\beta)-(1+\beta+r\beta)(1+\beta)^{-r}]}{[1-(1+\beta)^{-r}]^2} \\
\hline
  \small{\text{Probability generating function}} & \frac{[1-\beta(z-1)]^{-r}-(1+\beta)^{-r}}{1-(1+\beta)^{-r}} \\
  ~~P(z) & \\
\hline
  a \small{\text{ and }} b \small{\text{ for recursion}} & a=\frac{\beta}{1+\beta} \\
   & b=\frac{(r-1)\beta}{1+\beta} \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Probability mass function}} & \text{dztnbinom}(x=, size=r, prob=\frac{1}{1+\beta}) \\
\hline
  \small{\text{Distribution function}} & \text{pztnbinom}(p=, size=r, prob=\frac{1}{1+\beta}) \\
\hline
  \small{\text{Quantile function}} & \text{qztnbinom}(q=, size=r, prob=\frac{1}{1+\beta}) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rztnbinom}(n=, size=r, prob=\frac{1}{1+\beta}) \\
\hline
\end{array}
\end{matrix}
\]

\hypertarget{ztE}{}
Hide

Logarithmic

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
      \hline
  \small{\text{Parameter assumptions}} & \beta>0 \\
\hline
  ~~p^T_1 & \frac{\beta}{(1+\beta)ln(1+\beta)} \\
\hline
  \small{\text{Probability mass function}} & \frac{\beta^k}{k(1+\beta)^k \ln (1+\beta)} \\
  ~~p^T_k & \\
\hline
  \small{\text{Expected value}} & \frac{\beta}{\ln (1+\beta)} \\
  ~~\mathrm{E}[N] & \\
\hline
  \small{\text{Variance}} & \frac{\beta[1+\beta-\frac{\beta}{ln(1+\beta)}]}{\ln (1+\beta)} \\
\hline
  \small{\text{Probability generating function}} & 1-\frac{ln[1-\beta(z-1)]}{\ln (1+\beta)} \\
  ~~P(z) & \\
\hline
  a \small{\text{ and }} b \small{\text{ for recursion}} & a=\frac{\beta}{1+\beta} \\
   & b=\frac{-\beta}{1+\beta} \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Probability mass function}} & \text{dnbinom}(x=,prob=\frac{\beta}{1+\beta}) \\
\hline
  \small{\text{Distribution function}} & \text{pnbinom}(p=,prob=\frac{\beta}{1+\beta}) \\
\hline
  \small{\text{Quantile function}} & \text{qnbinom}(q=,prob=\frac{\beta}{1+\beta}) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rnbinom}(n=,prob=\frac{\beta}{1+\beta}) \\
\hline
\end{array}
\end{matrix}
\]

\section{Continuous Distributions}\label{continuous-distributions}

\textbf{Overview.} This section summarizes selected continuous
probability distributions used throughout \emph{Loss Data Analytics}.
Relevant functions, \texttt{R} code, and illustrative graphs are
provided.

\subsection{One Parameter
Distributions}\label{one-parameter-distributions}

Exponential

Inv Exponential

Single Parameter Pareto

\hypertarget{1pA}{}
Hide

Exponential

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & \theta>0 \\
\hline
  \small{\text{Probability density}} & \frac{1}{\theta}e^{-x/\theta} \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & 1-e^{-x/\theta} \\
   ~~F(x) & \\
\hline
 \textit{k}^{th}~\small{\text{raw moment}}  & \theta^k\Gamma(k+1) \\
  ~~\mathrm{E}[X^k]  & k>-1 \\
\hline
  VaR_p(x) & -\theta \ln (1-p) \\
\hline
  \small{\text{Limited Expected Value}} & \theta(1-e^{-x/\theta}) \\
  ~~\mathrm{E}[X\wedge x] & \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dexp}(x=, rate=1/\theta) \\
\hline
  \small{\text{Distribution function}} & \text{pexp}(p=, rate=1/\theta) \\
\hline
  \small{\text{Quantile function}} & \text{qexp}(q=, rate=1/\theta) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rexp}(n=, rate=1/\theta) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-218-1.pdf}

\hypertarget{1pB}{}
Hide

Inverse Exponential

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
    \hline
  \small{\text{Parameter assumptions}} & \theta>0 \\
\hline
  \small{\text{Probability density}} & \frac{\theta e^{-\theta/x}}{x^2} \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & e^{-\theta/x} \\
   ~~F(x) & \\
\hline
  \textit{k}^{th}~\small{\text{raw moment}} & \theta^k\Gamma(1-k) \\
  ~~\mathrm{E}[X^k]  & k<1 \\
\hline
  \mathrm{E}[(X\wedge x)^k] & \theta^kG(1-k;\theta/x)+x^k (1 - e^{-\theta/x}) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dinvexp}(x=, scale=\theta) \\
\hline
  \small{\text{Distribution function}} & \text{pinvexp}(p=, scale=\theta) \\
\hline
  \small{\text{Quantile function}} & \text{qinvexp}(q=, scale=\theta) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rinvexp}(n=, scale=\theta) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-219-1.pdf}

\hypertarget{odC}{}
Hide

Single Parameter Pareto

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & \theta~\text{is known},~x>\theta, \alpha > 0 \\
\hline
  \small{\text{Probability density}} & \frac{\alpha\theta^\alpha}{x^{\alpha+1}} \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & 1-(\theta/x)^\alpha \\
   ~~F(x) & \\
\hline
  \textit{k}^{th}~\small{\text{raw moment}} & \frac{\alpha\theta^k}{\alpha-k} \\
  ~~\mathrm{E}[X^k]  & k < \alpha \\
\hline
  \mathrm{E}[(X\wedge x)^k] & \frac{\alpha\theta^k}{\alpha-k}-\frac{k\theta^{\alpha}}{(\alpha-k)x^{\alpha-k}} \\
  & x \geq\theta \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dpareto1}(x=, shape=\alpha,min=\theta) \\
\hline
  \small{\text{Distribution function}} & \text{ppareto1}(p=, shape=\alpha,min=\theta) \\
\hline
  \small{\text{Quantile function}} & \text{qpareto1}(q=, shape=\alpha,min=\theta) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rpareto1}(n=, shape=\alpha,min=\theta) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-220-1.pdf}

\subsection{Two Parameter
Distributions}\label{two-parameter-distributions}

Pareto

Inv Pareto

Loglogistic

Paralogistic

Gamma

Inv Gamma

Weibull

Inv Weibull

Uniform

Normal

\hypertarget{2pA}{}
Hide

Pareto

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & \theta>0, \alpha>0 \\
\hline
  \small{\text{Probability density}} & \frac{\alpha\theta^\alpha}{(x+\theta)^{\alpha+1}} \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & 1-\Big(\frac{\theta}{x+\theta}\Big)^\alpha \\
   ~~F(x) & \\
\hline
  \textit{k}^{th}~\small{\text{raw moment}} & \frac{\theta^k\Gamma(k+1)\Gamma(\alpha-k)}{\Gamma(\alpha)} \\
  ~~\mathrm{E}[X^k] & -1<k<\alpha \\
\hline
  \small{\text{Limited Expected Value:}}~\alpha\neq1 & \frac{\theta}{\alpha-1}\Big[1-\Big(\frac{\theta}{x+\theta}\Big)^{\alpha-1}\Big] \\
  ~~\mathrm{E}[X\wedge x] & \\
\hline
  \small{\text{Limited Expected Value:}}~\alpha=1 & -\theta \ln \left(\frac{\theta}{x+\theta}\right) \\
  ~~\mathrm{E}[X\wedge x] & \\
\hline
  \mathrm{E}[(X\wedge x)^k] & \frac{\theta^k\Gamma(k+1)\Gamma(\alpha-k)}{\Gamma(\alpha)}\beta(k+1,\alpha-k;\frac{x}{x+\theta})+x^k(\frac{\theta}{x+\theta})^\alpha  \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dpareto2}(x=, shape=\alpha, scale=\theta) \\
\hline
  \small{\text{Distribution function}} & \text{ppareto2}(p=, shape=\alpha,scale=\theta) \\
\hline
  \small{\text{Quantile function}} & \text{qpareto2}(q=, shape=\alpha,scale=\theta) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rpareto2}(n=, shape=\alpha,scale=\theta) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-221-1.pdf}

\hypertarget{2pB}{}
Hide

Inverse Pareto

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
  \hline
  \small{\text{Parameter assumptions}} & \theta>0, \tau>0 \\
\hline
  \small{\text{Probability density}} & \frac{\tau\theta x^{\tau-1}}{(x+\theta)^\tau-1} \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & \Big(\frac{x}{x+\theta}\Big)^\tau \\
   ~~F(x) & \\
\hline
  \textit{k}^{th}~\small{\text{raw moment}} & \frac{\theta^k\Gamma(\tau+k)\Gamma(1-k)}{\Gamma(\tau)} \\
  ~~\mathrm{E}[X^k]  & -\tau<k<1 \\
\hline
  \mathrm{E}[(X\wedge x)^k] & \theta^k\tau\int^{x/(x+\theta)}_0~y^{\tau+k-1}(1-y)^{-k}dy+x^k[1-\Big(\frac{x}{x+\theta}\Big)^\tau] \\
  & k>-\tau \\
\hline
\end{array}
\end{matrix}
\] \textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dinvpareto}(x=, shape=\tau, scale=\theta) \\
\hline
  \small{\text{Distribution function}} & \text{pinvpareto}(p=, shape=\tau,scale=\theta) \\
\hline
  \small{\text{Quantile function}} & \text{qinvpareto}(q=, shape=\tau,scale=\theta) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rinvpareto}(n=, shape=\tau,scale=\theta) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-222-1.pdf}

\hypertarget{2pC}{}
Hide

Loglogistic

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & \theta>0, \gamma > 0, u=\frac{(x/\theta)^\gamma}{1+(x/\theta)^\gamma} \\
\hline
  \small{\text{Probability density}} & \frac{\gamma(x/\theta)^\gamma}{x[1+(x/\theta)^\gamma]^2} \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & u \\
   ~~F(x) & \\
\hline
  \textit{k}^{th}~\small{\text{raw moment}} & \theta^k\Gamma(1+(k/\gamma))\Gamma(1-(k/\gamma)) \\
  ~~\mathrm{E}[X^k]  & -\gamma<k<\gamma \\
\hline
  \mathrm{E}[(X\wedge x)^k] & \theta^k\Gamma(1+(k/\gamma))\Gamma(1-(k/\gamma))\beta(1+(k/\gamma),1-(k/\gamma);u)+x^k(1-u) \\
  & k>-\gamma \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-223-1.pdf}

\hypertarget{2pD}{}
Hide

Paralogistic

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & \theta>0, \alpha>0, u=\frac{1}{1+(x/\theta)^\alpha} \\
\hline
  \small{\text{Probability density}} & \frac{\alpha^2(x/\theta)^\alpha}{x[1+(x/\theta)^\alpha]^{\alpha+1}} \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & 1-u^\alpha \\
   ~~F(x) & \\
\hline
  \textit{k}^{th}~\small{\text{raw moment}} & \frac{\theta^k\Gamma(1+(k/\alpha))\Gamma(\alpha-(k/\alpha))}{\Gamma(\alpha)} \\
  ~~\mathrm{E}[X^k]  & -\alpha<k<\alpha^2 \\
\hline
  \mathrm{E}[(X\wedge x)^k] & \frac{\theta^k\Gamma(1+(k/\alpha))\Gamma(\alpha-(k/\alpha))}{\Gamma(\alpha)}\beta(1+(k/\alpha),\alpha-(k/\alpha);1-u)+x^ku^\alpha \\
  & k>-\alpha \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dparalogis}(x=, shape=\alpha, scale=\theta) \\
\hline
  \small{\text{Distribution function}} & \text{pparalogis}(p=, shape=\alpha,scale=\theta) \\
\hline
  \small{\text{Quantile function}} & \text{qparalogis}(q=, shape=\alpha,scale=\theta) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rparalogis}(n=, shape=\alpha,scale=\theta) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-224-1.pdf}

\hypertarget{2pE}{}
Hide

Gamma

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & \theta>0,~\alpha>0 \\
\hline
  \small{\text{Probability density}} & \frac{1}{\theta^{\alpha}\Gamma(\alpha)}x^{\alpha-1}e^{-x/\theta} \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & \Gamma(\alpha;\frac{x}{\theta}) \\
   ~~F(x) & \\
\hline
  \textit{k}^{th}~\small{\text{raw moment}}  & \theta^k\frac{\Gamma(\alpha+k)}{\Gamma(\alpha)} \\
  ~~\mathrm{E}[X^k]  & k>-\alpha \\
\hline
   & \frac{\theta^k\Gamma(k+\alpha)}{\Gamma(\alpha)}\Gamma(k+\alpha; x/\theta)+x^k[1-\Gamma(\alpha; x/\theta)]  \\
   ~~\mathrm{E}[X\wedge x]^k & k > -\alpha \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \small{\text{Density function}} & \text{dgamma}(x=, shape=\alpha, scale=\theta) \\
\hline
  \small{\text{Distribution function}} & \text{pgamma}(p=, shape=\alpha,scale=\theta) \\
\hline
  \small{\text{Quantile function}} & \text{qgamma}(q=, shape=\alpha,scale=\theta) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rgamma}(n=, shape=\alpha,scale=\theta) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-225-1.pdf}

\hypertarget{2pF}{}
Hide

Inverse Gamma

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Probability density}} & \frac{(\theta/x)^\alpha e^{-\theta/x}}{x\Gamma(\alpha)} \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & 1-\Gamma(\alpha;\theta/x) \\
   ~~F(x) & \\
\hline
  \textit{k}^{th}~\small{\text{raw moment}} & \frac{\theta^k\Gamma(\alpha-k)}{\Gamma(\alpha)} \\
  ~~\mathrm{E}[X^k]  & k<\alpha \\
\hline
  \mathrm{E}[(X\wedge x)^k] & \frac{\theta^k\Gamma(\alpha-k)}{\Gamma(\alpha)}[1-\Gamma(\alpha-k;\theta/x)]+x^k\Gamma(\alpha;\theta/x) \\
  &  \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dinvgamma}(x=, shape=\alpha, scale=\theta) \\
\hline
  \small{\text{Distribution function}} & \text{pinvgamma}(p=, shape=\alpha,scale=\theta) \\
\hline
  \small{\text{Quantile function}} & \text{qinvgamma}(q=, shape=\alpha,scale=\theta) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rinvgamma}(n=, shape=\alpha,scale=\theta) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-226-1.pdf}

\hypertarget{2pG}{}
Hide

Weibull

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & \theta>0,\alpha>0 \\
\hline\
  \small{\text{Probability density}} & \frac{\alpha \Big(\frac{x}{\theta}\Big)^\alpha \exp\Big(-\Big(\frac{x}{\theta}\Big)^\alpha\Big)}{x} \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & 1-\exp\Big(-\Big(\frac{x}{\theta}\Big)^\alpha\Big) \\
   ~~F(x) & \\
\hline
  \textit{k}^{th}~\small{\text{raw moment}} & \theta^k \Gamma(1 + \frac{k}{\alpha}) \\
  ~~\mathrm{E}[X^k]  & k>-\alpha \\
\hline
  \mathrm{E}[(X\wedge x)^k] & \theta^k\Gamma(1+\frac{k}{\alpha})\Gamma\Big[1+\frac{k}{\alpha};\Big(\frac{x}{\theta}\Big)^\alpha\Big]+x^k\exp\Big(-\Big(\frac{x}{\theta}\Big)^\alpha\Big)  \\
   & k>-\alpha \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dweibull}(x=, shape=\alpha, scale=\theta) \\
\hline
  \small{\text{Distribution function}} & \text{pweibull}(p=, shape=\alpha,scale=\theta) \\
\hline
  \small{\text{Quantile function}} & \text{qweibull}(q=, shape=\alpha,scale=\theta) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rweibull}(n=, shape=\alpha,scale=\theta) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-227-1.pdf}

\hypertarget{2pH}{}
Hide

Inverse Weibull

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & \theta>0,\tau>0 \\
\hline\
  \small{\text{Probability density}} & \frac{\tau(\theta/x)^\tau \exp\Big(-\Big(\frac{\theta}{x}\Big)^\tau\Big)}{x} \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & \exp\Big(-\Big(\frac{\theta}{x}\Big)^\tau\Big) \\
   ~~F(x) & \\
\hline
 \textit{k}^{th}~\small{\text{raw moment}} & \theta^k\Gamma(1-(k/\tau)) \\
  ~~\mathrm{E}[X^k]  & k<\tau \\
\hline
  \mathrm{E}[(X\wedge x)^k] & \theta^k\Gamma(1-(k/\tau))[1-\Gamma(1-(k/\tau);(\theta/x)^\tau)]+x^k[1-e^{-(\theta/x)^\tau}] \\
  &  \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dinvweibull}(x=, shape=\tau, scale=\theta) \\
\hline
  \small{\text{Distribution function}} & \text{pinvweibull}(p=, shape=\tau,scale=\theta) \\
\hline
  \small{\text{Quantile function}} & \text{qinvweibull}(q=, shape=\tau,scale=\theta) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rinvweibull}(n=, shape=\tau,scale=\theta) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-228-1.pdf}

\hypertarget{2pI}{}
Hide

Uniform

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & -\infty<\alpha<\beta<\infty \\
\hline
  \small{\text{Probability density}} & \frac{1}{\beta-\alpha} \\
  \text{f(x)} & \\
\hline
  \small{\text{Distribution function}} & \frac{x-\alpha}{\beta-\alpha} \\
 ~~F(x) & \\
\hline
  \text{Mean} & \frac{\beta+\alpha}{2} \\
  \text{E[X]} & \\
\hline
  \text{Variance} & \frac{(\beta-\alpha)^2}{12} \\
  E[(X-\mu)^2] & \\
\hline
  \mathrm{E}[(X-\mu)^k] & \mu_k=0~~~\text{for odd }\textit{k} \\
   & \mu_k=\frac{(\beta-\alpha)^k}{2^k (k+1)}~~~\text{for even }\textit{k} \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dunif}(x=, min=a, max=b) \\
\hline
  \small{\text{Distribution function}} & \text{punif}(p=, min=a, max=b) \\
\hline
  \small{\text{Quantile function}} & \text{qunif}(q=, min=a, max=b) \\ 
\hline
  \small{\text{Random sampling function}} & \text{runif}(n=, min=a, max=b) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-229-1.pdf}

\hypertarget{2pJ}{}
Hide

Normal

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & -\infty<\mu<\infty,~\sigma>0 \\
\hline
  \small{\text{Probability density}} & \frac{1}{\sqrt{2\pi}\sigma} \exp\left( -\frac{(x-\mu)^2}{2\sigma^2}\right) \\
  \text{f(x)} & \\
\hline
  \small{\text{Distribution function}} & \Phi\left(\frac{x-\mu}{\sigma}\right) \\
 ~~F(x) & \\
\hline
  \text{Mean} & \mu \\
  \text{E[X]} & \\
\hline
  \text{Variance} & \sigma^2 \\
  E[(X-\mu)^2] & \\
\hline
  \mathrm{E}[(x-\mu)^k] & \mu_k=0~~~\text{for even k} \\
   & \mu_k=\frac{k!\sigma^2}{(\frac{k}{2})! 2^{k/2}}~~~\text{for odd k} \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dnorm}(x=, mean=\mu, sd=\sigma) \\
\hline
  \small{\text{Distribution function}} & \text{pnorm}(p=, mean=\mu, sd=\sigma) \\
\hline
  \small{\text{Quantile function}} & \text{qnorm}(q=, mean=\mu, sd=\sigma) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rnorm}(n=, mean=\mu, sd=\sigma) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-230-1.pdf}

\hypertarget{2pK}{}
Hide

Cauchy

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & -\infty <\alpha <\infty, \beta>0 \\
\hline
  \small{\text{Probability density}} & \frac{1}{\pi\beta}[1+\left( \frac{x-\alpha}{\beta}\right)^2]^{-1} \\
 ~~ \small{\text{function }} f(x)& \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dcauchy}(x=, location=\alpha, scale=\beta) \\
\hline
  \small{\text{Distribution function}} & \text{pcauchy}(p=, location=\alpha, scale=\beta) \\
\hline
  \small{\text{Quantile function}} & \text{qcauchy}(q=, location=\alpha, scale=\beta) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rcauchy}(n=, location=\alpha, scale=\beta) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-231-1.pdf}

\subsection{Three Parameter
Distributions}\label{three-parameter-distributions}

Generalized Pareto

Burr

Inv Burr

\hypertarget{3pA}{}
Hide

Generalized Pareto

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & \theta>0, \alpha>0, \tau>0, u=\frac{x}{x+\theta} \\
\hline
  \small{\text{Probability density}} & \frac{\Gamma(\alpha+\tau)}{\Gamma(\alpha)\Gamma(\tau)}\frac{\theta^\alpha x^{\tau-1}}{(x+\theta)^{\alpha+\tau}} \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & \beta(\tau,\alpha;u) \\
   ~~F(x) & \\
\hline
  \textit{k}^{th}~\small{\text{raw moment}} & \frac{\theta^k\Gamma(\tau+1)\Gamma(\alpha-k)}{\Gamma(\alpha)\Gamma(\tau)} \\
  ~~~~\mathrm{E}[X^k]  & -\tau<k<\alpha \\
\hline
  \mathrm{E}[(X\wedge x)^k] & \frac{\theta^k\Gamma(\tau+k)\Gamma(\alpha-k)}{\Gamma(\alpha)\Gamma(\tau)}\beta(\tau+k,\alpha-k;u)+x^k[1-\beta(\tau,\alpha;u)]  \\
  & k>-\tau \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dgenpareto}(x=, shape1=\alpha, shape2=\tau, scale=\theta) \\
\hline
  \small{\text{Distribution function}} & \text{pgenpareto}(q=, shape1=\alpha, shape2=\tau, scale=\theta) \\
\hline
  \small{\text{Quantile function}} & \text{qgenpareto}(p=, shape1=\alpha, shape2=\tau, scale=\theta) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rgenpareto}(r=, shape1=\alpha, shape2=\tau, scale=\theta) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-232-1.pdf}

\hypertarget{3pB}{}
Hide

Burr

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & \theta>0, \alpha>0, \gamma>0, u=\frac{1}{1+(x/\theta)^\gamma} \\
\hline
  \small{\text{Probability density}} & \frac{\alpha\gamma(x/\theta)^\gamma}{x[1+(x/\theta)^\gamma]^{\alpha+1}} \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & 1-u^\alpha \\
   ~~F(x) & \\
\hline
  \textit{k}^{th}~\small{\text{raw moment}} & \frac{\theta^k\Gamma(1+(k/\gamma))\Gamma(\alpha-(k/\gamma))}{\Gamma(\alpha)} \\
  ~~~~\mathrm{E}[X^k]  & -\gamma<k<\alpha\gamma \\
\hline
  \mathrm{E}[(X\wedge x)^k] & \frac{\theta^k\Gamma(1+(k/\gamma))\Gamma(\alpha-(k/\gamma))}{\Gamma(\alpha)}\beta(1+(k/\gamma),\alpha-(k/\gamma);1-u)+x^ku^\alpha  \\
  & k>-\gamma \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dburr}(x=, shape1=\alpha, shape2=\gamma, scale=\theta) \\
\hline
  \small{\text{Distribution function}} & \text{pburr}(p=, shape1=\alpha, shape2=\gamma, scale=\theta) \\
\hline
  \small{\text{Quantile function}} & \text{qburr}(q=, shape1=\alpha, shape2=\gamma, scale=\theta) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rburr}(n=, shape1=\alpha, shape2=\gamma, scale=\theta) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-233-1.pdf}

\hypertarget{3pC}{}
Hide

Inverse Burr

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & \theta>0, \tau>0, \gamma>0, u=\frac{(x/\theta)^\gamma}{1+(x/\theta)^\gamma} \\
\hline
  \small{\text{Probability density}} & \frac{\tau\gamma(x/\theta)^{\tau \gamma}}{x[1+(x/\theta)^\gamma]^{\tau+1}} \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & u^\tau \\
   ~~F(x) & \\
\hline
  \textit{k}^{th}~\small{\text{raw moment}} & \frac{\theta^k\Gamma(\tau+(k/\gamma))\Gamma(1-(k/\gamma))}{\Gamma(\tau)} \\
  ~~~~\mathrm{E}[X^k]  & -\tau\gamma<k<\gamma \\
\hline
  \mathrm{E}[(X\wedge x)^k] & \frac{\theta^k\Gamma(\tau+(k/\gamma))\Gamma(1-(k/\gamma))}{\Gamma(\tau)}\beta(\tau+(k/\gamma),1-(k/\gamma);u)+x^k[1-u^\tau]  \\
  & k>-\tau\gamma \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dinvburr}(x=, shape1=\tau, shape2=\gamma, scale=\theta) \\
\hline
  \small{\text{Distribution function}} & \text{pinvburr}(p=, shape1=\tau, shape2=\gamma, scale=\theta) \\
\hline
  \small{\text{Quantile function}} & \text{qinvburr}(q=, shape1=\tau, shape2=\gamma, scale=\theta) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rinvburr}(n=, shape1=\tau, shape2=\gamma, scale=\theta) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-234-1.pdf}

\subsection{Four Parameter
Distribution}\label{four-parameter-distribution}

GB2

Hide

Generalized Beta of the Second Kind (GB2)

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & \theta>0, \alpha_1>0, \alpha_2>0, \sigma>0  \\
\hline
  \small{\text{Probability density}} & \frac{(x/\theta)^{\alpha_2/\sigma}}{x \sigma~\mathrm{B}\left( \alpha_1,\alpha_2\right)\left\lbrack 1 + \left( x/\theta \right)^{1/\sigma} \right\rbrack^{\alpha_1 + \alpha_2}}  \\
    ~~ \small{\text{function }} f(x) & \\
\hline
  \textit{k}^{th}~\small{\text{raw moment}} & \frac{\theta^{k}~\mathrm{B}\left( \alpha_1 +k \sigma,\alpha_2 - k \sigma \right)}{\mathrm{B}\left( \alpha_1,\alpha_2 \right)} \\
  ~~~~\mathrm{E}[X^k]  & \textit{k}>0 \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

Please see the \href{https://ewfrees.github.io/LDARcode/index.html}{R
Codes for Loss Data Analytics} site for information about this
distribution.

\subsection{Other Distributions}\label{other-distributions}

Lognormal

Inv Gaussian

\hypertarget{odA}{}
Hide

Lognormal

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & -\infty <\mu <\infty, \sigma>0 \\
\hline
  \small{\text{Probability density}} & \frac{1}{x\sqrt{2\pi}\sigma} \exp\left( -\frac{(\ln x-\mu)^2}{2\sigma^2}\right) \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & \Phi\left(\frac{\ln (x)-\mu}{\sigma}\right) \\
   ~~F(x) & \\
\hline
  \textit{k}^{th}~\small{\text{raw moment}} & \exp(k\mu+\frac{k^2\sigma^2}{2}) \\
  ~~\mathrm{E}[X^k]  &  \\
\hline
  \small{\text{Limited Expected Value}} & \exp\Big(k\mu+\frac{k^2\sigma^2}{2}\Big)\Phi\Big(\frac{\ln (x)-\mu-k\sigma^2}{\sigma}\Big)+x^k\Big[1-\Phi\Big(\frac{\ln (x)-\mu}{\sigma}\Big)\Big]  \\
  ~~\mathrm{E}[X\wedge x] & \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-235-1.pdf}

\hypertarget{odB}{}
Hide

Inverse Gaussian

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & \theta>0, \mu>0, z=\frac{x-\mu}{\mu}~,~y=\frac{x+\mu}{\mu} \\
\hline
  \small{\text{Probability density}} & \Big(\frac{\theta}{2\pi x^3}\Big)^{1/2}\exp\Big(\frac{-\theta z^2}{2x}\Big) \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & \Phi\Big[z\Big(\frac{\theta}{x}\Big)^{1/2}\Big]+\exp\Big(\frac{2\theta}{\mu}\Big)\Phi\Big[-y\Big(\frac{\theta}{x}\Big)^{1/2}\Big] \\
   ~~F(x) & \\
\hline
  \text{Mean} & \mu \\
  \mathrm{E}[X]  & \\
\hline
  \mathrm{Var[X]} & \frac{\mu^3}{\theta}\\
\hline
  \mathrm{E}[(X\wedge x)^k] & x-\mu x\Phi\Big[z\Big(\frac{\theta}{x}\Big)^{1/2}\Big]-(\mu y)\exp\Big(\frac{2\theta}{\mu}\Big)\Phi\Big[-y\Big(\frac{\theta}{x}\Big)^{1/2}\Big] \\
  &  \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dinvgauss}(x=, mean=\mu,dispersion=\theta) \\
\hline
  \small{\text{Distribution function}} & \text{pinvgauss}(p=, mean=\mu,dispersion=\theta) \\
\hline
  \small{\text{Quantile function}} & \text{qinvgauss}(q=, mean=\mu,dispersion=\theta) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rinvgauss}(n=, mean=\mu,dispersion=\theta) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-236-1.pdf}

\subsection{Distributions with Finite
Support}\label{distributions-with-finite-support}

Beta

Generalized Beta

\hypertarget{fsA}{}
Hide

Beta

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & \theta>0, ~a>0,~b>0, u=\frac{x}{\theta},~0<x<\theta \\
\hline
  \small{\text{Probability density}} & \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} u^a(1-u)^{b-1}\frac{1}{x} \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & \beta(a,b;u) \\
   ~~F(x) & \\
\hline
  \textit{k}^{th}~\small{\text{raw moment}} & \frac{\theta^k 
  \Gamma(a+b)\Gamma(a+k)}{\Gamma(a)\Gamma(a+b+k)} \\
  ~~\mathrm{E}[X^k]  & k>-a \\
\hline
  & \frac{\theta^k a(a+1)\cdots(a+k-1)}{(a+b)(a+b+1)\cdots(a+b+k-1)}\beta(a+k,b;u)+x^k[1-\beta(a,b;u)]  \\
   ~~\mathrm{E}[X\wedge x]^k &  \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dbeta}(x=, shape1=a,shape2=b,ncp=\theta) \\
\hline
  \small{\text{Distribution function}} & \text{pbeta}(p=, shape1=a,shape2=b,ncp=\theta) \\
\hline
  \small{\text{Quantile function}} & \text{qbeta}(q=, shape1=a,shape2=b,ncp=\theta) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rbeta}(n=, shape1=a,shape2=b,ncp=\theta) \\
\hline
\end{array}
\end{matrix}
\]

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-237-1.pdf}

\hypertarget{fsB}{}
Hide

Generalized Beta

\textbf{Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Name} & \text{Function} \\
\hline
  \small{\text{Parameter assumptions}} & \theta>0, a>0, b>0, \tau>0, 0<x<\theta~,~u=(x/\theta)^\tau \\
\hline
  \small{\text{Probability density}} & \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}u^\alpha(1-u)^{b-1}\frac{\tau}{x} \\
 ~~ \small{\text{function }} f(x)& \\
\hline
  \small{\text{Distribution function}} & \beta(a,b;u) \\
   ~~F(x) & \\
\hline
  \textit{k}^{th}~\small{\text{raw moment}} & \frac{\theta^k\Gamma(a+b)\Gamma(a+(k/\tau))}{\Gamma(a)\Gamma(a+b+(k/\tau))} \\
   ~~\mathrm{E}[X^k] & k>-\alpha\tau \\
\hline
  \mathrm{E}[(X\wedge x)^k] & \frac{\theta^k\Gamma(a+b)\Gamma(a+(k/\tau))}{\Gamma(a)\Gamma(a+b+(k/\tau))}\beta(a+(k/\tau),b;u)+x^k[1-\beta(a,b;u)] \\
\hline
\end{array}
\end{matrix}
\]

\textbf{\texttt{R} Commmands}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Function Name} & \text{R Command} \\
\hline
  \small{\text{Density function}} & \text{dgenbeta}(x=, shape1=a,shape2=b,shape3=\tau,scale=\theta) \\
\hline
  \small{\text{Distribution function}} & \text{pgenbeta}(p=, shape1=a,shape2=b,shape3=\tau,scale=\theta) \\
\hline
  \small{\text{Quantile function}} & \text{qgenbeta}(q=, shape1=a,shape2=b,shape3=\tau,scale=\theta) \\ 
\hline
  \small{\text{Random sampling function}} & \text{rgenbeta}(n=, shape1=a,shape2=b,shape3=\tau,scale=\theta) \\
\hline
\end{array}
\end{matrix}
\]

\textbf{Illustrative Graph}

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-238-1.pdf}

\section{Limited Expected Values}\label{limited-expected-values}

\textbf{Overview.} This section summarizes limited expected values for
selected continuous distributions.

Functions

Graph

Hide

Functions

\textbf{Limited Expected Value Functions}

\[
\begin{matrix}
\begin{array}{l|c}
\hline
  \text{Distribuion} & \text{Function} \\
\hline
  \text{GB2} & 
  \frac{\theta\Gamma(\tau+1)\Gamma(\alpha-1)}{\Gamma(\alpha)\Gamma(\tau)}\beta(\tau+1,\alpha-1;\frac{x}{x+\beta})+x[1-\beta(\tau,\alpha;\frac{x}{x+\beta})] \\
\hline
  \text{Burr} & \frac{\theta\Gamma(1+\frac{1}{\gamma})\Gamma(\alpha-\frac{1}{\gamma})}{\Gamma(\alpha)}\beta(1+\frac{1}{\gamma},\alpha-\frac{1}{\gamma};1-\frac{1}{1+(x/\theta)^\gamma})+x\Big(\frac{1}{1+(x/\theta)^\gamma}\Big)^\alpha \\
\hline
  \text{Inverse Burr} & \frac{\theta\Gamma(\tau+(1/\gamma))\Gamma(1-(1/\gamma))}{\Gamma(\tau)}\beta(\tau+\frac{1}{\gamma},1-\frac{1}{\gamma};\frac{(x/\theta)^\gamma}{1+(x/\theta)^\gamma})+x[1-\Big(\frac{(x/\theta)^\gamma}{1+(x/\theta)^\gamma}\Big)^\tau] \\
\hline
  \text{Pareto} & \\
  \alpha=1 & -\theta \ln \Big(\frac{\theta}{x+\theta}\Big) \\
  \alpha\neq1 & \frac{\theta}{\alpha-1}[1-\Big(\frac{\theta}{x+\theta}\Big)^{\alpha-1}] \\
\hline
  \text{Inverse Pareto} & \theta\tau\int^{x/(x+\theta)}_0~y^\tau(1-y)^{-1}dy+x[1-\Big(\frac{x}{x+\theta}\Big)^\tau] \\
\hline
  \text{Loglogistic} & \theta\Gamma(1+\frac{1}{\gamma})\Gamma(1-\frac{1}{\gamma})\beta(1+\frac{1}{\gamma},1-\frac{1}{\gamma};\frac{(x/\theta)^\gamma}{1+(x/\theta)^\gamma})+x(1-\frac{(x/\theta)^\gamma}{1+(x/\theta)^\gamma})  \\
\hline
  \text{Paralogistic} & \frac{\theta\Gamma(1+\frac{1}{\alpha})\Gamma(\alpha-\frac{1}{\alpha})}{\Gamma(\alpha)}\beta(1+\frac{1}{\alpha},\alpha-\frac{1}{\alpha};1-\frac{1}{1+(x/\theta)^\alpha})+x\Big(\frac{1}{1+(x/\theta)^\alpha}\Big)^\alpha \\
\hline
  \text{Inverse Paralogistic} & \frac{\theta\Gamma(\tau+\frac{1}{\tau})\Gamma(1-\frac{1}{\tau})}{\Gamma(\tau)}\beta(\tau+\frac{1}{\tau},1-\frac{1}{\tau};\frac{(x/\theta)^\tau}{1+(x/\theta)^\tau})+x[1-\Big(\frac{(x/\theta)^\tau}{1+(x/\theta)^\tau}\Big)^\tau] \\
\hline
  \text{Gamma} & \frac{\theta\Gamma(\alpha+1)}{\Gamma(\alpha)}\Gamma(\alpha+1;\frac{x}{\theta})+x[1-\Gamma(\alpha;\frac{x}{\theta})] \\
\hline
  \text{Inverse Gamma} & \frac{\theta\Gamma(\alpha-1)}{\Gamma(\alpha)}[1-\Gamma(\alpha-1;\frac{\theta}{x})]+x\Gamma(\alpha;\frac{\theta}{x}) \\
\hline
  \text{Weibull} & \theta\Gamma(1+\frac{1}{\alpha})\Gamma(1+\frac{1}{\alpha};\Big(\frac{x}{\theta}\Big)^\alpha)+x*\exp(-(x/\theta)^\alpha) \\
\hline
  \text{Inverse Weibull} & \theta\Gamma(1-\frac{1}{\alpha})[1-\Gamma(1-\frac{1}{\alpha};\Big(\frac{\theta}{x}\Big)^\alpha)]+x[1-\exp(-(\theta/x)^\alpha)] \\
\hline
  \text{Exponential} & \theta(1-\exp(-(x/\theta))) \\
\hline
  \text{Inverse Exponential} & \theta G(0;\frac{\theta}{x})+x(1-\exp(-(\theta/x))) \\
\hline
  \text{Lognormal} & \exp(\mu+\sigma^2/2)\Phi\Big(\frac{\ln (x)-\mu-\sigma^2}{\sigma}\Big)+x[1-\Phi\Big(\frac{\ln (x)-\mu}{\sigma}\Big)] \\
\hline
  \text{Inverse Gaussian} & x-\mu\Big(\frac{x-\mu}{\mu}\Big)\Phi\Big[\Big(\frac{x-\mu}{\mu}\Big)\Big(\frac{\theta}{x}\Big)^{1/2}\Big]-\mu\Big(\frac{x+\mu}{\mu}\Big)\exp\Big(\frac{2\theta}{\mu}\Big)\Phi\Big[-\Big(\frac{x+\mu}{\mu}\Big)\Big(\frac{\theta}{x}\Big)^{1/2}\Big] \\
\hline
  \text{Single-Parameter Pareto} & \frac{\alpha\theta}{\alpha-1}-\frac{\theta^\alpha}{(\alpha-1)x^{\alpha-1}} \\
\hline
  \text{Generalized Beta} & \frac{\theta\Gamma(a+b)\Gamma(a+\frac{1}{\tau})}{\Gamma(a)\Gamma(a+b+\frac{1}{\tau})}\beta(a+\frac{1}{\tau},b;\Big(\frac{x}{\theta}\Big)^\tau)+x\Big[1-\beta(a,b;\Big(\frac{x}{\theta}\Big)^\tau)\Big] \\
\hline
  \text{Beta} & \frac{\theta a}{(a+b)}\beta(a+1,b;\frac{x}{\theta})+x[1-\beta(a,b;\frac{x}{\theta})] \\
\hline
\end{array}
\end{matrix}
\]

\hypertarget{levB}{}
Hide

Illustrative Graph

\textbf{Comparison of Limited Expected Values for Selected
Distributions}

\[
\begin{matrix}
\begin{array}{l|c|c|c|c|c|c}
\hline
  \text{Distribution} & \text{Parameters} & \mathrm{E}[x] & E[X\wedge100] & E[X\wedge250] & E[X\wedge500] &E[X\wedge1000] \\
\hline
  \text{Pareto} & \alpha = 3, \theta = 200 & 100 & 55.55 &80.25 & 91.84 & 97.22  \\ 
\hline
  \text{Exponential} & \theta = 100 & 100 & 63.21 & 91.79 & 99.33 & 99.99 \\
\hline
  \text{Gamma} & \alpha = 2, \theta = 50 & 100 & 72.93 & 97.64 & 99.97 & 100 \\
\hline
  \text{Weibull} & \tau=2, \theta=\frac{200}{\sqrt[]{\pi}} & 100 & 78.99 & 99.82 & 100 & 100 \\
\hline
  \text{GB2} & \alpha = 3,\tau=2,\theta = 100 & 100 & 62.50 & 86.00 & 94.91 & 98.42  \\
\hline
\end{array}
\end{matrix}
\]

\includegraphics{LossDataAnalytics_files/figure-latex/unnamed-chunk-239-1.pdf}

\bibliography{References/LDAReferenceC.bib,References/articles.bib,References/books.bib,References/packagesA.bib,References/packagesB.bib}


\end{document}
